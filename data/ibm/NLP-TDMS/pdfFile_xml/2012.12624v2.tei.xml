<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Dense Representations of Phrases at Scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
							<email>jinhyuk_lee@korea.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mujeen</forename><surname>Sung</surname></persName>
							<email>mujeensung@korea.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
							<email>kangj@korea.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
							<email>danqic@cs.princeton.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Dense Representations of Phrases at Scale</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference <ref type="bibr" target="#b34">(Seo et al., 2019)</ref>. However, current phrase retrieval models heavily depend on their sparse representations while still underperforming retriever-reader approaches. In this work, we show for the first time that we can learn dense phrase representations alone that achieve much stronger performance in open-domain QA. Our approach includes (1) learning query-agnostic phrase representations via question generation and distillation; (2) novel negative-sampling methods for global normalization; (3) query-side fine-tuning for transfer learning. On five popular QA datasets, our model DensePhrases improves previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open-domain question answering (QA) aims to provide answers to natural-language questions using a large text corpus <ref type="bibr" target="#b36">(Voorhees et al., 1999;</ref><ref type="bibr" target="#b8">Ferrucci et al., 2010;</ref>. While a dominating approach is a two-stage retriever-reader approach <ref type="bibr" target="#b4">(Chen et al., 2017;</ref><ref type="bibr" target="#b9">Guu et al., 2020;</ref>, we focus on a recent new paradigm solely based on phrase retrieval <ref type="bibr" target="#b33">(Seo et al., 2018</ref><ref type="bibr" target="#b34">(Seo et al., , 2019</ref>.</p><p>Phrase retrieval highlights the use of phrase representations and finds answers purely based on the similarity search in the vector space of phrases. Without relying on an expensive reader model (e.g., a 12-layer BERT model ) for processing text passages, it has demonstrated great runtime efficiency at inference time. <ref type="table" target="#tab_13">Table 1</ref> compares the two approaches in detail.</p><p>Despite great promise, it remains a formidable challenge to build effective dense representations for each single phrase in a large corpus (e.g., Wikipedia). First, since phrase representations need to be decomposable from question representations, they are often less expressive than querydependent representations-this challenge brings the decomposability gap as stated in <ref type="bibr" target="#b33">(Seo et al., 2018</ref><ref type="bibr" target="#b34">(Seo et al., , 2019</ref>. Second, it requires retrieving answers correctly out of ten billions of phrases, which are more than four orders of magnitude larger than the number of documents in Wikipedia. Consequently, this approach heavily relies on sparse representations for locating relevant documents and paragraphs while still falling behind retriever-reader models <ref type="bibr" target="#b34">(Seo et al., 2019;</ref>.</p><p>In this work, we investigate whether we can build fully dense phrase representations at scale for opendomain QA. First, we attribute the cause of the decomposability gap to the sparsity of training data. We close this gap by generating questions for every answer phrase, as well as distilling knowledge from query-dependent models (Section 3). Second, we use negative sampling strategies such as inbatch negatives <ref type="bibr">(Henderson et al., 2017;</ref>, to approximate global normalization. We also propose a novel method called pre-batch negatives, which leverages preceding mini-batches as negative examples to compensate the need of large-batch training (Section 4). Lastly, for taskspecific adaptation of our model, we propose queryside fine-tuning that drastically improves the per-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category Model</head><p>Sparse? Storage #Q/sec NQ SQuAD (GB) (GPU, CPU) (Acc) (Acc)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retriever-Reader</head><p>DrQA <ref type="bibr" target="#b4">(Chen et al., 2017</ref>) 26 1.8, 0.6 -29.8 BERTSerini <ref type="bibr" target="#b38">(Yang et al., 2019)</ref> 21 2.0, 0.4 -38.6 ORQA  18 8.6, 1.2 33.3 20.2 REALM News <ref type="bibr" target="#b9">(Guu et al., 2020)</ref> 18 8.4, 1.2 40.4 -DPR-multi  76 0.9, 0.04 41.5 24.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phrase Retrieval</head><p>DenSPI <ref type="bibr" target="#b34">(Seo et al., 2019)</ref> 1,200 2.9, 2.4 8.1 36.2 DenSPI + Sparc  1,547 2.1, 1.7 14.5 40.7 DensePhrases (Ours) 320 20.6, 13.6 40.9 38.0 <ref type="table" target="#tab_13">Table 1</ref>: Retriever-reader and phrase retrieval approaches for open-domain QA. The retriever-reader approach retrieves a small number of relevant documents from which the answers are extracted. The phrase retrieval approach retrieves an answer out of ten billions of phrase representations pre-indexed from the entire Wikipedia. 2 NQ, SQuAD: the accuracy is measured on the test sets of Natural Questions  and SQuAD <ref type="bibr" target="#b31">(Rajpurkar et al., 2016)</ref> in the open-domain setting.</p><p>formance of phrase retrieval without re-building billions of phrase representations (Section 5). Consequently, all these improvements enable us to learn a much stronger phrase retrieval model, without the use of any sparse representations. We evaluate our final model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models <ref type="bibr" target="#b34">(Seo et al., 2019;</ref>)-a 15% to 25% absolute improvement in most datasets. Our model also matches performance of state-ofthe-art retriever-reader models <ref type="bibr" target="#b9">(Guu et al., 2020;</ref>. Because of the removal of sparse representations and careful design choices, we further reduce the storage footprint for the full English Wikipedia from 1.5TB to 320GB, as well as improving the throughput by processing more than 10 questions per second on CPUs.</p><p>Finally, we envision that DensePhrases acts as a neural interface for retrieving phrase-level knowledge from a large text corpus. As such, it can be integrated into other knowledge-intensive NLP tasks beyond question answering. To showcase this possibility, we demonstrate that we can directly use DensePhrases for fact extraction, without the need for re-building billions of phrase representations. With fine-tuning on a small number of subject-relation-object triples on the query representations alone, we achieve state-of-the-art performance on two slot filling tasks <ref type="bibr" target="#b26">(Petroni et al., 2020)</ref>, using only 5% of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Open-domain QA</head><p>We first formulate the task of open-domain question answering for a set of K documents D = {d 1 , . . . , d K }. We follow the recent work <ref type="bibr" target="#b4">(Chen et al., 2017;</ref> and treat all of English Wikipeida as D, hence K ≈ 5 × 10 6 . However, most approaches-including ours-are generic and could be applied to other collections of documents.</p><p>The task aims to provide an answerâ for the input question q based on D. In this work, we focus on the extractive QA setting, where each answer is a segment of text, or a phrase, that can be found in D. Denote the set of phrases in D as S(D) and each phrase s k ∈ S(D) consists of contiguous words w start(k) , . . . , w end(k) in its document d doc(k) . In practice, we consider all the phrases up to L = 20 words in D and S(D) comprises a large number of 6 × 10 10 phrases. An extractive QA system returns a phraseŝ = argmax s∈S(D) f (s | D, q) and f is a scoring function. The system finally mapsŝ to an answer stringâ: TEXT(ŝ) =â. Evaluation is typically done by comparing the predicted answer a with a gold answer a * (after normalization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retriever-reader A dominating paradigm in</head><p>open-domain QA is the retriever-reader approach <ref type="bibr" target="#b4">(Chen et al., 2017;</ref>, which leverages a firststage document retriever (f retr ) and only reads top K (K K) documents for finding the answer with a reader model (f read ). The scoring function f (s | D, q) can be decomposed as:</p><formula xml:id="formula_0">f (s | D, q) = f retr ({d j 1 , . . . , d j K } | D, q) × f read (s | {d j 1 , . . . , d j K }, q),<label>(1)</label></formula><p>where {j 1 , . . . , j K } ⊂ {1, . . . , K} and if s / ∈ S({d j 1 , . . . , d j K }), the score will be 0. The model can be easily adapted from documents to passages and has been studied extensively <ref type="bibr" target="#b38">(Yang et al., 2019;</ref><ref type="bibr" target="#b37">Wang et al., 2019)</ref>. However, this approach suffers from error propagation when incorrect documents are retrieved and can be slow as it usually requires running an expensive reader model on every retrieved document or passage at inference time. <ref type="bibr" target="#b34">Seo et al. (2019)</ref> introduce the phrase retrieval approach that encodes phrase and question representations independently and performs similarity search over the phrase representations to find an answer. Their scoring function f is computed as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phrase retrieval</head><formula xml:id="formula_1">f (s | D, q) = E s (s, D) E q (q)<label>(2)</label></formula><p>where E s and E q denote the phrase encoder and the question encoder respectively. As E s (·) and E q (·) representations are decomposable, it can support maximum inner product search (MIPS) and improve the efficiency of open-domain QA models. Previous approaches <ref type="bibr" target="#b34">(Seo et al., 2019;</ref>  Although we focus on the extractive QA setting, recent works propose to use a generative model as the reader <ref type="bibr" target="#b14">Izacard and Grave, 2020)</ref>, or learn a closed-book QA model , which predicts an answer directly without using an external knowledge source. The extractive setting provides two advantages: first, the model directly locates the source of the answer, which is more interpretable, and second, phraselevel knowledge retrieval can be uniquely adapted to other NLP tasks as we show in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Technical Challenges</head><p>Despite an appealing approach, phrase retrieval poses several key technical challenges. The first challenge is the decomposition constraint between question and phrase encoders as stated in Equation (2), which brings a significant degradation of performance. While a similar problem is observed in learning dual encoders for passage representations <ref type="bibr" target="#b13">(Humeau et al., 2019;</ref><ref type="bibr" target="#b19">Khattab and Zaharia, 2020)</ref>, phrase representations are even more difficult to learn due to the fine-grained representations. The second challenge arises from the scale. Compared to 5-million documents, or 21-million text blocks as used in previous work, each phrase representation has to be properly normalized over 60 billions of dense representations. This normalization problem is also implied in Equation <ref type="formula" target="#formula_1">(2)</ref> that E s (s, D) is defined over all the phrases in S(D). Lastly, since it is computationally expensive to build billions of phrase representations at the Wikipedia scale, it is prohibitive to update the phrase representations once they are obtained. As a result, current phrase retrieval models often rely on their zero-shot ability .</p><p>In this work, we introduce DensePhrases to tackle these technical challenges, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. In the following, we first describe our query-agnostic model in a single-passage setting and address the decomposability gap (Section 3). Then we propose several normalization techniques for scaling phrase representations to the full collection of documents D (Section 4). Finally, we detail how we adapt our model for transfer learning (Section 5), without the need for re-building of phrase representations at scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Phrase-Indexed Question Answering</head><p>In this section, we start by learning query-agnostic phrase representations in a reading comprehension setting, in which a gold passage p is given for a question-answer pair (q, a * ). Our goal is to build a strong reading comprehension model while enforcing the decomposability of query and phrase representations. This problem was first formulated by <ref type="bibr" target="#b33">Seo et al. (2018)</ref> and dubbed the phrase-indexed question answering (PIQA) task. In the following, we first describe our base model (Section 3.1) and propose two new solutions to close the decomposability gap: tackling data sparsity via question generation (Section 3.2) and distillation from querydependent models (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Base Model</head><p>We first describe our base architecture, which consists of a phrase encoder E s and a question encoder E q . Given a passage p of m tokens w 1 , . . . , w m , we consider phrases of up to L tokens and the full set of phrases is denoted by S(p). Each phrase s k has start and end indicies start(k) and end(k) and the gold phrase is s * ∈ S(p) 4 . Following previous work on phrase, or span representations <ref type="bibr" target="#b24">(Lee et al., 2017;</ref><ref type="bibr" target="#b33">Seo et al., 2018)</ref>, we first use a pretrained language model M p to obtain contextualized word representations for each passage token: h 1 , . . . , h m . Then, we can represent each phrase s k ∈ S(p) as the concatenation of corresponding start and end vectors:</p><formula xml:id="formula_2">E s (s k , p) = [h start(k) , h end(k) ].</formula><p>( <ref type="formula">3)</ref> Using contextualized word representations to construct phrase representations has another great advantage that we can eventually reduce the storage of phrase representations to word representations. Therefore, we only need to save |W(D)| (the total number of tokens in D) vectors, which is at least one magnitude order smaller than |S(D)|. Similarly, we need to learn a question encoder E q (·) that maps a question of n tokens w 1 , . . . , w n to a vector of the same dimension as E s (·). To do so, we use another two different pre-trained LMs M q,start and M q,end applied on q and finally obtain representations q start and q end pooled from the [CLS] token representations of M q,start and M q,end respectively. Finally, E q (·) simply takes their concatenation:</p><formula xml:id="formula_3">E q (q) = [q start , q end ].<label>(4)</label></formula><p>In summary, we have three different LMs in total, which are initialized from the same pre-trained LM. Since the start and end representations of phrases are produced by the same language model, we need two question representations q start and q end to differentiate the start and end positions. In our pilot experiments, we found that SpanBERT <ref type="bibr" target="#b16">(Joshi et al., 2020)</ref> leads to superior performance compared to BERT . SpanBERT is designed to predict the information in the entire span from its two endpoints, therefore it is well suited for our phrase representations. In our final model, we use SpanBERT-base-cased as our base LMs for E s and E q , and hence d = 768.</p><p>Single-passage training For the reading comprehension setting, we maximize the log-likelihood of the start and end positions of the gold phrase s * :</p><formula xml:id="formula_4">z start 1 , . . . , z start m =[h 1 , . . . , h m ] q start P start =softmax(z start 1 , . . . , z start m )<label>(5)</label></formula><p>We define L start = − log P start start(s * ) and L end in a similar way. The final loss is calculated as:</p><formula xml:id="formula_5">L single = L start + L end 2 .<label>(6)</label></formula><p>Differences from DenSPI We deviate from Den-SPI in the following ways: (1) Previous models split a hidden vector from a pre-trained LM into four vectors (start &amp; end vectors and two vectors for calculating a coherency score). We don't do any splitting of vectors and remove the use of coherency scalars. We find that it is beneficial to keep the output dimension of pre-trained LMs for fully utilizing their representational capacity;</p><p>(2) Previous models use a shared encoder for phrases and questions. However, we use two different language models for representing questions. <ref type="formula">(3)</ref> We use SpanBERT instead of BERT. See <ref type="table" target="#tab_8">Table 5</ref> for an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tackling Data Sparsity</head><p>The performance of query-agnostic models is always inferior to query-dependent models with cross-attention . We hypothesize that one key reason is most reading comprehension datasets only provide a few annotated questions in each passage, compared to the set of possible answer phrases. For instance, each passage in the training set of Natural Questions ) mostly has only one annotated question. Suppose that we are given a passage with the following question-answer pair in the training set: p = Queen Elizabeth II is the sovereign, and her heir apparent is her eldest son, Charles, Prince of Wales. (...) Third in line is Prince George, the eldest child of the Duke of Cambridge (...) q = who is next in line to be the monarch of england s * = Charles, Prince of Wales While cross-attention models only need to represent the passage focusing on "who is Queen Elizabeth II's heir apparent", our phrase encoder should take all the other phrases into account, (e.g., s = Prince George), because their representations will be re-used for other questions (e.g., q = who is the eldest child of the duke of cambridge).</p><p>Following this intuition, we propose to use a simple question generation model to generate questions for each training passage. We generate questions from a question generation (QG) model built upon a T5-large model . The input of the model is a passage p with the gold answer s * highlighted by inserting surrounding special tags and the model is trained to maximize the log-likelihood of the question words of q. Since we want to cover many possible answer phrases during inference, we extract all the entities in each training passage as candidate answers to extract named entities. to generate questions. We keep the questionanswer pairs only when a strong query-dependent (QD) reading comprehension model (SpanBERTlarge, 88.2 EM on SQuAD) gives correct prediction on the generated pair. The remaining generated QA pairs {(q 1 , s 1 ), (q 2 , s 2 ), . . . , (q r , s r )} are directly augmented to the original training set. Generated QA pairs can help learn phrase representations aligned with those of corresponding questions, instead of biased to few annotated questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distillation</head><p>As query-dependent (QD) models with crossattention are considered stronger models, we also propose improving our query-agnostic model by distilling knowledge from a cross-attention model. We minimize the Kullback-Leibler (KL) divergence between the probability distribution from our phrase encoder and that from a query-dependent model <ref type="bibr" target="#b12">(Hinton et al., 2015)</ref>. We use a SpanBERTbase QA model as the QD model. The distillation loss is computed as follows:</p><formula xml:id="formula_6">L distill = KL(P start ||P start qd ) + KL(P end ||P end qd ) 2 ,<label>(7)</label></formula><p>where P start (and P end ) is defined in Equation <ref type="formula" target="#formula_4">(5)</ref> and P start qd and P end qd denote the probability distributions, which are used to predict the start and end indices in the query-dependent model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Phrase Representations at Scale</head><p>Eventually, we need to build phrase representations for billions of phrases, therefore a bigger challenge is to incorporate more phrases during training so the representations can be better normalized. While <ref type="bibr" target="#b34">Seo et al. (2019)</ref> simply sample two negative questions from other passages based on question similarity, we propose two effective negative-sampling strategies, which are efficient to compute and highly useful in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">In-batch Negatives</head><p>We use in-batch negatives for our dense phrase representations, which also has been shown to be effective in learning dense passage representations before . Specifically, for the i-th example in a mini-batch of size B, we denote the hidden representations of the gold start and end positions (h start(s * ) and h end(s * ) ) as g start i and g end i , as well as the question representation as [q start i , q end i ]. Let G start , G end , Q start , Q end be the B × d matrices and each row corresponds to g start i , g end i , q start i , q end i respectively. Basically, we can treat all the gold phrases from other passages in the same mini-batch as negative examples. We compute S start = Q start G start and S end = Q end G end and the i-th row of S start and S end return B scores each, including a positive score and B − 1 negative scores: s start 1 , . . . , s start B and s end 1 , . . . , s end B . Similar to Equation <ref type="formula" target="#formula_4">(5)</ref>, we can compute the loss function for the i-th example as:</p><formula xml:id="formula_7">P start_ib = softmax(s start 1 , . . . , s start B ) P end_ib = softmax(s end 1 , . . . , s end B ) L neg = − log P start_ib i + log P end_ib i 2<label>(8)</label></formula><p>Finally, the loss is summed over B examples in the mini-batch. We also attempted using other nongold phrases from other passages as negatives but didn't find a meaningful improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-batch Negatives</head><p>The in-batch negatives are highly effective and they usually benefit from a large batch size . However, batch sizes are bounded by GPU memory and it is challenging to further increase them. In this section, we propose a novel negative sampling method called pre-batch negatives, which can effectively utilize the representations from the preceding C mini-batches. In each iteration, we maintain a FIFO queue of C minibatches to cache phrase representations (G start , G end ). The cached phrase representations are then used as negative samples for the next iteration, providing B × C additional negative samples in total. This approach is inspired by the momentum contrast idea recently proposed in unsupervised visual representation learning <ref type="bibr" target="#b10">(He et al., 2020)</ref>. However, our approach differs from theirs in that we have independent encoders for phrases and questions and back-propagate to both during training, without a momentum update. 5 These pre-batch negatives are used together with in-batch negatives and the training loss is the same as Equation <ref type="formula" target="#formula_7">(8)</ref>, except that the gradients are not back-propagated to the cached pre-batch negatives.</p><p>In practice, we found that pre-batch negatives work well, once the phrase encoder is warmed up with in-batch negatives. After the warm-up stage, we simply shift from in-batch negatives (B − 1 negatives) to in-batch and pre-batch negatives (hence a total number of B × C + B − 1 negatives). For simplicity, we use L neg to denote both in-batch negatives and pre-batch negatives during training. Since we do not retain the computational graph for forward and backward propagation, the memory consumption of pre-batch negatives is much more manageable while allowing increasing the number of negative samples. Empirically, we found that using a large number of pre-batch negatives does not always help since the phrase representations can get easily outdated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization, Indexing and Search</head><p>With our loss terms defined previously, we finally minimize the following loss function on a question answering dataset, together with the generated questions (Section 3.2):</p><formula xml:id="formula_8">L = λ 1 L single + λ 2 L distill + λ 3 L neg<label>(9)</label></formula><p>where λ 1 , λ 2 , λ 3 determine the importance of each loss term. We found that setting λ 1 = 1, λ 2 = 2, and λ 3 = 4 works well in practice. In our experiments (Section 6.2), we use reading comprehension datasets (a gold passage p is provided) such as SQuAD <ref type="bibr" target="#b31">(Rajpurkar et al., 2016)</ref> and Natural Questions  to train the phrase and question encoders.</p><p>Indexing After training the phrase encoder E s , we need to encode all the phrases S(D) in the entire English Wikipedia D and store an index of the phrase dump. We segment each document d i ∈ D into a set of natural paragraphs, from which we obtain token representations for each paragraph using E s (· Search During inference, for a given question q, we can find the answerŝ as follows <ref type="figure" target="#fig_1">(Figure 1)</ref>:</p><formula xml:id="formula_9">s = argmax s (i,j) E s (s (i,j) , D) E q (q) = argmax s (i,j) h i q start + h j q end = argmax s (i,j) (Hq start ) i + (Hq end ) j<label>(10)</label></formula><p>where s (i,j) denotes a phrase with start and end indices as i and j in the index H. We can compute the argmax of Hq start (or Hq end ) efficiently by performing MIPS over H with q start (or q end ). In practice, we search for the top k start and top k end positions separately and perform a constrained search over the start and end positions respectively such that 1 ≤ i ≤ j &lt; i + L ≤ |W(D)|. Since we share H for the start and end representations, q start and q end are also batched for MIPS to benefit from multi-threading. To avoid producing redundant answers, we only keep the best scoring phrases when the two phrases that have the same normalized string are retrieved from the same paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Query-side Fine-tuning</head><p>So far, we have created a phrase dump H as well as a question encoder E q that can be directly used for question answering. In this section, we propose a novel method called query-side fine-tuning, which can facilitate transfer learning on a new dataset, by training the question encoder E q to correctly retrieve a desired answer a * for a question q given H. There are several advantages for doing this: (1) It can help our model quickly adapt to new QA tasks without re-building billions of phrase representations. 7 (2) Even for the questionanswering datasets used to build H (SQuAD and NQ in our experiments), we also find that queryside fine-tuning can further improve performance because it can reduce the discrepancy between training and inference.</p><p>(3) It also creates a possibility to adapt our DensePhrases to non-QA tasks when the query is written in a different format. In Section 6.3, we show the possibility of directly using DensePhrases for slot filling tasks by using a query such as (Michael Jackson, is a singer of, x).</p><p>In this regard, we can view our model as a knowledge base that can be accessed by many different types of queries and it is able to return phraselevel knowledge efficiently. Formally speaking, we maximize the marginal log-likelihood of the gold answer a * for a question q, which resembles the weakly-supervised setting in the open-domain QA <ref type="bibr" target="#b28">Min et al., 2019)</ref>. The loss for query-side fine-tuning is computed as follows:</p><formula xml:id="formula_10">L open = − log s∈S(q) TEXT(s)=a * exp f (s|D, q) s i ∈S(q) exp f (s i |D, q) ,<label>(11)</label></formula><p>where f (s|D, q) is the score of the phrase s (Equation (2)) andS(q) denotes the top k phrases for q (Equation <ref type="formula" target="#formula_0">(10)</ref>). In practice, we use k = 100 for the query-side fine-tuning. Note that only the parameters of the question encoder E q are updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Setup</head><p>Datasets We use two reading comprehension datasets: SQuAD <ref type="bibr" target="#b31">(Rajpurkar et al., 2016)</ref> and Natural Questions , in which a gold passage is provided. For Natural Questions, we use the short answer as a ground truth answer a * and its long answer as a gold passage p (Appendix D). We train our phrase representations on these two datasets, and also report the performance of our query-agnostic models in the reading comprehension setting.</p><p>We evaluate our approach on five popular open-domain QA datasets: Natural Questions , WebQuestions <ref type="bibr" target="#b2">(Berant et al., 2013)</ref>, CuratedTREC <ref type="bibr" target="#b1">(Baudiš and Šedivỳ, 2015)</ref>, TriviaQA <ref type="bibr" target="#b17">(Joshi et al., 2017)</ref>, and SQuAD <ref type="bibr" target="#b31">(Rajpurkar et al., 2016)</ref> and the data statistics are provided in <ref type="table" target="#tab_13">Table D.</ref>3. Although many questions in SQuAD are context-dependent, we evaluate our model on SQuAD for the comparison with previous phrase retrieval models <ref type="bibr" target="#b34">(Seo et al., 2019;</ref>, which were mainly trained and evaluated on SQuAD.</p><p>Finally, we also evaluate our model on two slot filling tasks, to show how to adapt our DensePhrases for other knowledge-intensive NLP tasks. We focus on using two slot filling datasets from the KILT benchmark <ref type="bibr" target="#b26">(Petroni et al., 2020)</ref>: T-REx <ref type="bibr" target="#b7">(Elsahar et al., 2018)</ref> and zero-shot relation extraction <ref type="bibr" target="#b25">(Levy et al., 2017)</ref>. Each query is provided in the form of "{subject entity} <ref type="bibr">[SEP]</ref> {relation}", where the answer is the object entity.  <ref type="bibr" target="#b9">(Guu et al., 2020)</ref> {Wiki., CC-News} † 40.4 40.7 42.9 --DPR-multi     on each dataset using Equation <ref type="formula" target="#formula_0">(11)</ref>. While we use a single 48GB GPU (Quadro RTX 8000) for training the phrase encoders with Equation <ref type="formula" target="#formula_8">(9)</ref>, queryside fine-tuning is relatively cheap and uses a single 12GB GPU (TITAN Xp). For slot filling, we use the same set of hyperparameters used for queryside fine-tuning our models on open-domain QA datasets and we use the phrase dump obtained from DensePhrases (NQ + SQuAD). To see how rapidly our model adapts to the new query types, we train our models on randomly sampled 5k or 10k training samples. See Appendix E for more details on the hyperparameters for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiments: Question Answering</head><p>Baselines For reading comprehension, we report scores of query-agnostic models including Den-SPI <ref type="bibr" target="#b34">(Seo et al., 2019)</ref>, DenSPI + Sparc  as well as Deformer <ref type="bibr" target="#b3">(Cao et al., 2020)</ref> and DilBERT <ref type="bibr" target="#b35">(Siblini et al., 2020)</ref>, where they only allow a late interaction (cross-attention) in the last few layers of BERT. We report their results based on the interaction in the last layer, which mostly resembles the fully query-agnostic models.</p><p>For open-domain QA, we report the scores of extractive open-domain QA models including DrQA <ref type="bibr" target="#b4">(Chen et al., 2017)</ref>, BERT + BM25 , ORQA , REALM <ref type="bibr" target="#b9">(Guu et al., 2020)</ref>, and DPR-Multi . We also show the performance of previous phrase retrieval models: DenSPI and DenSPI + Sparc. In Appendix A, we provide a thorough analysis on the computational complexity of each open-domain QA model.   <ref type="bibr" target="#b26">(Petroni et al., 2020)</ref>. We report KILT-AC and KILT-F1 (denoted as Acc and F1 in the table), which consider both span-level accuracy and correct retrieval of evidence documents. We consider two settings, in which we use 5K and 10K training examples respectively.</p><p>Reading comprehension Experimental results on reading comprehension datasets are shown in <ref type="table" target="#tab_4">Table 3</ref>. Among query-agnostic models, our models achieve the best performance of 78.3 EM on SQuAD by improving the previous dense phrase retrieval model (DenSPI) by 4.7%. Despite it is still behind query-dependent models, the gap has been greatly reduced and serves as a strong starting point for the open-domain QA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open-domain QA</head><p>Experimental results on open-domain QA are summarized in <ref type="table" target="#tab_3">Table 2</ref>. Without using any sparse representations, DensePhrases outperforms previous phrase retrieval models by a large margin of a 15%-25% absolute improvement on all the datasets except SQuAD. As previous models only used SQuAD to train the phrase model and perform zero-shot prediction on other datasets, we add one more experiment training the model of  on C phrase = {NQ, SQuAD} for a fair comparison. However, it only increases the result from 14.5% to 16.5% on Natural Questions, demonstrating that it is not enough to simply add more datasets for training phrase representations. Our performance is also competitive with recent retriever-reader models <ref type="bibr" target="#b9">Guu et al., 2020)</ref>, while running much faster during inference <ref type="table" target="#tab_13">(Table 1)</ref>. We can also consider using distantly-supervised examples of TriviaQA, WebQuestions and TREC for training our phrase representations, as the DPR-multi model did, and we leave it to future work. Finally, we find that using C phrase = {SQuAD} or {NQ, SQuAD} doesn't make much difference after query-side fine-tuning in most datasets, except that including   <ref type="bibr" target="#b34">(Seo et al., 2019)</ref> also included a coherency scalar and see their paper for more details.</p><p>NQ in C phrase brings a large gain of 9.7% on the NQ evaluation.  <ref type="table" target="#tab_8">Table 5</ref> shows the ablation result of our model on SQuAD. We observe that not sharing phrase and question encoders (Share = ) and using the full output dimension (Split = ) together improves the performance by 2%. Using a stronger pre-trained LM SpanBERT leads to another 1.3% improvement. Augmenting training set with generated questions (QG = ) and performing distillation from query-dependent models (Distill = ) further improves performance up to EM = 78.3. We also attempt adding the generated questions to the training of the query-dependent model and only find a 0.3% improvement (SpanBERT-base), which validates our hypothesis that data sparsity is a bottleneck for query-agnostic models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experiments: Slot Filling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Effect of Batch Negatives</head><p>We further evaluate the effectiveness of various negative sampling methods introduced in Section 4.</p><p>Since it is computationally expensive to test each setting at the full Wikipedia scale, we use a smaller text corpus D small of all the gold passages in the development sets of Natural Questions, for the ablation study. We also report performance in the reading comprehension setting so D = {p} only consists of a gold passage. Empirically, we find that results are generally well correlated when we gradually increase the size of |D| and we encourage interested readers to experiment with these settings for model development. The results are summarized in <ref type="table" target="#tab_13">Table 6</ref>. While using a larger batch size (B = 84) is beneficial for in-batch negatives, the number of preceding batches in pre-batch negatives is optimal when C = 2. Somewhat surprisingly, the pre-batch negatives also improve the performance when D = {p}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Effect of Query-side Fine-tuning</head><p>The query-side fine-tuning further trains the question encoder E q with the phrase dump H and enables us to fine-tune DensePhrases on different types of questions. We use three different phrase encoders, each of which is trained on a different training dataset C phrase . We summarize the results in <ref type="table" target="#tab_13">Table 7</ref>. For the datasets that were not used for training the phrase encoders (TQA, WQ, TREC), we observe a 15% to 20% improvement after queryside fine-tuning, compared to zero-shot prediction. Even for the datasets that have been used (NQ, SQuAD), it leads to significant improvements (e.g., +8.3% improvement on NQ for C phrase = {NQ}) and it clearly demonstrates it can effectively reduce the discrepancy between training and inference. 28.9 18.9 34.9 31.9 33.2 40.9 37.5 51.0 50.7 38.0 <ref type="table" target="#tab_13">Table 7</ref>: Effect of query-side fine-tuning in DensePhrases on each test set. We report EM of each model before (QS = ) and after (QS = ) the query-side fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this study, we show that we can learn dense representations of phrases at the Wikipedia scale, which are readily retrievable for open-domain QA and other knowledge-intensive NLP tasks. We tackle the decomposability gap by mitigating data sparsity and introduce two batch-negative techniques for normalizing billions of phrase representations. We also introduce query-side finetuning that easily adapts our model to any type of query with a single 12GB GPU. As a result, we achieve much stronger performance on five popular open-domain QA datasets compared to previous phrase retrieval models, while reducing the storage footprint and improving latency significantly. We also achieve strong performance on two slot filling datasets using only a small number of training examples, showing the possibility of utilizing our DensePhrases as a dense knowledge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Complexity Analysis</head><p>We describe the resources and time spent during inference <ref type="table" target="#tab_13">(Table 1</ref> and A.1) and indexing (Table A.1). With our limited GPU resources (24GB × 4), it takes about 20 hours for indexing the entire phrase representations. We also largely reduced the storage from 1,547GB to 320GB by (1) removing sparse representations and (2) using our sharing and split strategy. See Appendix B for the details on the reduction of storage footprint and Appendix C for the specification of our server for the benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indexing</head><p>Resources  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Reducing Storage Footprint</head><p>As shown in <ref type="table" target="#tab_13">Table 1</ref>, we have reduced the storage footprint from 1,547GB  to 320GB. We detail how we can reduce the storage footprint in addition to the several techniques introduced by <ref type="bibr" target="#b34">Seo et al. (2019)</ref>. First, following <ref type="bibr" target="#b34">Seo et al. (2019)</ref>, we apply a linear transformation on the passage token representations to obtain a set of filter logits, which can be used to filter many token representations from W(D). This filter layer is supervised by applying the binary cross entropy with the gold start/end positions (trained together with Equation <ref type="formula" target="#formula_8">(9)</ref>). We tune the threshold for the filter logits on the reading comprehension development set to the point where the performance does not drop significantly while maximally filtering tokens.</p><p>Second, in our architecture, we use a base model (SpanBERT-base) for a smaller dimension of token representations (d = 768) and does not use any sparse representations including tf-idf or contextualized sparse representations . We also use the scalar quantization for storing float32 vectors as int8 during indexing.</p><p>Lastly, since the inference in Equation <ref type="formula" target="#formula_0">(10)</ref> is purely based on MIPS, we do not have to keep the original start and end vectors which takes about 500GB. However, when we perform query-side fine-tuning, we need the original start and end vectors for reconstructing them to compute Equation (11) since MIPS index only returns the top-k scores and their indices, but not the vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Server Specifications for Benchmark</head><p>To compare the complexity of open-domain QA models, we install all models in <ref type="table" target="#tab_13">Table 1</ref>  For DPR, due to its large memory consumption, we use a similar server with a 24GB GPU (TITAN RTX). For all models, we use 1,000 randomly sampled questions from the Natural Questions development set for the speed benchmark and measure #Q/sec. We set the batch size to 64 for all models except BERTSerini, ORQA and REALM, which do not support a batch size of more than 1 in their open-sources. #Q/sec for DPR includes retrieving passages and running a reader model and the batch size for the reader model is set to 8 to fit in the 24GB GPU (retriever batch size is still 64). For other hyperparameters, we use the default settings of each model. We also exclude the time and the number of questions in the first five iterations for warming up each model. Note that despite our effort to match the environment of each model, their latency can be affected by various different settings in their implementations such as the choice of library (PyTorch vs. Tensorflow).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Pre-processing for Single-Passage Training</head><p>We use two reading comprehension datasets (SQuAD and Natural Questions) for training our model on Equation <ref type="formula" target="#formula_8">(9)</ref>. For SQuAD, we use the original dataset provided by the authors <ref type="bibr" target="#b31">(Rajpurkar et al., 2016)</ref>. For Natural Questions (Kwiatkowski et al., 2019), we use the pre-processed version provided by <ref type="bibr" target="#b0">Asai et al. (2019)</ref>. <ref type="bibr">9</ref> We also match the gold passages in Natural Questions to the paragraphs in Wikipedia whenever possible. Since we want to check the performance changes of our model with the growing number of tokens, we follow the same split (train/dev/test) used in Natural Questions-Open for the reading comprehension setting as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Hyperparameters</head><p>We use the Adam optimizer <ref type="bibr" target="#b20">(Kingma and Ba, 2015)</ref> in all our experiments. For training our phrase and question encoders with Equation <ref type="formula" target="#formula_8">(9)</ref>, we use a learning rate of 3e-5 and the norm of the gradient is clipped at 1. We use a batch size of 84 and train each model for 4 epochs for all datasets where the loss of pre-batch negatives is applied in the last two epochs. We use spaCy 10 for extracting named entities in each training passage, which are used to generate questions. The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively. The number of preceding batches C is set to 2. For the query-side fine-tuning with Equation <ref type="formula" target="#formula_0">(11)</ref>, we use a learning rate of 3e-5 and the norm of the gradient is clipped at 1. We use a batch size of 12 and train each model for 10 epochs for all datasets. The top k for the Equation (11) is set to 100. Using the development set, we select the best performing model for each dataset, which are then evaluated on each test set. Since SpanBERT only supports cased models, we also truecase the questions <ref type="bibr" target="#b27">(Lita et al., 2003)</ref> that are originally provided in the lowercase (Natural Questions and WebQuestions).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>leverage both dense and sparse vectors for phrase and question representations by taking their concatenation: E s (s, D) = [E sparse (s, D), E dense (s, D)]. 3 However, since the sparse vectors are difficult to parallelize with dense vectors, their method is not purely implemented as MIPS in practice while requiring more storage. The goal of this work is to only use the dense representations, i.e., E s (s, D) = E dense (s, D), which can model f (s | D, q) solely with MIPS, as well as close the gap in performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>An overview of DensePhrases. (a) We learn a query-agnostic reading comprehension model (Section 3), as well as normalize phrase representations with an extensive use of negative examples (Section 4). (b) With the trained phrase encoder, we build a MIPS index for the phrase representations of the entire text corpus D of |W(D)| tokens in total (Section 4.3). (c) Our query-side fine-tuning further optimizes the question encoder and can also adapt our model to new types of questions such asq (Section 5). (d) During inference, we search for start and end positions with MIPS (Section 4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Open-domain QA results. We report exact match (EM) on the testing sets. We also show the additional (pre-)training datasets for learning the retriever models (C retr ) and creating the phrase dump (C phrase ).</figDesc><table><row><cell>Model</cell><cell cols="2">SQuAD</cell><cell cols="2">NQ (Long)</cell></row><row><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>Query-Dependent</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-base</cell><cell cols="4">80.8 88.5 69.9 78.2</cell></row><row><cell>SpanBERT-base</cell><cell cols="4">85.7 92.2 73.2 81.0</cell></row><row><cell>Query-Agnostic</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DilBERT (Siblini et al., 2020) 63.0 72.0</cell><cell>-</cell><cell>-</cell></row><row><cell>DeFormer (Cao et al., 2020)</cell><cell>-</cell><cell>72.1</cell><cell>-</cell><cell>-</cell></row><row><cell>DenSPI  †</cell><cell cols="4">73.6 81.7 68.2 76.1</cell></row><row><cell>DenSPI + Sparc  †</cell><cell cols="2">76.4 84.8</cell><cell>-</cell><cell>-</cell></row><row><cell>DensePhrases (ours)</cell><cell cols="4">78.3 86.3 71.9 79.6</cell></row></table><note>* : no supervi- sion using target training data (zero-shot).† : unlabeled data used for extra pre-training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Reading comprehension results, evaluated on the development sets of SQuAD and Natural Questions. Underlined numbers: estimated from the figures from the original papers. † : BERT-large model.</figDesc><table><row><cell>Implementation details Our phrase and ques-</cell></row><row><cell>tion encoders are trained on each training set with</cell></row><row><cell>Equation (9). We use SQuAD to train our QG</cell></row><row><cell>model. 8 We denote the training datasets used for</cell></row><row><cell>reading comprehension (Equation (9)) as C phrase .</cell></row><row><cell>For using DensePhrases in open-domain QA, we</cell></row><row><cell>train two versions of phrase encoders, each of</cell></row><row><cell>which are trained on C phrase = {SQuAD} and</cell></row><row><cell>{NQ, SQuAD}, respectively. Note that both Den-</cell></row><row><cell>SPI and DenSPI + Sparc are trained on C phrase =</cell></row><row><cell>{SQuAD}. We build the phrase dump H for the</cell></row><row><cell>2018-12-20 Wikipedia snapshot which contains</cell></row><row><cell>about 5.6 million documents and 3 billion words.</cell></row><row><cell>Then, we perform query-side fine-tuning our model</cell></row></table><note>8 The quality of generated questions from a QG model trained on Natural Questions is worse due to the ambiguity of information-seeking questions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Slot filling results on the test sets of T-REx and Zero shot RE (ZsRE) in the KILT benchmark</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Ablation of DensePhrases on the development</cell></row><row><cell>set of SQuAD. Bb: BERT-base, Sb: SpanBERT-base,</cell></row><row><cell>Bl: BERT-large. Share: whether question and phrase</cell></row><row><cell>encoders are shared or not. Split: whether the full hid-</cell></row><row><cell>den vectors are kept or split into start and end vectors.</cell></row><row><cell>DenSPI</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>summarizes the results of our model on the</cell></row><row><cell>two slot filling datasets, along with the scores of</cell></row><row><cell>baseline models provided by Petroni et al. (2020).</cell></row><row><cell>The only extractive baseline is DPR + BERT which</cell></row><row><cell>performs poorly in zero-shot relation extraction.</cell></row><row><cell>On the other hand, our model achieves competitive</cell></row><row><cell>performance on all datasets and achieves state-of-</cell></row><row><cell>the-art performance on two datasets using only 5K</cell></row><row><cell>training samples (less than 5% of the training data).</cell></row><row><cell>This showcases how DensePhrases can be easily</cell></row><row><cell>leveraged for knowledge-intensive NLP tasks.</cell></row><row><cell>7 Analysis</cell></row><row><cell>7.1 Ablation of Phrase Representations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A .</head><label>A</label><figDesc>1: Complexity analysis of three open-domain QA models during indexing and inference. For inference, we also report the minimum requirement of RAM and GPU memory for running each model with GPU. For computing #Q/s for CPU, we do not use GPUs but load all models on the RAM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>on the same server using their public open-source code. Our server has the following specifications:</figDesc><table><row><cell>Hardware</cell></row><row><cell>Intel Xeon CPU E5-2630 v4 @ 2.20GHz</cell></row><row><cell>128GB RAM</cell></row><row><cell>12GB GPU (TITAN Xp) × 2</cell></row><row><cell>2TB 970 EVO Plus NVMe M.2 SSD × 1</cell></row><row><cell>Table C.2: Server specification for the benchmark</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Statistics of five open-domain QA datasets and two slot filling datasets. We follow the same splits in open-domain QA for the two reading comprehension datasets (SQuAD and Natural Questions).</figDesc><table><row><cell>Dataset</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>Natural Questions</cell><cell cols="2">79,168 8,757</cell><cell>3,610</cell></row><row><cell>WebQuestions</cell><cell>3,417</cell><cell>361</cell><cell>2,032</cell></row><row><cell>CuratedTrec</cell><cell>1,353</cell><cell>133</cell><cell>694</cell></row><row><cell>TriviaQA</cell><cell cols="3">78,785 8,837 11,313</cell></row><row><cell>SQuAD</cell><cell cols="3">78,713 8,886 10,570</cell></row><row><cell>T-REx</cell><cell cols="2">2,284,168 5,000</cell><cell>5,000</cell></row><row><cell>Zero-Shot RE</cell><cell cols="2">147,909 3,724</cell><cell>4,966</cell></row><row><cell>Table D.3:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/ jhyuklee/DensePhrases.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Since we benchmark each model with a batch size of 64 as detailed in Appendix C, the speedup in previous phrase retrieval models<ref type="bibr" target="#b34">(Seo et al., 2019;</ref> is smaller than previously claimed, although they still performs much faster on CPUs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3"><ref type="bibr" target="#b34">Seo et al. (2019)</ref> use sparse representations of both paragraphs and documents and use contextualized sparse representations conditioned on the phrase.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In some reading comprehension datasets (e.g., SQuAD, Natural Questions), the gold phrase s * is given. While in some other datasets, only the gold answer a * is provided and we need to find a phrase s that matches TEXT(s) = a * .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In their approach, the query and key encoders are shared. Only the query encoder is back-propagated and the key encoder is updated as a moving average. Interested readers are referred to their paper for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We use IVFSQ4 with 1M clusters and set n-probe to 256.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Previous work used the question encoder Eq directly for additional QA tasks such as TREC and it is called a zero-shot prediction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://github.com/AkariAsai/ learning_to_retrieve_reasoning_paths 10 https://spacy.io/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Sewon Min </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to retrieve reasoning paths over wikipedia graph for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling of the question answering task in the YodaQA system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Baudiš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Cross-Language Evaluation Forum for European Languages (CLEF)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformer: Decomposing pre-trained transformers for faster question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Aruna Balasubramanian, and Niranjan Balasubramanian</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="34" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">T-REx: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building Watson: An overview of the deepqa project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">REALM: Retrieval-augmented language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">László</forename><surname>Lukács</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00652</idno>
		<title level="m">Balint Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply</title>
		<imprint>
			<date>Sanjiv Kumar</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01282</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with GPUs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Span-BERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ColBERT: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Transactions of the Association of Computational Linguistics (TACL</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contextualized sparse representations for real-time open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimi</forename><surname>Salant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandara</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Vlad Lita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A discrete hard EM approach for weakly supervised question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02252</idno>
		<title level="m">Vassilis Plachouras, Tim Rocktäschel, et al. 2020. KILT: a benchmark for knowledge intensive language tasks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Phraseindexed question answering: A new challenge for scalable document comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Real-time open-domain question answering with dense-sparse phrase index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Delaying interaction layers in transformer-based encoders for efficient open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wissam</forename><surname>Siblini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Challal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Pasqual</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08422</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The TREC-8 question answering track report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trec</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-passage BERT: A globally normalized bert model for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Ramesh Nallapati, and Bing Xiang</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end open-domain question answering with bertserini</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aileen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Demo</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
