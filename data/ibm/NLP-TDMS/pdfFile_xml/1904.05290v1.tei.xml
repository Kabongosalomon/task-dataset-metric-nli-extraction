<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country>TU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country>TU</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning approaches to optical flow estimation have seen rapid progress over the recent years. One common trait of many networks is that they refine an initial flow estimate either through multiple stages or across the levels of a coarse-to-fine representation. While leading to more accurate results, the downside of this is an increased number of parameters. Taking inspiration from both classical energy minimization approaches as well as residual networks, we propose an iterative residual refinement (IRR) scheme based on weight sharing that can be combined with several backbone networks. It reduces the number of parameters, improves the accuracy, or even achieves both. Moreover, we show that integrating occlusion prediction and bi-directional flow estimation into our IRR scheme can further boost the accuracy. Our full network achieves stateof-the-art results for both optical flow and occlusion estimation across several standard datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of parameters (million) AEPE on Sintel Train Clean</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FlowNetC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FlowNetS SpyNet</head><p>Ours (PWC-Net + Occ)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PWC-Net</head><p>Ours (PWC-Net + Bi)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours (IRR-PWC) LiteFlowNet</head><p>Ours (PWC-Net + IRR)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Akin to many areas of computer vision, deep learning has had a significant impact on optical flow estimation. But in contrast to, e.g., object detection <ref type="bibr" target="#b18">[19]</ref> or human pose estimation <ref type="bibr" target="#b54">[55]</ref>, the accuracy of deep learning-based flow methods on public benchmarks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41]</ref> had initially not surpassed that of classical approaches. Still, the efficient test-time inference has led to their widespread adoption as a sub-module in applications requiring to process temporal information, including video object segmentation <ref type="bibr" target="#b12">[13]</ref>, video recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b63">64]</ref>, and video style transfer <ref type="bibr" target="#b10">[11]</ref>.</p><p>FlowNet <ref type="bibr" target="#b13">[14]</ref> pioneered the use of convolutional neural networks (CNNs) for estimating optical flow and relied on a -by now standard -encoder-decoder architecture with skip connections, similar to semantic segmentation <ref type="bibr" target="#b35">[36]</ref>, among others. Since the flow accuracy remained behind that of classical methods based on energy minimization, later work has focused on designing more powerful CNN architectures for optical flow. FlowNet2 <ref type="bibr" target="#b25">[26]</ref> remedied the accuracy limitations of FlowNet and started to outperform classical approaches. Its main principle is to stack multiple FlowNet-  <ref type="figure">Figure 1</ref>. Accuracy / network size tradeoff of CNNs for optical flow: Combining our iterative residual refinement (IRR), as well as bi-directional (Bi) and occlusion estimation (Occ) with PWC-Net <ref type="bibr" target="#b51">[52]</ref> in comparison to previous work. Our full model (IRR-PWC), combining all three components, yields significant accuracy gains over <ref type="bibr" target="#b51">[52]</ref> while having many fewer parameters.</p><p>family networks <ref type="bibr" target="#b13">[14]</ref>, such that later stages effectively refine the output from the previous ones. However, one of the side effects of this stacking is the linearly and strongly increasing number of parameters, being a burden for the adoption in other applications. Also, stacked networks require training the stages sequentially rather than jointly, resulting in a complex training procedure in practice.</p><p>More recently, SpyNet <ref type="bibr" target="#b44">[45]</ref>, PWC-Net <ref type="bibr" target="#b51">[52]</ref>, and Lite-FlowNet <ref type="bibr" target="#b23">[24]</ref> proposed lightweight networks that still achieve competitive accuracy (cf . <ref type="figure">Fig. 1</ref>). SpyNet adopts coarse-to-fine estimation in the network design, a wellknown principle in classical approaches. It residually updates the flow across the levels of a spatial pyramid with individual trainable weights and demonstrates better accuracy than FlowNet but with far fewer model parameters. Lite-FlowNet and PWC-Net further combine the coarse-to-fine strategy with multiple ideas from both classical methods and recent deep learning approaches. Particularly PWC-Net outperformed all published methods on the common public benchmarks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Interestingly, many recent deep learning approaches for flow <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b51">52]</ref> have a common structure: From a rough first flow estimate, later modules or networks re-To appear in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, June 2019. c 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. <ref type="figure">Figure 2</ref>. From a standard network stack to our iterative residual refinement scheme with joint optical flow and occlusion estimation: The stacked version of FlowNet (a) can be converted into an iterative residual refinement model (b) with the half number of parameters. Note that modules with the same color share their weights. We can re-interpret (b) as a rolled version (c), making it more immediate to include occlusion estimation (d) for further improving the accuracy. fine the previous estimates across pyramid levels or through multiple chained networks. As illustrated in <ref type="figure">Fig. 2a</ref>, the later modules or networks have their own trainable weights, since each module assumes a particular functionality at the respective spatial resolution or conditioned on the output of the preceding modules. The downside is that this significantly increases the number of required model parameters.</p><p>In this paper, we take the inspiration from classical energy minimization-based optical flow approaches several steps further. Energy-based methods iteratively estimate the flow based on a consistent underlying energy with a single set of parameters <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b50">51]</ref>. Hence we ask: Can we iteratively refine flow with a deep network based on a single, shared set of weights? Moreover, energy-based methods have benefited from bi-directional estimation and occlusion reasoning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b59">60]</ref>. We thus ask: Can deep learning approaches to optical flow similarly benefit from bi-directional estimation with occlusion reasoning?</p><p>We address these questions and make a number of contributions: (i) We first propose an iterative residual refinement (IRR) scheme that takes the output from a previous iteration as input and iteratively refines it by only using a single network block with shared weights. (ii) We demonstrate the applicability to two popular networks, FlowNet <ref type="bibr" target="#b13">[14]</ref> ( <ref type="figure">Fig. 2c</ref>) and PWC-Net <ref type="bibr" target="#b51">[52]</ref>  <ref type="figure" target="#fig_2">(Fig. 4</ref>). For FlowNet, we can significantly increase the accuracy without adding parameters; for PWC-Net, we can reduce the number of parameters while even improving the accuracy ( <ref type="figure">Fig. 1</ref>). (iii) Next, we demonstrate the integration with occlusion estimation ( <ref type="figure">Fig. 2d</ref>). (iv) We further extend the scheme to bidirectional flow estimation, which turns out to be only beneficial when combined with occlusion estimation. Unlike previous work <ref type="bibr" target="#b26">[27]</ref>, our scheme enables the flow accuracy to benefit from joint occlusion estimation. (v) We finally propose lightweight bilateral filtering and occlusion upsampling layers for refined motion and occlusion boundaries.</p><p>Applying our proposed scheme to two backbone net-works, FlowNet and PWC-Net, yields significant improvements in flow accuracy of 18.5% and 17.7%, respectively, across multiple datasets. In case of PWC-Net, we achieve this accuracy gain using 26.4% fewer parameters. Note that occlusion estimation and bi-directional flow are additional outcomes as by-products of this improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Optical flow with CNNs. Starting with FlowNet <ref type="bibr" target="#b13">[14]</ref>, various deep network architectures for optical flow have been proposed, e.g., FlowNet2 <ref type="bibr" target="#b25">[26]</ref>, SpyNet <ref type="bibr" target="#b44">[45]</ref>, PWC-Net <ref type="bibr" target="#b51">[52]</ref>, and LiteFlowNet <ref type="bibr" target="#b23">[24]</ref>. They are based on an autoencoder design, allow for supervised end-to-end training, and enable fast inference during testing time. To alleviate the need for training data with ground truth in a specific domain, unsupervised <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b65">66]</ref> and semisupervised <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b64">65]</ref> alternatives have also been developed.</p><p>Other than such end-to-end approaches, CNNs can also serve to extract learned feature descriptors, which are combined with classical optimizers or well-designed correspondence search methods to find matches between extracted features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61]</ref>. Such optimization-based approaches can yield less blurry results than typical CNN decoders, but not all are end-to-end trainable and their runtime in the testing phase is significantly longer.</p><p>Here, we investigate how to improve generic autoencoder-based architectures by adapting an iterative residual scheme that is widely applicable.</p><p>Optical flow and occlusion. Occlusion has been regarded as an important cue for estimating more accurate optical flow. Because occluded pixels do not have correspondences in the other frame, several approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref> aim to filter out these outliers to minimize their ill effects and apply post-processing to refine the estimates <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b66">67]</ref>.</p><p>Other methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60]</ref> model occlusions in a joint energy and utilize them as additional evi-  <ref type="figure">Figure 3</ref>. Our Iterative Residual Refinement (IRR) version of FlowNetS <ref type="bibr" target="#b13">[14]</ref>. The model iteratively estimates residual flow from the previous output. Note that we apply warping after several encoder layers, see text for details. dence for flow through recursive joint estimation. Occlusion estimates can enable (i) a more accurate matching cost <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60]</ref>, (ii) bi-directional consistency <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>, or (iii) uniqueness constraints for pixel-level matching <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>Recently, <ref type="bibr" target="#b57">[58]</ref> proposed an unsupervised deep network that jointly estimates flow and occlusion. Occlusions are explicitly detected from the inverse of disocclusion <ref type="bibr" target="#b24">[25]</ref>; the per-pixel loss is disabled on occluded pixels. <ref type="bibr" target="#b26">[27]</ref> proposed a supervised network for jointly estimating optical flow and occlusion, as well as depth and motion boundaries. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42]</ref> integrate occlusion estimation into a PWC-Net backbone based on temporal propagation in longer sequences; <ref type="bibr" target="#b29">[30]</ref> is based on unsupervised learning. Our work also directly learns to estimate occlusion using ground-truth supervision signals, but requires only two frames and unlike <ref type="bibr" target="#b26">[27]</ref> enables to improve the flow using the estimated occlusions.</p><p>Iterative and residual refinement. Despite of the immense learning capacity of deep networks, early CNN approaches to optical flow did not outperform classical methods <ref type="bibr" target="#b13">[14]</ref>. Often, the CNN decoder yielded blurry, thus less accurate regression results. In order to overcome this and motivated by classical coarse-to-fine refinement <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b50">51]</ref>, SpyNet <ref type="bibr" target="#b44">[45]</ref> and PWC-Net <ref type="bibr" target="#b51">[52]</ref> residually update the flow along a pyramid structure. FlowNet2 <ref type="bibr" target="#b25">[26]</ref> instead stacks multiple networks to refine the previous estimates, though this linearly increases the network size. DeMoN <ref type="bibr" target="#b55">[56]</ref> uses a combined strategy of stacking and iteratively using one network, also requiring more parameters than one single baseline network. LiteFlowNet <ref type="bibr" target="#b23">[24]</ref> cascades extra convolution layers for refining the outputs and regularizes the outputs based on a feature-driven local convolution, which adaptively defines the convolution weights based on the estimated outputs. Related approaches have also been proposed in the stereo literature <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref>, successfully improving the accuracy but still increasing the network size.</p><p>In contrast, we propose a generic scheme that repetitively uses one baseline network to yield better accuracy without increasing the network size. For certain networks, we even reduce the size by removing repetitive modules while still enabling competitive or even improved accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Core concepts &amp; base networks</head><p>Iterative residual refinement (IRR) with shared weights. The basic problem setup is to estimate (forward) optical flow f fw from the reference frame I 1 to the target frame I 2 .</p><p>The main concept of our IRR scheme is to make a model learn to residually refine its previous estimate by iteratively re-using the same network block with shared weights. We pursue two scenarios: (i) We increase the accuracy without adding parameters or complicating the training procedure, by iteratively re-using a single network to keep refining its previous estimate; or (ii) we aim toward a more compact model by substituting multiple network blocks assuming the same basic functionality with only a single block.</p><p>IRR with FlowNet. Addressing the first scenario, we propose an iterative residual refinement version of FlowNetS <ref type="bibr" target="#b13">[14]</ref>, cf . <ref type="figure">Fig. 3</ref> for an overview. Our IRR version iteratively estimates residual flow with multiple iterations using one single FlowNetS; the final result is the sum of residual flows from all iteration steps. We use one shared encoder E for feature extraction from each input image I 1 and I 2 , similar to FlowNetC, and concatenate the two feature maps after warping the second feature map based on the estimated flow f i−1 fw from the previous iteration i − 1. Then we input the concatenated feature maps to the decoder D to estimate the residual (forward) flow at iteration i:</p><formula xml:id="formula_0">f i fw = D E(I 1 ), w E(I 2 ), f i−1 fw + f i−1 fw ,<label>(1)</label></formula><p>where w(·, ·) is a bilinear interpolation function for backward warping <ref type="bibr" target="#b28">[29]</ref>. Here, warping the second feature map is crucial as it yields a suitable input for estimating the appropriate residual flow. This yields much improved accuracy while re-using the same network with only slight modifications and not requiring additional training stages.</p><p>IRR with PWC-Net. Based on the classical coarse-to-fine principle, PWC-Net <ref type="bibr" target="#b51">[52]</ref> and SpyNet <ref type="bibr" target="#b44">[45]</ref> both use multiple repetitive modules for the same purpose but with separate weights. <ref type="figure" target="#fig_2">Fig. 4a</ref> shows a 3-level PWC-Net (for ease of visualization, originally 7-level) that incrementally updates the estimation across the pyramid levels with individual decoders for each level. Adopting our IRR scheme here to address the second scenario, we can substitute the multiple decoders with only one shared decoder that iteratively refines the output over all the pyramid levels, cf . <ref type="figure" target="#fig_2">Fig. 4b</ref>. We set the number of iterations equal to the number of pyramid levels, keeping the original pipeline but with fewer parameters and a more compact representation:</p><formula xml:id="formula_1">f i fw = D P i (I 1 ), c P i (I 1 ), w P i (I 2 ),f i−1 fw ,f i−1 fw +f i−1 fw (2a) withf i−1 fw = 2· ↑ (f i−1 fw ),<label>(2b)</label></formula><p>where P i is the feature map at pyramid level i, c(·, ·) calculates a cost volume, and ↑ performs 2× bilinear upsampling to twice the resolution of the previous flow field. As the dimension increases, we also scale the flow magnitude accordingly (Eq. 2b). One important change from the original PWC-Net <ref type="bibr" target="#b51">[52]</ref>, which estimates flow for each level on the original scale, is that we estimate flow for each level at its native spatial resolution. This enables us to use only one shared decoder and yet make it possible to handle different resolutions across all levels. When calculating the loss, we revert back to the original scale to use the same loss function.</p><p>In addition, we add a 1 × 1 convolution layer after the input feature map P i (I 1 ) to make the number of feature maps input to the decoder D be equal across the pyramid levels. This enables us to use one single shared decoder with a fixed number of input channels across the pyramid.</p><p>Occlusion estimation. It is widely reported that jointly localizing occlusions and estimating optical flow can benefit each other <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60]</ref>. Toward leveraging this in the setting of CNNs, we attach an additional decoder estimating occlusion o i 1 in the first frame at the end of the encoder, in parallel to the flow decoder as shown in <ref type="figure">Fig. 2d</ref>, similar to <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42]</ref>. The occlusion decoder has the same configuration as the flow decoder, but the number of output channels is 1 (instead of 2 for flow). The input to the occlusion decoder is the same as to the flow decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Joint optical flow and occlusion estimation</head><p>Iteratively re-using residual subnetworks and adding occlusion decoders are independent, easily combined together, and adaptable to many types of optical flow base networks. Beyond simply adopting these two concepts, we additionally propose several ideas to improve the accuracy further in a joint estimation setup: (i) bi-directional estimation, (ii) bilateral refinement of flow and occlusion, and (iii) an occlusion upsampling layer.</p><p>Bi-directional estimation. Based on the basic IRR setup for joint flow and occlusion estimation in <ref type="figure">Fig. 2d</ref>, we first perform bi-directional flow and occlusion estimation by simply switching the order of the input feature maps for the decoder <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b57">58]</ref>. This yields backward flow f i bw and occlusion o i 2 in the second frame. Note that bi-directional estimation requires no extra convolutional weights as it again re-uses the same shared decoders. As we shall see, estimating both forward and backward flow together yields at most minor accuracy improvements itself, but we find that exploiting forward-backward consistency is crucial for estimating more accurate occlusions.</p><p>Bilateral refinement of flow and occlusion. Blurry estimates, particularly near motion boundaries, have recently been identified as a main limitation of standard optical flow decoders in CNNs. To address this, bilateral filters or local attention maps <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref> have been proposed as viable solutions. We also adopt this idea in our setup, extend it to refine optical flow and occlusion using bilateral filters, but with weight sharing across all iteration steps.</p><p>Similar to Hui et al. <ref type="bibr" target="#b23">[24]</ref>, we construct learned bilateral filters individualized to each pixel and apply them to each flow component u, v and the occlusion separately:</p><formula xml:id="formula_2">f i fw,u (x, y) = g fw (x, y) * f i fw,u (x, y) (3a) f i fw,v (x, y) = g fw (x, y) * f i fw,v (x, y) (3b) o i 1 (x, y) = g o (x, y) * o i 1 (x, y),<label>(3c)</label></formula><p>where, e.g.,f i fw,u (x, y) is the filtered horizontal flow at (x, y), g fw (x, y) is the w × w learned bilateral filter kernel  for flow at (x, y), and f i fw,u (x, y) is the w × w patch of the horizontal flow centered at (x, y). Note that we construct the kernels for flow and occlusion separately as motion and occlusion boundaries are not necessarily aligned.</p><p>For constructing the bilateral filter for the flow, we follow the strategy of Hui et al. <ref type="bibr" target="#b23">[24]</ref>, and for occlusion we input occlusion estimates, a feature map, and a warped feature map from the other temporal direction. One important difference to <ref type="bibr" target="#b23">[24]</ref> is that we do not need separate learnable convolutional weights for every iteration step or every pyramid level. Our IRR design enables re-using the same weights for constructing the bilateral filters for all iteration steps or pyramid levels. In case of adapting to PWC-Net, our bilateral refinement adds only 0.69M parameters, which is 2.4× less than the scheme of <ref type="bibr" target="#b23">[24]</ref>, adding 1.66M parameters.</p><p>Occlusion upsampling layer. One common trait of FlowNet <ref type="bibr" target="#b13">[14]</ref> and PWC-Net <ref type="bibr" target="#b51">[52]</ref> is that the output resolution of flow from the CNN is a quarter ( 1 4 H × 1 4 W ) of the input resolution (H × W ), which is then bilinearly upscaled to the input resolution. The reasons for not directly estimating at full resolution are a marginal accuracy improvement and the GPU computation and memory overhead.</p><p>Yet for estimating occlusion, there is a significant accuracy loss when estimating only at a quarter resolution. On the Sintel dataset, we conduct an oracle study by downscaling the ground-truth occlusion maps to a quarter size and then upscaling them back. The F-score of the reconstructed occlusion maps was 0.777, suggesting a significant accuracy limitation. As seen in <ref type="figure" target="#fig_4">Fig. 6</ref>, quarter-resolution occlusion maps cannot really represent fine occlusions, through which the major loss in F-score occurs. This strongly emphasizes the importance of estimating at full resolution.</p><p>To estimate more accurate occlusion at higher resolution, we attach an upsampling layer at the end of network, cf . <ref type="figure">Fig. 5</ref>, to upscale optical flow and occlusion together back to the input resolution. <ref type="figure" target="#fig_5">Fig. 7</ref> illustrates our upsampling layer. For optical flow, we found bilinear upsampling to be sufficient. For occlusion, we first perform nearest-neighbor upsampling, which is fed into a CNN module to estimate the residual occlusion on the upsampled occlusion map. The CNN module consists of three residual blocks <ref type="bibr" target="#b34">[35]</ref>, which receive flow, a feature map from the encoder, warped flow, and a warped feature map from the other temporal direction. Putting the warped feature map and flow from the other direction enables exploiting the classical forward-backward consistency for estimating the occlusion. We provide further details in the supplemental material.</p><p>Initialization. To bootstrap our iterative estimation, we input zero as initial optical flow (i.e. f 0 fw and f 0 bw ) and occlusion (i.e. o 0 1 and o 0 2 ) into the first stage. Note that 0 indicates non-occluded (visible) and 1 indicates occluded.</p><p>Training loss. Let N be the total number of steps in our iterative setting. Then we predict a set of forward optical flow maps f i fw , backward optical flow f i bw , occlusion maps in the first image o i 1 and in the second image o i 2 for each iteration step, where i = 1, . . . , N . Forward and backward optical flow are supervised using the L 2,1 norm as</p><formula xml:id="formula_3">l i flow = 1 2 f i fw − f fw,GT 2 + f i bw − f bw,GT 2 ,<label>(4)</label></formula><p>whereas for the supervision of the two occlusion maps we use a weighted binary cross-entropy</p><formula xml:id="formula_4">l i occ = − 1 2 w i 1 o i 1 log o 1,GT +w i 1 (1−o i 1 ) log(1−o 1,GT ) +w i 2 o i 2 log o 2,GT +w i 2 (1−o i 2 ) log(1−o 2,GT ) .<label>(5)</label></formula><p>Here, we apply the weights w i 1 = <ref type="bibr">GT)</ref> to take into account the number of predictions and true labels.</p><formula xml:id="formula_5">H·W o i 1 + o1,GT andw i 1 = H·W (1−o i 1 )+ (1−o1,</formula><p>Our final loss is the weighted sum of the two losses above, taken over all iteration steps using the same multiscale weights α s as in the original papers. In case of FlowNet <ref type="bibr" target="#b13">[14]</ref>, the final loss becomes</p><formula xml:id="formula_6">l FlowNet = 1 N N i=1 S s=s0 α s (l i,s flow + λ · l i,s occ ),<label>(6)</label></formula><p>where s denotes the scale index given in <ref type="figure">Fig. 3</ref> of <ref type="bibr" target="#b13">[14]</ref>. In case of PWC-Net <ref type="bibr" target="#b51">[52]</ref>, the number of scales is equal to the number of iterations, hence the final loss is</p><formula xml:id="formula_7">l PWC-Net = 1 N N i=1 α i (l i flow + λ · l i occ ).<label>(7)</label></formula><p>λ weighs the flow against the occlusion loss. In every iteration, we calculate the λ that makes the loss of the flow and the occlusion be equal. We empirically found that this strategy yields better accuracy than just using a fixed trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">FlyingChairsOcc dataset</head><p>Lacking a suitable dataset, we create our own dataset for the supervision of bi-directional flow and the two occlusion maps, with ground truth for forward flow, backward flow, and occlusion maps at the first and second frame. To build the dataset, we follow the exact protocol of the FlyingChairs dataset <ref type="bibr" target="#b13">[14]</ref>. We refer to this dataset as FlyingChairsOcc.</p><p>We crawl 964 background images with a resolution of 1024 × 768 from Flickr and Google using the keywords cityscape, street, and mountain. As foreground objects, we use 809 chair images rendered from CAD models with varying views and angles <ref type="bibr" target="#b2">[3]</ref>. Then we follow the exact protocol of <ref type="bibr" target="#b13">[14]</ref> for generating image pairs, including the number of foreground objects, object size, and random parameters for generating the motion of each object. As the motion is parametrized by a 3 × 3 matrix, it is easy to calculate not only backward ground-truth flow but also occlusion maps by conducting visibility checks. The number of images in the training and validation sets are the same as in FlyingChairs (i.e. 22232 and 640, respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>Training details. We follow the training settings of FlowNet respective PWC-Net for a fair comparison. We use the same geometric and photometric augmentations with additive Gaussian noise as described in <ref type="bibr" target="#b25">[26]</ref>. After applying the geometric augmentation on the occlusion ground truth, we additionally check for pixels moving outside of the image boundary (i.e. out-of-bound pixels) and set them as occluded. Note that no multi-stage training is needed.</p><p>We first train the proposed model on our FlyingChairs-Occ dataset with learning rate schedule S short (instead of S long ), described in <ref type="bibr" target="#b25">[26]</ref>. Next, we fine-tune on the Flying-Things3D-subset dataset <ref type="bibr" target="#b38">[39]</ref>, which contains much larger displacements; we use half the S fine learning rate schedule <ref type="bibr" target="#b25">[26]</ref>. We empirically found that using shorter schedules was enough as our model converged faster. We finally finetune on different public benchmark datasets, including Sintel <ref type="bibr" target="#b9">[10]</ref> and KITTI <ref type="bibr" target="#b16">[17]</ref>, following the fine-tuning protocol of <ref type="bibr" target="#b52">[53]</ref>. We use a smaller minibatch size of 4, as our model implicitly increases the batch size by performing iterative bi-directional estimation with a single model.</p><p>Lacking other ground truth, we only use the forward flow and the occlusion map for the first frame for supervision on Sintel; for KITTI we only use the forward flow. Importantly, our model is still trainable when ground truth is available only for one direction (e.g., forward flow with occlusion map at the first frame), since both temporal directions share the same "unidirectional" decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>To see the effectiveness of each proposed component, we conduct an ablation study by training our model in multiple settings. All models are trained on the FlyingChair-sOcc dataset with the S short schedule and tested on multiple datasets to assess generalization across datasets. We use a minibatch size of 4 when either bi-directional estimation or iterative residual refinement is on, or the original minibatch size of 8, otherwise. For a simpler ablation study, we use two iteration steps when applying IRR on FlowNet <ref type="bibr" target="#b13">[14]</ref>. <ref type="table" target="#tab_0">Table 1</ref> assesses the optical flow in terms of the average end-point error (EPE) and occlusion estimation with the average F1-score, if applicable for the respective configuration. In contrast to findings in recent work <ref type="bibr" target="#b26">[27]</ref>, estimating occlusion together yields a gradual improvement of the flow of up to 5% on the training domain, and an even bigger im-  provement across different datasets when combined on top of bi-directional estimation (Bi) or IRR. We believe this to mainly stem from using a separate occlusion decoder instead of a joint decoder <ref type="bibr" target="#b26">[27]</ref>. Bi-directional estimation by itself yields at most a marginal improvement on flow, but it is important for the input of the occlusion upsampling layer, which brings very large benefits on occlusion estimation. Iterative residual refinement yields consistent improvements in flow accuracy on the training domain, and perhaps surprisingly a much better generalization across datasets, with up to 10% improvement in EPE. We presume that this better generalization comes from training a single decoder to handle feature maps from all iteration steps or pyramid levels, which encourages generalization even across datasets.  <ref type="table">Table 3</ref>. Comparison of our occlusion upsampling layer and the refinement network from FlowNet2 <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>layer significantly improve the accuracy of both flow and occlusion with a small overhead of only 0.83M parameters. For PWC-Net, we obtain a significant accuracy boost of 17.7% on average over the baseline, while reducing the number of parameters by 26.4%. We name the full versions of the models including all modules IRR-FlowNet and IRR-PWC. <ref type="figure" target="#fig_6">Fig. 8</ref> highlights the improvement of the flow from our proposed components with qualitative examples. Please note the completeness and sharp boundaries.</p><p>Bilateral refinement. We compare our bilateral refinement layer with the refinement layer of LiteFlowNet <ref type="bibr" target="#b23">[24]</ref> based on a PWC-Net with Bi, Occ, and IRR components enabled. <ref type="table">Table 2</ref> shows that the benefit of our design choice (i.e. sharing weights) holds for bilateral refinement as well, yielding better accuracy for flow and particularly for occlusion, with 2.5× fewer parameters than that of <ref type="bibr" target="#b23">[24]</ref>.</p><p>Occlusion upsampling layer. Similar to our upsampling layer, <ref type="bibr" target="#b26">[27]</ref> uses a refinement network from FlowNet2 <ref type="bibr" target="#b25">[26]</ref> to upsample the intermediate quarter-resolution outcome back to the original resolution. We compare our upsampling layer with the refinement network from <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, adding it to our network based on a PWC-Net backbone with Bi, Occ, IRR, and the bilateral refinement layer enabled. <ref type="table">Table 3</ref> shows the clear benefits of using our upsampling layer, yielding significant gains in both tasks while requiring fewer parameters. The refinement network from FlowNet2 <ref type="bibr" target="#b25">[26]</ref> actually degrades the accuracy of flow estimation. We presume this may stem from differences in   <ref type="table">Table 5</ref>. MPI Sintel Flow: Average end-point error (EPE) and number of CNN parameters. § using more than 2 frames, † using additional datasets (KITTI and HD1k) for better accuracy.</p><p>training. FlowNet2's refinement layer may require piecewise training, while our model is trained all at once.</p><p>Different IRR steps on FlowNet. For FlowNet, we can freely choose the number of IRR steps as we iteratively refine previous estimates by re-using a single network. We try different numbers of IRR steps on vanilla FlowNetS <ref type="bibr" target="#b13">[14]</ref> (i.e. without Bi or Occ) and compare with stacking multiple FlowNetS networks. All networks are trained on Fly-ingChairsOcc with the S short schedule, minibatch size of 8, and tested on Sintel Clean. As shown in <ref type="table" target="#tab_3">Table 4</ref>, the accuracy keeps improving with more IRR steps and stably settles at more than 4 steps. In contrast, stacking multiple FlowNetS networks overfits on the training data after 3 steps, and is consistently outperformed by IRR with the same number of stages. This clearly demonstrates the advantage of our IRR scheme over stacking: better accuracy without linearly increasing the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Optical flow benchmarks</head><p>We test the accuracy of our IRR-PWC on the public Sintel <ref type="bibr" target="#b9">[10]</ref> and KITTI <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41]</ref> benchmarks. When fine-tuning, we use the robust training loss as in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>   <ref type="bibr" target="#b52">[53]</ref> (1.45) (7.59%) 7.72% LiteFlowNet <ref type="bibr" target="#b23">[24]</ref> (1.62) (5.58%) 9.38% PWC-Net <ref type="bibr" target="#b51">[52]</ref> (2.16) (9.80%) 9.60% ContinualFlow ROB † § <ref type="bibr" target="#b41">[42]</ref> --10.03% MirrorFlow <ref type="bibr" target="#b24">[25]</ref> -9.98% 10.29% FlowNet2 <ref type="bibr" target="#b25">[26]</ref> (2.30) (8.61%) 10.41% vations are that our model (i) converges much faster than the baseline and (ii) overfits to the training split less, demonstrating much better accuracy on the test set despite slightly higher error on training split. This highlights the benefit of our IRR scheme: better generalization even on the training domain as well as across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Occlusion estimation</head><p>We finally evaluate the accuracy of occlusion estimation on the Sintel training set as no public benchmarks are available for the task. <ref type="table">Table 7</ref> shows the comparison with stateof-the-art algorithms. Supervised methods are trained on FlyingChairs and FlyingThings3D; unsupervised methods are trained on Sintel without the use of ground truth. We achieve state-of-the-art accuracy with far fewer parameters (6.00M instead of 110M) and much simpler training schedules than the previous state of the art <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed an iterative residual refinement (IRR) scheme based on weight sharing for generic optical flow networks, with additional components for bi-directional estimation and occlusion estimation. Applying our scheme on top of two representative flow networks, FlowNet and PWC-Net, significantly improves flow accuracy with a better generalization while even reducing the number of parameters in case of PWC-Net. We also show that our design choice of jointly estimating occlusion together with flow brings accuracy improvements on both domains, setting the state of the art on public benchmark datasets. We believe that our powerful IRR scheme can be combined with other baseline networks and can form the basis of other follow-up approaches, including multi-frame methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Junhwa Hur</head><p>Stefan Roth Department of Computer Science, TU Darmstadt Here, we provide additional details on IRR-PWC, our occlusion upsampling layer, more qualitative examples on the ablation study, as well as a qualitative comparison with the state of the art.</p><p>A. IRR-PWC <ref type="figure" target="#fig_8">Fig. 9</ref> shows our IRR-PWC model that jointly estimates optical flow and occlusion using bi-directional estimation, bilateral refinement, and the occlusion upsampling layer. Given a 7-level feature pyramid as in the original PWC-Net <ref type="bibr" target="#b51">[52]</ref>, our IRR-PWC first iteratively and residually estimates optical flow and occlusion up to a quarter resolution of the input image, as shown in <ref type="figure" target="#fig_8">Fig. 9a</ref>. Then, given the estimates at the 5 th level, we show how we use our occlusion upsampling layer in <ref type="figure" target="#fig_8">Fig. 9b</ref> to scale the estimates up to the original resolution. The upsampling layer upscales the resolution by 2× at once, and applying the upsampling layer at the 6 th and 7 th level scales the quarter resolution estimate back to the original resolution. <ref type="figure" target="#fig_5">Fig. 7</ref> in the main paper shows the detailed structure of the upsampling layer. In the following, we describe the details on the residual blocks in the upsampling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details on the Occlusion Upsampling Layer</head><p>In the occlusion upsampling layer shown in <ref type="figure" target="#fig_5">Fig. 7</ref> in the main paper, the residual blocks <ref type="bibr" target="#b34">[35]</ref> are fed a set of feature maps as input and output residual occlusion estimates to refine the upscaled occlusion map from the previous level. <ref type="figure" target="#fig_10">Fig. 10</ref> shows the details of the residual blocks. As shown in <ref type="figure" target="#fig_10">Fig. 10a</ref>, the subnetwork consists of 3 residual blocks (i.e. 3 ResBlocks) with 3 convolution layers. One ResBlock consists of Conv+ReLu+Conv+Mult operations as shown in <ref type="figure" target="#fig_10">Fig. 10b</ref>, cf . <ref type="bibr" target="#b34">[35]</ref>. This sequence of 3 ResBlocks with one convolution layer afterwards estimates the residuals over one convolution output of the input feature maps, and the final convolution layer of the residual blocks outputs the residual occlusion. The number of channels for all convolution layers here is 32, except for the final convolution layer, which has only 1 channel for the occlusion output.</p><p>We use weight sharing also on the upsampling lay-  ers between bi-directional estimations and between pyramid levels or iteration steps. Furthermore, the ResBlocks in <ref type="figure" target="#fig_10">Fig. 10a</ref> also share their weights, which is different from <ref type="bibr" target="#b34">[35]</ref>, where they are not shared. With this efficient weight-sharing scheme, the occlusion upsampling layer im-  proves the occlusion accuracy by 2.99% on the training domain (i.e. the FlyingChairsOcc dataset) and 4.08% across datasets (i.e. Sintel) with only adding 0.031 M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Qualitative Examples</head><p>Occlusion upsampling layer. <ref type="figure">Fig. 11</ref> provides qualitative examples of occlusion estimation and demonstrates the advantage of using the occlusion upsampling layer. The models used here are trained on the FlyingChairsOcc dataset only (no fine-tuning on the FlyingThings3D-subset dataset or Sintel) and tested on Sintel Train Clean. The occlusion upsampling layer enhances the occlusion estimates to be much sharper along motion boundaries and refines coarse estimates. Also, the upsampling layer further detects thinlyshaped occlusions that were not detected at the quarter resolution. Unlike optical flow, where a quarter resolution estimate is largely sufficient, we can see from these qualitative examples that estimating occlusions up to the original resolution is very critical for yielding high accuracy.</p><p>Ablation study on PWC-Net. In addition to <ref type="figure" target="#fig_6">Fig. 8</ref> in the main paper, we here give more qualitative examples for the ablation study. In <ref type="figure">Fig. 12</ref>, all models are also trained on the FlyingChairsOcc dataset and tested on Sintel Train Clean. Our proposed schemes significantly improve the accuracy over the baseline model (i.e. PWC-Net <ref type="bibr" target="#b51">[52]</ref>), yielding better generalization across datasets. <ref type="figure">Figure 13</ref> demonstrates a qualitative comparison with the state of the art on occlusion estimation. Qualitatively, MirrorFlow <ref type="bibr" target="#b24">[25]</ref> misses many occlusions in general, and FlowNet-CSSR-ft-sd <ref type="bibr" target="#b26">[27]</ref> is able to detect fine details of occlusion. In contrast, our method tries not to miss occlusions, which results in a better recall rate but somewhat lower precision than those of FlowNet-CSSR-ft-sd <ref type="bibr" target="#b26">[27]</ref>. Overall, our method demonstrates better F1-score than FlowNet-CSSR-ft-sd <ref type="bibr" target="#b26">[27]</ref>, achieving state-of-the-art results on the evaluation dataset (i.e. Sintel Train Clean and Final). Note that FlowNet-CSSR-ft-sd <ref type="bibr" target="#b26">[27]</ref> is additionally trained on the ChairsSDHom dataset <ref type="bibr" target="#b25">[26]</ref> for handling small-displacement motion, which is related to thinlyshaped occlusions. Our approach is not trained further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Occlusion estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Bi-directional flows and occlusion maps</head><p>MirrorFlow <ref type="bibr" target="#b24">[25]</ref> is one of the most recent related works that estimates bi-directional flow and occlusion maps. <ref type="figure" target="#fig_2">Fig. 14</ref>   <ref type="bibr" target="#b24">[25]</ref>, our model demonstrates far fewer artifacts and fewer missing details for both flow and occlusion estimation. Although there is no ground truth for backward flow nor an occlusion map for the second image available for supervision, our bi-directional model is able to estimate the backward flow and the second occlusion map well while only using the ground truth of forward flow and the occlusion map for the first image (latter is only available on Sintel) during fine-tuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Our IRR version of PWC-Net, which uses only one single shared decoder over the pyramid levels, see text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 Figure 5 .</head><label>25</label><figDesc>Joint optical flow and occlusion estimation: bi-directional estimation, bilateral refinement, and upsampling layer (in the FlowNet setting): We estimate flow in both temporal directions and occlusion maps in both frames by switching the order of inputs to the decoder. Bilateral refinement and the upsampling layer further improve the accuracy of flow and occlusion. Modules with the same color share their weights. C.f . supplemental material for the corresponding PWC-Net variant. (a) Ground-truth occlusion map. (b) Reconstructed occlusion map from a quarter resolution of (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Oracle study showing the limitation of outputting lowresolution occlusion maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Occlusion upsampling layer: Inputs are bi-linearly upsampled flow and upsampled occlusion using nearest neighbor. Residual blocks then improve the occlusion accuracy using residual occlusion updates at the full output resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative examples from the ablation study on PWC-Net: (left to right) overlapped input images, ground-truth flow, the original PWC-Net<ref type="bibr" target="#b51">[52]</ref>, our PWC-Net with IRR, our PWC-Net with Bi-Occ-IRR, and our full model (i.e. IRR-PWC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Jointly estimating optical flow and occlusion up to a quarter resolution of the original input, i.e. pyramid levels 1 ≤ i ≤ 5. Upsampling optical flow and occlusion using the upsampling layer, i.e. pyramid levels 6 ≤ i ≤ 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>IRR-PWC: Our PWC-Net variant with joint optical flow and occlusion estimation based on bi-directional estimation, bilateral refinement, and the occlusion upsampling layer. (a) Our IRR-PWC model jointly estimates optical flow and occlusion up to a quarter resolution of the input image (i.e. up to the 5 th level), the same as the original PWC-Net. (b) Then, we use our occlusion upsampling layer to upscale the outputs back to the original resolution while improving accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Residual blocks in the upsampling layer: (a)The residual blocks consist of 3 weight-shared ResBlocks with 3 convolution layers.(b) One ResBlock consists of Conv+ReLu+Conv+Mult operations<ref type="bibr" target="#b34">[35]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation study of our design choices on the two baseline models. The numbers indicate the average end-point error (EPE) for optical flow (the lower the better) and the average F1score for occlusion in parentheses, where available (the higher the better). Bi: Bi-directional estimation, Occ: Joint occlusion estimation, IRR: Iterative residual refinement, IRR+: Iterative residual refinement including bilateral refinement and occlusion upsampling layer. The final column reports the relative changes on the number of parameters comparing to the vanilla baseline.</figDesc><table><row><cell>Bi</cell><cell>Occ</cell><cell>IRR</cell><cell cols="2">Chairs ChairsOcc Full Validation</cell><cell>Sintel Clean Sintel Final Rel. Training Training Param.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.39</cell><cell>2.27</cell><cell>4.35</cell><cell>5.44</cell><cell>0 %</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.43</cell><cell>2.30</cell><cell>4.40</cell><cell>5.53</cell><cell>0 %</cell></row><row><cell>FlowNet [14]</cell><cell></cell><cell></cell><cell>2.29 2.36 2.31 2.14 2.22</cell><cell cols="2">2.18 (0.690) 4.26 (0.521) 5.51 (0.493) +38.5% 2.22 3.77 5.00 0 % 2.20 (0.691) 4.21 (0.515) 5.46 (0.488) +38.5% 2.00 3.45 4.96 0 % 2.10 (0.689) 3.56 (0.507) 5.03 (0.486) +38.5%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.05</cell><cell cols="2">1.91 (0.699) 3.40 (0.528) 5.08 (0.502) +38.5%</cell></row><row><cell></cell><cell></cell><cell cols="2">+ 1.92</cell><cell cols="2">1.77 (0.736) 3.32 (0.596) 4.92 (0.560) +40.7%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.03</cell><cell>1.89</cell><cell>3.13</cell><cell>4.41</cell><cell>0 %</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.06</cell><cell>1.87</cell><cell>2.98</cell><cell>4.14</cell><cell>0 %</cell></row><row><cell>PWC-Net [52]</cell><cell></cell><cell></cell><cell>1.94 2.01 1.99 2.08 1.91</cell><cell cols="2">1.79 (0.706) 3.16 (0.616) 4.35 (0.581) +87.4% 1.83 2.79 4.10 −61.2% 1.82 (0.696) 3.01 (0.618) 4.39 (0.581) +87.4% 1.90 2.80 4.13 −61.2% 1.73 (0.700) 2.64 (0.630) 4.09 (0.593) −34.7%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1.98</cell><cell cols="2">1.81 (0.698) 2.69 (0.633) 4.03 (0.598) −34.7%</cell></row><row><cell></cell><cell></cell><cell cols="2">+ 1.67</cell><cell cols="2">1.48 (0.757) 2.34 (0.677) 3.95 (0.624) −26.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>n× IRR vs. n× stacking: EPE on Sintel Clean.</figDesc><table><row><cell>Method</cell><cell cols="2">Training</cell><cell>Test</cell><cell></cell><cell>Parameters</cell></row><row><cell></cell><cell>Clean</cell><cell>Final</cell><cell>Clean</cell><cell>Final</cell><cell></cell></row><row><cell>ContinualFlow ROB  † § [42]</cell><cell>-</cell><cell>-</cell><cell>3.34</cell><cell>4.53</cell><cell>14.6 M</cell></row><row><cell>MFF  § [46]</cell><cell>-</cell><cell>-</cell><cell>3.42</cell><cell>4.57</cell><cell>N/A</cell></row><row><cell>IRR-PWC (Ours)</cell><cell>(1.92)</cell><cell>(2.51)</cell><cell>3.84</cell><cell>4.58</cell><cell>6.36M</cell></row><row><cell>PWC-Net+  † [53]</cell><cell>(1.71)</cell><cell>(2.34)</cell><cell>3.45</cell><cell>4.60</cell><cell>8.75M</cell></row><row><cell>ProFlow  § [37]</cell><cell>-</cell><cell>-</cell><cell>2.82</cell><cell>5.02</cell><cell>-</cell></row><row><cell>PWC-Net-ft-final [53]</cell><cell>(2.02)</cell><cell>(2.08)</cell><cell>4.39</cell><cell>5.04</cell><cell>8.75M</cell></row><row><cell>DCFlow [61]</cell><cell>-</cell><cell>-</cell><cell>3.54</cell><cell>5.12</cell><cell>-</cell></row><row><cell>FlowFieldsCNN [6]</cell><cell>-</cell><cell>-</cell><cell>3.78</cell><cell>5.36</cell><cell>5.00M</cell></row><row><cell>MR-Flow [59]</cell><cell>1.83</cell><cell>3.59</cell><cell>2.53</cell><cell>5.38</cell><cell>-</cell></row><row><cell>LiteFlowNet [24]</cell><cell>(1.35)</cell><cell>(1.78)</cell><cell>4.54</cell><cell>5.38</cell><cell>5.37M</cell></row><row><cell>S2F-IF [62]</cell><cell>-</cell><cell>-</cell><cell>3.50</cell><cell>5.42</cell><cell>-</cell></row><row><cell>SfM-PM [38]</cell><cell>-</cell><cell>-</cell><cell>2.91</cell><cell>5.47</cell><cell>-</cell></row><row><cell>FlowFields++ [49]</cell><cell>-</cell><cell>-</cell><cell>2.94</cell><cell>5.49</cell><cell>-</cell></row><row><cell>FlowNet2 [26]</cell><cell>(2.02)</cell><cell>(3.14)</cell><cell>3.96</cell><cell>6.02</cell><cell>162.5 M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>for flow, and standard binary cross-entropy for occlusion. On Sintel Final, our IRR-PWC achieves a new state of the art among 2-frame methods. Comparing to the PWC-Net baseline (i.e. PWC-Net-ft-final) trained in the identical setting, our contributions improve the flow accuracy by 9.18% on Final and 12.36% on Clean, while using 26.4% fewer parameters. On KITTI 2015, our IRR-PWC again outperforms all published 2-frame methods, improving over the baseline PWC-Net.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Training</cell><cell>Test</cell></row><row><cell></cell><cell>AEPE</cell><cell>Fl-all</cell><cell>Fl-All</cell></row><row><cell>MFF  § [46]</cell><cell>-</cell><cell>-</cell><cell>7.17%</cell></row><row><cell>IRR-PWC (Ours)</cell><cell>(1.63)</cell><cell>(5.32%)</cell><cell>7.65%</cell></row><row><cell>PWC-Net+</cell><cell></cell><cell></cell><cell></cell></row><row><cell>When fine-tuning on benchmarks, our important obser-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>KITTI Optical Flow 2015: Average end-point error (EPE) and outlier rates (Fl-Noc and Fl-all).</figDesc><table><row><cell>Method</cell><cell>Type</cell><cell cols="2">Sintel Training</cell></row><row><cell></cell><cell></cell><cell>Clean</cell><cell>Final</cell></row><row><cell>IRR-PWC (Ours)</cell><cell>supervised</cell><cell>0.712</cell><cell>0.669</cell></row><row><cell>FlowNet-CSSR [27]</cell><cell>supervised</cell><cell>0.703</cell><cell>0.654</cell></row><row><cell>OccAwareFlow [58]</cell><cell>unsupervised</cell><cell>0.54</cell><cell>0.48</cell></row><row><cell>Back2FutureFlow [30]</cell><cell>unsupervised</cell><cell>0.49</cell><cell>0.44</cell></row><row><cell>MirrorFlow [25]</cell><cell>estimated</cell><cell>0.390</cell><cell>-</cell></row><row><cell cols="3">Table 7. Occlusion estimation results on Sintel Training.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>provides a qualitative comparison with MirrorFlow [25] on the Sintel and KITTI 2015 datasets. In this comparison, we use our model fine-tuned on the training set of each dataset and display qualitative examples from the validation split. Comparing to MirrorFlow</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv Conv</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 11</ref><p>. Qualitative examples of using the occlusion upsampling layer: (a) overlapped input images, (b) occlusion ground truth, (c) without using the occlusion upsampling layer, and (d) with using the occlusion upsampling layer. The occlusion upsampling layer makes occlusion estimates much sharper along motion boundaries and detects additional thinly-shaped occlusions.   <ref type="bibr" target="#b26">[27]</ref>, and (e) ours. In the result image of each method, blue pixels denote false positives, red pixels denote false negatives, and white ones denote true positives (i.e. correctly estimated occlusion). We include the Fscore of each method in the top-right corner. Our model yields a better F-score on the second and the third scene than FlowNet-CSSR-ft-sd <ref type="bibr" target="#b26">[27]</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised convolutional neural networks for motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1629" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Symmetrical dense optical flow estimation with occlusions detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachid</forename><surname>Luisálvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théodore</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">Sánchez</forename><surname>Papadopoulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Seeing 3D chairs: Exemplar partbased 2D-3D alignment using a large dataset of CAD models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3762" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting semantic information and deep matching for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="154" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Flow Fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4015" to="4023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CNNbased patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2710" to="2719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vanel Lazcano, and Vicent Caselles. A TV-L1 optical flow method with occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coloma</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Garrido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Und</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coherent online video style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1114" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Full Flow: Optical flow estimation by global optimization over regular grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4706" to="4714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SegFlow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="686" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic video CNNs through representation warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4463" to="4472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PatchBatch: A batch augmented loss for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gadot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4236" to="4245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detect, replace, refine: Deep structured prediction for pixel wise labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7187" to="7196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="158" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep discrete flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="207" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Segmentation-aware convolutional networks using local attention masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5048" to="5057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust interpolation of correspondences for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4791" to="4799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient coarseto-fine PatchMatch for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5704" to="5712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lite-FlowNet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8981" to="8989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MirrorFlow: Exploiting symmetries in joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Occlusion-aware optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serdar</forename><surname>Ince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janusz</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2015</title>
		<imprint>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="713" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for optical flow with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2017</title>
		<imprint>
			<biblScope unit="page" from="354" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SPM-BP: Sped-up PatchMatch belief propagation for continuous MRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4006" to="4014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast guided global interpolation for depth and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="717" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2811" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ProFlow: Learning to predict optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Bruhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structure-from-Motion-Aware PatchMatch for adaptive optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Marniok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Goldluecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Bruhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="575" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Continual occlusions and optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Neoral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janšochman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic video segmentation by gated recurrent flow propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6819" to="6828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Sj</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="878" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A fusion approach for multi-frame optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2077" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1495" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaïd</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">FlowFields++: Accurate optical flow correspondences meet robust interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wasenmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1463" to="1467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Local layering for joint motion estimation and occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A quantitative analysis of current practices in optical flow estimation and the principles behind them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Models matter, so does training: An empirical study of CNNs for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>to appear. 6, 8</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Symmetric stereo matching for occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2014</title>
		<imprint>
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">DeMoN: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5622" to="5631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joint motion estimation and segmentation of complex scenes with label costs and occlusion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4884" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Optical flow in mostly rigid scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6911" to="6920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bilateral filtering-based optical flow estimation with occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjian</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harpreet</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cen</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Isnardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5807" to="5815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">S2F: Slow-to-fast interpolator flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3767" to="3776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Derpanis. Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4141" to="4150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Guided optical flow learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">DenseNet for dense flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><forename type="middle">D</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="790" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">InterpoNet, a brain inspired neural network for optical flow dense interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6363" to="6372" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
