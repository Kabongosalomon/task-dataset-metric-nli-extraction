<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 3-7, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Li</surname></persName>
							<email>lizekunlee@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
							<email>zeyu.cui@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
							<email>shu.wu@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
							<email>zhangxiaoyu@iie.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<email>wangliang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation, Chinese Academy of Sciences University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation and Artificial Intelligence Research, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Institute of Information Engineering, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institute of Automation, Chinese Academy of Sciences University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction</title>
					</analytic>
					<monogr>
						<title level="m">CIKM &apos;19</title>
						<meeting> <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">November 3-7, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3357384.3357951</idno>
					<note>The first two authors Zekun Li and Zeyu Cui contribute to this work equally. Shu Wu and Xiaoyu Zhang are both corresponding authors. ACM ISBN 978-1-4503-6976-3/19/11. . . $15.00 ACM Reference Format:. 2019. Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction. In The 28th ACM International Conference on Information and Knowledge Management (CIKM &apos;19), November 3-7, 2019, Beijing, China. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3357384.3357951</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Information systems → Recommender systems</term>
					<term>Business in- telligence</term>
					<term>• Applied computing → Online shopping KEYWORDS Feature interactions, Graph neural networks, CTR prediction, Rec- ommender system</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-through rate (CTR) prediction is an essential task in web applications such as online advertising and recommender systems, whose features are usually in multi-field form. The key of this task is to model feature interactions among different feature fields. Recently proposed deep learning based models follow a general paradigm: raw sparse input multi-filed features are first mapped into dense field embedding vectors, and then simply concatenated together to feed into deep neural networks (DNN) or other specifically designed networks to learn high-order feature interactions. However, the simple unstructured combination of feature fields will inevitably limit the capability to model sophisticated interactions among different fields in a sufficiently flexible and explicit fashion.</p><p>In this work, we propose to represent the multi-field features in a graph structure intuitively, where each node corresponds to a feature field and different fields can interact through edges. The task of modeling feature interactions can be thus converted to modeling node interactions on the corresponding graph. To this end, we design a novel model Feature Interaction Graph Neural Networks (Fi-GNN). Taking advantage of the strong representative power of graphs, our proposed model can not only model sophisticated feature interactions in a flexible and explicit fashion, but also provide good model explanations for CTR prediction. Experimental results on two real-world datasets show its superiority over the state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The goal of click-through rate prediction is to predict the probabilities of users clicking ads or items, which is critical to many web applications such as online advertising and recommender systems. Modeling sophisticated feature interactions plays a central role in the success of CTR prediction. Distinct from continuous features which can be naturally found in images and audios, the features for web applications are mostly in multi-field categorical form. For example, the four-fields categorical features for movies may be: (1) Language = {English, Chinese, Japanese, ... }, (2) Genre = {action, fiction, ... }, (3) Director = {Ang Lee, Christopher Nolan, ... }, and (4) Starring = {Bruce Lee, Leonardo DiCaprio, ... } (noted that there are much more feature fields in real applications). These multi-field categorical features are usually converted to sparse one-hot encoding vectors, and then embedded to dense real-value vectors, which can be used to model feature interactions.</p><p>Factorization machine (FM) <ref type="bibr" target="#b23">[23]</ref> is a well-known model proposed to learn second-order feature interactions from vector inner products. Field-aware factorization machine (FFM) <ref type="bibr" target="#b9">[9]</ref> further considers the field information and introduces field-aware embedding. Regrettably, these FM-based models can only model second-order interaction and the linearity modeling limits its representative power. Recently, many deep learning based models have been proposed to learn high-order feature interactions, which follow a general paradigm: simply concatenate the field embedding vectors together and feed them into DNN or other specifically designed models to learn interactions. For example, Factorisation-machine supported Neural Networks (FNN) <ref type="bibr" target="#b35">[35]</ref>, Neural Factorization Machine (NFM) <ref type="bibr" target="#b8">[8]</ref>, Wide&amp;Deep <ref type="bibr" target="#b1">[2]</ref> and DeepFM <ref type="bibr" target="#b6">[6]</ref> utilize DNN to model interactions. However, these model based on DNN learn high-order feature interactions in a bit-wise, implicit fashion, which lacks good model explanations. Some models try to learn high order interactions explicitly by introducing specifically designed networks. For example, Deep&amp;Cross <ref type="bibr" target="#b31">[31]</ref> introduces Cross Network (CrossNet) and xDeepFM <ref type="bibr" target="#b15">[15]</ref> introduces Compressed Interaction Network (CIN). Nevertheless, we argue that they are still not sufficiently effective and explicit, since they still follow the general paradigm of combining feature fields together to model their interactions. The simple unstructured combination will inevitably limit the capability to model sophisticated interactions among different feature fields in a flexible and explicit fashion.</p><p>In this work, we take the structure of multi-field features into consideration. Specifically, we represent the multi-field features in a graph structure named feature graph. Intuitively, each node in the graph corresponds to a feature field and different fields can interact through edges. The task of modeling sophisticated interactions among feature fields can be thus converted to modeling node interactions on the feature graph. To this end, we design a novel model Feature interaction Graph Neural Networks (Fi-GNN) based on Graph Neural Networks (GNN), which is able to model sophisticated node (feature) interactions in a flexible and explicit fashion. In Fi-GNN, the nodes will interact by communicating the node states with neighbors and update themselves in a recurrent fashion. At every time step, the model interact with neighbors at one hop deeper. Therefore, the number of interaction steps equals to the order of feature interactions. Moreover, the edge weights reflecting importances of different feature interactions and node weights reflecting importances of each feature field on the final CTR prediction can be learnt by Fi-GNN, which can provide good explanations. Overall, our proposed model can model sophisticated feature interactions in an explicit, flexible fashion and also provide good model explanations.</p><p>Our contributions can be summarized in threefold:</p><p>• We point out the limitation of the existing works which consider multi-field features as an unstructured combination of feature fields. To this end, we propose to represent the multi-field features in a graph structure for the first time. • We design a novel model Feature Interaction Graph Neural Networks (Fi-GNN) to model sophisticated interactions among feature fields on the graph-structured features in a more flexible and explicit fashion. • Extensive experiments on two real-world datasets show that our proposed method can not only outperform the state-ofthe-arts but also provide good model explanations.</p><p>The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 provides an elaborative description of our proposed method. The extensive experiments and detailed analysis are presented in Section 4, followed by the conclusion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we briefly review the existing models that model feature interactions for CTR prediction and graph neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Interaction in CTR Prediction</head><p>Modeling feature interactions is the key to success of CTR prediction and therefore extensively studied in the literature. LR is a linear approach, which can only models the first-order interaction on the linear combination of raw individual features. FM <ref type="bibr" target="#b23">[23]</ref> learns second-order feature interactions from vector inner products. Afterwards, different variants of FM have been proposed. Field-aware factorization machine (FFM) <ref type="bibr" target="#b9">[9]</ref> considers the field information and introduces field-aware embedding. AFM <ref type="bibr" target="#b34">[34]</ref> considers the weight of different second-order feature interactions. However, these approaches can only model second-order interaction which is not sufficient.</p><p>With the success of DNN in various fields, researchers start to use it to learn high-order feature interactions due to its deeper structures and nonlinear activation functions. The general paradigm is to concatenate the field embedding vectors together and feed them into DNN to learn the high-order feature interactions. <ref type="bibr" target="#b16">[16]</ref> utilizes convolutional networks to model feature interactions. Factorisation-machine supported Neural Networks (FNNs) <ref type="bibr" target="#b35">[35]</ref> uses the pre-trained factorization machines for field embedding before applying DNN. Product-based Neural Network (PNN) <ref type="bibr" target="#b21">[21]</ref> models both second-order and high-order interactions by introducing a product layer between field embedding layer and DNN layer. Similarly, Neural Factorization Machine (NFM) <ref type="bibr" target="#b8">[8]</ref> has a Bi-Interaction Pooling layer between embedding layer and DNN layer to model second-order interactions, but the followed operation is summation instead of concatenation as in PNN. Some works on another line try to model the second-order and high-order interactions jointly via a hybrid architectures. The Wide&amp;Deep <ref type="bibr" target="#b1">[2]</ref> and DeepFM <ref type="bibr" target="#b6">[6]</ref> contain a wide part to model the low-order interaction and a deep part to model the high-order interaction. However, all these approaches leveraging DNN learn the high-order feature interactions in an implicit, bit-wise way and therefore lack good model explainability. Recently, some work try to learn feature interactions in an explicit fashion via specifically designed networks. Deep&amp;Cross <ref type="bibr" target="#b31">[31]</ref> introduces a CrossNet which takes outer product of features at the bit level. On the contrary, xDeepFM <ref type="bibr" target="#b15">[15]</ref> introduces a CIN to take outer product at the vector level. Nevertheless, they still don't solve the most fundamental problem, that is to concatenate the field embedding vectors together. The simple unstructured combination of feature fields will inevitably limit the capability to model sophisticated interactions among different fields in a flexible and explicit fashion. To this end, we proposed to represent the multi-field features in a graph structure, where each node represents a field and different feature fields can interact through the edges. Accordingly, we can model the flexible interactions among different feature fields on the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>Graph is a kind of data structure which models a set of objects (nodes) and their relationships (edges). Recently, researches of analyzing graphs with machine learning have been receiving more  <ref type="figure">Figure 1</ref>: Overview of our proposed method. The input raw multi-field feature vector is first converted to field embedding vectors via an embedding layer and represented as a feature graph, which is then feed into Fi-GNN to model feature interactions. An attention layer is applied on the output of Fi-GNN to predict the click through rateŷ. Details of embedding layer and Fi-GNN are illustrated in <ref type="figure">Figure 2</ref> and <ref type="figure" target="#fig_2">Figure 3</ref> respectively. and more attention because of the great representative power of graphs. Early works usually convert graph-structured data into sequence-structured data to deal with. Inspired by word2vec <ref type="bibr" target="#b18">[18]</ref>, the work <ref type="bibr" target="#b19">[19]</ref> proposed an unsupervised DeepWalk algorithm to learn node embedding in graph based on random walks. After that, <ref type="bibr" target="#b27">[27]</ref> proposed a network embedding algorithm LINE, which preserve the first-and second-order structural information. <ref type="bibr" target="#b5">[5]</ref> proposed node2vec which introduces a biased random walk. However, these methods can be computationally expensive and non-optimal for large graphs. Graph neural networks (GNN) are designed to tackle these problems, which are deep learning based methods that operate on the graph domain. The concept of GNN is first proposed by <ref type="bibr" target="#b24">[24]</ref>. Generally, nodes in GNNs interact with neighbors by aggregating information from neighborhoods and updating their hidden states. There have been many variants of GNN with various kinds of aggregators and updaters proposed these days. Here we only present some representative and classical methods. Gated Graph Neural Networks (GGNN) <ref type="bibr" target="#b12">[12]</ref> uses GRU <ref type="bibr" target="#b3">[3]</ref> as updater. Graph Convolutional Networks (GCN) <ref type="bibr" target="#b10">[10]</ref> considers the spectral structure of graphs and utilizes the convolutional aggregator. GraphSAGE <ref type="bibr" target="#b7">[7]</ref> considers the spatial information. It introduces three kinds of aggregators: mean aggregator, LSTM aggregator and Pooling aggregator. Graph attention network (GAT) <ref type="bibr" target="#b30">[30]</ref> incorporates the attention mechanism into the propagation step. There are some surveys <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b36">36]</ref> which provide more elaborative introduction of various kinds of GNN models.</p><p>Due to convincing performance and high interpretability, GNN has been a widely applied graph analysis method. Recently, there are many application of GNN like neural machine translation <ref type="bibr" target="#b0">[1]</ref>, semantic segmentation <ref type="bibr" target="#b20">[20]</ref>, image classification <ref type="bibr" target="#b17">[17]</ref>, situation recognition <ref type="bibr" target="#b11">[11]</ref>, recommendation <ref type="bibr" target="#b32">[32]</ref>, script event prediction <ref type="bibr" target="#b14">[14]</ref>, fashion analysis <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b13">13]</ref>. GNN is suitable for modeling node interactions on graph-structured features intrinsically. In this work, we proposed a model Fi-GNN based on GGNN to model feature interactions on the graph-structured features for CTR prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR PROPOSED METHOD</head><p>We first formulate the problem and then introduce the overview of our proposed method, followed by the elaborate detail of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Suppose the training dataset consists of m-fields categorical features (m is the number of feature fields) and the associated labels y ∈ {0, 1} which indicate user click behaviors. The task of CTR prediction is to predictŷ for the input m-fields features, which estimates the probability of a user clicking. The key of the task is to model the sophisticated interactions among different feature fields. <ref type="figure">Figure 1</ref> is the overview of our proposed method (m=4). The input sparse m-field feature vector is first mapped into sparse one-hot embedding vectors and then embedded to dense field embedding vectors via the embedding layer and the multi-head self-attention layer. The field embedding vectors are then represented as a feature graph, where each node corresponds to a feature field and different feature fields can interact through edges. The task of modeling interaction can be thus converted to modeling node interactions on the feature graph. Therefore, the feature graph is feed into our proposed Fi-GNN to model node interactions. An attention scoring layer is applied on the output of Fi-GNN to estimate the clickthrough rateŷ. In the following, we will introduce the details of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Embedding Layer</head><p>The multi-field categorical feature x is usually sparse and of huge dimension. Following previous works <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35]</ref>, we represent each field as a one-hot encoding vector and then embed it to a dense vector, noted as field embedding vector. Let us consider the example in Section 1, a movie {Language: English, Genre: fiction, Director: Christopher Nolan, Starring: Leonardo DiCaprio } is first transformed into a high-dimensional sparse features via one-hot encoding:</p><formula xml:id="formula_0">[1, 0, ..., 0] Language , [0, 1, ..., 0] Genre , [0, 1, ..., 0] Director , [0, 1, ..., 0]</formula><p>Starring A field-aware embedding layer is then applied upon the one-hot vectors to embed them to low dimensional, dense real-value field embedding vectors as shown in <ref type="figure">Figure ?</ref>?. Likewise, the field embedding vectors of m-field feature can be obtained:</p><formula xml:id="formula_1">E = [e 1 , e 2 , e 3 , ..., e m ] ,</formula><p>where e i ∈ R d denotes the embedding vector of field i and d denotes the dimension of field embedding vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-head Self-attention Layer</head><p>Transformer <ref type="bibr" target="#b29">[29]</ref> is prevalent in NLP and has achieved great success in many tasks. At the core of Transformer, the multi-head self-attention mechanism is able to model complicated dependencies between word pairs in multiple semantic subspaces. In the literature of CTR prediction, we take advantage of the multi-head self-attention mechanism to capture the complex dependencies between feature field pairs, i.e, pairwise feature interactions, in different semantic subspaces.</p><p>Following <ref type="bibr" target="#b26">[26]</ref>, given the feature embeddings E, we obtain the feature representation of features that cover the pairwise interactions of an attention head i via scaled dot-product:</p><formula xml:id="formula_2">H i = softmax i ( QK T √ d K )V, Q = W (Q ) i E, K = W (K ) i E, V = W (V ) i E. The matrices W (Q ) i ∈ R d i ×d , W (K ) i ∈ R d i ×d , W (V ) i ∈ R d i ×d are three weight parameters for attention head i, d i is the dimension size of head i, and H i ∈ R m×d i .</formula><p>Then we combine the learnt feature representations of each head to preserve the pairwise feature interactions in each semantic subspace:</p><formula xml:id="formula_3">H 1 = ReLU(H 1 ⊕ H 2 ⊕ · · · ⊕ H h ),</formula><p>where ⊕ denotes the concatenation operation and h denotes the number of attention heads. The learnt feature representations H 1 ∈ R m×d ′ are used for the initial node states of the graph neural net-</p><formula xml:id="formula_4">work, where d ′ = h i=1 d i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Feature Graph</head><p>Distinguished from the previous works which simply concatenate the field embedding vectors together and feed them into designed models to learn feature interactions, we represent them in a graph structure. In particular, We represent each input multi-field feature as a feature graph G = (N, E), where each node n i ∈ N corresponds to a feature field i and different fields can interact through the edges, so that |N | = m. Since each two fields ought to interact, it is a weighted fully connected graph while the edge weights reflect importances of different feature interactions. Accordingly, the task of modeling feature interactions can be converted to modeling node interactions on the feature graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Feature Interaction Graph Neural Network</head><p>Fi-GNN is designed to model node interactions on the feature graph, which is based on GGNN <ref type="bibr" target="#b12">[12]</ref>. It is able to model the interactions in a flexible and explicit fashion.</p><p>Preliminaries. In Fi-GNN, each node n i is associated with a hidden state vector h t i and the state of graph is composed of these node states</p><formula xml:id="formula_5">H t = h t 1 , h t 2 , h t 3 , ..., h t m ,</formula><p>where t denote the interaction step. The learnt feature representations by the multi-head self-attention layer are used for their initial node states H 1 . As shown in <ref type="figure">Figure 2</ref>, the nodes interact and update their states in a recurrent fashion. At each interaction step, the nodes aggregate the transformed state information with neighbors, <ref type="figure">Figure 2</ref>: Framework of Fi-GNN. The nodes interact with neighbors and update their states in a recurrent fashion. At each interaction step, each node will first aggregate transformed state information from neighbors and then update its state according to the aggregated information and history via GRU and residual connection. and then update their node states according to the aggregated information and history via GRU and residual connection. Next, we will introduce the details of Fi-GNN elaborately. State Aggregation. At interaction step t, each node will aggregate the state information from neighbors. Formally, the aggregated information of node n i is sum of its neighbors' transformed state information,</p><formula xml:id="formula_6">a t i = n j →n i ∈E A[n j , n i ]W p h t −1 j ,<label>(1)</label></formula><p>where W p is the transformation function. A ∈ R m×m is the adjacency matrix containing the edge weights. For example, A[n j , n i ] is the weight of edge from node n j to n i , which can reflect the importance of their interaction. Apparently, the transformation function and adjacency matrix decide on the node interactions. Since the interaction on each edge ought to differ, we aim to achieve edge-wise interaction, which requires a unique weight and transformation function for each edge.</p><p>(1) Attentional Edge Weights. The adjacency matrix in the conventional GNN models is usually in the binary form, i.e., only contains 0 and 1. It can only reflect the connected relation of nodes but fails to reflect the importances of their relations. In order to infer the importances of interactions between different nodes, we propose to learn the edge weights via an attention mechanism. In particular, the weight of edge from node n i to node n j is calculated with their initial node states, i.e., the corresponding field embedding vectors. Formally,</p><formula xml:id="formula_7">w(n i , n j ) = exp(LeakyRelu(W w e i || e j )) k exp(LeakyRelu(W w [e i || e k ])) ,<label>(2)</label></formula><p>where W w ∈ R 2d ′ is a weight matrix, || is the concatenation operation. The softmax function is utilized to make weights easily comparable across different nodes. Therefore, the adjacency matrix is,</p><formula xml:id="formula_8">A[n i , n j ] = w(n i , n j ), if i j, 0, else .<label>(3)</label></formula><p>Since the edge weights reflects the importances of different interaction, Fi-GNN can provide good explanations on the relation of different feature fields of input instance, which will be further discussed in Section 4.5.</p><p>(2) Edge-wise Transformation. As discussed before, a fixed transformed function on all the edges is unable to model the flexible interactions and a unique transformation for each edge is essential. Nevertheless, our graph is complete graph with a huge number of edges. Simply assigning a unique transformation weight to each edge will consuming too much parameter space and running time. To reduce the time and space complexity and also achieve edge-wise transformation, we assign an output matrix W i out and an input matrix W i in to each node n i similar with <ref type="bibr" target="#b4">[4]</ref>. As shown in <ref type="figure">Figure  2</ref>, when node n i sends its state information to node n j , the state information will first be transformed by its output matrix W i out and then transformed by node n j 's input matrix W j in before n j receives it. The transformation function of edge n i → n j from node n i to node n j thus could be written as,</p><formula xml:id="formula_9">W n i →n j p = W i out W j in .<label>(4)</label></formula><p>Likewise, the transformation function of edge n j → n i from node n j to node n j is</p><formula xml:id="formula_10">W n j →n i p = W j out W i in .<label>(5)</label></formula><p>Accordingly, the Equation 1 could be rewritten as,</p><formula xml:id="formula_11">a t i = n j →n i ∈ E A[n j , n i ]W j out W i in h t −1 j + b p .<label>(6)</label></formula><p>In this way, the number of parameters is proportional to the number of nodes rather than numerous edges, which greatly reduces the space and time complexity and meanwhile achieves edge-wise interaction.</p><p>State Update. After aggregating state information, the nodes will update the state vectors via GRU and residual connections.</p><p>(1) State update via GRU. In traditional GGNN, the state vector of node n i is updated via GRU based on the aggregated state information a t i and its state at last step. Formally,</p><formula xml:id="formula_12">h t i = GRU (h t −1 i , a t i ).<label>(7)</label></formula><p>It can be formalized in detail as:</p><formula xml:id="formula_13">z t i = σ (W z a t i + U z h t −1 i + b z ),<label>(8)</label></formula><formula xml:id="formula_14">r t i = σ (W r a t i + U r h t −1 i + b r ),<label>(9)</label></formula><formula xml:id="formula_15">h t i = tanh(W h a t i + U h (r t i ⊙ h t −1 i ) + b h ),<label>(10)</label></formula><formula xml:id="formula_16">h t i =h t i ⊙ z t i + h t −1 i ⊙ (1 − z t i ),<label>(11)</label></formula><p>where, W z , W r , W h , b z , b r , b h are weights and biases of the updating function Gated Recurrent Unit (GRU) <ref type="bibr" target="#b12">[12]</ref>. z t i and r t i are update gate vector and reset gate vector, respectively.</p><p>(2) State update via Residual Connections. Previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref> have proved that it's effective to combine the low-order and high-order interactions together. We thus introduce extra residual connections to update note states along with GRU, which can facilitate low-order feature reuse and gradients back-propagation. Therefore, the Eq. <ref type="formula" target="#formula_12">(7)</ref> can be rewritten as,</p><formula xml:id="formula_17">h t i = GRU (h t −1 i , a t i ) + h 1 i .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Attentional Scoring Layer</head><p>After T propagation steps, we can obtain the node states H T = h T 1 , h T 2 , ..., h T m . Since the nodes have interacted with their T -order neighbors, the T -order feature interactions is modeled. We need a graph-level output to predict CTR. Attentional Node Weights The final state of each field node has captured the global information. In other words, these field nodes are neighborhood-aware. Here we predict a score on the final state of each field respectively and sum them up with an attention mechanism which measures their influences on the overall prediction. Formally, the prediction score of each node n i and its attentional node weight can be estimated via two multiple layers perceptions respectively as,ŷ</p><formula xml:id="formula_18">i = MLP 1 (h p i )),<label>(13)</label></formula><formula xml:id="formula_19">a i = MLP 2 (h p i )).<label>(14)</label></formula><p>The overall prediction is a summation of all nodes:</p><formula xml:id="formula_20">y = m i=1 a iŷi .<label>(15)</label></formula><p>Note that it is actually same as the work <ref type="bibr" target="#b12">[12]</ref>. Intuitively, MLP 1 is used to model the prediction score of each field aware of the global information and MLP 2 is used to model the weights of each field (i.e., importance of fields' influence on the overall prediction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Training</head><p>Our loss function is Log loss, which is defined as follows:</p><formula xml:id="formula_21">L = − 1 N N i=1 (y i loд(ŷ i ) + (1 − y i )loд(1 −ŷ i )),<label>(16)</label></formula><p>where N is the total number of training samples and i indexes the training samples. The parameters are updated via minimizing the Log Loss using RMSProp <ref type="bibr" target="#b28">[28]</ref>. Most CTR datasets have unbalanced proportion of positive and negative samples, which will mislead the predictions. To balance the proportion, we randomly select equal number of positive and negative samples in each batch during training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.8.1</head><p>Parameter Space. The parameter needed to be learnt mainly consists of the parameters correlated to nodes and the perception networks in attention mechanism. For each node n i , we have an input matrix W i in and an output matrix W i out to transform state information. Totally we have 2m matrices, which are proportional to the number of nodes m. Besides, the multi-head self-attention layer contains the following weight matrices W</p><formula xml:id="formula_22">(Q ) i , W (K ) i , W (V ) i</formula><p>for each head, and the number of parameters of the entire layer is (3dd ′ + hdd ′ ). In addition, we have two matrices of perception networks in the self-attention mechanism and also parameters in GRU. Overall, there are O(2m + hdd ′ ) matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Model Analysis</head><p>3.9.1 Comparison with Previous CTR Models. As discussed before, the previous deep learning based CTR models model highorder interactions in a general paradigm: raw sparse input multifiled features are first mapped into dense field embedding vectors, then simply concatenated together and feed into deep neural networks (DNN) or other specifically designed networks to learn highorder feature interactions. The simple unstructured combination of feature fields inevitably limits the capability to model sophisticated interactions among different fields in a sufficiently flexible and explicit fashion. In this way, the interaction between different fields is conducted in a fixed fashion, no matter how sophisticated the used network is. In addition, they lack good model explanation.</p><p>Since we represent the multi-field features in a graph structure, our proposed model Fi-GNN is able to model interactions among different fields in the form of node interactions. Compared with the previous CTR models, Fi-GNN can model the sophisticated feature interaction via flexible edge-wise interaction function, which is more effective and explicit. Moreover, the edge weights reflecting importance of different interactions can be learnt in Fi-GNN, which provides good model explanations for CTR prediction. In fact, if the edge weight is all 1 and the transformation matrix on each edge is same, our model Fi-GNN collapses into FM. Taking advantage of the great power of GNN, we can apply flexible interactions on different feature fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.9.2</head><p>Comparison with Previous GNN Models. Our proposed model Fi-GNN is designed based on GGNN, upon which we mainly make two improvements: (1) we achieve edge-wise interaction via attentional edge weights and edge-wise transformation; <ref type="bibr" target="#b1">(2)</ref> we introduce an extra residual connection along with GRU to update states, which can help regain the low-order information.</p><p>As discussed before, the node interaction on each edge in GNN depends on the edge weight and the transformation function on the edge. The conventional GGNN uses binary edge weights which fails to reflect the importance of the relations, and a fixed transformation function on all the edges. In contrast, our proposed Fi-GNN can model edge-wise interactions via attention edge weights and edgewise transformation functions. When the interaction order is high, the node states tend to be smooth, i.e., the states of all the nodes tend to be similar. The residual connections can help identity the nodes by adding initial node states. We first present some fundamental experimental settings before answering these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>4.1.1 Datasets. We evaluate our proposed models on the following two datasets, whose statistics are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>1. Criteo 1 . This is a famous industry benchmark dataset for CTR prediction, which has 45 million users' click records in 39 anonymous feature fields on displayed ads. Given a user and the page he is visiting, the goal is to predict the probability that he will click on a given ad.</p><p>2. Avazu 2 . This dataset contains users' click behaviors on displayed mobile ads. There are 23 feature fields including user/device features and ad attributes. The fields are partial anonymous.</p><p>For the two datasets, we remove the infrequent features appearing in less than 10, 5 times respectively and treat them as a single feature "&lt;unknown&gt;". Since the numerical features may have large variance, we normalize numerical values by transforming a value z to loд 2 (z) if z &gt; 2, which is proposed by the winner of Criteo Competition 3 . The instances are randomly split in 8:1:1 for training, validation and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation</head><p>Metrics. We use the following two metrics for model evaluation: AUC (Area Under the ROC curve) and Logloss (cross entropy).</p><p>AUC measures the probability that a positive instance will be ranked higher than a randomly chosen negative one. A higher AUC indicates a better performance.</p><p>Logloss measures the distance between the predicted score and the true label for each instance. A lower Logloss indicates a better performance.</p><p>Relative Improvement (RI). It should be noted that a small improvement with respect to AUC is regarded significant for realworld CTR tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b31">31]</ref>. In order to estimate the relative improvement of our model achieves over the compared models, we here measure RI-AUC and RI-Logloss, which can be formulated as,</p><formula xml:id="formula_23">RI-X = |X(model) − X(base)| X(base) * 100% ,<label>(17)</label></formula><p>where |x | returns the absolute value of x, X can be either AUC or Logloss, model refers to our proposed model and base refers to the compared model. We select the following representative methods of three types to compare with ours.</p><p>LR (A) models first-order interaction on the linear combination of raw individual features.</p><p>FM <ref type="bibr" target="#b23">[23]</ref> (B) models second-order feature interactions from vector inner products. AFM <ref type="bibr" target="#b34">[34]</ref> (B) is a extent of FM, which considers the weight of different second-order feature interactions by using attention mechanism. It is one of the state-of-the-art models that model second-order feature interactions.</p><p>DeepCrossing <ref type="bibr" target="#b25">[25]</ref> (C) utilizes DNN with residual connections to learn high-order feature interactions in an implicit fashion.</p><p>NFM <ref type="bibr" target="#b8">[8]</ref> (C) utilizes a Bi-Interaction Pooling layer to model the second-order interactions, and then feeds the concatenated second-order combinatorial features into DNNs to model highorder interactions.</p><p>CrossNet (Deep&amp;Cross) <ref type="bibr" target="#b31">[31]</ref> (C) is the core of Deep&amp;Cross model, which tries to model feature interactions explicitly by taking outer product of concatenated feature vector at the bit-wise level.</p><p>CIN (xDeepFM) <ref type="bibr" target="#b15">[15]</ref> (C) is the core of xDeepFM model, which takes outer product of stacked feature matrix at vector-wise level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Implementation Details.</head><p>We implement our method using Tensorflow 4 . The optimal hyper-parameters are determined by the grid search strategy. Implementation of baselines follows <ref type="bibr" target="#b26">[26]</ref>. Dimension of field embedding vectors is 16 and batch size is 1024 for all methods. DeepCrossing has four feed-forward layers, each with 100 hidden units. NFM has one hidden layer of size 200 on top of Bi-Interaction layer as recommended in the paper <ref type="bibr" target="#b8">[8]</ref>. There are three interaction layers for both CrossNet and CIN. All the experiments were conducted over a sever equipped with 8 NVIDIA Titan X GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Comparison (RQ1)</head><p>The performance of different methods is summarized in <ref type="table" target="#tab_1">Table 2</ref>, from which we can obtain the following observations:</p><p>(1) LR achieves the worst performance among these baselines, which proves that the individual features is insufficient in CTR prediction. (2) FM and AFM, which model second-order feature interactions, outperform LR on all datasets, indicating that it's effective to model pair-wise interaction between feature fields. In addition, AFM achieves better performance than FM, which proves the effectiveness of attention on different interactions. (3) The methods modeling high-order interaction mostly outperform the methods that model second-order interactions. <ref type="bibr" target="#b4">4</ref> The code is released at https://github.com/CRIPAC-DIG/Fi_GNN This indicates the second-order feature interactions is not sufficient. (4) DeepCrossing outperforms NFM, proving the effectiveness of residual connections in CTR prediction. (5) Our proposed Fi-GNN achieves best performance among all these methods on two datasets. Considering the fact that previous improvements with respect to AUC at 0.001-level are regarded significant for CTR prediction task, our proposed method shows great superiority over these state-of-the-arts especially on Criteo dataset, owing to the great representative power of graph structure and the effectiveness of GNN on modeling node interactions. (6) Compared with these baselines, the relative improvement of our model achieves on Criteo dataset is higher than that on Avazu dataset. This might be attributed to that there are more feature fields in Criteo dataset, which can take more advantage of the representative power of graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study (RQ2)</head><p>Our proposed model Fi-GNN is based on GGNN, upon which we mainly make two improvements: (1) we achieve edge-wise node interactions via attentional edge weights and edge-wise transformation; <ref type="bibr" target="#b1">(2)</ref> we introduce extra residual connections to update state along with GRU. To evaluate the effectiveness of the two improvements on modeling node interactions, we conduct ablation study and compare the following three variants of Fi-GNN: Fi-GNN(-E/R): Fi-GNN without the two above mentioned improvements: edge-wise node interactions (E) and residual connections (R).</p><p>Fi-GNN(-E): Fi-GNN without edge-wise interactions (E).</p><p>Fi-GNN(-R): Fi-GNN without residual connections (R), which is also GGNN with edge-wise interactions.</p><p>The performance comparison is shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a), from which we can obtain the following observations:</p><p>(1) Compared with FiGNNïĳŇthe performance of Fi-GNN(-E) drops by a large margin, suggesting that it's crucial to model the edge-wise interaction. Fi-GNN(-E) achieves better performance than Fi-GNN(-E/R), proving that the residual connections can indeed provide useful information. <ref type="bibr">(</ref>2) The full model Fi-GNN outperforms the three variants, indicating that the two improvements we make, i.e., residual   connections and edge-wise interactions, can jointly boost the performance.</p><p>We take two measures to achieve edge-wise node interactions in Fi-GNN: attentional edge weight (W) and edge-wise transformation (T). To further investigate where dose the great improvement come from, we conduct another ablation study and compare the following three variants of Fi-GNN:</p><p>Fi-GNN(-W/T): Fi-GNN without self-adaptive adjacency matrix (W) and edge-wise transformation (T), i.e., uses binary adjacency matrix (all the edge weights are 1) and a shared transformation matrix on all the edges. It is also Fi-GNN-(E), Fi-GNN(-W): FI-GNN without attentional edge weights, i.e., uses binary adjacency matrix.</p><p>Fi-GNN(-T): FI-GNN without edge-wise transformation, i.e., uses a shared transformation on all the edges.</p><p>The performance comparison is shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a). We can see that Fi-GNN(-T) and Fi-GNN(-W) both outperform Fi-GNN(-W/T), which proves their effectiveness. Nevertheless, Fi-GNN(-W) achieves greater improvements than Fi-GNN(-T), suggesting that the edge-wise transformation is more effective than attentional edge weights in modeling edge-wise interaction. This is quite reasonable since the transformation matrix oughts to have stronger influence on interactions than a scalar attentional edge weight. In addition, Fi-GNN achieves the best performance demonstrates that it's crucial to take both the two measures to model edge-wise interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hyper-Parameter Study (RQ3)</head><p>4.4.1 Influence of different state dimensionality. We first investigate how the performance changes w.r.t. the dimension of the node states d ′ , which is also the output size of the initial multihead self-attention layer. The results on Criteo and Avazu datasets are shown in <ref type="figure" target="#fig_4">Figure 4</ref>(a). On Avazu dataset, the performance first increases and then begins to decrease when the dimension size reaches 32, which indicates that state size of 32 has been represented enough information and the model is overfitted when too many parameters are used. Nevertheless, on Criteo dataset, the performance peaks with the dimension size of 64, which is reasonable since the dataset is more complexed which needs larger dimension size to carry out enough information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.2</head><p>Influence of different interaction steps. We are interested in what the optimal highest order of feature interactions is. Our proposed Fi-GNN can answer the question, since the interaction step T equals to the highest order of feature interaction. Therefore, we conduct experiments on how the performance changes w.r.t. the highest order of feature interaction, i.e., the interaction step T . The results on Criteo and Avazu datasets are shown in <ref type="figure" target="#fig_4">Figure 4(b)</ref>. On Avazu datasets, we can see that the performance increases along with the increasing of T until it reaches 2, after that the performance starts to decrease. By contrast, the performance peaks when T = 3 on Criteo dataset. This finding suggests 2-order and 3-order interactions are enough for Avazu and Criteo dataset, respectively. It is reasonable since the Avazu and Criteo datasets have 23 and 39 feature fields, respectively. Thus the Criteo dataset needs more interaction steps for the field nodes to fully interact with other nodes in the feature graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Explanation (RQ4)</head><p>In this section, we will answer the question that can Fi-GNN provide explanations. We apply attention mechanisms on the edges and nodes in the feature graphs and obtain attentional edge weights and attentional node weights respectively, which can provide explanations from different aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5.1</head><p>Attentional Edge weights. The attentional edge weight reflects the importance of interaction between the two connected field nodes, which can also reflect the relation of the two feature fields. Higher the weight is, stronger the relation is. <ref type="figure" target="#fig_5">Figure 5</ref> presents the heat map of the globally averaged adjacency matrix of all the samples in Avazu dataset, which can reflect the relations between different fields in a global level. Since they are some anonymous feature fields, we only show the remaining 13 feature fields with real meanings.</p><p>As can be seen, some feature fields tend to have a strong relations with others, such as site_category and site_id. This makes sense since the two feature field both corresponds to the website where the impressions are put on. They contain the main contextual information of impressions. Hour is another feature which have close relations with others. It is reasonable since Avazu focuses on mobile scene, where user surfing online at any time of a day. The surfing time has strong influence on other advertising features. On the other hand, device_ip and device_id seem to have weak relations with other feature fields. This may due to that they nearly equal to user identity, which is relatively fixed and hard to be influenced by other features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5.2</head><p>Attentional Node weights. The attentional node weights reflect the importances of feature fields' influence on the overall prediction score. <ref type="figure">Figure 6</ref> presents the heat map of global-level and case-level attentional node weights. The leftmost is an globally averaged one of all the samples in Avazu dataset. The left four are randomly selected, whose predicted scores are [0.97, 0.12, 0.91, 0.99], and labels are [1, 0, 1, 1] respectively. At the global level, we can see that the feature field app_category have the strongest influence on the clicking behaviors. It is reasonable since Avazu focuses on mobile scene, where the app is the most important factor. At the case level, we observe that the final clicking behavior mainly depends on one critical feature field in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we point out the limitations of the previous CTR models which consider multi-field features as an unstructured combination of feature fields. To overcome these limitations, we propose to represent the multi-field features in a graph structure for the global average cases <ref type="figure">Figure 6</ref>: Heat map of attentional node weights at both global-and case-level on Avazu, which reflects the importance of different feature fields on the final prediction.</p><p>first time, where each node corresponds to a feature field and different fields can interact through edges. Therefore, modeling feature interactions can be converted to modeling node interaction on the graph. To this end, we design a novel model Fi-GNN which is able to model sophisticated interactions among feature fields in a flexible and explicit fashion. Overall, we propose a new paradigm of CTR prediction: represent multi-field features in a graph structure and convert the task of modeling feature interactions to modeling node interactions on graphs, which may motivate the future work in this line.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Field</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 1 . 3</head><label>13</label><figDesc>Baselines. As described in Section 2.1, the early approaches can be categorized into three types: (A) Logistic Regression (LR) which models first-order interaction; (B) Factorization Machine (FM) based linear models which model second-order interactions; (C) Deep learning based models which model high-order interactions on the concatenated field embedding vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>edge-wise interaction (E) and residual connections (R) attentional edge weight (W) and edge-wise transformation (T) Two groups of ablation studies on Fi-GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>AUC performance with different state dimensionality D (left) and interaction step T (right) on Criteo and Avazu dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Heat map of attentional edge weights at the globallevel on Avazu, which reflects the importance of relations between different feature fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of evaluation datasets.</figDesc><table><row><cell cols="3">Dataset #Instances #Fields #Features (sparse)</cell></row><row><cell>Criteo 45,840,617</cell><cell>39</cell><cell>998,960</cell></row><row><cell>Avazu 40,428,967</cell><cell>23</cell><cell>1,544,488</cell></row><row><cell>4 EXPERIMENTS</cell><cell></cell><cell></cell></row><row><cell cols="3">In this section, we conduct extensive experiments to answer the</cell></row><row><cell>following questions:</cell><cell></cell><cell></cell></row><row><cell cols="3">RQ1 How does our proposed Fi-GNN perform in modeling high-</cell></row><row><cell cols="3">order feature interactions compared with the state-of-the-art</cell></row><row><cell>models?</cell><cell></cell><cell></cell></row><row><cell cols="3">RQ2 Does our proposed Fi-GNN perform better than original</cell></row><row><cell cols="3">GGNN in modeling high-order feature interactions?</cell></row><row><cell cols="3">RQ3 What are the influences of different model configurations?</cell></row><row><cell cols="3">RQ4 What are the relations between features of different fields?</cell></row><row><cell cols="2">Is our proposed model explainable?</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance Comparison of Different methods. The best performance on each dataset and metric are highlighted. Further analysis is provided in Section 4.2.</figDesc><table><row><cell>Model Type</cell><cell>Model</cell><cell>AUC</cell><cell cols="3">Criteo RI-AUC Logloss RI-Logloss</cell><cell>AUC</cell><cell cols="3">Avazu RI-AUC Logloss RI-Logloss</cell></row><row><cell>First-order</cell><cell>LR</cell><cell>0.7820</cell><cell>3.00%</cell><cell>0.4695</cell><cell>5.43%</cell><cell>0.7560</cell><cell>2.60%</cell><cell>0.3964</cell><cell>3.63%</cell></row><row><cell>Second-order</cell><cell>FM [23] AFM[34]</cell><cell>0.7836 0.7938</cell><cell>2.80% 1.54%</cell><cell>0.4700 0.4584</cell><cell>5.55% 2.94%</cell><cell>0.7706 0.7718</cell><cell>0.72% 0.57%</cell><cell>0.3856 0.3854</cell><cell>0.76% 0.81%</cell></row><row><cell></cell><cell cols="2">DeepCrossing [25] 0.8009</cell><cell>0.66%</cell><cell>0.4513</cell><cell>1.35%</cell><cell>0.7643</cell><cell>1.53%</cell><cell>0.3889</cell><cell>1.67%</cell></row><row><cell></cell><cell>NFM [8]</cell><cell>0.7957</cell><cell>1.57%</cell><cell>0.4562</cell><cell>2.45%</cell><cell>0.7708</cell><cell>0.70%</cell><cell>0.3864</cell><cell>1.02%</cell></row><row><cell>High-order</cell><cell>CrossNet [31]</cell><cell>0.7907</cell><cell>1.92%</cell><cell>0.4591</cell><cell>3.10%</cell><cell>0.7667</cell><cell>1.22%</cell><cell>0.3868</cell><cell>1.12%</cell></row><row><cell></cell><cell>CIN [15]</cell><cell>0.8009</cell><cell>0.63%</cell><cell>0.4517</cell><cell>1.44%</cell><cell>0.7758</cell><cell>0.05%</cell><cell>0.3829</cell><cell>0.10%</cell></row><row><cell></cell><cell>Fi-GNN (ours)</cell><cell>0.8062</cell><cell>0.00%</cell><cell>0.4453</cell><cell>0.00%</cell><cell>0.7762</cell><cell>0.00%</cell><cell>0.3825</cell><cell>0.00%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.kaggle.com/c/criteo-display-ad-challenge 2 https://www.kaggle.com/c/avazu-ctr-prediction 3 https://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Graph-to-sequence learning using gated graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09835</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st workshop on deep learning for recommender systems</title>
		<meeting>the 1st workshop on deep learning for recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08009</idno>
		<title level="m">Dressing as a Whole: Outfit Compatibility Learning Based on Node-wise Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DeepFM: a factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural factorization machines for sparse predictive analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fieldaware factorization machines for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Situation recognition with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4173" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-Supervised Compatibility Learning Across Categories for Clothing Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="484" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Constructing Narrative Event Evolutionary Graph for Script Event Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.05081</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">xDeepFM: Combining explicit and implicit feature interactions for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A convolutional click prediction model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international on conference on information and knowledge management</title>
		<meeting>the 24th ACM international on conference on information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1743" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The More You Know: Using Knowledge Graphs for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 2017. 3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 16th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Product-Based Neural Networks for User Response Prediction over Multi-Field Categorical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minzhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Data Mining. IEEE</title>
		<imprint>
			<biblScope unit="page" from="995" to="1000" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep crossing: Web-scale modeling without manually crafted combinatorial features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11921</idno>
		<title level="m">AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 24th international conference on world wide web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD&apos;17</title>
		<meeting>the ADKDD&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sessionbased Recommendation with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attentional factorization machines: Learning the weight of feature interactions via attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep Learning over Multifield Categorical Data: A Case Study on User Response Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.02376</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
