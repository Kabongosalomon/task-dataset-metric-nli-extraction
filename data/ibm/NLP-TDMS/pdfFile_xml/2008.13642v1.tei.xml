<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RADAR+RGB ATTENTIVE FUSION FOR ROBUST OBJECT DETECTION IN AUTONOMOUS VEHICLES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritu</forename><surname>Yadav</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universitt Kaiserslautern</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Vierling</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universitt Kaiserslautern</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Berns</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universitt Kaiserslautern</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RADAR+RGB ATTENTIVE FUSION FOR ROBUST OBJECT DETECTION IN AUTONOMOUS VEHICLES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Object Detection</term>
					<term>Radar Signals</term>
					<term>Vision</term>
					<term>RGB Cameras</term>
					<term>Fusion</term>
					<term>Deep Learning</term>
					<term>Autonomous Vehicle</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents two variations of architecture referred to as RANet and BIRANet. The proposed architecture aims to use radar signal data along with RGB camera images to form a robust detection network that works efficiently, even in variable lighting and weather conditions such as rain, dust, fog, and others. First, radar information is fused in the feature extractor network. Second, radar points are used to generate guided anchors. Third, a method is proposed to improve region proposal network [1] targets. BIRANet yields 72.3/75.3% average AP/AR on the NuScenes [2] dataset, which is better than the performance of our base network Faster-RCNN with Feature pyramid network(FFPN) <ref type="bibr" target="#b2">[3]</ref>. RANet gives 69.6/71.9% average AP/AR on the same dataset, which is reasonably acceptable performance. Also, both BI-RANet and RANet are evaluated to be robust towards the noise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With increasing demand and research in autonomous vehicles, object detection has become a significant task. RGB images through cameras are the standard input of such object detectors. However, bad weather conditions or dark environment conditions distort the images or videos captured by a camera in a driving scenario. Unpredictable environmental conditions make it troublesome for single modality based object detectors to detect the object's location, and it is category precisely. To overcome this limitation, Vehicle industries are using different sensors in autonomous vehicles such as LIDAR, radar, infrared cameras, and others. Each sensor has it is own benefits and drawbacks. However, LIDAR and radar sensors have most commonly come into the picture when analyzed on the usability factor. LIDAR sensors are highly efficient in making 3d images of objects but at the cost of the reliability issue. LIDAR has more moving parts hence have room for more noise. Radar sensors, on the other hand, provide very few points hence are not suitable for 3d object image construction. However, radar sensors have a high range, due to which they can detect objects at a long distance.</p><p>They are robust, reliable, and cheaper than LIDAR sensors. Long-range object detection is beneficial for autonomous vehicles, especially commercial vehicles such as trucks, where they need more time to slow down for an incoming obstacle or object. Radar sensors and RGB cameras, make a very reliable combination for object detection. Even after these significant merits of radar sensors, there is limited significant research work conducted on radar sensor and vision fusion.</p><p>In this paper, an architecture for object detection is introduced, which utilizes RGB camera images and radar sensor data. Radar points store location information of objects and their distance from the sensor with high confidence. We use this high confidence radar sensor information for better feature extraction and region proposals along with other intuitive ideas. Proposed approaches are implemented to build a robust object detection network for the autonomous driving environment. For the demonstration of the effectiveness of the proposed architecture, the NuScenes dataset is used, which has recorded data from a full autonomous sensor suite(6 RGB cameras, 1 LIDAR, 5 RADAR, GPS, IMU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In autonomous vehicles, radar-based object detection is explored in work such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. All these works are based on traditional techniques such as Haar-like features <ref type="bibr" target="#b7">[8]</ref>, Extended Kalman Filter(EKF) <ref type="bibr" target="#b3">[4]</ref>, occupancy grid map <ref type="bibr" target="#b4">[5]</ref>, and others. With the introduction of Convolutional Neural Network(CNN), an informative representation of objects is learned, and it shows drastic improvement in tasks such as classification and regression. CNN based object detectors are very efficient and common these days. Two-stage object detectors generally overpower single-stage detectors in terms of accuracy. The main strength of the two-stage object detector is the region proposal network. Faster-RCNN <ref type="bibr" target="#b0">[1]</ref> introduced region proposal network(RPN) where they made network to learn anchor proposals. RPN was a tremendous success in the field of object detection. RPN is further explored in multiple works such as GA-RPN <ref type="bibr" target="#b8">[9]</ref>, Iterative RPN <ref type="bibr" target="#b9">[10]</ref>, Cascade RPN <ref type="bibr" target="#b10">[11]</ref>, and others.</p><p>RPN is much explored on RGB camera images and very less on radar data. However, there are few works such as <ref type="bibr" target="#b11">[12]</ref>, where authors presented a 3D object detection network for car detection using AVOD architecture on radar pointclouds and camera images. Authors in <ref type="bibr" target="#b12">[13]</ref>, use range-doppler spectrums from FMCW radar signals and camera images for 3D object detection and estimation.</p><p>Authors in <ref type="bibr" target="#b13">[14]</ref>, presented radar points-based anchors where radar points are considered towards the center, left, right, and bottom of the object. However, the authors did not provide any specific reason for not considering the radar point at the top of the object. The idea of radar point-based anchor generation is accommodated in the presented work with some intuitive changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED OBJECT DETECTION NETWORK</head><p>The idea of sensor fusion is inspired by humans. We humans use multiple sensors to sense different attributes or specifications, which are refined together to come to a final observation. When it comes to artificial sensor fusion, the same approach is followed. In the proposed architecture, input from RGB cameras and radar sensors are fused, refined, and processed for object detection. Two variations of networks are proposed in this work, namely RANet and BIRANet. Both follow the same architecture with two differences; First, RANet works only on radar point-based anchors. However, BIRANet also utilizes anchors generated similar to that in FFPN. Second, BIRANet has a different RPN target generation method, "Best of Two," which is explained in subsection 3.4. Below is a detailed description of the proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Radar + RGB Fusion</head><p>Radar sensors provide points, distance, velocity, azimuth, and others. Radar points are in the 3D coordinate system(x,y,z) where x and y coordinate of radar points provide the 2D location of reflective objects in the view, and the third dimension z corresponds to the distance of the object from the radar sensor. Before feeding the radar data into the network, radar data points are first converted from vehicle coordinates to cameraview coordinates, which will align radar points with the RGB image. Second, radar points are resized and then converted into a feature map of a size equivalent to that of the input image. At each radar point location x, y of the feature map, the z distance value is embedded and assign 0 to all other locations where there is no radar point present. The resulting radar fea-ture map is the input of radar feature extractor and has one channel. In the proposed architecture, a separate radar feature extractor branch is formed, as represented in <ref type="figure" target="#fig_0">Fig 1.</ref> The idea of residual network is adapted to extract radar features efficiently, and one convolution along with two identity blocks is applied in the radar feature extractor branch. The radar feature extractor branch and RGB feature extractor branch are fused in the feature extractor backbone. The feature extractor used for RGB images is ResNet <ref type="bibr" target="#b14">[15]</ref>. While we experimented with radar and RGB data fusion at different stages of the feature extractor, the proposed configuration shown in <ref type="figure" target="#fig_1">Fig 2 proved</ref> to be the best. For fusion, element-wise addition operation is performed on the output feature map of the radar feature extractor branch and second stage output feature map of the RGB feature extractor branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attentive FPN</head><p>In Feature pyramid network(FPN) of FFPN 4 feature map output, P2 to P5 are extracted from the feature extractor backbone. Extracted feature maps are the features on which the performance of the detection network is primarily based. Hence these features have to be as good as possible. It is experimented and found that in the radar+RGB fused network, the output feature maps can be boosted further with the help of attention. With this motivation, an Attentive FPN is proposed, which uses Concurrent Spatial and Channel Squeeze &amp; Excitation(scSE) blocks <ref type="bibr" target="#b15">[16]</ref>. The scSE block acts as attention and highlights important spatial features as well as significant channels. In the attentive FPN, scSE attention block is applied to the radar feature map. scSE blocks adaptively boost activation of areas where we have radar points and suppress the activation at other locations. Boosted good feature maps are then fused with RGB features map as shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Radar Point-Based Anchors and RoIAlign Layer</head><p>Radar signals are reliable in providing an object's location even if they are far away hence have a long-range. When radar points are superimposed on images, although radar signals do not point to all object's in the image but incorporate most of the objects and also those in long range. Therefore, it is reasonable to use radar points for generating region proposals. Another observation is that radar points are not only present towards the center of the surface of the object but also towards the top, bottom, left, and right edges of objects. Thus, in a good proposal scheme, one cannot just consider the radar point at the center of the bounding box but should also consider the radar point on all four edges of the bounding box.</p><p>Anchor boxes are generated with 3 ratios (0.5, 1.0, 2.0) at each radar point and with the consideration that each radar point is at the top, down, left, right edge, and at the center(TDLRC) of an anchor box, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. In RPN, Radar points are mapped to each output feature map of FPN. Anchors for each radar point are extracted with different scales based on the level of the feature map.</p><p>In the region proposal network of FFPN, RoIPooling [3] is used to obtain fixed-size(e.g., 7X7) feature maps from each region of interest. RoIPool, cause harsh quantization over features which creates alignment issue between the input image and extracted features. For a good point based proposals, it is essential to have an accurate mapping of radar points in each feature map. For feature alignment reason, the RoIPool layer of FFPN is replaced with the RoIAlign <ref type="bibr" target="#b16">[17]</ref> layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Best of Two -RPN Target Generation Method</head><p>In RPN target generation, anchors for each input image are compared to the ground truth bounding boxes, and the IOU score is calculated. Anchors, whose IOU score is greater than 0.7 are assigned with the positive label, and those with IOU score less than 0.3 are assigned with the negative label. Out of all anchor boxes, 128 positive and 128 negative labeled anchor boxes are selected for further process.</p><p>In cases where few ground truth bounding boxes of an image do not have any corresponding positive anchor box(i.e., Anchors with IOU&gt;0.7), the IOU scores of all corresponding anchor boxes are compared, and the one with maximum IOU score is assigned with a positive label. In such scenarios, radar point-based anchoring offers anchors with higher IOU scores provided there is a radar point corresponding to those ground truth objects. However, it is known that radar sensors do not provide reflected points for all objects in the scene; hence there are no anchors for them.</p><p>The Proposed Best of two method as outlined in Algorithm 1 help in overcoming this limitation where all anchors as per FFPN along with radar point-based anchors are utilized in RPN target generation. The benefits of this strategy are; first, objects which are not detected by using radar points can now be detected by using RGB anchors similar to FFPN. Second, this method increases the probability of having better anchor candidates for the RPN. The effect of this method can be seen in the performance comparison present in <ref type="table" target="#tab_1">Table 1</ref> and 2. We also want to highlight that the stated method causes a negligible overhead at training time and no increment in inference time.</p><p>Algorithm 1: Best of Two RPN target generation method.</p><p>Input: Ground-truth bounding box(gt bbox), RGB anchors(i anch), radar based anchors(r anch). Output: 128 positive target anchors. <ref type="bibr" target="#b0">1</ref> Assign positive/negative labels to RGB anchors and radar based anchors separately. 2 for positive label RGB anchors(P i anch) do <ref type="bibr" target="#b2">3</ref> for positive label radar anchors(P r anch) do <ref type="bibr" target="#b3">4</ref> if IOU(P r anch, gt bbox) &gt; IOU(P i anch, gt bbox) then 5 P i anch = P r anch. <ref type="bibr" target="#b5">6</ref> Select random 128 positive anchors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DATASET</head><p>For training and evaluation, we used the NuScenes dataset which is inspired by the KITTI dataset and covers a large variety of outdoor conditions. To our knowledge, it is the only publicly available dataset where radar data and camera images are synchronized. For object detection, NuScenes provide 3D bounding boxes with 23 classes. Before using the NuScenes dataset, 3D bounding boxes are converted into 2D and merged relevant classes to obtain 6 classes, which are Car, Truck, Person, Motorcycle, Bicycle, and Bus. In the dataset, there are ground truth bounding box annotations that are not visible in the image due to full occlusion. Such cases are discarded by setting the visibility level parameter from NuScenes as two. Also, the dataset is split into 80:20% ratio for training and testing, respectively. To investigate the robustness of networks, extra noise is added to evaluation images. Noise added are average blur noise with kernel size three and additive Gaussian noise with 0.05 standard deviation. We also evaluated our networks on extreme </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">IMPLEMENTATION AND TRAINING</head><p>Faster R-CNN with feature pyramid network is our conceptual base on which our proposed network is built. Experiments are conducted on images of size 1024x1024 and 512x512. Scale used for anchor generation are <ref type="bibr" target="#b15">(16,</ref><ref type="bibr">32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256)</ref> and <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">32,</ref><ref type="bibr">64,</ref><ref type="bibr">128)</ref> respectively. Maximum number of objects per image is set to 100. Training is conducted on one TITAN Pascal GPU with batch size of two. First, base network FFPN is trained on COCO dataset <ref type="bibr" target="#b17">[18]</ref> with ResNet50 feature extractor. Then the trained weights are used to initialize the weights of our proposed fused networks(RANet, BIRANet). These network are further trained on the NuScenes training dataset. For training the network, we use step-wise training with 0.001 learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RESULTS AND EVALUATION</head><p>For evaluation, COCO evaluation metric <ref type="bibr" target="#b17">[18]</ref> is used with an additional AP over 0.85 IOU for a more detailed evaluation. FFPN generates anchors at each pixel of the image, whereas RANet generates anchors only for locations corresponding to which we get radar points. In the presented result Tables 1, 2, the performance of RANet lack by ∼5% in comparison to that of FFPN, which is a reasonably good performance. Generally, we do not get radar points corresponding to each object in the scene due to the limited field of view, multiple surface reflection, and others. So, it is reasonable to say that a network trained on radar point-based network will not be able to detect objects for which there is no corresponding radar point. BIRANet took care of these missing objects, and provide better detection results in comparison to FFPN. Also, on further observation, we find many cases such as <ref type="figure" target="#fig_4">Fig 4,</ref>    On further noise addition, we see an expected decrease in performance. However, the drop in performance of BIRANet is less in comparison to that of FFPN. Both RANet and BI-RANet show a minimal drop in performance, which proves their robustness. Results presented in <ref type="table" target="#tab_2">Table 2</ref> over 512x512 size images, shows that proposed fusion networks are more robust in low-resolution images. Hence, they can be efficient object detectors for vehicles with low-resolution cameras.</p><p>Based on the stated findings, we can say that radar data help in making detection more robust towards noise and can perform better. To our knowledge, RRPN is the only work where RGB and radar sensor data of the NuScenes dataset is used in the object detection task. In comparison, our network's performance is found to be better than theirs.</p><p>Proposed radar and RGB camera image fusion methods are implemented on FFPN base network. We expect similar behavior when proposed radar and RGB camera image fusion is applied with state of the art object detection networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Radar Feature extractor branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Radar+RGB featuremap fusion. Small purple circles represents element-wise addition. Red dots in radar signal input represent radar points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Radar point-based Anchors. From left to right, anchor boxes towards the center, right, left, bottom, and top edge center of radar point(blue dot).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Detection sample images. From left to right; Ground-Truth bounding box with Radar point superimposed on Image, FFPN Prediction on RGB, BIRANet Prediction on RGB+Radar. amount of noise. Because of page limitations all evaluation and results are not presented here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where BIRANet detects objects which are not detected by FFPN.</figDesc><table><row><cell>Method Used</cell><cell>AP</cell><cell cols="7">AP50 AP75 AP85 AP(s) AP(m) AP(l) AR</cell><cell cols="3">AR(s) AR(m) AR(l)</cell></row><row><cell>FFPN(RGB)</cell><cell cols="2">69.7 88.2</cell><cell>82</cell><cell>60.9</cell><cell>50.3</cell><cell>68</cell><cell>73.1</cell><cell>73</cell><cell>53.2</cell><cell>71.3</cell><cell>76.4</cell></row><row><cell>RANet(Radar)</cell><cell>69</cell><cell>83.9</cell><cell>80.1</cell><cell>64.4</cell><cell>44.8</cell><cell>67.8</cell><cell>73.3</cell><cell cols="2">71.9 47.3</cell><cell>70.9</cell><cell>76.2</cell></row><row><cell cols="3">BIRANet(RGB+Radar) 72.3 88.9</cell><cell>84.3</cell><cell>65.7</cell><cell>53.5</cell><cell>70.1</cell><cell>76.9</cell><cell cols="2">75.3 56.2</cell><cell>73.2</cell><cell>79.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">With Added Noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FFPN(RGB)</cell><cell cols="2">68.9 88.1</cell><cell>81.7</cell><cell>59</cell><cell>49.7</cell><cell>67.3</cell><cell>72.2</cell><cell cols="2">72.1 52.4</cell><cell>70.6</cell><cell>75.3</cell></row><row><cell>RANet(Radar)</cell><cell cols="2">68.3 83.3</cell><cell>79.1</cell><cell>62.6</cell><cell>45</cell><cell>66.9</cell><cell>73</cell><cell cols="2">71.3 47.1</cell><cell>70</cell><cell>75.9</cell></row><row><cell cols="3">BIRANet(RGB+Radar) 71.9 88.9</cell><cell>83.9</cell><cell>65.1</cell><cell>51</cell><cell>70.2</cell><cell>76.6</cell><cell cols="2">74.4 53.5</cell><cell>73.4</cell><cell>79.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of detection results on 1024x1024 image size.</figDesc><table><row><cell>Method Used</cell><cell>AP</cell><cell cols="7">AP50 AP75 AP85 AP(s) AP(m) AP(l) AR</cell><cell cols="3">AR(s) AR(m) AR(l)</cell></row><row><cell>FFPN on RGB</cell><cell cols="2">65.4 87</cell><cell>76.8</cell><cell>50.2</cell><cell>44.6</cell><cell>63</cell><cell>69.9</cell><cell cols="2">68.9 48.4</cell><cell>66.5</cell><cell>73.6</cell></row><row><cell>RANet(Radar)</cell><cell cols="2">64.7 82.1</cell><cell>75.1</cell><cell>57.4</cell><cell>41</cell><cell>62.6</cell><cell>70.4</cell><cell cols="2">67.5 44.1</cell><cell>65.5</cell><cell>72.9</cell></row><row><cell cols="3">BIRANet(RGB+Radar) 68.7 87.6</cell><cell>79.7</cell><cell>58.2</cell><cell>45.9</cell><cell>65.9</cell><cell>74.2</cell><cell>72</cell><cell>49.8</cell><cell>69.5</cell><cell>77.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">With Added Noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FFPN on RGB</cell><cell cols="2">63.7 85.7</cell><cell>75</cell><cell>47.9</cell><cell>40.4</cell><cell>60.8</cell><cell>68.9</cell><cell cols="2">67.6 45.2</cell><cell>64.7</cell><cell>72.8</cell></row><row><cell>RANet(Radar)</cell><cell>63</cell><cell>81.4</cell><cell>73.3</cell><cell>53.5</cell><cell>37.4</cell><cell>60.5</cell><cell>68.6</cell><cell cols="2">65.9 40.6</cell><cell>63.6</cell><cell>71.3</cell></row><row><cell cols="3">BIRANet(RGB+Radar) 67.4 87.4</cell><cell>77.7</cell><cell>56.2</cell><cell>42.7</cell><cell>64</cell><cell>73.8</cell><cell cols="2">70.7 46.8</cell><cell>67.5</cell><cell>76.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of detection results on 512x512 image size.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11027</idno>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Frontal object perception using radar and mono-vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Chavez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Burlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung-Dung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Aycard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="159" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Moving objects detection in evidential occupancy grids using laser radar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 8th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="73" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An obstacle detection method by fusion of radar and motion stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiki</forename><surname>Ninomiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Masaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="182" to="188" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Radar based object detection and tracking for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankith</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Engstle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE MTT-S International Conference on Microwaves for Intelligent Mobility (ICMIM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online vehicle detection using haar-like, lbp and hog feature based image classifiers with stereo vision preselection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fritz</forename><surname>Ulbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorothee</forename><surname>Spitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Goehring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="773" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2965" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Attend refine repeat: Active box proposal generation via in-out localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04446</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cascade rpn: Delving into high-quality region proposal network with adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1430" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning based 3d object detection for automotive radar and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kuschk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 16th European Radar Conference</title>
		<meeting><address><addrLine>Eu-RAD</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="133" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Object detection and 3d estimation via an fmcw radar using a fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Wenger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05394</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Rrpn: Radar region proposal network for object detection in autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Nabati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00526</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Concurrent spatial and channel squeeze &amp; excitationin fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Abhijit Guha Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="421" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
