<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention Guided Anomaly Localization in Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
							<email>shashankv@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<region>FL</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Siemens Corporate Technology</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention Guided Anomaly Localization in Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>]</term>
					<term>Kuan-Chuan Peng †[0000−0002−2682−9912]</term>
					<term>Rajat Vikram Singh ‡[0000−0002−1416−8344]</term>
					<term>and Abhijit Mahalanobis [0000−0002−2782−8655] Keywords: guided attention</term>
					<term>anomaly localization</term>
					<term>convolutional ad- versarial variational autoencoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly localization is an important problem in computer vision which involves localizing anomalous regions within images with applications in industrial inspection, surveillance, and medical imaging. This task is challenging due to the small sample size and pixel coverage of the anomaly in real-world scenarios. Most prior works need to use anomalous training images to compute a class-specific threshold to localize anomalies. Without the need of anomalous training images, we propose Convolutional Adversarial Variational autoencoder with Guided Attention (CAVGA), which localizes the anomaly with a convolutional latent variable to preserve the spatial information. In the unsupervised setting, we propose an attention expansion loss where we encourage CAVGA to focus on all normal regions in the image. Furthermore, in the weaklysupervised setting we propose a complementary guided attention loss, where we encourage the attention map to focus on all normal regions while minimizing the attention map corresponding to anomalous regions in the image. CAVGA outperforms the state-of-the-art (SOTA) anomaly localization methods on MVTec Anomaly Detection (MVTAD), modified ShanghaiTech Campus (mSTC) and Large-scale Attention based Glaucoma (LAG) datasets in the unsupervised setting and when using only 2% anomalous images in the weakly-supervised setting. CAVGA also outperforms SOTA anomaly detection methods on the MNIST, CIFAR-10, Fashion-MNIST, MVTAD, mSTC and LAG datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recognizing whether an image is homogeneous with its previously observed distribution or whether it belongs to a novel or anomalous distribution has been identified as an important problem <ref type="bibr" target="#b4">[5]</ref>. In this work, we focus on a related task, anomaly localization in images, which involves segmenting the anomalous regions (i) CAVGA main idea dataset task improvement MVTAD <ref type="bibr" target="#b4">[5]</ref> l (9/15; 4∼85%) MVTAD <ref type="bibr" target="#b4">[5]</ref> d (9/15; 2∼30%) mSTC <ref type="bibr" target="#b30">[31]</ref> l (7/12; 2∼42%) mSTC <ref type="bibr" target="#b30">[31]</ref> d (8/12; 1∼38%) LAG <ref type="bibr" target="#b28">[29]</ref> l (1/1; 16%) LAG <ref type="bibr" target="#b28">[29]</ref> d (1/1; 1.1%) MNIST <ref type="bibr" target="#b26">[27]</ref> d (8/10; 0.1∼2.5%) CIFAR-10 [25] d (7/10; 3∼31%) F-MNIST <ref type="bibr" target="#b56">[57]</ref> d (8/10; 2∼24%) l: localization; d: detection F-MNIST: Fashion-MNIST <ref type="bibr" target="#b56">[57]</ref> • metric for l : IoU • metric for d in MVTAD, mSTC, and LAG: classification accuracy • metric for d in MNIST, CIFAR-10 and Fashion-MNIST: area under ROC curve (ii) improvement summary <ref type="figure">Fig. 1</ref>: (i) CAVGA uses the proposed complementary guided attention loss to encourage the attention map to cover the entire normal regions while suppressing the attention map corresponding to anomalous class in the training image. This enables the trained network to generate the anomalous attention map to localize the anomaly better at testing (ii) CAVGA's improvement over SOTA in the form of (number of outperforming/total categories; improvement (%) in its metric) within them. Anomaly localization has been applied in industrial inspection settings to segment defective product parts <ref type="bibr" target="#b4">[5]</ref>, in surveillance to locate intruders <ref type="bibr" target="#b37">[38]</ref>, in medical imaging to segment tumor in brain MRI or glaucoma in retina images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref>, etc. There has been an increase in analysis towards segmenting potential anomalous regions in images as acknowledged in <ref type="bibr" target="#b12">[13]</ref>.</p><p>Existing state-of-the-art (SOTA) anomaly localization methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b46">47]</ref> are based on deep learning. However, developing deep learning based algorithms for this task can be challenging due to the small pixel coverage of the anomaly and lack of suitable data since images with anomalies are rarely available in real-world scenarios <ref type="bibr" target="#b4">[5]</ref>. Existing SOTA methods tackle this challenge using autoencoders <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b46">47]</ref> and GAN based approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b58">59]</ref>, which use a thresholded pixelwise difference between the input and reconstructed image to localize anomalies. But, their methods need to determine class-specific thresholds using anomalous training images which can be unavailable in real-world scenarios.</p><p>To tackle these drawbacks of using anomalous training images, we propose Convolutional Adversarial Variational autoencoder with Guided Attention (CAVGA), an unsupervised anomaly localization method which requires no anomalous training images. CAVGA comprises of a convolutional latent variable to preserve the spatial relation between the input and latent variable. Since real-world applications may have access to only limited training data <ref type="bibr" target="#b4">[5]</ref>, we propose to localize the anomalies by using supervision on attention maps. This is motivated by the finding in <ref type="bibr" target="#b27">[28]</ref> that attention based supervision can alleviate the need of using large amount of training data. Intuitively, without any prior knowledge of the anomaly, humans need to look at the entire image to identify the anomalous regions. Based on this idea, we propose an attention expansion loss where we encourage the network to generate an attention map that focuses on all normal regions of the image.</p><p>Since annotating segmentation training data can be laborious <ref type="bibr" target="#b21">[22]</ref>, in the case when the annotator provides few anomalous training images without ground truth segmented anomalous regions, we extend CAVGA to a weakly supervised setting. Here, we introduce a classifier in CAVGA and propose a complementary guided attention loss computed only for the normal images correctly predicted by the classifier. Using this complementary guided attention loss, we expand the normal attention but suppress the anomalous attention on the normal image, where normal/anomalous attention represents the areas affecting the classifier's normal/anomalous prediction identified by existing network visualization methods (e.g. Grad-CAM <ref type="bibr" target="#b48">[49]</ref>). <ref type="figure">Fig. 1</ref> (i) (a) illustrates our attention mechanism during training, and <ref type="figure">Fig. 1</ref> (i) (b) demonstrates that the resulting normal attention and anomalous attention on the anomalous testing images are visually complementary, which is consistent with our intuition. Furthermore, <ref type="figure">Fig. 1</ref> (ii) summarizes CAVGA's ability to outperform SOTA methods in anomaly localization on industrial inspection (MVTAD) <ref type="bibr" target="#b4">[5]</ref>, surveillance (mSTC) <ref type="bibr" target="#b30">[31]</ref> and medical imaging (LAG) <ref type="bibr" target="#b28">[29]</ref> datasets. We also show CAVGA's ability to outperform SOTA methods in anomaly detection on common benchmarks.</p><p>To the best of our knowledge, we are the first in anomaly localization to propose an end-to-end trainable framework with attention guidance which explicitly enforces the network to learn representations from the entire normal image. As compared to the prior works, our proposed approach CAVGA needs no anomalous training images to determine a class-specific threshold to localize the anomaly. Our contributions are: -An attention expansion loss (L ae ), where we encourage the network to focus on the entire normal images in the unsupervised setting. -A complementary guided attention loss (L cga ), which we use to minimize the anomalous attention and simultaneously expand the normal attention for the normal images correctly predicted by the classifier. -New SOTA: In anomaly localization, CAVGA outperforms SOTA methods on the MVTAD and mSTC datasets in IoU and mean Area under ROC curve (AuROC) and also outperforms SOTA anomaly localization methods on LAG dataset in IoU. We also show CAVGA's ability to outperform SOTA methods for anomaly detection on the MVTAD, mSTC, LAG, MNIST <ref type="bibr" target="#b26">[27]</ref>, CIFAR-10 <ref type="bibr" target="#b24">[25]</ref> and Fashion-MNIST <ref type="bibr" target="#b56">[57]</ref> datasets in classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Often used interchangeably, the terms anomaly localization and anomaly segmentation involve pixel-accurate segmentation of anomalous regions within an </p><formula xml:id="formula_0">N N Y Y Y Y localize multiple modes of anomalies Y N N N Y Y pixel (not patch) based localization Y Y N Y Y Y use convolutional latent variable N Y N N N Y</formula><p>image <ref type="bibr" target="#b4">[5]</ref>. They have been applied to industrial inspection settings to segment defective product parts <ref type="bibr" target="#b4">[5]</ref>, medical imaging to segment glaucoma in retina images <ref type="bibr" target="#b28">[29]</ref>, etc. Image based anomaly localization has not been fully studied as compared to anomaly detection, where methods such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48</ref>] employ a thresholded pixel wise difference between the input and reconstructed image to segment the anomalous regions. <ref type="bibr" target="#b46">[47]</ref> proposes an inpainter-detector network for patch-based localization in images. <ref type="bibr" target="#b12">[13]</ref> proposes gradient descent on a regularized autoencoder while Liu et al. <ref type="bibr" target="#b31">[32]</ref> (denoted as ADVAE) generate gradient based attention maps from the latent space of the trained model. We compare CAVGA with the existing methods relevant to anomaly localization in the unsupervised setting in <ref type="table" target="#tab_0">Table 1</ref> and show that among the listed methods, only CAVGA shows all the listed properties. Anomaly detection involves determining an image as normal or anomalous <ref type="bibr" target="#b2">[3]</ref>. One-class classification and anomaly detection are related to novelty detection <ref type="bibr" target="#b40">[41]</ref> which has been widely studied in computer vision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53]</ref> and applied to video analysis <ref type="bibr" target="#b9">[10]</ref>, remote sensing <ref type="bibr" target="#b35">[36]</ref>, etc. With the advance in GANs <ref type="bibr" target="#b16">[17]</ref>, SOTA methods perform anomaly detection by generating realistic normal images during training <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref>. <ref type="bibr" target="#b11">[12]</ref> proposes to search the latent space of the generator for detecting anomalies. <ref type="bibr" target="#b40">[41]</ref> introduces latent-spacesampling-based network with information-negative mining while <ref type="bibr" target="#b29">[30]</ref> proposes normality score function based on capsule network's activation and reconstruction error. <ref type="bibr" target="#b1">[2]</ref> proposes a deep autoencoder that learns the distribution of latent representation through autoregressive procedure. Unlike <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b54">55]</ref> where anomalous training images are used for anomaly detection, CAVGA does not need anomalous training images.   During testing, we localize the anomaly from the areas of the image that the attention map does not focus on.</p><p>Convolutional latent variable Variational Autoencoder (VAE) <ref type="bibr" target="#b22">[23]</ref> is a generative model widely used for anomaly detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>. The loss function of training a vanilla VAE can be formulated as:</p><formula xml:id="formula_1">L = L R (x,x) + KL(q φ (z|x)||p θ (z|x)),<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">L R (x,x) = −1 N N i=1 x i log(x i )+(1−x i )log(1−x i )</formula><p>is the reconstruction loss between the input (x) and reconstructed images (x), and N is the total number of images. The posterior p θ (z|x) is modeled using a standard Gaussian distribution prior p(z) with the help of Kullback-Liebler (KL) divergence through q φ (z|x). Since the vanilla VAE results in blurry reconstruction <ref type="bibr" target="#b25">[26]</ref>, we use a discriminator (D(.)) to improve the stability of the training and generate sharper reconstructed imagesx using adversarial learning <ref type="bibr" target="#b33">[34]</ref> formulated as follows:</p><formula xml:id="formula_3">L adv = − 1 N N i=1 log(D(x i )) + log(1 − D(x i ))<label>(2)</label></formula><p>Unlike traditional autoencoders <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> where the latent variable is flattened, inspired from <ref type="bibr" target="#b3">[4]</ref>, we use a convolutional latent variable to preserve the spatial relation between the input and the latent variable.</p><p>Attention expansion loss L ae The main contribution of our work involves using supervision on attention maps to spatially localize the anomaly in the image. Most methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">53</ref>] employ a thresholded pixel-wise difference between the reconstructed image and the input image to localize the anomaly where the threshold is determined by using anomalous training images. However, CAVGA u learns to localize the anomaly using an attention map reflected through an endto-end training process without the need of any anomalous training images. We use the feature representation of the latent variable z to compute the attention map (A). A is computed using Grad-CAM <ref type="bibr" target="#b48">[49]</ref> </p><formula xml:id="formula_4">such that A i,j ∈ [0, 1], where A i,j is the (i, j) element of A.</formula><p>Intuitively, A obtained from feature maps focuses on the regions of the image based on the activation of neurons in the feature maps and its respective importance <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b59">60]</ref>. Due to the lack of prior knowledge about the anomaly, in general, humans need to look at the entire image to identify anomalous regions. We use this notion to learn the feature representation of the entire normal image by proposing an attention expansion loss, where we encourage the network to generate an attention map covering all the normal regions. This attention expansion loss for each image L ae,1 is defined as:</p><formula xml:id="formula_5">L ae,1 = 1 |A| i,j (1 − A i,j )<label>(3)</label></formula><p>where |A| is the total number of elements in A. The final attention expansion loss L ae is the average of L ae,1 over the N images. Since the idea of attention mechanisms involves locating the most salient regions in the image <ref type="bibr" target="#b28">[29]</ref> which typically does not cover the entire image, we use L ae as an additional supervision on the network, such that the trained network generates an attention map that covers all the normal regions. <ref type="figure">Fig. 1</ref> (i) (a) shows that before using L ae i.e. training CAVGA u only with adversarial learning (L adv + L) does not encode all the normal regions into the latent variable, and that the attention map fails to cover the entire image, which is overcome after using L ae . Furthermore, supervising on attention maps prevents the trained model to make inference based on incorrect areas and also alleviates the need of using large amount of training data as shown in <ref type="bibr" target="#b27">[28]</ref>, which is not explicitly enforced in existing methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>We form the final objective function L f inal below:</p><formula xml:id="formula_6">L f inal = w r L + w adv L adv + w ae L ae ,<label>(4)</label></formula><p>where w r , w adv , and w ae are empirically set as 1, 1, and 0.01 respectively. During testing, we feed an image x test into the encoder followed by the decoder, which reconstructs an imagex test . As defined in <ref type="bibr" target="#b47">[48]</ref>, we compute the pixel-wise difference betweenx test and x test as the anomalous score s a . Intuitively, if x test is drawn from the learnt distribution of z, then s a is small. Without using any anomalous training images in the unsupervised setting, we normalize s a between [0, 1] and empirically set 0.5 as the threshold to detect an image as anomalous. The attention map A test is computed from z using Grad-CAM and is inverted (1 -A test ) to obtain an anomalous attention map which localizes the anomaly. Here, 1 refers to a matrix of all ones with the same dimensions as A test . We empirically choose 0.5 as the threshold on the anomalous attention map to evaluate the localization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weakly Supervised Approach: CAVGA w</head><p>CAVGA u can be further extended to a weakly supervised setting (denoted as CAVGA w ) where we explore the possibility of using few anomalous training images to improve the performance of anomaly localization. Given the labels of the anomalous and normal images without the pixel-wise annotation of the anomaly during training, we modify CAVGA u by introducing a binary classifier C at the output of z as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (b) and train C using the binary cross entropy loss L bce . Given an image x and its ground truth label y, we define p ∈ {c a , c n } as the prediction of C, where c a and c n are anomalous and normal classes respectively. From <ref type="figure" target="#fig_1">Fig. 2 (b)</ref> we clone z into a new tensor, flatten it to form a fully connected layer z f c , and add a 2-node output layer to form C. z and z f c share parameters. Flattening z f c enables a higher magnitude of gradient backpropagation from p <ref type="bibr" target="#b48">[49]</ref>.</p><p>Complementary guided attention loss L cga Although, attention maps generated from a trained classifier have been used in weakly supervised semantic segmentation tasks <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b48">49]</ref>, to the best of our knowledge, we are the first to propose supervision on attention maps for anomaly localization in the weakly supervised setting. Since the attention map depends on the performance of C <ref type="bibr" target="#b27">[28]</ref>, we propose the complementary guided attention loss L cga based on C's prediction to improve anomaly localization. We use Grad-CAM to compute the attention map for the anomalous class A ca x and the attention map for the normal class A cn x on the normal image x (y = c n ). Using A ca x and A cn x , we propose L cga where we minimize the areas covered by A ca x but simultaneously enforce A cn x to cover the entire normal image. Since the attention map is computed by backpropagating the gradients from p, any incorrect p would generate an undesired attention map. This would lead to the network learning to focus on erroneous areas of the image during training, which we avoid using L cga . We compute L cga only for the normal images correctly classified by the classifier i.e. if p = y = c n . We define L cga,1 , the complementary guided attention loss for each image, in the weakly supervised setting as:</p><formula xml:id="formula_7">L cga,1 = 1 (p = y = c n ) |A cn x | i,j (1 − (A cn x ) i,j + (A ca x ) i,j ),<label>(5)</label></formula><p>where 1 (·) is an indicator function. L cga is the average of L cga,1 over the N images. Our final objective function L f inal is defined as: where w r , w adv , w c , and w cga are empirically set as 1, 1, 0.001, and 0.01 respectively. During testing, we use C to predict the input image x test as anomalous or normal. The anomalous attention map A test of x test is computed when y = c a . We use the same evaluation method as that in Sec. 3.1 for anomaly localization.</p><formula xml:id="formula_8">L f inal = w r L + w adv L adv + w c L bce + w cga L cga ,<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Benchmark datasets: We evaluate CAVGA on the MVTAD <ref type="bibr" target="#b4">[5]</ref>, mSTC <ref type="bibr" target="#b30">[31]</ref> and LAG <ref type="bibr" target="#b28">[29]</ref> datasets for anomaly localization, and the MVTAD, mSTC, LAG, MNIST <ref type="bibr" target="#b26">[27]</ref>, CIFAR-10 <ref type="bibr" target="#b24">[25]</ref> and Fashion-MNIST <ref type="bibr" target="#b56">[57]</ref> datasets for anomaly detection. Since STC dataset <ref type="bibr" target="#b30">[31]</ref> is designed for video instead of image anomaly detection, we extract every 5 th frame of the video from each scene for training and testing without using any temporal information. We term the modified STC dataset as mSTC and summarize the experimental settings in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Baseline methods: For anomaly localization, we compare CAVGA with AVID <ref type="bibr" target="#b46">[47]</ref>, AE L2 <ref type="bibr" target="#b5">[6]</ref>, AE SSIM <ref type="bibr" target="#b5">[6]</ref>, AnoGAN <ref type="bibr" target="#b47">[48]</ref>, CNN feature dictionary (CN-NFD) <ref type="bibr" target="#b36">[37]</ref>, texture inspection (TI) <ref type="bibr" target="#b7">[8]</ref>, γ-VAE grad <ref type="bibr" target="#b12">[13]</ref> (denoted as γ-VAE g ), LSA <ref type="bibr" target="#b1">[2]</ref>, ADVAE <ref type="bibr" target="#b31">[32]</ref> and variation model (VM) <ref type="bibr" target="#b51">[52]</ref> based approaches on the MVTAD and mSTC datasets. Since <ref type="bibr" target="#b12">[13]</ref> does not provide the code for their method, we adapt the code from <ref type="bibr" target="#b0">[1]</ref> and report its best result using our experimental settings. We also compare CAVGA u with CAM <ref type="bibr" target="#b59">[60]</ref>, GBP <ref type="bibr" target="#b50">[51]</ref>, Smooth-Grad <ref type="bibr" target="#b49">[50]</ref> and Patho-GAN <ref type="bibr" target="#b53">[54]</ref> on the LAG dataset. In addition, we compare CAVGA u with LSA <ref type="bibr" target="#b1">[2]</ref>, OCGAN <ref type="bibr" target="#b40">[41]</ref>, ULSLM <ref type="bibr" target="#b55">[56]</ref>, CapsNet PP-based and Cap-sNet RE-based <ref type="bibr" target="#b29">[30]</ref> (denoted as CapsNet PP and CapsNet RE ), AnoGAN <ref type="bibr" target="#b47">[48]</ref>, ADGAN <ref type="bibr" target="#b11">[12]</ref>, and β-VAE [21] on the MNIST, CIFAR-10 and Fashion-MNIST datasets for anomaly detection.</p><p>Architecture details: Based on the framework in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>, we use the convolution layers of ResNet-18 <ref type="bibr" target="#b18">[19]</ref> as our encoder pretrained from ImageNet <ref type="bibr" target="#b44">[45]</ref> and finetuned on each category / scene individually. Inspired from <ref type="bibr" target="#b8">[9]</ref>, we propose to use the residual generator as our residual decoder by modifying it with a convolution layer interleaved between two upsampling layers. The skip connection added from the output of the upsampling layer to the output of the convolution layer, increases mutual information between observations and latent variable and also avoids latent variable collapse <ref type="bibr" target="#b13">[14]</ref>. We use the discriminator of DC-GAN [42] pretrained on the Celeb-A dataset <ref type="bibr" target="#b32">[33]</ref> and finetuned on our data as our discriminator and term this network as CAVGA-R. For fair comparisons with the baseline approaches in terms of network architecture, we use the discriminator and generator of DC-GAN pretrained on the Celeb-A dataset as our encoder and decoder respectively. We keep the same discriminator as discussed previously and term this network as CAVGA-D. CAVGA-D u and CAVGA-R u are termed as CAVGA u in the unsupervised setting, and CAVGA-D w and CAVGA-R w as CAVGA w in weakly supervised setting respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and evaluation:</head><p>For anomaly localization and detection on the MVTAD, mSTC and LAG datasets, the network is trained only on normal images in the unsupervised setting. In the weakly supervised setting, since none of the baseline methods provide the number of anomalous training images they use to compute the threshold, we randomly choose 2% of the anomalous images along with all the normal training images for training. On the MNIST, CIFAR-10 and Fashion-MNIST datasets, we follow the same procedure as defined in <ref type="bibr" target="#b11">[12]</ref> (training/testing uses single class as normal and the rest of the classes as anomalous. We train CAVGA-D u using this normal class). For anomaly localization, we show the AuROC <ref type="bibr" target="#b4">[5]</ref> and the Intersection-over-Union (IoU) between the generated attention map and the ground truth. Following <ref type="bibr" target="#b4">[5]</ref>, we use the mean of accuracy of correctly classified anomalous images and normal images to evaluate the performance of anomaly detection on both the normal and anomalous images on the MVTAD, mSTC and LAG datasets. On the MNIST, CIFAR-10, and Fashion-MNIST datasets, same as <ref type="bibr" target="#b11">[12]</ref>, we use AuROC for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We use the cell color in the quantitative result tables to denote the performance ranking in that row, where darker cell color means better performance.</p><p>Performance on anomaly localization: <ref type="figure" target="#fig_3">Fig. 3 (a)</ref> shows the qualitative results and <ref type="table" target="#tab_2">Table 3</ref> shows that CAVGA u localizes the anomaly better compared to the baselines on the MVTAD dataset. CAVGA-D u outperforms the best performing baseline method (γ-VAE g ) in mean IoU by 5%. Most baselines use anomalous training images to compute class-specific threshold to localize anomalies. Needing no anomalous training images, CAVGA-D u still outperforms all the mentioned baselines in mean IoU. In terms of mean AuROC, CAVGA-D u outperforms CNNFD, TI and VM by 9%, 12% and 10% respectively and achieves comparable results with best baseline method. <ref type="table" target="#tab_2">Table 3</ref> also shows that CAVGA-D w outperforms CAVGA-D u by 22% and 8% on mean IoU and mean AuROC respectively. CAVGA-D w also outperforms the baselines in mean AuROC. <ref type="figure" target="#fig_5">Fig. 4</ref> illustrates that one challenge in anomaly localization is the low contrast between the anomalous regions and their background. In such scenarios, although still outperforming the baselines, CAVGA does not localize the anomaly well.   <ref type="table">Table 4</ref> shows that CAVGA also outperforms the baseline methods in mean IoU and mean AuROC on the mSTC dataset. <ref type="table" target="#tab_4">Table 5</ref> shows that CAVGA outperforms the most competitive baseline Patho-GAN <ref type="bibr" target="#b53">[54]</ref> by 16% in IoU on the LAG dataset. CAVGA is practically reasonable to train on a single GTX 1080Ti GPU, having comparable training and testing time with baseline methods. <ref type="table">Table 4</ref>: Performance comparison of anomaly localization in IoU and its mean (IoU) along with anomaly detection in terms of mean of accuracy of correctly classified anomalous images and normal images on the mSTC dataset for each scene ID s i . For anomaly localization, we also list the mean AuROC (AuROC)    Performance on anomaly detection: <ref type="table" target="#tab_5">Table 6</ref> shows that CAVGA u outperforms the baselines in the mean of accuracy of correctly classified anomalous images and normal images on the MVTAD dataset. CAVGA-D u outperforms the best performing baseline (γ-VAE g ) in mean of classification accuracy by 1.3%. <ref type="table">Table 4</ref> and <ref type="table" target="#tab_4">Table 5</ref> show that CAVGA outperforms the baseline methods in classification accuracy on both the mSTC and LAG datasets by 2.6% and 1.1% respectively. Furthermore, <ref type="table" target="#tab_6">Table 7</ref> shows that CAVGA-D u outperforms all the baselines in mean AuROC in the unsupervised setting on the MNIST, CIFAR-10 and Fashion-MNIST datasets. CAVGA-D u also outperforms MemAE <ref type="bibr" target="#b15">[16]</ref> and β-VAE [21] by 1.1% and 8% on MNIST and by 21% and 38% on CIFAR-10 datasets respectively. CAVGA-D u also outperforms all the listed baselines in mean AuROC on the Fashion-MNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ablation Study</head><p>All the ablation studies are performed on 15 categories on the MVTAD dataset, of which 5 are reported here. The mean of all 15 categories is shown in <ref type="table" target="#tab_7">Table  8</ref>. We illustrate the effectiveness of the convolutional z in CAVGA, L ae in the unsupervised setting, and L cga in the weakly supervised setting. The qualitative results are shown in <ref type="figure" target="#fig_6">Fig. 5</ref>. The column IDs to refer to the columns in <ref type="table" target="#tab_7">Table 8</ref>.</p><p>Effect of convolutional latent variable z: To show the effectiveness of the convolutional z, we flatten the output of the encoder of CAVGA-R u and CAVGA-R w , and connect it to a fully connected layer as latent variable. Following <ref type="bibr" target="#b5">[6]</ref>, the dimension of latent variable is chosen as 100. We call these network as CAVGA-R * u and CAVGA-R * w in the unsupervised and weakly supervised settings respectively. In the unsupervised setting, we train CAVGA-R u and CAVGA-R * u using L + L adv as our objective function and compute the anomalous attention  map from the feature map of the latent variable during inference. Similarly, in the weakly supervised setting, we train CAVGA-R w and CAVGA-R * w using L + L adv + L bce as our objective function and compute the anomalous attention map from the classifier's prediction during inference. Comparing column c 1 with  <ref type="table" target="#tab_7">Table 8</ref>, we observe that preserving the spatial relation of the input and latent variable through the convolutional z improves the IoU in anomaly localization without the use of L ae in the unsupervised setting and L cga in the weakly supervised setting. Furthermore, comparing column c 2 with c 4 and c 6 with c 8 in <ref type="table" target="#tab_7">Table 8</ref>, we observe that using convolutional z in CAVGA-R u and CAVGA-R w outperforms using a flattened latent variable even with the help of L ae in the unsupervised setting and L cga in the weakly supervised setting.</p><p>Effect of attention expansion loss L ae : To test the effectiveness of using L ae in the unsupervised setting, we train CAVGA-R * u and CAVGA-R u with eq. 4. During inference, the anomalous attention map is computed to localize the anomaly. Comparing column c 1 with c 2 and c 3 with c 4 in <ref type="table" target="#tab_7">Table 8</ref>, we observe that L ae enhances the IoU regardless of a flattened or convolutional latent variable.</p><p>Effect of complementary guided attention loss L cga : We show the effectiveness of L cga by training CAVGA-R * w and CAVGA-R w using eq. 6. Comparing column c 5 with c 6 and c 7 with c 8 in <ref type="table" target="#tab_7">Table 8</ref>, we find that using L cga enhances the IoU regardless of a flattened or convolutional latent variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose an end-to-end convolutional adversarial variational autoencoder using guided attention which is a novel use of this technique for anomaly localization. Applicable to different network architectures, our attention expansion loss and complementary guided attention loss improve the performance of anomaly localization in the unsupervised and weakly supervised (with only 2% extra anomalous images for training) settings respectively. We quantitatively and qualitatively show that CAVGA outperforms the state-of-the-art (SOTA) anomaly localization methods on the MVTAD, mSTC and LAG datasets. We also show CAVGA's ability to outperform SOTA anomaly detection methods on the MV-TAD, mSTC, LAG, MNIST, Fashion-MNIST and CIFAR-10 datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3</head><label></label><figDesc>Proposed Approach: CAVGA 3.1 Unsupervised Approach: CAVGA u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 (</head><label>2</label><figDesc>a) illustrates CAVGA in the unsupervised setting (denoted as CAVGA u ). CAVGA u comprises of a convolutional latent variable to preserve the spatial information between the input and latent variable. Since attention maps obtained from feature maps illustrate the regions of the image responsible for specific</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>(a) The framework of CAVGA u where the attention expansion loss L ae guides the attention map A computed from the latent variable z to cover the entire normal image. (b) Illustration of CAVGA w with the complementary guided attention loss L cga to minimize the anomalous attention A ca x and expand the normal attention A cn x for the normal images correctly predicted by the classifier activation of neurons in the feature maps [58], we propose an attention expansion loss such that the feature representation of the latent variable encodes all the normal regions. This loss encourages the attention map generated from the latent variable to cover the entire normal training image as illustrated in Fig. 1 (i) (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative results on (a) MVTAD &amp; (b) mSTC datasets respectively. The anomalous attention map (in red) depicts the localization of the anomaly</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 (</head><label>3</label><figDesc>b) illustrates the qualitative results and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Examples of incorrect localization of the anomaly on the MVTAD dataset by CAVGA-R u and CAVGA-R w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative results of the ablation study to illustrate the performance of the anomaly localization on the MVTAD dataset c 3 and c 5 with c 7 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between CAVGA and other anomaly localization methods in the unsupervised setting in terms of the working properties. Among all the listed methods, only CAVGA satisfies all the listed properties</figDesc><table><row><cell cols="3">Does the method satisfy each property? [3, 48] [4] [47] [54] [13, 32] CAVGA</cell></row><row><cell>[6, 43]</cell><cell>[50]</cell><cell>[2]</cell></row><row><cell>not using anomalous training images</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Our experimental settings. Notations: u: unsupervised; w: weakly supervised; D M : MNIST<ref type="bibr" target="#b26">[27]</ref>; D F : Fashion-MNIST<ref type="bibr" target="#b56">[57]</ref>; D C : CIFAR-10<ref type="bibr" target="#b24">[25]</ref> </figDesc><table><row><cell>property \ dataset</cell><cell cols="2">MVTAD [5]</cell><cell cols="2">mSTC [31]</cell><cell cols="4">LAG [29] D M D F D C</cell></row><row><cell>setting</cell><cell>u</cell><cell>w</cell><cell>u</cell><cell>w</cell><cell>u</cell><cell>u</cell><cell>u</cell><cell>u</cell></row><row><cell># total classes</cell><cell>15</cell><cell>15</cell><cell>13</cell><cell>13</cell><cell>1</cell><cell>10</cell><cell cols="2">10 10</cell></row><row><cell># normal training images</cell><cell cols="4">3629 3629 244875 244875</cell><cell>2632</cell><cell cols="3">∼6k 6k 5k</cell></row><row><cell># anomalous training images</cell><cell>0</cell><cell>35</cell><cell>0</cell><cell>1763</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell># normal testing images</cell><cell>467</cell><cell>467</cell><cell cols="2">21147 21147</cell><cell>800</cell><cell cols="3">∼1k 1k 1k</cell></row><row><cell cols="5"># anomalous testing images 1223 1223 86404 86404</cell><cell>2392</cell><cell cols="3">∼9k 9k 9k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of anomaly localization in category-specific IoU, mean IoU (IoU), and mean AuROC (AuROC) on the MVTAD dataset. The darker cell color indicates better performance ranking in each row</figDesc><table><row><cell cols="12">Category AVID AESSIM AEL2 AnoGAN γ-VAEg LSA ADVAE CAVGA CAVGA CAVGA CAVGA</cell></row><row><cell></cell><cell>[47]</cell><cell>[6]</cell><cell>[6]</cell><cell>[48]</cell><cell>[13]</cell><cell>[2]</cell><cell>[32]</cell><cell>-Du</cell><cell>-Ru</cell><cell>-Dw</cell><cell>-Rw</cell></row><row><cell>Bottle</cell><cell>0.28</cell><cell>0.15</cell><cell>0.22</cell><cell>0.05</cell><cell cols="3">0.27 0.27 0.27</cell><cell>0.30</cell><cell>0.34</cell><cell>0.36</cell><cell>0.39</cell></row><row><cell cols="2">Hazelnut 0.54</cell><cell>0.00</cell><cell>0.41</cell><cell>0.02</cell><cell cols="3">0.63 0.41 0.44</cell><cell>0.44</cell><cell>0.51</cell><cell>0.58</cell><cell>0.79</cell></row><row><cell>Capsule</cell><cell>0.21</cell><cell>0.09</cell><cell>0.11</cell><cell>0.04</cell><cell cols="3">0.24 0.22 0.11</cell><cell>0.25</cell><cell>0.31</cell><cell>0.38</cell><cell>0.41</cell></row><row><cell cols="2">Metal Nut 0.05</cell><cell>0.01</cell><cell>0.26</cell><cell>0.00</cell><cell cols="3">0.22 0.38 0.49</cell><cell>0.39</cell><cell>0.45</cell><cell>0.46</cell><cell>0.46</cell></row><row><cell>Leather</cell><cell>0.32</cell><cell>0.34</cell><cell>0.67</cell><cell>0.34</cell><cell cols="3">0.41 0.77 0.24</cell><cell>0.76</cell><cell>0.79</cell><cell>0.80</cell><cell>0.84</cell></row><row><cell>Pill</cell><cell>0.11</cell><cell>0.07</cell><cell>0.25</cell><cell>0.17</cell><cell cols="3">0.48 0.18 0.18</cell><cell>0.34</cell><cell>0.40</cell><cell>0.44</cell><cell>0.53</cell></row><row><cell>Wood</cell><cell>0.14</cell><cell>0.36</cell><cell>0.29</cell><cell>0.14</cell><cell cols="3">0.45 0.41 0.14</cell><cell>0.56</cell><cell>0.59</cell><cell>0.61</cell><cell>0.66</cell></row><row><cell>Carpet</cell><cell>0.25</cell><cell>0.69</cell><cell>0.38</cell><cell>0.34</cell><cell cols="3">0.79 0.76 0.10</cell><cell>0.71</cell><cell>0.73</cell><cell>0.70</cell><cell>0.81</cell></row><row><cell>Tile</cell><cell>0.09</cell><cell>0.04</cell><cell>0.23</cell><cell>0.08</cell><cell cols="3">0.38 0.32 0.23</cell><cell>0.31</cell><cell>0.38</cell><cell>0.47</cell><cell>0.81</cell></row><row><cell>Grid</cell><cell>0.51</cell><cell>0.88</cell><cell>0.83</cell><cell>0.04</cell><cell cols="3">0.36 0.20 0.02</cell><cell>0.32</cell><cell>0.38</cell><cell>0.42</cell><cell>0.55</cell></row><row><cell>Cable</cell><cell>0.27</cell><cell>0.01</cell><cell>0.05</cell><cell>0.01</cell><cell cols="3">0.26 0.36 0.18</cell><cell>0.37</cell><cell>0.44</cell><cell>0.49</cell><cell>0.51</cell></row><row><cell cols="2">Transistor 0.18</cell><cell>0.01</cell><cell>0.22</cell><cell>0.08</cell><cell cols="3">0.44 0.21 0.30</cell><cell>0.30</cell><cell>0.35</cell><cell>0.38</cell><cell>0.45</cell></row><row><cell cols="2">Toothbrush 0.43</cell><cell>0.08</cell><cell>0.51</cell><cell>0.07</cell><cell cols="3">0.37 0.48 0.14</cell><cell>0.54</cell><cell>0.57</cell><cell>0.60</cell><cell>0.63</cell></row><row><cell>Screw</cell><cell>0.22</cell><cell>0.03</cell><cell>0.34</cell><cell>0.01</cell><cell cols="3">0.38 0.38 0.17</cell><cell>0.42</cell><cell>0.48</cell><cell>0.51</cell><cell>0.66</cell></row><row><cell>Zipper</cell><cell>0.25</cell><cell>0.10</cell><cell>0.13</cell><cell>0.01</cell><cell cols="3">0.17 0.14 0.06</cell><cell>0.20</cell><cell>0.26</cell><cell>0.29</cell><cell>0.31</cell></row><row><cell>IoU</cell><cell>0.26</cell><cell>0.19</cell><cell>0.33</cell><cell>0.09</cell><cell cols="3">0.39 0.37 0.20</cell><cell>0.41</cell><cell>0.47</cell><cell>0.50</cell><cell>0.59</cell></row><row><cell>AuROC</cell><cell>0.78</cell><cell>0.87</cell><cell>0.82</cell><cell>0.74</cell><cell cols="3">0.86 0.79 0.86</cell><cell>0.85</cell><cell>0.89</cell><cell>0.92</cell><cell>0.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison of anomaly localization in IoU along with anomaly detection in terms of classification accuracy on the LAG dataset<ref type="bibr" target="#b28">[29]</ref> </figDesc><table><row><cell cols="6">Task \ Method CAM [60] GBP [51] SmoothGrad [50] Patho-GAN [54] CAVGA-Du</cell></row><row><cell>Localization</cell><cell>0.13</cell><cell>0.09</cell><cell>0.14</cell><cell>0.37</cell><cell>0.43</cell></row><row><cell>Detection</cell><cell>0.68</cell><cell>0.84</cell><cell>0.79</cell><cell>0.89</cell><cell>0.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The mean of accuracy of correctly classified anomalous images and normal images in anomaly detection on the MVTAD dataset</figDesc><table><row><cell cols="11">Category AVID AESSIM AEL2 AnoGAN γ-VAEg LSA CAVGA CAVGA CAVGA CAVGA</cell></row><row><cell></cell><cell>[47]</cell><cell>[6]</cell><cell>[6]</cell><cell>[48]</cell><cell>[13]</cell><cell>[2]</cell><cell>-Du</cell><cell>-Ru</cell><cell>-Dw</cell><cell>-Rw</cell></row><row><cell>Bottle</cell><cell>0.88</cell><cell>0.88</cell><cell>0.80</cell><cell>0.69</cell><cell>0.86</cell><cell>0.86</cell><cell>0.89</cell><cell>0.91</cell><cell>0.93</cell><cell>0.96</cell></row><row><cell>Hazelnut</cell><cell>0.86</cell><cell>0.54</cell><cell>0.88</cell><cell>0.50</cell><cell>0.74</cell><cell>0.80</cell><cell>0.84</cell><cell>0.87</cell><cell>0.90</cell><cell>0.92</cell></row><row><cell>Capsule</cell><cell>0.85</cell><cell>0.61</cell><cell>0.62</cell><cell>0.58</cell><cell>0.86</cell><cell>0.71</cell><cell>0.83</cell><cell>0.87</cell><cell>0.89</cell><cell>0.93</cell></row><row><cell cols="2">Metal Nut 0.63</cell><cell>0.54</cell><cell>0.73</cell><cell>0.50</cell><cell>0.78</cell><cell>0.67</cell><cell>0.67</cell><cell>0.71</cell><cell>0.81</cell><cell>0.88</cell></row><row><cell>Leather</cell><cell>0.58</cell><cell>0.46</cell><cell>0.44</cell><cell>0.52</cell><cell>0.71</cell><cell>0.70</cell><cell>0.71</cell><cell>0.75</cell><cell>0.80</cell><cell>0.84</cell></row><row><cell>Pill</cell><cell>0.86</cell><cell>0.60</cell><cell>0.62</cell><cell>0.62</cell><cell>0.80</cell><cell>0.85</cell><cell>0.88</cell><cell>0.91</cell><cell>0.93</cell><cell>0.97</cell></row><row><cell>Wood</cell><cell>0.83</cell><cell>0.83</cell><cell>0.74</cell><cell>0.68</cell><cell>0.89</cell><cell>0.75</cell><cell>0.85</cell><cell>0.88</cell><cell>0.89</cell><cell>0.89</cell></row><row><cell>Carpet</cell><cell>0.70</cell><cell>0.67</cell><cell>0.50</cell><cell>0.49</cell><cell>0.67</cell><cell>0.74</cell><cell>0.73</cell><cell>0.78</cell><cell>0.80</cell><cell>0.82</cell></row><row><cell>Tile</cell><cell>0.66</cell><cell>0.52</cell><cell>0.77</cell><cell>0.51</cell><cell>0.81</cell><cell>0.70</cell><cell>0.70</cell><cell>0.72</cell><cell>0.81</cell><cell>0.86</cell></row><row><cell>Grid</cell><cell>0.59</cell><cell>0.69</cell><cell>0.78</cell><cell>0.51</cell><cell>0.83</cell><cell>0.54</cell><cell>0.75</cell><cell>0.78</cell><cell>0.79</cell><cell>0.81</cell></row><row><cell>Cable</cell><cell>0.64</cell><cell>0.61</cell><cell>0.56</cell><cell>0.53</cell><cell>0.56</cell><cell>0.61</cell><cell>0.63</cell><cell>0.67</cell><cell>0.86</cell><cell>0.97</cell></row><row><cell cols="2">Transistor 0.58</cell><cell>0.52</cell><cell>0.71</cell><cell>0.67</cell><cell>0.70</cell><cell>0.50</cell><cell>0.73</cell><cell>0.75</cell><cell>0.80</cell><cell>0.89</cell></row><row><cell cols="2">Toothbrush 0.73</cell><cell>0.74</cell><cell>0.98</cell><cell>0.57</cell><cell>0.89</cell><cell>0.89</cell><cell>0.91</cell><cell>0.97</cell><cell>0.96</cell><cell>0.99</cell></row><row><cell>Screw</cell><cell>0.66</cell><cell>0.51</cell><cell>0.69</cell><cell>0.35</cell><cell>0.71</cell><cell>0.75</cell><cell>0.77</cell><cell>0.78</cell><cell>0.79</cell><cell>0.79</cell></row><row><cell>Zipper</cell><cell>0.84</cell><cell>0.80</cell><cell>0.80</cell><cell>0.59</cell><cell>0.67</cell><cell>0.88</cell><cell>0.87</cell><cell>0.94</cell><cell>0.95</cell><cell>0.96</cell></row><row><cell>mean</cell><cell>0.73</cell><cell>0.63</cell><cell>0.71</cell><cell>0.55</cell><cell>0.77</cell><cell>0.73</cell><cell>0.78</cell><cell>0.82</cell><cell>0.86</cell><cell>0.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison of anomaly detection in terms of AuROC and mean AuROC with the SOTA methods on MNIST (D M ) and CIFAR-10 (D C ) datasets . We also report the mean AuROC on Fashion-MNIST (D F ) dataset</figDesc><table><row><cell cols="10">Dataset Class γ-VAEg LSA OCGAN ULSLM CapsNetPP CapsNetRE AnoGAN ADGAN CAVGA</cell></row><row><cell></cell><cell></cell><cell>[13]</cell><cell>[2]</cell><cell>[41]</cell><cell>[56]</cell><cell>[30]</cell><cell>[30]</cell><cell>[48]</cell><cell>[12]</cell><cell>-Du</cell></row><row><cell></cell><cell>0</cell><cell cols="3">0.991 0.993 0.998</cell><cell>0.991</cell><cell>0.998</cell><cell>0.947</cell><cell>0.990</cell><cell>0.999</cell><cell>0.994</cell></row><row><cell></cell><cell>1</cell><cell cols="3">0.996 0.999 0.999</cell><cell>0.972</cell><cell>0.990</cell><cell>0.907</cell><cell>0.998</cell><cell>0.992</cell><cell>0.997</cell></row><row><cell></cell><cell>2</cell><cell cols="3">0.983 0.959 0.942</cell><cell>0.919</cell><cell>0.984</cell><cell>0.970</cell><cell>0.888</cell><cell>0.968</cell><cell>0.989</cell></row><row><cell></cell><cell>3</cell><cell cols="3">0.978 0.966 0.963</cell><cell>0.943</cell><cell>0.976</cell><cell>0.949</cell><cell>0.913</cell><cell>0.953</cell><cell>0.983</cell></row><row><cell></cell><cell>4</cell><cell cols="3">0.976 0.956 0.975</cell><cell>0.942</cell><cell>0.935</cell><cell>0.872</cell><cell>0.944</cell><cell>0.960</cell><cell>0.977</cell></row><row><cell cols="2">D M [27] 5</cell><cell cols="3">0.972 0.964 0.980</cell><cell>0.872</cell><cell>0.970</cell><cell>0.966</cell><cell>0.912</cell><cell>0.955</cell><cell>0.968</cell></row><row><cell></cell><cell>6</cell><cell cols="3">0.993 0.994 0.991</cell><cell>0.988</cell><cell>0.942</cell><cell>0.909</cell><cell>0.925</cell><cell>0.980</cell><cell>0.988</cell></row><row><cell></cell><cell>7</cell><cell cols="3">0.981 0.980 0.981</cell><cell>0.939</cell><cell>0.987</cell><cell>0.934</cell><cell>0.964</cell><cell>0.950</cell><cell>0.986</cell></row><row><cell></cell><cell>8</cell><cell cols="3">0.980 0.953 0.939</cell><cell>0.960</cell><cell>0.993</cell><cell>0.929</cell><cell>0.883</cell><cell>0.959</cell><cell>0.988</cell></row><row><cell></cell><cell>9</cell><cell cols="3">0.967 0.981 0.981</cell><cell>0.967</cell><cell>0.990</cell><cell>0.871</cell><cell>0.958</cell><cell>0.965</cell><cell>0.991</cell></row><row><cell></cell><cell cols="4">mean 0.982 0.975 0.975</cell><cell>0.949</cell><cell>0.977</cell><cell>0.925</cell><cell>0.937</cell><cell>0.968</cell><cell>0.986</cell></row><row><cell></cell><cell>0</cell><cell cols="3">0.702 0.735 0.757</cell><cell>0.740</cell><cell>0.622</cell><cell>0.371</cell><cell>0.610</cell><cell>0.661</cell><cell>0.653</cell></row><row><cell></cell><cell>1</cell><cell cols="3">0.663 0.580 0.531</cell><cell>0.747</cell><cell>0.455</cell><cell>0.737</cell><cell>0.565</cell><cell>0.435</cell><cell>0.784</cell></row><row><cell></cell><cell>2</cell><cell cols="3">0.680 0.690 0.640</cell><cell>0.628</cell><cell>0.671</cell><cell>0.421</cell><cell>0.648</cell><cell>0.636</cell><cell>0.761</cell></row><row><cell></cell><cell>3</cell><cell cols="3">0.713 0.542 0.620</cell><cell>0.572</cell><cell>0.675</cell><cell>0.588</cell><cell>0.528</cell><cell>0.488</cell><cell>0.747</cell></row><row><cell></cell><cell>4</cell><cell cols="3">0.770 0.761 0.723</cell><cell>0.678</cell><cell>0.683</cell><cell>0.388</cell><cell>0.670</cell><cell>0.794</cell><cell>0.775</cell></row><row><cell>D C [25]</cell><cell>5</cell><cell cols="3">0.689 0.546 0.620</cell><cell>0.602</cell><cell>0.635</cell><cell>0.601</cell><cell>0.592</cell><cell>0.640</cell><cell>0.552</cell></row><row><cell></cell><cell>6</cell><cell cols="3">0.805 0.751 0.723</cell><cell>0.753</cell><cell>0.727</cell><cell>0.491</cell><cell>0.625</cell><cell>0.685</cell><cell>0.813</cell></row><row><cell></cell><cell>7</cell><cell cols="3">0.588 0.535 0.575</cell><cell>0.685</cell><cell>0.673</cell><cell>0.631</cell><cell>0.576</cell><cell>0.559</cell><cell>0.745</cell></row><row><cell></cell><cell>8</cell><cell cols="3">0.813 0.717 0.820</cell><cell>0.781</cell><cell>0.710</cell><cell>0.410</cell><cell>0.723</cell><cell>0.798</cell><cell>0.801</cell></row><row><cell></cell><cell>9</cell><cell cols="3">0.744 0.548 0.554</cell><cell>0.795</cell><cell>0.466</cell><cell>0.671</cell><cell>0.582</cell><cell>0.643</cell><cell>0.741</cell></row><row><cell></cell><cell cols="4">mean 0.717 0.641 0.656</cell><cell>0.736</cell><cell>0.612</cell><cell>0.531</cell><cell>0.612</cell><cell>0.634</cell><cell>0.737</cell></row><row><cell cols="4">D F [57] mean 0.873 0.876</cell><cell>-</cell><cell>-</cell><cell>0.765</cell><cell>0.679</cell><cell>-</cell><cell>-</cell><cell>0.885</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>The ablation study on 5 randomly chosen categories showing anomaly localization in IoU on the MVTAD dataset. The mean of all 15 categories is reported. CAVGA-R * u and CAVGA-R * w are our base architecture with a flattened z in the unsupervised and weakly supervised settings respectively. "conv z" means using convolutional z</figDesc><table><row><cell cols="9">Method CAVGA CAVGA CAVGA CAVGA CAVGA CAVGA CAVGA CAVGA -R  *  u -R  *  u -Ru -Ru -R  *  w -R  *  -Rw -Rw w + Lae + conv z + conv z + Lcga + conv z + conv z</cell></row><row><cell>Category</cell><cell></cell><cell></cell><cell></cell><cell>+ Lae</cell><cell></cell><cell></cell><cell></cell><cell>+ Lcga</cell></row><row><cell>Column ID</cell><cell>c1</cell><cell>c2</cell><cell>c3</cell><cell>c4</cell><cell>c5</cell><cell>c6</cell><cell>c7</cell><cell>c8</cell></row><row><cell>Bottle</cell><cell>0.24</cell><cell>0.27</cell><cell>0.26</cell><cell>0.33</cell><cell>0.16</cell><cell>0.34</cell><cell>0.28</cell><cell>0.39</cell></row><row><cell>Hazelnut</cell><cell>0.16</cell><cell>0.26</cell><cell>0.31</cell><cell>0.47</cell><cell>0.51</cell><cell>0.76</cell><cell>0.67</cell><cell>0.79</cell></row><row><cell>Capsule</cell><cell>0.09</cell><cell>0.22</cell><cell>0.14</cell><cell>0.31</cell><cell>0.18</cell><cell>0.36</cell><cell>0.27</cell><cell>0.41</cell></row><row><cell>Metal Nut</cell><cell>0.28</cell><cell>0.38</cell><cell>0.34</cell><cell>0.45</cell><cell>0.25</cell><cell>0.38</cell><cell>0.28</cell><cell>0.46</cell></row><row><cell>Leather</cell><cell>0.55</cell><cell>0.71</cell><cell>0.64</cell><cell>0.79</cell><cell>0.72</cell><cell>0.79</cell><cell>0.75</cell><cell>0.84</cell></row><row><cell>mean</cell><cell>0.24</cell><cell>0.34</cell><cell>0.33</cell><cell>0.47</cell><cell>0.39</cell><cell>0.52</cell><cell>0.48</cell><cell>0.60</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments : This work was done when Shashanka was an intern and Kuan-Chuan was a Staff Scientist at Siemens. Shashanka's effort was partially supported by DARPA under Grant D19AP00032.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Code for iterative energy-based projection on a normal data manifold for anomaly localization</title>
		<ptr target="https://qiita.com/kogepan102/items/122b2862ad5a51180656" />
		<imprint>
			<biblScope unit="page" from="2020" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GANomaly: Semisupervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MVTec AD-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A novel and efficient cvaegan-based approach with informative manifold for semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="88903" to="88916" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time texture error detection on textured surfaces with compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Böttger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition and Image Analysis</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="94" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection and localization using maximum sub-sequence search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM/IEEE international workshop on Analysis and retrieval of tracked events and motion in imagery stream</title>
		<meeting>the 4th ACM/IEEE international workshop on Analysis and retrieval of tracked events and motion in imagery stream</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep variational semi-supervised novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurutach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04971</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iterative energy-based projection on a normal data manifold for anomaly localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Combrexelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Avoiding latent variable collapse with generative skip models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2397" to="2405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adversarial autoencoders for anomalous event detection in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dimokranitou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detection of video anomalies using convolutional autoencoders and one-class support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M R</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lazzaretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">XIII Brazilian Congress on Computational Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<title level="m">beta-VAE: Learning basic visual concepts with a constrained variational framework. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adversarial discriminative attention for robust anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Munawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tachibana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parakkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9215" to="9223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention based glaucoma detection: A large-scale database and cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kiringa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yeap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Exploring deep anomaly detection methods based on capsule net. International Conference on Machine Learning 2019 Workshop on Uncertainty and Robustness in Deep Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<title level="m">Towards visually explaining variational autoencoders. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Metric learning for novelty and anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An overview of background modeling for detection of targets and anomalies in hyperspectral remotely sensed imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matteoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Theiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2317" to="2336" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by CNN-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">209</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6752" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised lesion detection in brain CT using bayesian convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khetani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">OCGAN: One-class novelty detection using GANs with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Training adversarial discriminators for cross-channel abnormal event detection in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1896" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Görnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarially learned oneclass classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3379" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Avid: Adversarial visual irregularity detection. In: Asian Conference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="488" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03825</idno>
		<title level="m">SmoothGrad: removing noise by adding noise</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Similarity measures for occlusion, clutter, and illumination invariant object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="148" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ueta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pranata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06924</idno>
		<title level="m">Anomaly detection with adversarial dual autoencoders</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pathology-aware deep network visualization and its application in glaucoma image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="423" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Inductive multi-view semisupervised anomaly detection via probabilistic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muknahallipatna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Knowledge (ICBK)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised learning of the set of local maxima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Galanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<title level="m">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zenati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lecouat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Manek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06222</idno>
		<title level="m">Efficient GAN-based anomaly detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
