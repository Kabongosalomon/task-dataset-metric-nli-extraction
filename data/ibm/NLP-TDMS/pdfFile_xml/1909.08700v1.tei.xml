<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémien</forename><surname>Kocher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Architecture of Fribourg</orgName>
								<orgName type="institution">HES-SO</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">iCoSys Institute ♠ Ecole Polytechnique Fédérale de Lausanne (EPFL) ♥ Swisscom ♣ University of Fribourg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Scuito</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Tarantino</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Lazaridis</surname></persName>
							<email>alexandros.lazaridis|claudiu.musat@swisscom.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Fischer</surname></persName>
							<email>andreas.fischer@hefr.ch</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Architecture of Fribourg</orgName>
								<orgName type="institution">HES-SO</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">iCoSys Institute ♠ Ecole Polytechnique Fédérale de Lausanne (EPFL) ♥ Swisscom ♣ University of Fribourg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
						</author>
						<title level="a" type="main">Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In sequence modeling tasks the token order matters, but this information can be partially lost due to the discretization of the sequence into data points. In this paper, we study the imbalance between the way certain token pairs are included in data points and others are not. We denote this a token order imbalance (TOI) and we link the partial sequence information loss to a diminished performance of the system as a whole, both in text and speech processing tasks. We then provide a mechanism to leverage the full token order information-Alleviated TOI-by iteratively overlapping the token composition of data points. For recurrent networks, we use prime numbers for the batch size to avoid redundancies when building batches from overlapped data points. The proposed method achieved state of the art performance in both text and speech related tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling sequences is a necessity. From time series <ref type="bibr" target="#b3">(Connor et al., 1994;</ref><ref type="bibr" target="#b11">Lane and Brodley, 1999)</ref> to text <ref type="bibr" target="#b23">(Sutskever et al., 2011)</ref> and voice <ref type="bibr" target="#b20">(Robinson, 1994;</ref><ref type="bibr" target="#b25">Vinyals et al., 2012)</ref>, ordered sequences account for a large part of the data we process and learn from. The data are discretized and become, in this paradigm, a list of tokens.</p><p>The key to processing these token sequences is to model the interactions between them. Traditionally <ref type="bibr" target="#b21">(Rosenfeld, 2000)</ref> this has been achieved with statistical methods, like N-grams.</p><p>With the advances in computing power and the rebirth of neural networks, the dominant paradigm has become the use of recurrent neural networks (RNNs) <ref type="bibr" target="#b16">(Mikolov et al., 2010)</ref>.</p><p>The dominance of RNNs has been recently challenged with great success by self-attention based models <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref>. Instead</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contiguous tokens</head><p>Data-point Order knowledge lost <ref type="figure">Figure 1</ref>: The common way of building data points given a dataset of contiguous tokens. Here we illustrate a dataset with a contiguous list of 13 tokens, from which we build 3 data points of 4 tokens each. This process keeps the order of the tokens inside the data points, but loses the order information from token pairs that happen to fall between adjacent data points. of modeling the sequence linearly, Transformerbased models use learned correlations within the input to weight each element of the input sequence based on their relevance for the given task.</p><p>Series discretization. Both RNNs and selfattention models take as input data points-token sequences of a maximum predefined length-and then create outputs for each of them. These tend to be much shorter in size, compared to the size of the full dataset. While for humans time seems to pass continuously, this discretization step is important for the machine understanding of the sequence.</p><p>A side effect of this step is a partial loss of the token order information. As portrayed in <ref type="figure">Figure 1</ref>, we notice that the token order information within a data point are kept. On the other hand, the knowledge about the token order at the boundaries of data points is lost. We name the situation Token Order Imbalance (TOI).</p><p>As the discretization in <ref type="figure">Figure 1</ref> is the current standard of sequence processing, we denote this as standard Token Order Imbalance (TOI). We hypothesize that this loss of information unnecessarily affects the output of the neural network models.</p><p>Alleviated Token Order Imbalance. A first contribution in this work is a mechanism to en-sure that all token sequences are taken into account, i.e. every token pair is included in a data point and does not always fall between two data point boundaries. Thus, all sequence information is available for subsequent processing. The proposed method, denoted Alleviated TOI, employs a token offset in the data point creation to create overlapped data point sequences in order to achieve this effect.</p><p>Batch Creation with Alleviated TOI. A second contribution is a strategy for batch creation when using the proposed Alleviated TOI method. We have observed an unintended data redundancy within batches introduced by the overlapped data point sequences. A strategy for avoiding this data redundancy is surprisingly simple but effective: Always use a prime number for the batch size. The intuition behind the prime batch size is that it ensures a good distribution of the batches over the entire dataset. If used naively, the Alleviated TOI policy leads to very similar data points being selected in a batch, which hinders learning. By decoupling the batch size and the token offset used in the token creation, this negative effect is effectively removed.</p><p>We then compare the Alleviated TOI with the Standard TOI and show that, on the same dataset and with the same computation allocated, the Alleviated TOI yields better results. The novel TOI reduction method is applicable to a multitude of sequence modeling tasks. We show its benefits in both text and voice processing. We employ several basic and state of the art RNNs as well as Transformers and the results are consistent-the additional information provided by the Alleviated TOI improves the final results in the studied tasks.</p><p>For text processing we focus on a well-studied task-language modeling-where capturing the sequence information is crucial. Using Alleviated TOI (P) with the Maximum Over Softmax (MoS) technique on top of a recurrent cell <ref type="bibr" target="#b28">(Yang et al., 2017)</ref> we get the new state of the art on the Penn-Tree-Bank dataset without fine-tuning with 54.58 perplexity on the test set. We also obtain results comparable to the state of the art on speech emotion recognition on the IEMOCAP <ref type="bibr" target="#b1">(Busso et al., 2008</ref>) dataset 1 .</p><p>The paper continues with an overview of the related work in Section 2, a description of the al-leviated TOI mechanism in Section 3 and a detailed description of the batch generation in Section 4. The experimental design follows in Section 5 and the results are detailed and interpreted in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>At the core of our work is the idea that the way that data samples are provided for training a model can affect speed or capabilities of the model. This field is broad and there are several distinct approaches to achieve it. Notable examples include curriculum learning <ref type="bibr" target="#b0">(Bengio et al., 2009</ref>) and self-paced learning <ref type="bibr" target="#b10">(Kumar et al., 2010)</ref>, where data points for training are selected based on a metric of easiness or hardness. In Bayesian approaches <ref type="bibr" target="#b9">(Klein et al., 2016)</ref>, the goal is to create sub-samples of data points, whose traits can be extrapolated as the full dataset.</p><p>Our work thus differs from the aforementioned methods in the fact that we focus on exploiting valuable but overlooked information from sequences of tokens. We change the way data points are generated from token sequences and extend the expressivity of a model by providing an augmented, and well sorted, sequence of data points. This method has a related effect of a randomized-length backpropagation through time (BPTT) <ref type="bibr" target="#b13">(Merity et al., 2017)</ref>, which yields different data points between epochs. It also resembles classical text data-augmentation methods, such as data-augmentation using thesaurus <ref type="bibr" target="#b29">(Zhang and LeCun, 2015)</ref>.</p><p>Our method takes a step forward and proposes a systematic and deterministic approach on building data points that provides the needed variety of data points without the need of randomized-length backpropagation through time (BPTT). This has the effect of producing a text-augmentation without the need of using external resources such as a thesaurus, but only requires the dataset itself. Our method uses a concept of overlapped data points, which can be found in many areas such as data-mining <ref type="bibr" target="#b5">(Dong and Pei, 2007)</ref>, DNA sequencing <ref type="bibr" target="#b18">(Ng, 2017)</ref>, spectral analysis <ref type="bibr" target="#b4">(Ding et al., 2000)</ref>, or temporal data <ref type="bibr" target="#b11">(Lane and Brodley, 1999)</ref>. In language modeling however, this approach of overlapped data points has not yet been fully exploited. On the other hand, extracting frame-based acoustic features such as mel-fequency cepstral coefficients (MFCCs) using overlapping windows is a common technique in speech processing and more specifically in automatic speech recognition (ASR) <ref type="bibr" target="#b2">(Chiu et al., 2018;</ref><ref type="bibr" target="#b8">Kim and Stern, 2016)</ref>. We hypothesize that extending the current overlapping technique to a higher level, that is using a sliding overlapping window over the already extracted features, will be proven beneficial. We believe this to have a positive impact on speech processing tasks such as speech emotion recognition (SER). This is because the emotional load in an spoken utterance expands over larger windows than frame-, phoneme-or syllable-based ones <ref type="bibr" target="#b7">(Frijda, 1986)</ref>.</p><p>We investigate the proposed method using a simple LSTM model and a small-size Transformer model on the IEMOCAP dataset <ref type="bibr" target="#b1">(Busso et al., 2008)</ref>, composed of five acted sessions, for a fourclass emotions classification and we compare to the state of the art <ref type="bibr" target="#b17">(Mirsamadi et al., 2017)</ref> model, a local attention based BiLSTM. <ref type="bibr" target="#b19">Ramet et al. (2018)</ref> showed in their work a new model that is competitive to the one previously cited, following a cross-valiadation evaluation schema. For a fair comparison, in this paper we focus on a non-crossvaliation schema and thus compare our results to the work of <ref type="bibr" target="#b17">Mirsamadi et al. (2017)</ref>, where a similar schema is followed using as evaluation set the fifth session of IEMOCAP database. It is noteworthy that with a much simpler method than presented in <ref type="bibr" target="#b19">Ramet et al. (2018)</ref>, we achieve comparable results, underscoring the importance of the proposed method for this task as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Alleviated Token Order Imbalance</head><p>Let a token pair denote an ordered pair of tokensfor instance token A followed by token B, as in the sequence "ABCDEF G...". When splitting a token sequence into data points "D1, D2, ..", if the split is fixed, as in D1 always being equal to "ABC", D2 always being equal to "DEF ", etc., then the information contained in the order of tokens C and D for instance is partially lost. This occurs as there is no data point that contains this token pair explicitly. We call the "CD" token pair a split token pair and its tokens, C and D, are denoted as split tokens.</p><p>In its most extreme form, split token pair order information is lost completely. In other cases, it is partially taken into account implicitly. In recurrent cells, for instance, the internal state of the cell allows for the order information of split tokens pairs Here, an Alleviated TOI (3) splits the contiguous list of tokens 3 times with each time a different offset (0, 1, 2 respectively). This finally leads to a list of 11 data points coming from the 3 appended overlapped sequences.</p><p>to be used. This is due to the serial processing of the data points containing the split tokens.</p><p>As some token pairs are taken into account fully, others partially and others not at all, we denote this situation as token order imbalance (TOI).</p><p>In this paper, we propose to alleviate the TOI by means of overlapping sequences of data points. The aim is to avoid the loss of information between the last token of a data point and the first token of its subsequent data point. Instead of splitting the sequence of tokens only once, we repeat this process multiple times using different offsets. Each time we subdivide the sequence of tokens with a new offset, we include the links that were missing in the previous step. Finally, the overlapping sequences of data points are concatenated into a single sequence, forming the final dataset. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates an Alleviated TOI <ref type="formula">(3)</ref>, which means the sequence of data points is split three times instead of only once, producing 3 overlapped sequences that will then be concatenated.</p><p>Our Alleviated TOI (P) method is detailed in the pseudo-code below, where olp_sequence holds an overlapped sequence and P is the number of times we subdivide the sequence of tokens with a different offset: When we apply an Alleviated TOI (P), this means that we are going to create P times a sequence of data points with different offsets. Therefore, the final dataset will be the concatenation of P repetitions of the original dataset, with data points shifted by a specific and increasing offset at token level for each repetition.</p><formula xml:id="formula_0">Let N =</formula><p>For example, given a sequence S 1 with N = 70 tokens per data point and an Alleviated TOI (P) with P = 10, the step size will be N P = 7 tokens. Therefore, starting from the sequence S 1 , nine additional sequences of data points will be created: S 2 starting from token 7, S 3 starting from token 14, S 4 starting from token 21 and so on until S 10 .</p><p>When using Alleviated TOI (P), with P smaller than the data point size, within an epoch, a split token pair-that is a token pair that is split in the original data point splitting-becomes part of a data point P − 1 times. A token pair that is never split will be part of the data point P times.</p><p>We can thus define a token order imbalance ratio that describes the imbalance between the number of times we include split token pairs and the number of times we include pairs that are not split:</p><formula xml:id="formula_1">(P − 1)/P</formula><p>We notice that the higher P , the closer the ratio becomes to 1. We hypothesize that the closer the ratio becomes to 1, the better we leverage the information in the dataset. We thus expect that for higher values of P the Alleviated TOI (P) method will outperform versions with lower values, with Alleviated TOI (1) being the Standard TOI, which is now prevalent.</p><p>We quantify the additional computational cost of Alleviated TOI (P). Since our method only results in P (shifted) repetitions of the dataset, each epoch using the augmented dataset would take ∼ P times longer than an epoch over the original dataset. Therefore, we ensure fair comparison by allowing baseline models to train for P times more epochs than a model using Alleviated TOI (P).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Batch Creation with Alleviated TOI</head><p>Series discretization may also occur at higher levels than data points, in particular when building batches for mini-batch training of neural net-  <ref type="figure">Figure 3</ref>: Three levels of data representation used to create distributed batches. The dataset is a sequence of tokens on which data points are built by splitting the sequence into subsequences of N tokens. Batches of K data points are then built by subdividing the sequence of data points into K equal parts. Here, the first part contains the first two data points, the second part the following two, and the last data point is dropped. Each batch then uses one element of each part.</p><p>works. We can distinguish two types of batches, i.e. sequential and distributed batches. The former keep the data point sequences intact, thus creating split token pairs only between two consecutive batches. The latter distribute data points from different parts of the dataset to approximate the global distribution, thus creating split token pairs between all data points in batches.</p><p>In principle, our proposed method alleviates the TOI in both cases, since multiple overlapping sequences of data points are generated. However, we have observed an unintended interference with the batch creation in the case of distributed batches. In this section we explain the problem in detail and propose a simple but effective solution-choosing a prime batch size. <ref type="figure">Figure 3</ref> illustrates the three levels of data representation in the case of distributed batches. Data points are built from N consecutive tokens to capture the sequential information. Batches are then built from K parts of the data point sequence to capture the global distribution. An example of this approach is the batching procedure used in Zoph and Le <ref type="formula">(2016)</ref> The batching mechanism can be seen as building a 2-dimensional matrix, where each row contains a batch. Consider a sequence of M data points and a batch size of K. In order to build batches, the data points are split into K parts, represented as M K × 1 column vectors. They are concatenated to form a M K × K matrix, such that the rows correspond to batches.</p><p>When applying the proposed Alleviated TOI (P) method (see Section 3), we augment the original  <ref type="figure">Figure 4</ref>: Illustrations of the 2D matrix of batches with different P -values of Alleviated TOI (P). On the left we used a batch size of 20 and on the right we used a prime batch size of 19. Each data point is a pixel and each row is a batch. The grayscale value models the proximity of the data points with respect to the dataset. Therefore, two pixels with similar color represents two data points that are close in the dataset. The illustrations demonstrate how different values of P affect the content of the batches, which can lack a good distribution over the dataset. Ideally, each row should contain a gradient of different grayscale values. We can observe how using a prime batch size affects the distribution of data points within the batches, where the matrices on the right offer a better distribution. This effect is especially well visible for the Alleviated TOI 10.  dataset to a total of P · M data points, adding additional data points with token offsets. Therefore, the P ·M K × K matrix used for batch creation may contain repeated data points within the same batch as illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>. A repeated data point differs from the previous data point only marginally due to the token offset. This redundancy can be problematic, as the batches are not well-distributed over the entire dataset anymore.</p><p>With respect to the batch matrix, a repeated data point occurs iff P ·M K ·q = n·M with period q &lt; K and q, n ∈ N. This is equivalent to P K · q = n, q &lt; K, q, n ∈ N independent of the number of data points M . A repetition thus occurs iff the greatest common divisor (GCD) of P and K is larger than 1. Otherwise, for GCD(P, K) = 1 a data point repeats only after period q = K, i.e. there is no repetition within the same batch. <ref type="table" target="#tab_2">Table 1</ref> lists exemplary periods for a batch size of K = 20 and different values of P for the Alleviated TOI (P). The worst case is P = 10 with 10 repetitions of the same data point within the same batch and the best case is P = 7, which avoids any redundancy because the GCD of P and K is 1. <ref type="figure">Figure 4</ref> illustrates the repetition with grayscale values, where similar grayscale values indicate that two data points are close within the original data points sequence.</p><p>In general, while we aim for large values of P for reducing the TOI, a simple solution for avoiding redundancy within batches is to choose a prime number for the batch size K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>To validate the generalization capability of the proposed technique, we apply it on both text and speech related tasks. We thus run the Alleviated TOI (P) with language modeling (text) and emotion recognition (speech). The text datasets used are Penn-Tree-Bank (PTB) <ref type="bibr" target="#b12">(Marcus et al., 1993)</ref> as preprocessed in <ref type="bibr" target="#b15">Mikolov et al. (2011)</ref>, Wikitext-2 (WT2), and Wikitext-103 (WT103) <ref type="bibr" target="#b14">(Merity et al., 2016)</ref>. The speech dataset is the IEMOCAP database <ref type="bibr" target="#b1">(Busso et al., 2008)</ref>, a collection of more than 12 hours of recorded emotional speech of 10 native-English speakers, men and women. The audio data is filtered down to 5.5 hours containing only angry, happy, neutral and sad utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">TOI in Language Modelling</head><p>For language modeling, we use three different methods:</p><p>• A simple LSTM that does not benefit from extensive hyper-parameter optimization.</p><p>• An Average Stochastic Gradient Descent Weight-Dropped LSTM (AWD-LSTM) as described in <ref type="bibr" target="#b13">Merity et al. (2017)</ref>, with the same hyper-parameters.</p><p>• The latest State-of-the-Art model: Mixture of Softmaxes (MoS) <ref type="bibr" target="#b28">(Yang et al., 2017)</ref>.</p><p>We compare our results against the original process of building data points, i.e. Standard TOI, and use the same computation load allocated for each experiment. We use the same set of hyperparameters as described in the base papers, except for the batch size with Alleviated TOI (P), where we use a prime batch size in order to prevent any repetitions in batches, as described in Section 4. That is, on the PTB dataset, we use a sequence length of 70 for all the models. For the Simple LSTM and AWD-LSTM, we use a batch size of 20 and a hidden size of 400. AWD-LSTM and MoS are trained on 1000 epochs, and the Simple LSTM on 100 epochs. For the MoS model, embedding size used is 280, batch size 12, and hidden size 980. All the models use SGD as the optimizer. We set up experiments to compare 4 different token order imbalance setups: Extreme TOI, Interbatch TOI, Standard TOI, and Alleviated TOI (P).</p><p>Extreme TOI The Extreme TOI setup builds batches using a random sequence of data points. This removes any order inside the batches (i.e. among data points within a batch), and among batches.</p><p>Inter-batch TOI In the Inter-batch TOI setup, batches are built using an ordered sequence of data points, but the sequence of batches is shuffled. This keeps the order inside batches, but removes it among batches. Looking at the 2D matrix of batches, in <ref type="figure">Figure 4</ref>, this results in shuffling the rows of the matrix.</p><p>Standard TOI In the Standard TOI setup, the process of building batches is untouched, as described in section 3. This keeps the order inside, and among batches.</p><p>Alleviated TOI (P) In the Alleviated TOI (P) setup, we apply our proposed TOI reduction by creating P overlapped data point sequences (see Sections 3 and 4). This strategy not only keeps the order inside and among batches, but it also restores the full token order information in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TOI in Speech Emotion Recognition</head><p>For Speech Emotion Recognition (SER) we use two different models: the encoder of the Transformer <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref> followed by convolutional layers, and the simple LSTM used in text domain case. Since the Transformer is stateless and uses self-attention instead, we are able to investigate the effect of Alleviated TOI (P) independently of LSTM cells.</p><p>As with language modeling, we set up experiments to compare the 4 different token order imbalance strategies: Extreme TOI, Inter-batch TOI, Standard TOI, and Alleviated TOI (P).</p><p>We apply the methodology used in text on the SER task, using the simple LSTM and a window size of 300 frames. In this case, a data point, instead of being a sequence of words, is a sequence of frames coming from the same utterance. Each frame is described by a 384-dimensional features vector. OpenSMILE <ref type="bibr" target="#b6">(Eyben et al., 2013)</ref> is used for extracting the features. We opt for the IS09 features set <ref type="bibr" target="#b22">(Schuller et al., 2009)</ref> as proposed by <ref type="bibr" target="#b19">Ramet et al. (2018)</ref> and commonly used for SER.</p><p>Finally, to investigate the effect of the Alleviated TOI (P) strategy independently of LSTM cells, we design a final experiment in the SER task. We investigate whether or not we have improved results as we increase P , the number of overlapped data point sequences in a stateless scenario. For this reason, we use the Transformer model described above.  <ref type="table">Table 2</ref>: Perplexity score (PPL) comparison of the AWD model, on the three datasets, with batch sizes K = 20 (PTB), K = 80 (WT2) and K = 60 (WT103), with different levels of Token Order Imbalance (TOI). With Alleviated TOI (P), we use a prime batch size of K = 19 (PTB), K = 79 (WT2) and K = 59 (WT103). <ref type="table">Table 2</ref> compares the 4 token order imbalance strategies using the AWD model and three text datasets. We use the test perplexity after the same equivalent number of epochs. The different Alleviated TOI (P) experiments use a different number of overlapped sequence: An Alleviated TOI (P) means building and concatenating P overlapped sequences. Our results indicate that an Alleviated TOI (P) is better than the Standard TOI, which is better than an Extreme or Inter-batch TOI. We note a tendency that higher values of P lead to better results, which is in accordance with our hypothesis that a higher TOI ratio (P − 1)/P improves the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Language Modelling</head><p>Comparison with State of the Art and Simple LSTM. With the MoS model and an Alleviated TOI, we improve the current state of the art without fine tuning for the PTB dataset with 54.58 perplexity on the test set. <ref type="table">Table 3</ref> demonstrates how models can be improved by applying our Alleviated TOI method on 2 latest state-of-the-art models: AWD-LSTM <ref type="bibr" target="#b13">(Merity et al., 2017)</ref> and AWD-LSTM-MoS <ref type="bibr" target="#b28">(Yang et al., 2017)</ref>, and the Simple LSTM model. We compare the results with the same hyper-parameters used on the original papers with the only exception of the batch size, that must be prime. To ensure fairness, we allocate the same computational resources for the base model as well the model with Alleviated TOI, i.e. we train with the equivalent number of epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>test ppl AWD-LSTM <ref type="bibr" target="#b13">(Merity et al., 2017)</ref> 58.8 AWD-LSTM + Alleviated TOI 56.46 AWD-LSTM-MoS <ref type="bibr" target="#b28">(Yang et al., 2017)</ref>   <ref type="table">Table 3</ref>: Comparison between state-of-the-art models <ref type="bibr" target="#b13">(Merity et al., 2017;</ref><ref type="bibr" target="#b28">Yang et al., 2017)</ref>   <ref type="table">Table 4</ref>: Perplexity score (PPL) comparison on the PTB dataset and the AWD model. We use two different values for the batch size K -the original one with K = 20, and a prime one with K = 19. The results directly corroborate the observation portrayed in <ref type="figure">Figure 4</ref>, where the obtained score is related to the diversity of grayscale values in each row.</p><p>Comparison without prime batch size. In <ref type="table">Table 4</ref> we demonstrate how using a prime batch size with Alleviated TOI (P) actually impacts the scores. We compare the scores of a prime batch size K = 19 with the scores of the original batch size K = 20 for the AWD model with Alleviated TOI (P). When using a prime batch size, we observe consistent and increasing results as P increases. This is due to the good distribution of data points in the batches regardless of the value of P , which is visible in <ref type="figure">Figure 4(b)</ref> where each row contains a high diversity of grayscale values. With the original batch size K = 20, we observe a strong performance for P = 7, but a low performance for P = 10. Again, this effect is related to the distribution of data points in the batches, which is visible in <ref type="figure">Figure 4(a)</ref>. The matrix with P = 7 shows a good distribution-corresponding to the strong performance-and the matrix with P = 10 shows that each row contains a low diversity of data points.  <ref type="table">Table 5</ref>: Token order imbalance (TOI) comparison for the IEMOCAP dataset on a SER task using angry, happy, neutral and sad classes with a simple LSTM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Speech Emotion Recognition Results</head><p>The results on the IEMOCAP database are evaluated in terms of weighted (WA) and unweighted accuracy (UA). The first metric is the accuracy on the entire evaluation dataset, while the second is the average of the accuracies of each class of the evaluation set. UA is often used when the database is unbalanced, which is true in our case, since the happy class has a total duration that is half of the second smallest class in speech duration. <ref type="table">Table 5</ref> shows that our proposed method brings value in the speech related task as well. When choosing the Extreme TOI instead of the Standard TOI approach we observe a smaller effect than in text related task: this is due to the different nature of the text datasets (large "continuous" corpuses) and the IEMOCAP one (composed of shorter utterances). The fact that we can still observe improvements on a dataset with short utterances is a proof of the robustness of the method.</p><p>A greater effect is obtained when we increase the size of the dataset with the proposed Alleviated TOI (P) approach: Due to the increasing offset at each overlapped sequence, the data fed into the model contains utterances where the emotions are expressed in slightly different ways. For this reason, the performance notably increases. <ref type="table">Table 6</ref> reports the result of a final experiment that aims to investigate the effect of Alleviated TOI (P) independently of LSTM cells. For each Alleviated TOI (P) setup and Standard TOI described in <ref type="table">Table 6</ref>, we repeat the training and evaluation for each of the following window sizes: 100, 200, 300, 400 and 500 frames. The previously described Transformer model is used in these experiments. The results reported in <ref type="table">Table 6</ref> are the mean ± the standard deviation computed for different P-values of Alleviated TOI (P). Local attention 0.635 0.588 <ref type="table">Table 6</ref>: Token order imbalance (TOI) comparison for the IEMOCAP dataset on a SER task using angry, happy, neutral and sad classes for 60 epochs using the Transformer model.</p><p>The last line of <ref type="table">Table 6</ref> refers to <ref type="bibr" target="#b17">Mirsamadi et al. (2017)</ref> results. We want to highlight the fact that the goal of these experiments is to show the direct contribution of the Alleviated TOI technique for a different model. For this reason we use a smaller version of the Transformer in order to reduce the computational cost. We believe that with a more expressive model and more repetitions, the proposed method may further improve the results.</p><p>The results from <ref type="table">Table 6</ref> demonstrate that, as we increase the value of P , more significant improvements are achieved. This is in accordance with our hypothesis that a higher TOI ratio (P − 1)/P improves the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work, the importance of overlapping and token order in sequence modelling tasks were investigated. Series discretization is an essential step in machine learning processes which nonetheless can be responsible for the loss of the continuation of the tokens, through the token order imbalance (TOI) phenomenon. The proposed method, Alleviated TOI, has managed to overcome this drawback and ensures that all token sequences are taken into account. The proposed method was validated in sequence modelling tasks both in the text and speech domain outperforming the state of the art techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of an Alleviated TOI (3) made from a single contiguous list of 13 tokens. With a Standard TOI and N =3 (ie. 3 tokens per data point), a contiguous list of 13 tokens would produce 4 data points, which is illustrated by the first overlapped sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>; Merity et al. (2017); Yang et al. (2017); Zołna et al. (2017) for word language modeling, where the basic token is a word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Data point repetition with period q for M data points, K batches, and Alleviated TOI (P). Data point 1' is the same as data point 1 with a token offset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Data point repetition with period q for batch size K = 20 and Alleviated TOI (P).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To make our results reproducible, all relevant source codes are publicly available at https://github.com/ nkcr/overlap-ml</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Iemocap: interactive emotional dyadic motion capture database. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">Mower</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent neural networks and robust time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jerome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><forename type="middle">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="240" to="254" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Short-window spectral analysis of cortical event-related potentials by adaptive multivariate autoregressive modeling: data preprocessing, model validation, and variability assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhou</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hualou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sequence data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guozhu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recent developments in opensmile, the munich open-source multimedia feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="DOI">10.1145/2502081.2502224</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Multimedia, MM &apos;13</title>
		<meeting>the 21st ACM International Conference on Multimedia, MM &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="835" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><forename type="middle">H</forename><surname>Frijda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Powernormalized cepstral coefficients (pncc) for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1315" to="1329" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast bayesian optimization of machine learning hyperparameters on large datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1605.07079</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Temporal sequence learning and data reduction for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terran</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information and System Security (TISSEC)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="331" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1708.02182</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Empirical evaluation and combination of advanced language modeling techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jančernockỳ</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelfth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic speech emotion recognition using recurrent neural networks with local attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyedmahdad</forename><surname>Mirsamadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2227" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06279</idno>
		<title level="m">Consistent vector representations of variable-length k-mers</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Context-aware attention mechanism for speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaetan</forename><surname>Ramet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Baeriswyl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Lazaridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="126" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An application of recurrent nets to phone probability estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.279192</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Neur. Netw</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="298" to="305" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two decades of statistical language modeling: Where do we go from here?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1270" to="1278" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The interspeech 2009 emotion challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Batliner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revisiting recurrent neural networks for robust asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Suman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Povey</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2012.6288816</idno>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acoustics</surname></persName>
		</author>
		<title level="m">Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<biblScope unit="page" from="4085" to="4088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Achieving human parity in conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05256</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno>abs/1711.03953</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01710</idno>
		<title level="m">Text understanding from scratch</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Zołna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dendi</forename><surname>Suhubdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Fraternal dropout. stat</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
