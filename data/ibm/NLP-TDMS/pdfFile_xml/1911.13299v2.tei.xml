<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What&apos;s Hidden in a Randomly Weighted Neural Network?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Ramanujan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
						</author>
						<title level="a" type="main">What&apos;s Hidden in a Randomly Weighted Neural Network?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever modifying the weight values. Hidden in a randomly weighted Wide ResNet-50 <ref type="bibr" target="#b31">[32]</ref> we find a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 [9] trained on ImageNet <ref type="bibr" target="#b4">[4]</ref>. Not only do these "untrained subnetworks" exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an "untrained subnetwork" approaches a network with learned weights in accuracy. Our code and pretrained models are available at: https://github.com/allenai/hidden-networks. * equal contribution A subnetwork ‚åß 0 of N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P l j 5 z t 5</p><p>c 0 m y x J t l J K f 5 E X / 4 o e B B X x 6 s 8 w / T i o d S D k 8 d 4 M b + Z F m e D G + v 6 r N z e / s L i 0 X F g p r q 6 t b 2 y W t r b v j M o 1 w y p T Q u l 6 B A Y F l 1 i 1 3 A q s Z x o h j Q T W o u 7 F S K / 1 U B u u 5 K 3 t Z 9 h M o S N 5 m z O w j m q V L k O p u I x R W n p O J e Y a h P v s v d L d M C z u h x b y f X q f c J Z Q Y A n H H h r H d 5 S K a Y a 6 r X Q K k m G r V P Y r / r j o L A i m o E y m d d 0 q P Y e x Y n n q f J k A Y x q B n 9 n m A L T l T O C w G O Y G M 2 B d 6 G D D Q Q k p m u Z g f O 6 Q H j g m p s 7 b P b f 3 m P 0 5 M Y D U m H 4 a u c 4 U b G L + a i P y P 6 2 R 2 / Z p c 8 B l l l u U b G L U z g W 1 i o 6 y o z H X y K z o O w B M c 7 c r Z Q l o Y N Y l X H Q h B H 9 P n g V 3 R 5 X A r w Q 3 R + W z 4 2 k c B b J L 9 s g h C c g J O S N X 5 J p U C S M P 5 I m 8 k X f v 0 X v x P r z P S e u c N 5 3 Z I b / K + / o G R t K o p A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / F 2 E / q G S V 4 5 o 6 3 y Z b 4 D + + o a c J n 8 = " &gt; A A A C L n i c b V B N S w M x E M 3 6 W e t X 1 a O X Y B U 8 l V 0 P 6 l E R w a O C t Y V u K b P Z a T c 0 m y x J t l J K f 5 E X / 4 o e B B X x 6 s 8 w / T i o d S D k 8 d 4 M b + Z F m e D G + v 6 r N z e / s L i 0 X F g p r q 6 t b 2 y W t r b v j M o 1 w y p T Q u l 6 B A Y F l 1 i 1 3 A q s Z x o h j Q T W o u 7 F S K / 1 U B u u 5 K 3 t Z 9 h M o S N 5 m z O w j m q V L k O p u I x R W n p O J e Y a h P v s v d L d M C z u h x b y f X q f c J Z Q Y A n H H h r H d 5 S K a Y a 6 r X Q K k m G r V P Y r / r j o L A i m o E y m d d 0 q P Y e x Y n n q f J k A Y x q B n 9 n m A L T l T O C w G O Y G M 2 B d 6 G D D Q Q k p m u Z g f O 6 Q H j g m p s 7 b P b f 3 m P 0 5 M Y D U m H 4 a u c 4 U b G L + a i P y P 6 2 R 2 / Z p c 8 B l l l u U b G L U z g W 1 i o 6 y o z H X y K z o O w B M c 7 c r Z Q l o Y N Y l X H Q h B H 9 P n g V 3 R 5 X A r w Q 3 R + W z 4 2 k c B b J L 9 s g h C c g J O S N X 5 J p U C S M P 5 I m 8 k X f v 0 X v x P r z P S e u c N 5 3 Z I b / K + / o G R t K o p A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / F 2 E / q G S V 4 5 o 6 3 y Z b 4 D + + o a c J n 8 = " &gt; A A A C L n i c b V B N S w M x E M 3 6 W e t X 1 a O X Y B U 8 l V 0 P 6 l E R w a O C t Y V u K b P Z a T c 0 m y x J t l J K f 5 E X / 4 o e B B X x 6 s 8 w / T i o d S D k 8 d 4 M b + Z F m e D G + v 6 r N z e / s L i 0 X F g p r q 6 t b 2 y W t r b v j M o 1 w y p T Q u l 6 B A Y F l 1 i 1 3 A q s Z x o h j Q T W o u 7 F S K / 1 U B u u 5 K 3 t Z 9 h M o S N 5 m z O w j m q V L k O p u I x R W n p O J e Y a h P v s v d L d M C z u h x b y f X q f c J Z Q Y A n H H h r H d 5 S K a Y a 6 r X Q K k m G r V P Y r / r j o L A i m o E y m d d 0 q P Y e x Y n n q f J k A Y x q B n 9 n m A L</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>What lies hidden in an overparameterized neural network with random weights? If the distribution is properly scaled, then it contains a subnetwork which performs well without ever modifying the values of the weights (as illustrated by <ref type="figure" target="#fig_5">Figure 1</ref>).</p><p>The number of subnetworks is combinatorial in the size of the network, and modern neural networks contain millions or even billions of parameters <ref type="bibr" target="#b23">[24]</ref>. We should expect that even a randomly weighted neural network contains a subnetwork that performs well on a given task. In this work, we provide an algorithm to find these subnetworks.</p><p>Finding subnetworks contrasts with the prevailing paradigm for neural network training -learning the values of the weights by stochastic gradient descent. Traditionally, the network structure is either fixed during training (e.g. ResNet <ref type="bibr" target="#b9">[9]</ref> or MobileNet <ref type="bibr" target="#b10">[10]</ref>), or optimized in conjunction with the weight values (e.g. Neural Architecture Search (NAS)). We instead optimize to find a good subnet- <ref type="figure" target="#fig_5">Figure 1</ref>. If a neural network with random weights (center) is sufficiently overparameterized, it will contain a subnetwork (right) that perform as well as a trained neural network (left) with the same number of parameters. work within a fixed, randomly weighted network. We do not ever tune the value of any weights in the network, not even the batch norm <ref type="bibr" target="#b11">[11]</ref> parameters or first or last layer.</p><p>In <ref type="bibr" target="#b5">[5]</ref>, Frankle and Carbin articulate The Lottery Ticket Hypothesis: neural networks contain sparse subnetworks that can be effectively trained from scratch when reset to their initialization. We offer a complimentary conjecture: within a sufficiently overparameterized neural network with random weights (e.g. at initialization), there exists a subnetwork that achieves competitive accuracy. Specifically, the test accuracy of the subnetwork is able to match the accuracy of a trained network with the same number of parameters.</p><p>This work is catalyzed by the recent advances of Zhou et al. <ref type="bibr" target="#b32">[33]</ref>. By sampling subnetworks in the forward pass, they first demonstrate that subnetworks of randomly weighted neural networks can achieve impressive accuracy. However, we hypothesize that stochasticity may limit their performance. As the number of parameters in the network grows, they are likely to have a high variability in their sampled networks.</p><p>To this end we propose the edge-popup algorithm for finding effective subnetworks within randomly weighted neural networks. We show a significant boost in performance and scale to ImageNet. For each fixed random weight in the network, we consider a positive real-valued score. To choose a subnetwork we take the weights with the top-k% highest scores. With a gradient estimator we optimize the scores via SGD. We are therefore able to find a good neural network without ever changing the values of the weights. We empirically demonstrate the efficacy of our algorithm and show that (under certain technical assumptions) the loss decreases on the mini-batch with each modification of the subnetwork.</p><p>We experiment on small and large scale datasets for image recognition, namely CIFAR-10 <ref type="bibr" target="#b13">[13]</ref> and Imagenet <ref type="bibr" target="#b4">[4]</ref>. On CIFAR-10 we empirically demonstrate that as networks grow wider and deeper, untrained subnetworks perform just as well as the dense network with learned weights. On ImageNet, we find a subnetwork of a randomly weighted Wide ResNet50 which is smaller than, but matches the performance of a trained ResNet-34. Moreover, a randomly weighted ResNet-101 <ref type="bibr" target="#b9">[9]</ref> with fixed weights contains a subnetwork that is much smaller, but surpasses the performance of VGG-16 <ref type="bibr" target="#b26">[27]</ref>. In short, we validate the unreasonable effectiveness of randomly weighted neural networks for image recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work Lottery Tickets and Supermasks</head><p>In <ref type="bibr" target="#b5">[5]</ref>, Frankle and Carbin offer an intriguing hypothesis: neural networks contain sparse subnetworks that can be effectively trained from scratch when reset to their initialization. These so-called winning tickets have won the "initialization lottery". Frankle and Carbin find winning tickets by iteratively shrinking the size of the network, masking out weights which have the lowest magnitude at the end of each training run.</p><p>Follow up work by Zhou et al. <ref type="bibr" target="#b32">[33]</ref> demonstrates that winning tickets achieve better than random performance without training. Motivated by this result they propose an algorithm to identify a "supermask" -a subnetwork of a randomly initialized neural network that achieves high accuracy without training. On CIFAR-10, they are able to find subnetworks of randomly initialized neural networks that achieve 65.4% accuracy.</p><p>The algorithm presented by Zhou et al. is as follows: for each weight w in the network they learn an associated probability p. On the forward pass they include weight w with probability p and otherwise zero it out. Equivalently, they use weightw = wX where X is a Bernoulli(p) random variable (X is 1 with probability p and 0 otherwise). The probabilities p are the output of a sigmoid, and are learned using stochastic gradient descent. The terminology supermask" arises as finding a subnetwork is equivalent to learning a binary mask for the weights.</p><p>Our work builds upon Zhou et al., though we recognize that the stochasticity of their algorithm may limit performance. In section 3.1 we provide more intuition for this claim. We show a significant boost in performance with an algorithm that does not sample supermasks on the forward pass. For the first time we are able to match the performance of a dense network with a supermask. Neural Architecture Search (NAS)</p><p>The advent of modern neural networks has shifted the focus from feature engineering to feature learning. However, researchers may now find themselves manually engineering the architecture of the network. Methods of Neural Architecture Search (NAS) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b27">28]</ref> instead provide a mechanism for learning the architecture of neural network jointly with the weights. Models powered by NAS have recently obtained state of the art classification performance on ImageNet <ref type="bibr" target="#b28">[29]</ref>.</p><p>As highlighted by Xie et al. <ref type="bibr" target="#b30">[31]</ref>, the connectivity patterns in methods of NAS remain largely constrained. Surprisingly, Xie et al. establish that randomly wired neural networks can achieve competitive performance. Accordingly, Wortsman et al. <ref type="bibr" target="#b29">[30]</ref> propose a method of Discovering Neural Wirings (DNW) -where the weights and structure are jointly optimized free from the typical constraints of NAS. We highlight DNW as we use a similar method of analysis and gradient estimator to optimize our supermasks. In DNW, however, the subnetwork is chosen by taking the weights with the highest magnitude. There is therefore no way to learn supermasks with DNW as the weights and connectivity are inextricably linked -there is no way to separate the weights and the structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight Agnostic Neural Networks</head><p>In Weight Agnostic Neural Networks (WANNs) <ref type="bibr" target="#b6">[6]</ref>, Gaier and Ha question if an architecture alone may encode the solution to a problem. They present a mechanism for building neural networks that achieve high performance when each weight in the network has the same shared value. Importantly, the performance of the network is agnostic to the value itself. They are able to obtain ‚àº 92% accuracy on MNIST <ref type="bibr" target="#b16">[16]</ref>.</p><p>We are quite inspired by WANNs, though we would like to highlight some important distinctions. Instead of each weight having the same value, we explore the setting where each weight has a random value. In Section A.2.2 of their appendix, Gaier and Ha mention that they were not successful in this setting. However, we find a good subnetwork for a given random initialization -the supermasks we find are not agnostic to the weights. Finally, <ref type="bibr">Gaier</ref>  </p><formula xml:id="formula_0">) '( ‚Üê ) '( ‚àí / 0‚Ñí 0, ( &amp; '( + ' # Figure 2.</formula><p>In the edge-popup Algorithm, we associate a score with each edge. On the forward pass we choose the top edges by score. On the backward pass we update the scores of all the edges with the straight-through estimator, allowing helpful edges that are "dead" to re-enter the subnetwork. We never update the value of any weight in the network, only the score associated with each weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Classifiers and Pruning at Initialization</head><p>Linear classifiers on top of randomly weighted neural networks are often used as baselines in unsupervised learning <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b2">3]</ref>. This work is different in motivation, we search for untrained subnetworks which achieve high performance without changing any weight values. This also differs from methods which prune at initialization and modify the weights of the discovered subnetwork <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b17">17]</ref> or methods which modify a subset of the weights <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section we present our optimization method for finding effective subnetworks within randomly weighted neural networks. We begin by building intuition in an unusual setting -the infinite width limit. Next we motivate and present our algorithm for finding effective subnetworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Intuition The Existence of Good Subnetworks</head><p>Modern neural networks have a staggering number of possible subnetworks. Consequently, even at initialization, a neural network should contain a subnetwork which performs well.</p><p>To build intuition we will consider an extreme case -a neural network N in the infinite width limit (for a convolutional neural networks, the width of the network is the number of channels). As in <ref type="figure" target="#fig_5">Figure 1</ref>, let œÑ be a network with the same structure of N that achieves good accuracy. If the weights of N are initialized using any standard scaling of a normal distribution, e.g. xavier <ref type="bibr" target="#b7">[7]</ref> or kaiming <ref type="bibr" target="#b8">[8]</ref>, then we may show there exists a subnetwork of N that achieves the same performance as œÑ without training. Let q be the probability that a given subnetwork of N has weights that are close enough to œÑ to obtain the same accuracy. This probability q is extremely small, but it is still nonzero. Therefore, the probability that no subnetwork of N is close enough to œÑ is effectively (1 ‚àí q) S where S is the number of subnetworks. S grows very quickly with the width of the network, and this probability becomes arbitrarily small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How Should We Find A Good Subnetwork</head><p>Even if there are good subnetworks in randomly weighted neural networks, how should we find them?</p><p>Zhou et al. learn an associated probability p with each weight w in the network. On the forward pass they include weight w with probability p (where p is the output of a sigmoid) and otherwise zero it out. The infinite width limit provides intuition for a possible shortcoming of the algorithm presented by Zhou et al. <ref type="bibr" target="#b32">[33]</ref>. Even if the parameters p are fixed, the algorithm will likely never observe the same subnetwork twice. As such, the gradient estimate becomes more unstable, and this in turn may make training difficult.</p><p>Our algorithm for finding a good subnetwork is illustrated by <ref type="figure">Figure 2</ref>. With each weight w in the neural network we learn a positive, real valued popup score s. The subnetwork is then chosen by selecting the weights in each layer corresponding to the top-k% highest scores. For simplicity we use the same value of k for all layers.</p><p>How should we update the score s uv ? Consider a single edge in a fully connected layer which connects neuron u to neuron v. Let w uv be the weight of this edge, and s uv the associated score. If this score is initially low then w uv is not selected in the forward pass. But we would still like a way to update its score to allow it to pop back up. Informally, with backprop <ref type="bibr" target="#b25">[26]</ref> we compute how the loss "wants" node v's input to change (i.e. the negative gradient). We then examine the weighted output of node u. If this weighted output is aligned with the negative gradient, then node u can take node v's output where the loss "wants" it to go. Accordingly, we should increase the score. If this alignment happens consistently, then the score will continue to increase and the edge will re-enter the chosen subnetwork (i.e. popup).</p><p>More formally, if w uv Z u denotes the weighted output of neuron u, and I v denotes the input of neuron v, then we update s uv as</p><formula xml:id="formula_1">s uv ‚Üê s uv ‚àí Œ± ‚àÇL ‚àÇI v Z u w uv .<label>(1)</label></formula><p>This argument and the analysis that follows is motivated and guided by the work of <ref type="bibr" target="#b29">[30]</ref>. In their work, however, they do not consider a score and are instead directly updating the weights. In the forward pass they use the top k% of edges by magnitude, and therefore there is no way of learning a subnetwork without learning the weights. Their goal is to train sparse neural networks, while we aim to showcase the efficacy of randomly weighted neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The edge-popup Algorithm and Analysis</head><p>We now formally detail the edge-popup algorithm. For clarity, we first describe our algorithm for a fully connected neural network. In Section B.2 we provide the straightforward extension to convolutions along with code in PyTorch <ref type="bibr" target="#b22">[23]</ref>.</p><p>A fully connected neural network consists of layers 1, ..., L where layer has n nodes</p><formula xml:id="formula_2">V ( ) = {v ( ) 1 , ..., v ( ) n }.</formula><p>We let I v denote the input to node v and let Z v denote the output, where Z v = œÉ(I v ) for some non-linear activation function œÉ (e.g. ReLU <ref type="bibr" target="#b14">[14]</ref>). The input to neuron v in layer is a weighted sum of all neurons in the preceding layer. Accordingly, we write I v as</p><formula xml:id="formula_3">I v = u‚ààV ( ‚àí1) w uv Z u<label>(2)</label></formula><p>where w uv are the network parameters for layer . The output of the network is taken from the final layer while the input data is given to the very first layer. Before training, the weights w uv for layer are initialized by independently sampling from distribution D . For example, if we are using kaiming normal initialization <ref type="bibr" target="#b8">[8]</ref> with ReLU activations, then D = N 0, 2/n ‚àí1 where N denotes the normal distribution. Normally, the weights w uv are optimized via stochastic gradient descent. In our edge-popup algorithm, we instead keep the weights at their random initialization, and optimize to find a subnetwork G = (V, E). We then compute the input of node v in layer as</p><formula xml:id="formula_4">I v = (u,v)‚ààE w uv Z u<label>(3)</label></formula><p>where G is a subgraph of the original fully connected network 1 . As mentioned above, for each weight w uv in the original network we learn a popup score s uv . We choose the subnetwork G by selecting the weights in each layer which have the top-k% highest scores. Equation 3 may therefore be written equivalently as</p><formula xml:id="formula_5">I v = u‚ààV ( ‚àí1) w uv Z u h(s uv )<label>(4)</label></formula><p>where h(s uv ) = 1 if s uv is among the top k% highest scores in layer and h(s uv ) = 0 otherwise. Since the gradient of h is 0 everywhere it is not possible to directly compute the gradient of the loss with respect to s uv . We instead use the straight-through gradient estimator <ref type="bibr" target="#b0">[1]</ref>, in which h is treated as the identity in the backwards pass -the gradient goes "straight-through" h. Consequently, we approximate the gradient to s uv a≈ù</p><formula xml:id="formula_6">g suv = ‚àÇL ‚àÇI v ‚àÇI v ‚àÇs uv = ‚àÇL ‚àÇI v w uv Z u<label>(5)</label></formula><p>where L is the loss we are trying to minimize. The scores s uv are then updated via stochastic gradient descent with learning rate Œ±. If we ignore momentum and weight decay <ref type="bibr" target="#b15">[15]</ref> then we update s uv as</p><formula xml:id="formula_7">s uv = s uv ‚àí Œ± ‚àÇL ‚àÇI v w uv Z u<label>(6)</label></formula><p>wheres uv denotes the score after the gradient step 2 .</p><p>As the scores change certain edges in the subnetwork will be replaced with others. Motivated by the analysis of <ref type="bibr" target="#b29">[30]</ref> we show that when swapping does occur, the loss decreases for the mini-batch. Theorem 1: When edge (i, œÅ) replaces (j, œÅ) and the rest of the subnetwork remains fixed then the loss decreases for the mini-batch (provided the loss is sufficiently smooth). Proof. Lets uv denote the score of weight w uv after the gradient update. If edge (i, œÅ) replaces (j, œÅ) then our algorithm dictates that s iœÅ &lt; s jœÅ buts iœÅ &gt;s jœÅ . Accordingly, s iœÅ ‚àí s iœÅ &gt;s jœÅ ‚àí s jœÅ <ref type="bibr" target="#b7">(7)</ref> which implies that</p><formula xml:id="formula_8">‚àíŒ± ‚àÇL ‚àÇI œÅ w iœÅ Z i &gt; ‚àíŒ± ‚àÇL ‚àÇI œÅ w jœÅ Z j<label>(8)</label></formula><p>by the update rule given in Equation <ref type="bibr" target="#b6">6</ref>. Letƒ® œÅ denote the input to node k after the swap is made and I œÅ denote the original input. Note thatƒ® œÅ ‚àí I œÅ = w iœÅ Z i ‚àí w jœÅ Z j by Equation <ref type="bibr" target="#b2">3</ref>. We now wish to show that L(ƒ® œÅ ) &lt; L (I œÅ ).</p><p>If the loss is smooth andƒ® œÅ is close to I œÅ and ignore second-order terms in a Taylor expansion:</p><formula xml:id="formula_9">L ƒ® œÅ = L I œÅ + ƒ® œÅ ‚àí I œÅ (9) ‚âà L (I œÅ ) + ‚àÇL ‚àÇI œÅ ƒ® œÅ ‚àí I œÅ (10) = L (I œÅ ) + ‚àÇL ‚àÇI œÅ (w iœÅ Z i ‚àí w jœÅ Z j )<label>(11)</label></formula><p>and from equation 8 we have that ‚àÇL ‚àÇIœÅ (w iœÅ Z i ‚àíw jœÅ Z j ) &lt; 0 and so L(ƒ® œÅ ) &lt; L (I œÅ ) as needed. We examine a more general case of Theorem 1 in Section B.1 of the appendix.  <ref type="figure">Figure 3</ref>. Going Deeper: Experimenting with shallow to deep neural networks on CIFAR-10 <ref type="bibr" target="#b13">[13]</ref>. As the network becomes deeper, we are able to find subnetworks at initialization that perform as well as the dense original network when trained. The baselines are drawn as a horizontal line as we are not varying the % of weights. When we write Weights ‚àº D we mean that the weights are randomly drawn from distribution D and are never tuned. Instead we find subnetworks with size (% of Weights)/100 * (Total # of Weights).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We demonstrate the efficacy of randomly weighted neural networks for image recognition on standard benchmark datasets CIFAR-10 <ref type="bibr" target="#b13">[13]</ref> and ImageNet <ref type="bibr" target="#b4">[4]</ref>. This section is organized as follows: in Section 4.1 we discuss the experimental setup and hyperparameters. We perform a series of ablations at small scale: we examine the effect of k, the % of Weights which remain in the subnetwork, and the effect of width. In Section 4.4 we compare against the algorithm of Zhou et al., followed by Section 4.5 in which we study the effect of the distribution used to sample the weights. We conclude with Section 4.6, where we optimize to find subnetworks of randomly weighted neural networks which achieve good performance on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We use two different distributions for the weights in our network:</p><p>‚Ä¢ Kaiming Normal <ref type="bibr" target="#b8">[8]</ref>, which we denote N k . Following the notation in section 3.2 the Kaiming Normal distribution is defined as N k = N 0, 2/n ‚àí1 where N denotes the normal distribution.</p><p>‚Ä¢ Signed Kaiming Constant which we denote U k . Here we set each weight to be a constant and randomly choose its sign to be + or ‚àí. The constant we choose is the standard deviation of Kaiming Normal, and as a result the variance is the same. We use the notation U k as we are sampling uniformly from the set {‚àíœÉ k , œÉ k } where œÉ k is the standard deviation for Kaiming Normal (i.e. 2/n ‚àí1 ).</p><p>In Section 4.5 we reflect on the importance of the random distribution and experiment with alternatives.  <ref type="table">Table 1</ref>. For completeness we provide the architecture of the simple VGG-like <ref type="bibr" target="#b26">[27]</ref> architectures used for CIFAR-10 <ref type="bibr" target="#b13">[13]</ref>, which are identical to those used by Frankle and Carbin <ref type="bibr" target="#b5">[5]</ref> and Zhou et al. <ref type="bibr" target="#b32">[33]</ref>. However, the slightly deeper Conv8 does not appear in the previous work. Each model first performs convolutions followed by the fully connected (FC) layers, and pool denotes max-pooling.</p><p>On CIFAR-10 <ref type="bibr" target="#b13">[13]</ref> we experiment with simple VGG-like architectures of varying depth. These architectures are also used by Frankle and Carbin <ref type="bibr" target="#b5">[5]</ref> and Zhou et al. <ref type="bibr" target="#b32">[33]</ref> and are provided in <ref type="table">Table 1</ref>. On ImageNet we experiment with ResNet-50 and ResNet-101 <ref type="bibr" target="#b9">[9]</ref>, as well as their wide variants <ref type="bibr" target="#b31">[32]</ref>. In every experiment (for all baselines, datasets, and our algorithm) we optimize for 100 epochs and report the last epoch accuracy on the validation set. When we optimize with Adam <ref type="bibr" target="#b12">[12]</ref> we do not decay the learning rate. When we optimize with SGD we use cosine learning rate decay <ref type="bibr" target="#b20">[20]</ref>. On CIFAR-10 <ref type="bibr" target="#b13">[13]</ref> we train our models with weight decay 1e-4, momentum 0.9, batch size 128, and learning rate 0.1. We also often run both an Adam and SGD baseline where the weights are learned. The Adam baseline uses the same learning rate and batch size as in <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b32">33]</ref>  <ref type="bibr" target="#b2">3</ref> . For the SGD baseline we find that training does not converge with learning rate 0.1, and so we use 0.01. As standard we also use weight decay 1e-4, momentum 0.9, and batch size 128. For the ImageNet experiments we use the hyperparam-0.25 0.50 0.75 1.00 1. <ref type="bibr" target="#b24">25</ref>   <ref type="bibr" target="#b8">[8]</ref>. This discussion has encompassed the extent of the hyperparameter tuning for our models. We do, however, perform hyperparameter tuning for the Zhou et al. <ref type="bibr" target="#b32">[33]</ref> baseline and improve accuracy significantly. We include further discussion of this in Section 4.4.</p><p>In all experiments on CIFAR-10 <ref type="bibr" target="#b13">[13]</ref> we use 5 different random seeds and plot the mean accuracy ¬± one standard deviation. Moreover, on all figures, Learned Dense Weights denotes the standard training the full model (all weights remaining).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Varying the % of Weights</head><p>Our algorithm has one associated parameter: the % of weights which remain in the subnetwork, which we refer to as k. <ref type="figure">Figure 3</ref> illustrates how the accuracy of the subnetwork we find varies with k, a trend which we will now dissect. We consider k ‚àà <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">50,</ref><ref type="bibr">70,</ref><ref type="bibr">90]</ref> and plot the dense model when it is trained as a horizontal line (as it has 100% of the weights).</p><p>We recieve the worst accuracy when k approaches 0 or 100. When k approaches 0, we are not able to perform well as our subnetwork has very few weights. On the other hand, when k approaches 100, our network outputs are random.</p><p>The best accuracy occurs when k ‚àà <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">70]</ref>, and we make a combinatorial argument for this trend. We are choosing kn weights out of n, and there are n kn ways of doing so. The number of possible subnetworks is therefore maximized when k ‚âà 0.5, and at this value our search space is at its largest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Varying the Width</head><p>Our intuition from Section 3.1 suggests that as the network gets wider, a subnetwork of a randomly weighted model should approach the trained model in accuracy. How wide is wide enough? In <ref type="figure">Figure 4</ref> we vary the width of Conv4 and Conv6. The width of a linear layer is the number of "neurons", and the width of a convolution layer is the number of channels. The width multiplier is the factor by which the width of all layers is scaled. A width multiplier of 1 corresponds to the models tested in <ref type="figure">Figure 3</ref>. As the width multiplier increases, the gap shrinks between the accuracy a subnetwork found with edge-popup and the dense model when it is trained. Notably, when Conv6 is wide enough, a subnetwork of the randomly weighted model (with %Weights = 50) performs just as well as the dense model when it is trained.</p><p>Moreover, this boost in performance is not solely from the subnetwork having more parameters. Even when the # of parameters is fixed, increasing the width and therefore the search space leads to better performance. In <ref type="figure" target="#fig_0">Figure 5</ref> we fix the number of parameters and while modifying k and the width multiplier. Specifically, we test k ‚àà [30, 50, 70] for subnetworks of constant size c 1 , c 2 and c 3 . On <ref type="figure" target="#fig_0">Figure 5</ref> we use |E| denote the size of the subnetwork.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparing with Zhou et al. [33]</head><p>In <ref type="figure">Figure 6</ref> we compare the performance of edge-popup with Zhou et al. Their work considers distributions N x and U x , which are identical to those presented in Section 4.1 but with xavier normal <ref type="bibr" target="#b7">[7]</ref> instead of kaiming normal <ref type="bibr" target="#b8">[8]</ref> -the factor of ‚àö 2 is omitted from the standard deviation. By running their algorithm with N k and U k we witness a significant improvement. However, even the N x and U x results exceed those in the paper as we perform some hyperparameter tuning. As in our experiments on CIFAR-10, we use SGD with weight decay 1e-4, momentum 0.9, batch size 128, and a cosine scheduler <ref type="bibr" target="#b20">[20]</ref>. We double the learning rate until we see the performance become worse, and settle on 200 4 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Effect of The Distribution</head><p>The distribution that the random weights are sampled from is very important. As illustrated by <ref type="figure" target="#fig_2">Figure 7</ref>, the performance of our algorithm vastly decreases when we switch to using xavier normal <ref type="bibr" target="#b7">[7]</ref> or kaiming uniform <ref type="bibr" target="#b8">[8]</ref>.</p><p>Following the derivation in <ref type="bibr" target="#b8">[8]</ref>, the variance of the forward pass is not exactly 1 when we consider a subnetwork with only k% of the weights. To reconcile for this we could scale standard deviation by 1/k. This distribution is referred to as "Scaled Kaiming Normal" on <ref type="figure" target="#fig_2">Figure 7</ref>. We may also consider this scaling for the Signed Kaiming Constant distribution which is described in Section 4.1. <ref type="figure">Figure 8</ref>. Testing our Algorithm on ImageNet <ref type="bibr" target="#b4">[4]</ref>. We use a fixed k = 30%, and find subnetworks within a randomly weighted ResNet-50 <ref type="bibr" target="#b9">[9]</ref>, Wide ResNet-50 <ref type="bibr" target="#b31">[32]</ref>, and ResNet-101. Notably, a randomly weighted Wide ResNet-50 contains a subnetwork which is smaller than, but matches the performance of ResNet-34. Note that for the non-dense models, # of Parameters denotes the size of the subnetwork.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">ImageNet [4] Experiments</head><p>On ImageNet we observe similar trends to CIFAR-10. As ImageNet is a much harder dataset, computationally feasible models are not overparameterized to the same degree. As a consequence, the performance of a randomly weighted subnetwork does not match the full model with learned weights. However, we still witness a very encouraging trend -the performance increases with the width and depth of the network.</p><p>As illustrated by <ref type="figure">Figure 8</ref>, a randomly weighted Wide ResNet-50 contains a subnetwork that is smaller than, but matches the accuracy of ResNet-34 when trained on Ima-geNet <ref type="bibr" target="#b4">[4]</ref>. As strongly suggested by our trends, better and larger "parent" networks would result in even stronger performance on ImageNet <ref type="bibr" target="#b4">[4]</ref>. A table which reports the numbers in <ref type="figure">Figure 8</ref> may be found in Section A of the appendix. <ref type="figure" target="#fig_3">Figure 9</ref> illustrates the effect of k, which follows an almost identical trend: k ‚àà [30, 70] performs best though 30 now provides the best performance. <ref type="figure" target="#fig_3">Figure 9</ref> also demonstrates that we significantly outperform Zhou et al. at scale (in their original work they do not consider ImageNet). For Zhou et al. on ImageNet we report the best top-1 accuracy as we find their performance degrades towards the end of training. This is the only case where we do not report last epoch accuracy.</p><p>The choice of the random distribution matters more for ImageNet. The "Scaled" distribution we discuss in Section 4.5 did not show any discernable difference on CIFAR-10. However, <ref type="figure" target="#fig_4">Figure 10</ref> illustrates that on ImageNet it is much  better. Recall that the "Scaled" distribution adds a factor of 1/k, which has less of an effect when k approaches 100% = 1. This result highlights the possibility of finding better distributions which work better for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Hidden within randomly weighted neural networks we find subnetworks with compelling accuracy. This work provides an avenue for many areas of exploration. Finally, we hope that our findings serve as a useful step in the pursuit of understanding the optimization and initialization of neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Table of ImageNet Results</head><p>In <ref type="table">Table 2</ref> we provide a table of the results for image classification with ImageNet <ref type="bibr" target="#b4">[4]</ref>. These results correspond exactly to <ref type="figure">Figure  8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Technical Details</head><p>In this section we first prove a more general case of Theorem 1 then provide an extension of edge-popup for convolutions along with code in PyTorch <ref type="bibr" target="#b22">[23]</ref>, found in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. A More General Case of Theorem 1</head><p>Theorem 1 (more general): When a nonzero number of edges are swapped in one layer and the rest of the network remains fixed then the loss decreases for the mini-batch (provided the loss is sufficiently smooth). Proof. As before, we letsuv denote the score of weight wuv after the gradient update. Additionally, letƒ®v denote the input to node v after the gradient update whereas Iv is the input to node v before the update. Finally, let i1, ..., in denote the n nodes in layer ‚àí 1 and j1, ..., jm denote the m notes in layer . Our goal is to show that</p><formula xml:id="formula_10">L ƒ® j 1 , ...,ƒ®j m &lt; L Ij 1 , ..., Ij m<label>(12)</label></formula><p>where the loss is written as a function of layer 's input for brevity. If the loss is smooth andƒ®j k is close to Ij k we may ignore secondorder terms in a Taylor expansion: </p><formula xml:id="formula_11">L ƒ® j 1 , ...,</formula><formula xml:id="formula_12">‚àÇL ‚àÇIj k ƒ® j k ‚àí Ij k &lt; 0.<label>(16)</label></formula><p>It is helpful to rewrite the sum to be over edges. Specifically, we will consider the sets Eold and Enew where Enew contains all edges that entered the network after the gradient update and Eold consists of edges which were previously in the subnetwork, but have now exited. As the total number of edges is conserved we know that |Enew| = |Eold|, and by assumption |Enew| &gt; 0.</p><p>Using the definition of I k andƒ® k from Equation 3 we may rewrite <ref type="bibr">Equation 16</ref> as</p><formula xml:id="formula_13">(ia,j b )‚ààEnew ‚àÇL ‚àÇIj b wi a j b Zi a ‚àí (ic,j d )‚ààE old ‚àÇL ‚àÇIj d wi c j d Zi c &lt; 0<label>(17)</label></formula><p>which, by Equation 6 and factoring out 1/Œ± becomes</p><formula xml:id="formula_14">(ia,j b )‚ààEnew (si a j b ‚àísi ajb ) ‚àí (ic,j d )‚ààE old (si c j d ‚àísi c j d ) &lt; 0.<label>(18)</label></formula><p>We now show that</p><formula xml:id="formula_15">(si a j b ‚àísi a j b ) ‚àí (si c j d ‚àísi c j d ) &lt; 0<label>(19)</label></formula><p>for any pair of edges (ia, j b ) ‚àà Enew and (ic, j d ) ‚àà Eold. Since |Enew| = |Eold| &gt; 0 we are then able to conclude that <ref type="bibr">Equation 18</ref> holds.</p><p>As (ia, j b ) was not in the edge set before the gradient update, but (ic, j d ) was, we can conclude</p><formula xml:id="formula_16">si a j b ‚àí si c j d &lt; 0.<label>(20)</label></formula><p>Likewise, since (ia, j b ) is in the edge set after the gradient update, but (ic, j d ) isn't, we can conclud·∫Ω si c j d ‚àísi a j b &lt; 0.</p><p>By adding Equation <ref type="bibr" target="#b21">21</ref> and <ref type="bibr">Equation 20</ref> we find that Equation 19 is satisfied as needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Extension to Convolutional Neural Networks</head><p>In order to show that our method extends to convolutional layers we recall that convolutions may be written in a form that resembles Equation 2. Let Œ∫ be the kernel size which we assume is odd for simplicity, then for w ‚àà {1, ..., W } and h ‚àà {1, ..., H} we have</p><formula xml:id="formula_18">I w,h v = u‚ààV ( ‚àí1) Œ∫ Œ∫ 1 =1 Œ∫ Œ∫ 2 =1 w (Œ∫ 1 ,Œ∫ 2 ) uv Z (w+Œ∫1‚àí Œ∫ 2 ,h+Œ∫ 2 ‚àí Œ∫ 2 ) u<label>(22)</label></formula><p>where instead of "neurons", we now have "channels". The input Iv and output Zv are now two dimensional and so Z (w,h) v is a scalar. As before, Zv = œÉ (Iv) where œÉ is a nonlinear function. However, in the convolutional case œÉ is often batch norm <ref type="bibr" target="#b11">[11]</ref> followed by ReLU (and then implicitly followed by zero padding).</p><p>Instead of simply having weights wuv we now have weights w (Œ∫ 1 ,Œ∫ 2 ) uv for Œ∫1 ‚àà {1, ..., Œ∫}, Œ∫2 ‚àà {1, ..., Œ∫}. Likewise, in our edge-popup Algorithm we now consider scores s (Œ∫ 1 ,Œ∫ 2 ) uv and again use the top k% in the forwards pass. As before, let h s The update for the scores is quite similar, though we must now sum over all spatial (i.e. w and h) locations as given below:  <ref type="table">Table 2</ref>. ImageNet <ref type="bibr" target="#b4">[4]</ref> classification results corresponding to <ref type="figure">Figure 8</ref>. Note that for the non-dense models, # of Parameters denotes the size of the subnetwork.</p><formula xml:id="formula_19">s (Œ∫ 1 ,Œ∫ 2 ) uv ‚Üê s (Œ∫ 1 ,Œ∫ 2 ) uv ‚àí Œ± W w=1 H h=1 ‚àÇL ‚àÇI w,h v w (Œ∫ 1 ,Œ∫ 2 ) uv Z (w+Œ∫1‚àí Œ∫ 2 ,h+Œ∫ 2 ‚àí Œ∫ 2 ) u<label>(24)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 .</head><label>5</label><figDesc>of Parameters | | = c 1 , Weights k | | = c 2 , Weights k | | = c 3 , Weights k Varying the width of Conv4 on CIFAR-10 [13] while modifying k so that the # of Parameters is fixed along each curve. c1, c2, c3 are constants which coincide with # of Parameters for k = [30, 50, 70] for width multiplier 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 6 Figure 6 .</head><label>66</label><figDesc>Zhou et al. ) Weights k (Zhou et al. ) Weights x (Zhou et al. ) Weights U k (Zhou et al. ) Weights U x Comparing the performance of edge-popup with the algorithm presented by Zhou et al. [33] on CIFAR-10 [13].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Testing different weight distributions on CIFAR-10<ref type="bibr" target="#b13">[13]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 .</head><label>9</label><figDesc>Examining the effect of % weights on ImageNet for edge-popup and the method of Zhou et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 .</head><label>10</label><figDesc>Examining the effect of using the "Scaled" initialization detailed in Section 4.5 on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(Œ∫ 1</head><label>1</label><figDesc>,Œ∫ 2 ) uv = 1 if s (Œ∫ 1 ,Œ∫ 2 ) uv is among the top k% highest scores in the layer and h s (Œ∫ 1 ,Œ∫ 2 ) uv = 0 otherwise. Then in edge-popup we are performing a convolution as I formulation of edge-popup in Equation 4. In fact, when Œ∫ = W = H = 1 (i.e. a 1x1 convolution on a 1x1 feature map) then Equation 23 and Equation 4 are equivalent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>+ ƒ® j 1 ‚àí Ij 1 , ..., Ij m + ƒ® jm ‚àí Ij m (14) = L (Ij 1 , ..., Ij m ) +</figDesc><table><row><cell>ƒ®j m</cell><cell></cell><cell></cell><cell>(13)</cell></row><row><cell>= L Ij 1 m k=1</cell><cell>‚àÇL ‚àÇIj k</cell><cell>ƒ® j k ‚àí Ij k</cell><cell>(15)</cell></row><row><cell cols="4">And so, in order to show Equation 12 it suffices to show that</cell></row><row><cell>m</cell><cell></cell><cell></cell><cell></cell></row><row><cell>k=1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The original network has edges E fc = L‚àí1 =1 (V √ó V +1 ) where √ó denotes the cross-product.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">To ensure that the scores are positive we take the absolute value.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Batch size 60, learning rate 2e-4, 3e-4 and 3e-4 for Conv2, Conv4, and Conv6 respectively Conv8 is not tested in<ref type="bibr" target="#b5">[5]</ref>, though we use find that learning rate 3e-4 still performs well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">A very high learning rate is required as mentioned in their work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Jesse Dodge and Nicholas Lourie and Gabriel Ilharco for helpful discussions and Sarah Pratt for n 2 . This work is in part supported by NSF IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, 67102239, gifts from Allen Institute for Artificial Intelligence, and the AI2 Fellowship for AI.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p># self.k is the % of weights remaining, a real number in [0,1] # self.popup_scores is a Parameter which has the same shape as self.weight # Gradients to self.weight, self.bias have been turned off. def forward(self, x): # Get the subnetwork by sorting the scores. adj = GetSubnet.apply( self.popup_scores.abs(), self.k) # Use only the subnetwork in the forward pass. w = self.weight * adj x = F.conv2d( x, w, self.bias, self.stride, self.padding, self.dilation, self.groups ) return x</p><p>In summary, we now have Œ∫ 2 edges between each u and v. The PyTorch <ref type="bibr" target="#b22">[23]</ref> code is given by Algorithm 1, where h is GetSubnet. The gradient goes straight through h in the backward pass, and PyTorch handles the implementation of these equations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Resnet18 on CIFAR-10</head><p>In <ref type="figure">figure 11</ref> we experiment with a more advanced network architecture on CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Are these subnetworks lottery tickets?</head><p>What happens when we train the weights of the subnetworks form <ref type="figure">Figure 8</ref> and <ref type="table">Table 2</ref> on ImageNet? They do not train to the same accuracy as a dense network, and do not perform substantially better than training a random subnetwork. This suggests that the good performance of these subnetworks at initialization does not explain the lottery phenomena described in <ref type="bibr" target="#b5">[5]</ref>. The results can be found in <ref type="table">Table 3</ref>  <ref type="table">Table 3</ref>. ImageNet <ref type="bibr" target="#b4">[4]</ref> classification results after training the discovered subnetworks. Surprisingly, the accuracy is not substantially better than training a random subnetwork. This suggests that the good performance of these subnetworks does not explain the lottery phenomena described in <ref type="bibr" target="#b5">[5]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyanaarachchi</forename><surname>Lekamalage Chamara Kasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Man</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>et al. Extreme learning machines [trends &amp; controversies</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE intelligent systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="30" to="59" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Weight agnostic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Gaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<ptr target="https://weightagnostic.github.io.2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Yee Whye Teh and Mike Titterington</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>Chia Laguna Resort</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A signal propagation perspective for pruning neural networks at initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06307</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02340</idno>
		<title level="m">Snip: Single-shot network pruning based on connection sensitivity</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Intriguing properties of randomly weighted networks: Generalizing while learning next to nothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John K Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 16th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1807.11626</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Discovering neural wirings. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01569</idno>
		<title level="m">Exploring randomly wired neural networks for image recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deconstructing lottery tickets: Zeros, signs, and the supermask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hattie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
