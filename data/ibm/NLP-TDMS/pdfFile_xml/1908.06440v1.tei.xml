<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengju</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqiang</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">SenseTime Research 4 YouTu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<email>qianchen@sensetime.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">SenseTime Research 4 YouTu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial landmark detection, or face alignment, is a fundamental task that has been extensively studied. In this paper, we investigate a new perspective of facial landmark detection and demonstrate it leads to further notable improvement. Given that any face images can be factored into space of style that captures lighting, texture and image environment, and a style-invariant structure space, our key idea is to leverage disentangled style and shape space of each individual to augment existing structures via style translation. With these augmented synthetic samples, our semi-supervised model surprisingly outperforms the fully-supervised one by a large margin. Extensive experiments verify the effectiveness of our idea with state-of-the-art results on WFLW [69], 300W [56], COFW <ref type="bibr" target="#b6">[7]</ref>, and AFLW [36] datasets. Our proposed structure is general and could be assembled into any face alignment frameworks. The code is made publicly available at https://github.com/thesouthfrog/stylealign.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial landmark detection is a fundamentally important step in many face applications, such as face recognition <ref type="bibr" target="#b43">[44]</ref>, 3D face reconstruction <ref type="bibr" target="#b16">[17]</ref>, face tracking <ref type="bibr" target="#b32">[33]</ref> and face editing <ref type="bibr" target="#b60">[61]</ref>. Accurate facial landmark localization was intensively studied with impressive progress made in these years. The main streams are learning a robust and discriminative model through effective network structure <ref type="bibr" target="#b68">[69]</ref>, usage of geometric information <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref>, and correction of loss functions <ref type="bibr" target="#b18">[19]</ref>.</p><p>It is common wisdom now that factors such as variation of expression, pose, shape, and occlusion could greatly affect performance of landmark localization. Almost all prior work aims to alleviate these problems from the perspective of structural characteristics, such as disentangling 3D pose to provide shape constraint <ref type="bibr" target="#b36">[37]</ref>, and utilizing dense bound- <ref type="figure">Figure 1</ref>: Problem in a well-trained facial landmark detector. It is biased towards unconstrained environment factors, including lighting, image quality, and occlusion. We regard these degradations as "style" in our analysis. ary information <ref type="bibr" target="#b68">[69]</ref>. The influence of "environment" still lacks principled discussion beyond structure. Also, considering limited labeled data for this task, how to optimally utilize limited training samples remains unexplored.</p><p>About "environment" effect, distortion brought by explicit image style variance was observed recently <ref type="bibr" target="#b14">[15]</ref>. We instead utilize style transfer <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21]</ref> and disentangled representation learning <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref> to tackle the face alignment problem, since style transfer aims at altering style while preserving content. In practice, image content refers to objects, semantics and sharp edge maps, whereas style could be color and texture.</p><p>Our idea is based on the purpose of facial landmark detection, which is to regress "facial content" -the principal component of facial geometry -by filtering unconstrained "styles". The fundamental difference to define "style" from that of <ref type="bibr" target="#b14">[15]</ref> is that we refer it to image background, lighting, quality, existence of glasses, and other factors that prevent detectors from recognizing facial geometry. We note every face image can be decomposed into its facial structure along with a distinctive attribute. It is a natural conjecture that face alignment could be more robust if we augment images only regarding their styles.</p><p>To this end, we propose a new framework to augment training for facial landmark detection without using extra knowledge. Instead of directly generating images, we first map face images into the space of structure and style. To guarantee the disentanglement of these two spaces, we design a conditional variational auto-encoder <ref type="bibr" target="#b34">[35]</ref> model, in which Kullback-Leiber (KL) divergence loss and skip connections are incorporated for compact representation of style and structure respectively. By factoring these features, we perform visual style translation between existing facial geometry. Given existing facial structure, faces with glasses, of poor quality, under blur or strong lighting are rerendered from corresponding style, which are used to further train the facial landmark detectors for a rather general and robust system to recognize facial geometry.</p><p>Our main contribution is as follows.</p><p>1. We offer a new perspective for facial landmark localization by factoring style and structure. Consequently, a face image is decomposed and rendered from distinctive image style and facial geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A novel semi-supervised framework based on conditional variational auto-encoder is built upon this new perspective. By disentangling style and structure, our model generates style-augmented images via style translation, further boosting facial landmark detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>We propose a new dataset based on AFLW <ref type="bibr" target="#b35">[36]</ref> with new 68-point annotation. It provides challenging benchmark considering large pose variation.</p><p>With extensive experiments on popular benchmark datasets including WFLW <ref type="bibr" target="#b68">[69]</ref>, 300W <ref type="bibr" target="#b55">[56]</ref>, COFW <ref type="bibr" target="#b6">[7]</ref> and AFLW <ref type="bibr" target="#b35">[36]</ref>, our approach outperforms previous state-ofthe-arts by a large margin. It is general to be incorporated into various frameworks for further performance improvement. Our method also works well under limited training computation resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This work has close connection with the areas of facial landmark detection, disentangled representation and selfsupervised learning.</p><p>Facial Landmark Detection This area has been extensively studied over past years. Classic parameterized methods, such as active appearance models (AAMs) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b31">32]</ref> and constrained local models (CLMs) <ref type="bibr" target="#b11">[12]</ref> provide satisfying results. SDM <ref type="bibr" target="#b72">[73]</ref>, cascaded regression, and their variants <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b17">18]</ref> were also proposed.</p><p>Recently, with the power of deep neural networks, regression-based models are able to produce better results. They are mainly divided into two streams of direct coordinate regression <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b49">50]</ref> and heatmap-based regression <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b47">48]</ref>. Meanwhile, in <ref type="bibr" target="#b79">[80]</ref>, auxiliary attributes were used to learn a discriminative representation. Recurrent modules <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b51">52]</ref> were introduced then. Lately, methods improved performance via semi-supervised learning <ref type="bibr" target="#b25">[26]</ref>. Influence of style variance was also discussed in <ref type="bibr" target="#b14">[15]</ref>, where a style aggregated component provides a stationary environment for landmark detector. Our solutions are distinct with definition of "style", different from prior work. Our solution does not rely on the aggregation architecture, and instead is based on a semi-supervised scheme.</p><p>Disentangled Representation Our work is also related to disentangled representation learning. Disentanglement is necessary to control and further alter the latent information in generated images. Under the unsupervised setting, InfoGAN <ref type="bibr" target="#b9">[10]</ref> and MINE <ref type="bibr">[3]</ref> learned disentangled representation by maximizing the mutual information between latent code and data observation. Recently, imageto-image translation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b28">29]</ref> explored the disentanglement between style and content without supervision. In structured tasks such as conditional image synthesis <ref type="bibr" target="#b45">[46]</ref>, keypoints <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b52">53]</ref> and person mask <ref type="bibr">[2]</ref> were utilized as self-supervision signals to disentangle factors, such as foreground, background and pose information. As our "style" is more complex while "content" is represented by facial geometry, traditional style transfer <ref type="bibr" target="#b20">[21]</ref> is inapplicable since it may suffer from structural distortion. In our setting, by leveraging the structure information base on landmarks, our separation component extracts the style factor from each face image.</p><p>Self-Supervised Learning Our method also connects to self-supervised learning. The mainstream work, such as <ref type="bibr" target="#b75">[76]</ref>, directly uses image data to provide proxy supervision through multi-task feature learning. Another widelyadopted approach is to use video data <ref type="bibr" target="#b65">[66]</ref>. Visual invariance of the same instance could be captured in a consecutive sequence of video frames <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b65">66]</ref>. Also, there is work focusing on fixed characteristics of objects from data statistics <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, such as image patch level information <ref type="bibr" target="#b13">[14]</ref>. These methods learn visual invariance, which could essentially provide a generalized feature of objects.</p><p>Our landmark localization involves computing the visual invariance. But our approach is different from prior selfsupervised frameworks. Our goal lies in extracting facial structure and keypoints considering different environment factors, including occlusion, lighting, makeup and so on. Eliminating the influence of style makes it possible to reliably alter or process face structure and accordingly recog- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Framework</head><p>Our framework consists of two parts. One learns the disentangled representation of facial appearance and structure, while the other can be any facial landmark detectors. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, during the first phase, conditional variational auto-encoder is proposed for learning disentangled representation between style and structure. In the second phase, after translating style from other faces, "stylized" images with their structures are available for boosting training performance and our style-invariant detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning Disentangled Style and Structure</head><p>Given an image x, and its corresponding structure y. Two essential descriptors of a face image are facial geometry and image style. Facial geometry is represented by labeled landmarks, while style captures all environmental factors that are mostly implicit, as described above. With this setting, if the latent space of style and shape is mostly uncorrelated, using Cartesian product of z and y latent space should capture all variation included in a face image. Therefore, the generator that re-renders a face image based on style and structure can be modeled as p(x|y, z).</p><p>To encode the style and structure information and compute the parametric distribution p(x|y, z), a conditional variational auto-encoder based network, which introduces two encoders, is applied. Our network consists of a structure estimator E struct to encode landmark heatmaps into structure latent space, a style encoder E style that learns the style embedding of images, and a decoder that re-renders the style and structure to image space.</p><p>As landmarks available in this task, the facial geometry is represented by stacking landmarks to heat maps. Our goal therefore becomes inferring disentangled style code z from a face image and its structure by maximizing the conditional likelihood of</p><formula xml:id="formula_0">log p(x|y) = log z p(x, z|y)dz ≥ E q [log p(x, z|y) q(z|x, y) ] = E q [log p(x|z, y)] − D KL [q(z|x, y), p(z|y)].<label>(1)</label></formula><p>In particular, the generator G f ull θ contains two encoders and a decoder (renderer), i.e., E style φ , E struct and D render , where G f ull θ and E style φ respectively estimate parameters of p(x|y, z) and q(z|x, y). Consequently, the full loss function on learning separating information of style and structure is written as</p><formula xml:id="formula_1">L disentangle (x, θ, φ) = −KL(q φ (z|x, y))||p θ (z|y)) +L rec (x, G f ull (E style (x, y), E struct (y)).<label>(2)</label></formula><p>KL-Divergence Loss Kullback-Leiber (KL) divergence loss severs as a key component in our design to help the encoder to learn decent representation. Basically, the KLdivergence measures the similarity between the variational posterior and prior distribution. In our framework, it is taken as regularization that discourages E style to encode structure-related information. As the prior distribution is commonly assumed to be a unit Gaussian distribution p ∼ N (0, 1), the learned style feature is regularized to suppress contained structure information through reconstruction. The KL-divergence loss limits the distribution range and capacity of the style feature. By fusing inferred style code z with encoded structure representation, sufficient structure information can be obtained from prior through multi-level skip connection. Extra structure encoded in z incurs penalty of the likelihood p(x|y, z) during training with no new information captured. In this way, E style is discouraged from learning structure information that is provided by E struct during training. To better reconstruct the original image, E style is enforced to learn structure-invariant style information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction Loss</head><p>The second term L rec in Eq. (2) refers to the reconstruction loss in the auto-encoder frame- work. As widely discussed <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b29">30]</ref>, basic pixel-wise L 1 or L 2 loss cannot model rich information within images well. We instead adopt perceptual loss to capture style information and better visual quality. L rec is formulated as</p><formula xml:id="formula_2">L rec (x, θ, φ) = l ||(Φ l (x) − Φ l (G f ull (x, y))|| 2 2 ,<label>(3)</label></formula><p>where we use VGG-19 network Φ structure that measures perceptual quality. l indexes the layer of network Φ.</p><p>Since the style definition could be complicated, E style here encodes semantics of the style signal that simulates different types of degradation. It does not have to maintain fine-grained visual details. Besides, to reserve the strong prior on structure information encoded from landmarks y, skip connection between E struct and D render is established to avoid landmark inaccuracy through style translation.</p><p>In this design, the model is capable of learning complementary representation of facial geometry and image style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Augmenting Training via Style Translation</head><p>Disentanglement of structure and style forms a solid foundation for diverse stylized face images under invariant structure prior.</p><p>Given a dataset X that contains n face images with landmarks annotation, each face image x i (1 ≤ i ≤ n) within the dataset has its explicit structure denoted by landmark y i , as well as an implicit style code z i depicted and embedded by E style . To perform style translation between two images x i and x j , we pass their latent style and structure code embedded by E style and E struct to D render . To put the style of image x j on x i 's structure, the stylized synthetic image is denoted as</p><formula xml:id="formula_3">x ij = D render (E style (x j , y i ), E struct (y i )).<label>(4)</label></formula><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, the first stage of our framework is to train the disentangling components. In the second phase, by augmenting and rendering a given sample x in the original dataset X with styles from random k other faces, we produce k × n "stylized" synthetic face images with respective annotated landmarks. These samples are then fed into training of facial landmark detectors together with the original dataset. Visualization of style translation results is provided in <ref type="figure" target="#fig_1">Fig. 3</ref>. The input facial geometry is maintained under severe style variation, indicating its potential at augmenting training of facial landmark detectors. Albeit with cohesive structure, the decoder generally does not re-render perfect-quality images, since the complexity of plentiful style information has been diminished to a parametric Gaussian distribution, confined by its capacity. Also, as discussed before, each face image x i has its own style. Theoretically, the renderer could synthesize n 2 images by rendering each available landmark with any other images' style. To understand how the quantity of stylized synthetic samples helps improve the facial landmark detectors, we analyze the effect of our design in following experiments and ablation study.  To provide a better benchmark for evaluating pose variation and allow cross-dataset evaluation, we re-annotate it with 68 facial landmarks, which follow the common standard in 300W <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b57">58]</ref>. Based on the new 68-point annotation, we conduct more precise evaluation. Cross-dataset evaluation is also provided among existing datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b40">41]</ref>. COFW dataset [7] contains 1,345 images for training and 507 images for testing, focusing on occlusion. The whole dataset is originally annotated with 29 landmarks and has been re-annotated with 68 landmarks in <ref type="bibr" target="#b21">[22]</ref> to allow crossdataset evaluation. We utilize 68 annotated landmarks provided by <ref type="bibr" target="#b21">[22]</ref> to conduct comparison with other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setting</head><p>Evaluation Metrics We evaluate performance of facial landmark detection using normalized landmarks mean error and Cumulative Errors Distribution (CED) curve. For the 300W dataset, we normalize the error using inter-pupil distance. In <ref type="table" target="#tab_2">Table 2</ref>, we also report the NME using inter-ocular distance to compare with algorithms of <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b36">37]</ref>, which also use it as the normalizing factor. For other datasets, we follow the protocol used in <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b62">63]</ref> and apply inter-ocular distance for normalization.</p><p>Implementation Details Before training, all images are cropped and resized to 256 × 256 using provided bounding boxes. For the detailed conditional variational autoencoder network structures, we use a two-branch encoderdecoder structure as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. We use 6 residual encoder blocks for downsampling the input feature maps, where batch normalization is removed for better synthetic results. The facial landmark detector backbone is substitutable and different detectors are usable to achieve improvement, which we will discuss later. For training of the disentangling step, we use Adam <ref type="bibr" target="#b33">[34]</ref> with an initial learning rate of 0.01, which descends linearly to 0.0001 with no augmentation. For training of detectors, we first augment each landmark map with k random styles sampled from other face images. The number is set to 8 if not specially mentioned in experiments. For the detector architecture, a simple baseline network based on ResNet-18 <ref type="bibr" target="#b23">[24]</ref> is chosen by changing the output dimension of the last FC layers to landmark × 2 to demonstrate the increase brought by style translation. To compare with state-of-thearts and further validate the effectiveness of our approach, we replace our baseline model with similar structures proposed in <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b14">15]</ref>, with the same affine augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-arts</head><p>WFLW We evaluate our approach on WFLW <ref type="bibr" target="#b68">[69]</ref> dataset. WFLW is a recently proposed challenging dataset with images from in-the-wild environment. We compare algorithm in terms of NME(%), Failure Rate(%) and AUC(@0.1) following protocols used in <ref type="bibr" target="#b68">[69]</ref>.</p><p>The Res-18 baseline receives strong enhancement using synthetic images. To further verify the effectiveness and generality of using style information, we replace the network by two strong baselines <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b68">69]</ref> and report the result in <ref type="table">Table 1</ref>. The light-weight Res-18 is improved by 13.8%. By utilizing a stronger baseline, our model achieves 4.39% NME under style-augmented training, outperforms state-ofthe-art entries by a large margin. In particular, for the strong baselines, our method also brings 15.9% improvement to SAN <ref type="bibr" target="#b14">[15]</ref> model, and 9% boost to LAB [69] from 5.27% NME to 4.76%. The elevation is also determined by the model capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>300W</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, we report different facial landmark detector performance (in terms of normalized mean error) on 300W dataset. The baseline network follows Res-18 structure. With additional "style-augmented" synthetic training samples, our model based on a simple backbone outperforms previous state-of-the-art methods. We also report results of models that are trained on original data, which reflect the performance gain brought by our approach.</p><p>Similarly, we replace the baseline model with a state-ofthe-art method <ref type="bibr" target="#b14">[15]</ref>. Following the same setting, this baseline is also much elevated. Note that the 4-stack LAB <ref type="bibr" target="#b68">[69]</ref> and SAN <ref type="bibr" target="#b14">[15]</ref> are open-source frameworks. We train the models from scratch, which perform less well than those reported in their original papers. However, our model still yields 1.8% and 3.1% improvement on LAB and SAN respectively, which manifest the consistent benefit when using the "style-augmented" strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Common   Cross-dataset Evaluation on COFW To comprehensively evaluate the robustness of our method towards occlusion, COFW-68 is also utilized for cross-dataset evaluation. We perform comparison against several state-of-theart methods in <ref type="figure" target="#fig_2">Fig. 4</ref>. Our model performs the best with 4.43% mean error and 2.82% failure rate, which indicates high robustness to occlusion due to our proper utilization of style translation.  AFLW We further evaluate our algorithm on the AFLW <ref type="bibr" target="#b35">[36]</ref> dataset following the AFLW Full protocol. AFLW is also challenging for its large pose variation. It is originally annotated with 19 facial landmarks, which are relatively sparse. To make it more useful, we richen the dataset by re-annotating it with 68-point facial landmarks. This new set of data is also publicly available. We compare our approach with several models in <ref type="table" target="#tab_4">Table 3</ref>, by re-implementing their algorithms on the new dataset along with our style-augmented samples. Exploiting style information also boosts landmark detectors with a large-scale training set (25, 000 images in AFLW). Interestingly, our method improves SAN baseline in terms of NME on Full set from 6.94% to 6.01%, which indicates that augmenting in style level brings promising improvement on solving large pose variation. The visual comparison in <ref type="figure">Fig.  5</ref> shows hidden face part is better modeled with our strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Improvement on Limited Data</head><p>Disentanglement of style and structure is the key that influences quality of style-augmented samples. We evaluate the   <ref type="table" target="#tab_3">Table 5</ref>: Normalized mean error (%) on 300W common and WFLW datasets when using different percentages of the training set, with the same protocol as in <ref type="table" target="#tab_6">Table 4</ref> on a stronger baseline. The baseline network here follows SAN <ref type="bibr" target="#b14">[15]</ref> structure.</p><p>show the relative improvement on different training samples. Style-augmented synthetic images improve detectors' performance by a large margin, while the improvement is even larger when the number of training images is quite small. In <ref type="table" target="#tab_3">Table 5</ref>, a stronger baseline SAN <ref type="bibr" target="#b14">[15]</ref> is chosen. Surprisingly, the baseline easily reaches state-of-theart performance using only 50% labeled images, compared to former methods provided in <ref type="table">Table 1</ref>. Besides, <ref type="figure" target="#fig_4">Fig. 6</ref> provides an intuitive visualization of the resulting generated faces when part of the data is used. Each column contains output that is rendered from the input structure and given style, when using a portion of face image data. It shows when the data is limited, our separation component tends to capture weak style information, such as color and lighting. Given more data as examples, the style becomes complex and captures detailed texture and degradation, like occlusion.</p><p>The results verify that even using limited labeled images, our design is capable of disentangling style information and keeps improve those baseline methods that are already very strong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Estimating the Upper-bound</head><p>As discussed before, our method conceptually and empirically augments training with n 2 synthetic samples. By aug-  menting each face image with k random styles, the training set could be very large and slows down convergence. In this section, we experiment with choosing the style augmenting factor k and test the upper bound of style translation. We evaluate our method by adding the number of random sampled styles k of each annotated landmarks on a ResNet-50 baseline.</p><p>The result is reported in <ref type="table" target="#tab_7">Table 6</ref>. By adding a number of augmented styles, the model continue gaining improvement. However, when k ≥ 8, the performance grow slows down. It begins to decrease if k reaches 32. The reason is that due to the quantity imbalance between real and synthetic faces, a very large k makes the model overfit to synthetic image texture when the generated image quantity is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we have analyzed the well-studied facial landmark detection problem from a new perspective of implicit style and environmental factor separation and utilization. Our approach exploits the disentanglement representation of facial geometry and unconstrained style to provide synthetic faces via style translation, further boosting quality of facial landmarks. Extensive experimental results manifest its effectiveness and superiority.</p><p>We also note that utilizing synthetic data for more highlevel vision tasks still remains an open problem, mainly due to the large domain gap between generated and real images. In our future work, we plan to model style in a more realistic way by taking into account the detailed degradation types and visual quality. We also plan to generalize our structure to other vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1. More Ablation Studies</head><p>In this section, we provide additional analysis about each design in our framework to facilitate understanding of our structure. Two key loss terms in our framework are studied to give insights into their respective roles. Qualitative visualization and quantitative results are reported for a comprehensive comparison.</p><p>KL divergence loss and perceptual loss, are incorporated into our framework during the disentangled learning procedure. <ref type="figure" target="#fig_5">Fig. 7</ref> shows their respective effect on style translation via visual comparisons of several incomplete variants. Through visual observations, their roles could be inferred intuitively. The perceptual loss, as discussed, is designed to capture better style information and visual quality. Thus, removing this term leads to "over-smoothness" and poor diversity on synthetic images. Removing KL divergence term shows severe structure distortion on translated results, which indicates that KL divergence loss plays a key role on disentangling structure and style information.</p><p>Quantitative results of each variants are also reported in <ref type="table">Table.</ref> 7. The normalized mean error(NME) is evaluated on WFLW <ref type="bibr" target="#b68">[69]</ref> test set when the model is trained on style augmented dataset using each variant. We observe that NME will increase if any loss function is removed. In particular, the detector performance drops significantly lower than the baseline if L KL is removed. Both the qualitative and quantitative result interprets the role of each component, indicating their essentialness in our framework.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. Additional Discussion</head><p>In this section, we provide more discussion on our approach along with our analysis towards some existing alternatives. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.1 Comparison with GAN-based approaches</head><p>Generative adversarial network (GAN) and its applications are widely studied these days, using GAN-synthetic data to aid training, has also been explored along this line. Some works <ref type="bibr">[1]</ref> have utilized GANs to perform data augmentation. However, its effect still remains questionable especially on high-level vision challenges. For instance, in our task, face images need to be labeled with accurate landmarks. Existing generative models are incapable of handling these tasks with fine-grained annotations, e.g. semantic segmentation, constrained by its limited generalizability. We choose to escape the difficulties of GAN training, starting from a new perspective of internal representation. With decent representation of separating style and structure, different interactions within a face image can be simulated by re-rendering from existing style and structure code. In other words, our choice depends upon fully exploiting available information by mixing them, instead of creating new information and visually perfect results via adversarial learning procedure. However, if two codes of structure and style are factored well, advances on high fidelity images synthesis could theoretically bring more gains based on our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.2 Comparison with Style Transfer</head><p>Our method is motivated by advances in style transfer. A common doubt could be why not directly conducting style transfer as a augmentation or how basic style transfer could help training. As discussed, our definition of style includes environments and degradation that prevent the model from recognizing while content refers to facial geometry. Applying "vanilla" style transfer would leads to structural distortion on stylized images, as illustrated in <ref type="figure">Fig. 8</ref>.Our definition of "style" helps preserve structure on synthetic images. Besides, synthetic images using style transfer have a large domain gap with real-world face images. Simply augmenting training with these samples would instead hurt model's localization ability on real images. <ref type="figure">Figure 8</ref>: Visual comparison with style transfer approach. For the style transfer algorithm, we use <ref type="bibr" target="#b20">[21]</ref>. Our results are more realistic than stylized images, with better structure coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Database</head><p>Environment Number Multi-PIE <ref type="bibr" target="#b22">[23]</ref> Controlled 750000 XM2VTS <ref type="bibr" target="#b48">[49]</ref> 2360 LFPW <ref type="bibr" target="#b3">[4]</ref> 1035 HELEN <ref type="bibr" target="#b40">[41]</ref> In-the-wild 2330 AFW <ref type="bibr" target="#b53">[54]</ref> 468 IBUG 135 COFW-68 <ref type="bibr" target="#b21">[22]</ref> 507 AFLW-68(Ours) 25993 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3. Details of AFLW 68-point dataset</head><p>We propose a new facial landmark dataset based on AFLW <ref type="bibr" target="#b35">[36]</ref>, to facilitate benchmarking on large pose performance. To allow a more precise evaluation and crossdataset comparison, we follow the widely-used Multi- </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Our framework. It consists of two stages. The first stage is to train the network to disentangle face images to style and structure space. At the second stage, style translation is performed to augment training of facial landmark detectors. nize invariant features. It thus better deals with style variation, which commonly exists in natural images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of style translation. Given the input images in red 4 different styles are provided to perform translation towards input structure. The synthetic images along with input original landmarks are provided to demonstrate the strong coherence of structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Cumulative error distribution curve on COFW 68point test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Results of style translation using different numbers of data. The left 2 images are the input, with 2 different reference styles. The percentage refers to how much data is used to train the disentangle module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative analysis of each component in our framework. Given input images in red, 3 different styles are provided to perform translation towards input structure. 3 incomplete variants of our framework are used to show the functionality of each component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Sampled annotated images in the proposed AFLW 68point dataset, including in-the-wild faces under large pose variations PIE [23] and 300W [56] 68-point protocol. Annotated samples are provided at Fig. 9, which contains extreme pose variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The testing images include two subsets, where 554 test samples from LFPW and HELEN form the common subset and 135 images from IBUG constitute the challenging subset. AFLW<ref type="bibr" target="#b35">[36]</ref> dataset is widely used for benchmarking facial landmark localization. It contains 24,386 in-the-wild faces with a wide range of yaw, pitch and roll angles ([−120 • , 120 • ] for yaw, [−90 • , 90 • ] for pitch and roll). Following the widely-adopted protocol<ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b82">83]</ref>, the AFLW-full dataset has 20,000 images for training and 4,386 for testing. It is originally annotated with 19 sparse facial landmarks.</figDesc><table><row><cell>Metric</cell><cell>Method</cell><cell>Fullset</cell><cell>Pose</cell><cell>Expression</cell><cell>Illumination</cell><cell>Make-Up</cell><cell>Occlusion</cell><cell>Blur</cell></row><row><cell></cell><cell>CFSS [82]</cell><cell>9.07</cell><cell>21.36</cell><cell>10.09</cell><cell>8.30</cell><cell>8.74</cell><cell>11.76</cell><cell>9.96</cell></row><row><cell></cell><cell>DVLN [70]</cell><cell>6.08</cell><cell>11.54</cell><cell>6.78</cell><cell>5.73</cell><cell>5.98</cell><cell>7.33</cell><cell>6.88</cell></row><row><cell></cell><cell>LAB [69]</cell><cell>5.27</cell><cell>10.24</cell><cell>5.51</cell><cell>5.23</cell><cell>5.15</cell><cell>6.79</cell><cell>6.32</cell></row><row><cell></cell><cell>SAN [15]</cell><cell>5.22</cell><cell>10.39</cell><cell>5.71</cell><cell>5.19</cell><cell>5.49</cell><cell>6.83</cell><cell>5.80</cell></row><row><cell></cell><cell>WING [19]</cell><cell>5.11</cell><cell>8.75</cell><cell>5.36</cell><cell>4.93</cell><cell>5.41</cell><cell>6.37</cell><cell>5.81</cell></row><row><cell>Mean Error (%)</cell><cell>Res-18</cell><cell>6.09</cell><cell>10.76</cell><cell>6.97</cell><cell>5.83</cell><cell>6.19</cell><cell>7.15</cell><cell>6.67</cell></row><row><cell></cell><cell>Ours w. Res-18</cell><cell>5.25</cell><cell>9.10</cell><cell>5.83</cell><cell>4.93</cell><cell>5.47</cell><cell>6.26</cell><cell>5.86</cell></row><row><cell></cell><cell>Ours w. LAB</cell><cell>4.76</cell><cell>8.21</cell><cell>5.14</cell><cell>4.51</cell><cell>5.00</cell><cell>5.76</cell><cell>5.43</cell></row><row><cell></cell><cell>Ours w. SAN</cell><cell>4.39</cell><cell>8.42</cell><cell>4.68</cell><cell>4.24</cell><cell>4.37</cell><cell>5.60</cell><cell>4.86</cell></row><row><cell></cell><cell>CFSS [82]</cell><cell>20.56</cell><cell>66.26</cell><cell>23.25</cell><cell>17.34</cell><cell>21.84</cell><cell>32.88</cell><cell>23.67</cell></row><row><cell></cell><cell>DVLN [70]</cell><cell>10.84</cell><cell>46.93</cell><cell>11.15</cell><cell>7.31</cell><cell>11.65</cell><cell>16.30</cell><cell>13.71</cell></row><row><cell></cell><cell>LAB [69]</cell><cell>7.56</cell><cell>28.83</cell><cell>6.37</cell><cell>6.73</cell><cell>7.77</cell><cell>13.72</cell><cell>10.74</cell></row><row><cell></cell><cell>SAN [15]</cell><cell>6.32</cell><cell>27.91</cell><cell>7.01</cell><cell>4.87</cell><cell>6.31</cell><cell>11.28</cell><cell>6.60</cell></row><row><cell></cell><cell>WING [19]</cell><cell>6.00</cell><cell>22.70</cell><cell>4.78</cell><cell>4.30</cell><cell>7.77</cell><cell>12.50</cell><cell>7.76</cell></row><row><cell>Failure Rate (%)</cell><cell>Res-18</cell><cell>10.92</cell><cell>43.87</cell><cell>13.38</cell><cell>7.31</cell><cell>11.17</cell><cell>16.30</cell><cell>11.90</cell></row><row><cell></cell><cell>Ours w. Res-18</cell><cell>7.44</cell><cell>32.52</cell><cell>8.60</cell><cell>4.30</cell><cell>8.25</cell><cell>12.77</cell><cell>9.06</cell></row><row><cell></cell><cell>Ours w. LAB</cell><cell>5.24</cell><cell>20.86</cell><cell>4.78</cell><cell>3.72</cell><cell>6.31</cell><cell>9.51</cell><cell>7.24</cell></row><row><cell></cell><cell>Ours w. SAN</cell><cell>4.08</cell><cell>18.10</cell><cell>4.46</cell><cell>2.72</cell><cell>4.37</cell><cell>7.74</cell><cell>4.40</cell></row><row><cell></cell><cell>CFSS [82]</cell><cell cols="2">0.3659 0.0632</cell><cell>0.3157</cell><cell>0.3854</cell><cell>0.3691</cell><cell>0.2688</cell><cell>0.3037</cell></row><row><cell></cell><cell>DVLN [70]</cell><cell cols="2">0.4551 0.1474</cell><cell>0.3889</cell><cell>0.4743</cell><cell>0.4494</cell><cell>0.3794</cell><cell>0.3973</cell></row><row><cell></cell><cell>LAB [69]</cell><cell cols="2">0.5323 0.2345</cell><cell>0.4951</cell><cell>0.5433</cell><cell>0.5394</cell><cell>0.4490</cell><cell>0.4630</cell></row><row><cell></cell><cell>SAN [15]</cell><cell cols="2">0.5355 0.2355</cell><cell>0.4620</cell><cell>0.5552</cell><cell>0.5222</cell><cell>0.4560</cell><cell>0.4932</cell></row><row><cell></cell><cell>WING [19]</cell><cell cols="2">0.5504 0.3100</cell><cell>0.4959</cell><cell>0.5408</cell><cell>0.5582</cell><cell>0.4885</cell><cell>0.4918</cell></row><row><cell>AUC @0.1</cell><cell>Res-18</cell><cell cols="2">0.4385 0.1527</cell><cell>0.3718</cell><cell>0.4559</cell><cell>0.4366</cell><cell>0.3655</cell><cell>0.3931</cell></row><row><cell></cell><cell cols="3">Ours w. Res-18 0.5034 0.2294</cell><cell>0.4534</cell><cell>0.5252</cell><cell>0.4849</cell><cell>0.4318</cell><cell>0.4532</cell></row><row><cell></cell><cell>Ours w. LAB</cell><cell cols="2">0.5460 0.2764</cell><cell>0.5098</cell><cell>0.5660</cell><cell>0.5349</cell><cell>0.4700</cell><cell>0.4923</cell></row><row><cell></cell><cell>Ours w. SAN</cell><cell cols="2">0.5913 0.3109</cell><cell>0.5490</cell><cell>0.6089</cell><cell>0.5812</cell><cell>0.5164</cell><cell>0.5513</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">. Experiments</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">4.1. Datasets</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">WFLW [69] dataset is a challenging one, which contains</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">7,500 faces for training and 2,500 faces for testing, based</cell></row></table><note>Table 1: Evaluation of our approach on WFLW dataset. Top-2 results are highlighted in bold font.on WIDER Face [74] with 98 manually annotated land- marks [69]. The dataset is partitioned into 6 subsets ac- cording to challenging attribute annotation of large pose, expression, illumination, makeup, occlusion, and blur. 300W [56] provides multiple face datasets including LFPW [4], AFW [54], HELEN [41], XM2VTS [49], and IBUG with 68 automatically-annotated landmarks. Follow- ing the protocol used in [55], 3,148 training images and 689 testing images are used.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Normalized mean error (%) on 300W common, challenging subset and the full set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Figure 5 :</head><label>5</label><figDesc>Visual comparison on WFLW test set between the original baseline model and the boosted framework via style translation.</figDesc><table><row><cell>Method</cell><cell cols="2">NME(%) Frontal Full</cell><cell cols="2">AUC@ 0.1 FR(%) Full Full</cell></row><row><cell>LAB [69]</cell><cell>2.23</cell><cell>7.15</cell><cell>0.39</cell><cell>11.28</cell></row><row><cell>SAN [15]</cell><cell>2.01</cell><cell>6.94</cell><cell>0.44</cell><cell>10.43</cell></row><row><cell>Res-18</cell><cell>2.30</cell><cell>7.23</cell><cell>0.37</cell><cell>11.89</cell></row><row><cell>Ours w. Res-18</cell><cell>2.20</cell><cell>7.17</cell><cell>0.38</cell><cell>11.91</cell></row><row><cell>Ours w. LAB</cell><cell>2.10</cell><cell>7.06</cell><cell>0.42</cell><cell>10.01</cell></row><row><cell>Ours w. SAN</cell><cell>1.86</cell><cell>6.01</cell><cell>0.58</cell><cell>9.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Normalized mean error (%) on re-annotated 68-pt AFLW frontal subset and the full set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Normalized mean error (%) on 300W common and WFLW datasets when the training images are split into 10 folds. Each row represents NME on test set when the model is trained using a percentage (PCT%) of the training set. The landmark detector backbone is Res-18.</figDesc><table><row><cell>completeness of disentanglement especially when the train-</cell></row><row><cell>ing samples are limited. To evaluate the performance and</cell></row><row><cell>relative gain of our approach when training data is limited.</cell></row><row><cell>The training set is split into 10 subsets and respectively we</cell></row><row><cell>evaluate our model on different portions of training data.</cell></row><row><cell>Note that for different portions, we train the model from</cell></row><row><cell>scratch with no extra data used. The quantitative result is</cell></row><row><cell>reported in Tables 4 and 5.</cell></row><row><cell>In Table 4, a light baseline network Res-18 is used to</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Normalized mean error (%) on WFLW test set using different numbers of style translation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Quantitative ablative results. Normalized mean error (%) on WFLW test set using different variants of our framework.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Widely-used 68-pt facial landmark datasets. Dataset names their the environment and number are reported.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The content of our supplementary material is organized as follows.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Data augmentation generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthesizing images of humans in unseen poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Guttag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convolutional aggregation of local evidence for large pose face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timothy F Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature detection and tracking with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bmvc</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Joint multi-view face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06023</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>ICCV. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A variational u-net for conditional appearance and shape generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cascaded collaborative regression for robust facial landmark detection trained using a mixture of synthetic and real images with dynamic weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3425" to="3440" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning invariance from transformation sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Földiák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="200" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Occlusion coherence: Localizing occluded faces with a hierarchical deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multi-pie. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving landmark localization with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recombinator networks: Learning coarse-to-fine feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Poseinvariant face alignment with a single cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An active illumination and appearance (aia) model for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Kahraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>Gokmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sune</forename><surname>Darkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Synergy between face alignment and tracking via discriminative global consensus optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Muhammad Haris Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A largescale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshops. IEEE</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Disentangling 3d pose in a dendritic cnn for unconstrained 2d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Serena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Vuong Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjing</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tinne Tuytelaars, and Luc Van Gool. Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Active appearance models revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via a fully-convolutional localglobal context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Merget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Xm2vtsdb: The extended m2vts database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieron</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilbert</forename><surname>Maitre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second international conference on audio and video-based biometric person authentication</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">964</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Vassilis Athitsos, and Heng Huang. Direct shape regression networks for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<idno>CVPR. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A recurrent encoder-decoder for sequential face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxs</surname></persName>
		</author>
		<idno>ECCV. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised person image synthesis in arbitrary poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A nonlinear discriminative approach to aam fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The first facial landmark tracking in-the-wild challenge: Benchmark and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grigoris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant features using video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stavens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Christian Theobalt, and Matthias Nießner. Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Robust face alignment using a mixture of invariant experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salil</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tambe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A deeply-initialized coarse-to-fine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Valdes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Slow feature analysis: Unsupervised learning of invariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurenz</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="715" to="770" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Leveraging intra and interdataset variations for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Recurrent 3d-2d dual learning for large-pose facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengtao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentive-refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengtao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Global supervised descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The menpo facial landmark localisation challenge: A step towards the solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Deep learning of invariant features via simulated fixations in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">More ablation studies and detailed analysis of components in our framework</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Additional discussion about related directions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Details of our annotated AFLW-68 dataset and some representative visualized samples</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
