<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CURL: Contrastive Unsupervised Representations for Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Laskin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
						</author>
						<title level="a" type="main">CURL: Contrastive Unsupervised Representations for Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs offpolicy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at https://www. github.com/MishaLaskin/curl.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Developing agents that can perform complex control tasks from high dimensional observations such as pixels has been possible by combining the expressive power of deep neural networks with the long-term credit assignment power of reinforcement learning algorithms. Notable successes include learning to play a diverse set of video games from raw pixels <ref type="bibr" target="#b32">(Mnih et al., 2015)</ref>, continuous control tasks such as controlling a simulated car from a dashboard camera <ref type="bibr" target="#b30">(Lillicrap et al., 2015)</ref> and subsequent algorithmic developments and applications to agents that successfully navigate mazes and solve complex tasks from first-person camera observations <ref type="bibr" target="#b18">(Jaderberg et al., 2016;</ref><ref type="bibr" target="#b9">Espeholt et al., 2018;</ref><ref type="bibr" target="#b19">Jaderberg et al., 2019)</ref>; and robots that successfully grasp objects in the real world <ref type="bibr" target="#b21">(Kalashnikov et al., 2018)</ref>.</p><p>However, it has been empirically observed that reinforcement learning from high dimensional observations such as raw pixels is sample-inefficient <ref type="bibr" target="#b26">(Lake et al., 2017;</ref><ref type="bibr" target="#b20">Kaiser</ref> Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. Contrastive Unsupervised Representations for</head><p>Reinforcement Learning (CURL) combines instance contrastive learning and reinforcement learning. CURL trains a visual representation encoder by ensuring that the embeddings of data-augmented versions oq and o k of observation o match using a contrastive loss. The query observations oq are treated as the anchor while the key observations o k contain the positive and negatives, all constructed from the minibatch sampled for the RL update. The keys are encoded with a momentum averaged version of the query encoder. The RL policy and (or) value function are built on top of the query encoder which is jointly trained with the contrastive and reinforcement learning objectives. CURL is a generic framework that can be plugged into any RL algorithm that relies on learning representations from high dimensional images. <ref type="bibr">et al., 2019)</ref>. Moreover, it is widely accepted that learning policies from physical state based features is significantly more sample-efficient than learning from pixels <ref type="bibr">(Tassa et al., 2018)</ref>. In principle, if the state information is present in the pixel data, then we should be able to learn representations that extract the relevant state information. For this reason, it may be possible to learn from pixels as fast as from state given the right representation.</p><p>From a practical standpoint, although high rendering speeds in simulated environments enable RL agents to solve complex tasks within reasonable wall clock time, learning in the real world means that agents are bound to work within the limitations of physics. <ref type="bibr" target="#b21">Kalashnikov et al. (2018)</ref> needed a farm of robotic arms that collected large scale robot in-arXiv:2004.04136v4 <ref type="bibr">[cs.</ref>LG] 21 Sep 2020 teraction data over several months to develop their robot grasp value functions and policies. The data-efficiency of the whole pipeline thus has significant room for improvement. Similarly, in simulated worlds which are limited by rendering speeds in the absence of GPU accelerators, data efficiency is extremely crucial to have a fast experimental turnover and iteration. Therefore, improving the sample efficiency of reinforcement learning (RL) methods that operate from high dimensional observations is of paramount importance to RL research both in simulation and the real world and allows for faster progress towards the broader goal of developing intelligent autonomous agents.</p><p>A number of approaches have been proposed in the literature to address the sample inefficiency of deep RL algorithms. Broadly, they can be classified into two streams of research, though not mutually exclusive: (i) Auxiliary tasks on the agent's sensory observations; (ii) World models that predict the future. While the former class of methods use auxiliary self-supervision tasks to accelerate the learning progress of model-free RL methods <ref type="bibr" target="#b18">(Jaderberg et al., 2016;</ref><ref type="bibr" target="#b31">Mirowski et al., 2016)</ref>, the latter class of methods build explicit predictive models of the world and use those models to plan through or collect fictitious rollouts for model-free methods to learn from <ref type="bibr">(Sutton, 1990;</ref><ref type="bibr" target="#b12">Ha &amp; Schmidhuber, 2018;</ref><ref type="bibr" target="#b20">Kaiser et al., 2019;</ref><ref type="bibr" target="#b35">Schrittwieser et al., 2019)</ref>.</p><p>Our work falls into the first class of models, which use auxiliary tasks to improve sample efficiency. Our hypothesis is simple: If an agent learns a useful semantic representation from high dimensional observations, control algorithms built on top of those representations should be significantly more data-efficient. Self-supervised representation learning has seen dramatic progress in the last couple of years with huge advances in masked language modeling <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> and contrastive learning <ref type="bibr">(HÃ©naff et al., 2019;</ref><ref type="bibr">He et al., 2019a;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref> for language and vision respectively. The representations uncovered by these objectives improve the performance of any supervised learning system especially in scenarios where the amount of labeled data available for the downstream task is really low.</p><p>We take inspiration from the contrastive pre-training successes in computer vision. However, there are a couple of key differences: (i) There is no giant unlabeled dataset of millions of images available beforehand -the dataset is collected online from the agent's interactions and changes dynamically with the agent's experience; (ii) The agent has to perform unsupervised and reinforcement learning simultaneously as opposed to fine-tuning a pre-trained network for a specific downstream task. These two differences introduce a different challenge: How can we use contrastive learning for improving agents that can learn to control effectively and efficiently from online interactions?</p><p>To address this challenge, we propose CURL -Contrastive Uunsupervised Representations for Reinforcement Learning. CURL uses a form of contrastive learning that maximizes agreement between augmented versions of the same observation, where each observation is a stack of temporally sequential frames. We show that CURL significantly improves sample-efficiency over prior pixel-based methods by performing contrastive learning simultaneously with an off-policy RL algorithm. CURL coupled with the Soft-Actor-Critic (SAC) <ref type="bibr" target="#b13">(Haarnoja et al., 2018)</ref> results in 1.9x median higher performance over Dreamer, a prior state-of-the-art algorithm on DMControl environments, benchmarked at 100k environment steps and matches the performance of state-based SAC on the majority of 16 environments tested, a first for pixel-based methods. In the Atari setting benchmarked at 100k interaction steps, we show that CURL coupled with a data-efficient version of Rainbow <ref type="bibr">DQN (van Hasselt et al., 2019)</ref> results in 1.2x median higher performance over prior methods such as SimPLe <ref type="bibr" target="#b20">(Kaiser et al., 2019</ref><ref type="bibr">), improving upon Efficient Rainbow (van Hasselt et al., 2019</ref> on 19 out of 26 Atari games, surpassing human efficiency on two games.</p><p>While contrastive learning in aid of model-free RL has been studied in the past by van den <ref type="bibr">Oord et al. (2018)</ref> using Contrastive Predictive Coding (CPC), the results were mixed with marginal gains in a few DMLab <ref type="bibr" target="#b9">(Espeholt et al., 2018)</ref> environments. CURL is the first model to show substantial data-efficiency gains from using a contrastive self-supervised learning objective for model-free RL agents across a multitude of pixel based continuous and discrete control tasks in DMControl and Atari.</p><p>We prioritize designing a simple and easily reproducible pipeline. While the promise of auxiliary tasks and learning world models for RL agents has been demonstrated in prior work, there's an added layer of complexity when introducing components like modeling the future in a latent space <ref type="bibr">(van den Oord et al., 2018;</ref><ref type="bibr" target="#b12">Ha &amp; Schmidhuber, 2018)</ref>. CURL is designed to add minimal overhead in terms of architecture and model learning. The contrastive learning objective in CURL operates with the same latent space and architecture typically used for model-free RL and seamlessly integrates with the training pipeline without the need to introduce multiple additional hyperparameters.</p><p>Our paper makes the following key contributions: We present CURL, a simple framework that integrates contrastive learning with model-free RL with minimal changes to the architecture and training pipeline. Using 16 complex control tasks from the DeepMind control (DMControl) suite and 26 Atari games, we empirically show that contrastive learning combined with model-free RL outperforms the prior state-of-the-art by 1.9x on DMControl and 1.2x on Atari compared across leading prior pixel-based methods. CURL is also the first algorithm across both model-based and model-free methods that operates purely from pixels, and nearly matches the performance and sample-efficiency of a SAC algorithm trained from the state based features on the DMControl suite. Finally, our design is simple and does not require any custom architectural choices or hyperparameters which is crucial for reproducible end-to-end training. Through these strong empirical results, we demonstrate that a contrastive objective is the preferred self-supervised auxiliary task for achieving sample-efficiency compared to reconstruction based methods, and enables model-free methods to outperform state-of-the-art model-based methods in terms of data-efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-Supervised Learning: Self-Supervised Learning is aimed at learning rich representations of high dimensional unlabeled data to be useful for a wide variety of tasks. The fields of natural language processing and computer vision have seen dramatic advances in self-supervised methods such as BERT <ref type="bibr" target="#b7">(Devlin et al., 2018</ref><ref type="bibr">), CPC, MoCo, SimCLR (HÃ©naff et al., 2019</ref><ref type="bibr">He et al., 2019a;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref>.</p><p>Contrastive Learning: Contrastive Learning is a framework to learn representations that obey similarity constraints in a dataset typically organized by similar and dissimilar pairs. This is often best understood as performing a dictionary lookup task wherein the positive and negatives represent a set of keys with respect to a query (or an anchor). A simple instantiation of contrastive learning is Instance Discrimination <ref type="bibr">(Wu et al., 2018)</ref> wherein a query and key are positive pairs if they are data-augmentations of the same instance (example, image) and negative otherwise. A key challenge in contrastive learning is the choice of negatives which can decide the quality of the underlying representations learned. The loss functions used to contrast could be among several choices such as InfoNCE <ref type="bibr">(van den Oord et al., 2018</ref><ref type="bibr">), Triplet (Wang &amp; Gupta, 2015</ref>, Siamese <ref type="bibr" target="#b5">(Chopra et al., 2005)</ref> and so forth.</p><p>Self-Supervised Learning for RL: Auxiliary tasks such as predicting the future conditioned on the past observation(s) and action(s) <ref type="bibr" target="#b18">(Jaderberg et al., 2016;</ref><ref type="bibr" target="#b38">Shelhamer et al., 2016;</ref><ref type="bibr">van den Oord et al., 2018;</ref><ref type="bibr" target="#b34">Schmidhuber, 1990</ref>) are a few representative examples of using auxiliary tasks to improve the sample-efficiency of model-free RL algorithms. The future prediction is either done in a pixel space <ref type="bibr" target="#b18">(Jaderberg et al., 2016)</ref> or latent space <ref type="bibr">(van den Oord et al., 2018)</ref>. The sample-efficiency gains from reconstruction-based auxiliary losses have been benchmarked in <ref type="bibr" target="#b18">Jaderberg et al. (2016);</ref><ref type="bibr">Higgins et al. (2017);</ref><ref type="bibr">Yarats et al. (2019)</ref>. Contrastive learning has been used to extract reward signals in the latent space <ref type="bibr" target="#b8">Dwibedi et al., 2018;</ref><ref type="bibr">Warde-Farley et al., 2018)</ref>; and study representation learning on Atari games by <ref type="bibr" target="#b0">Anand et al. (2019)</ref>.</p><p>World Models for sample-efficiency: While joint learning of an auxiliary unsupervised task with model-free RL is one way to improve the sample-efficiency of agents, there has also been another line of research that has tried to learn world models of the environment and use them to sample rollouts and plan. An early instantiation of the generic principle was put forth by <ref type="bibr">Sutton (1990)</ref> in Dyna where fictitious samples rolled out from a learned world model are used in addition to the agent's experience for sample-efficient learning. Planning through a learned world model <ref type="bibr" target="#b39">(Srinivas et al., 2018)</ref> is another way to improve sample-efficiency. While <ref type="bibr" target="#b18">Jaderberg et al. (2016)</ref>; van den <ref type="bibr">Oord et al. (2018)</ref>; <ref type="bibr" target="#b29">Lee et al. (2019)</ref> also learn pixel and latent space forward models, the models are learned to shape the latent representations, and there is no explicit Dyna or planning. Planning through learned world models has been successfully demonstrated in <ref type="bibr" target="#b12">Ha &amp; Schmidhuber (2018)</ref>; <ref type="bibr" target="#b15">Hafner et al. (2018;</ref>. <ref type="bibr" target="#b20">Kaiser et al. (2019)</ref> introduce SimPLe which implements Dyna with expressive deep neural networks for the world model for sample-efficiency on Atari games.</p><p>Sample-efficient RL for image-based control: CURL encompasses the areas of self-supervision, contrastive learning and using auxiliary tasks for sample-efficient RL. We benchmark for sample-efficiency on the DMControl suite <ref type="bibr">(Tassa et al., 2018)</ref> and Atari Games benchmarks <ref type="bibr" target="#b2">(Bellemare et al., 2013)</ref>. The DMControl suite has been used widely by <ref type="bibr">Yarats et al. (2019)</ref>, <ref type="bibr" target="#b15">Hafner et al. (2018)</ref>, <ref type="bibr" target="#b16">Hafner et al. (2019)</ref> and <ref type="bibr" target="#b29">Lee et al. (2019)</ref> for benchmarking sample-efficiency for image based continuous control methods. As for Atari, <ref type="bibr" target="#b20">Kaiser et al. (2019)</ref> propose to use the 100k interaction steps benchmark for sample-efficiency which has been adopted in <ref type="bibr" target="#b22">Kielak (2020);</ref><ref type="bibr">van Hasselt et al. (2019)</ref>. The Rainbow <ref type="bibr">DQN (Hessel et al., 2017)</ref> was originally proposed for maximum sample-efficiency on the Atari benchmark and in recent times has been adapted to a version known as <ref type="bibr">Data-Efficient Rainbow (van Hasselt et al., 2019)</ref> with competitive performance to SimPLe without learning world models. We benchmark extensively against both model-based and model-free algorithms in our experiments. For the DM-Control experiments, we compare our method to Dreamer, PlaNet, SLAC, SAC+AE whereas for Atari experiments we compare to SimPLe, Rainbow, and OverTrained Rainbow (OTRainbow) and Efficient Rainbow (Eff. Rainbow).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>CURL is a general framework for combining contrastive learning with RL. In principle, one could use any RL algorithm in the CURL pipeline, be it on-policy or off-policy. We use the widely adopted Soft Actor Critic (SAC) <ref type="bibr" target="#b13">(Haarnoja et al., 2018)</ref> for continuous control benchmarks (DM Control) and Rainbow <ref type="bibr">DQN (Hessel et al., 2017;</ref><ref type="bibr">van Hasselt et al., 2019)</ref> for discrete control benchmarks (Atari). Below, we review SAC, Rainbow DQN and Contrastive Learning.  <ref type="figure">Figure 2</ref>. CURL Architecture: A batch of transitions is sampled from the replay buffer. Observations are then data-augmented twice to form query and key observations, which are then encoded with the query encoder and key encoders, respectively. The queries are passed to the RL algorithm while query-key pairs are passed to the contrastive learning objective. During the gradient update step, only the query encoder is updated. The key encoder weights are the moving average (EMA) of the query weights similar to MoCo (He et al., 2019a).</p><formula xml:id="formula_0">q = f Î¸ q (o q ) k = f Î¸ k (o k ) o k o q</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Soft Actor Critic</head><p>SAC is an off-policy RL algorithm that optimizes a stochastic policy for maximizing the expected trajectory returns. Like other state-of-the-art end-to-end RL algorithms, SAC is effective when solving tasks from state observations but fails to learn efficient policies from pixels. SAC is an actorcritic method that learns a policy Ï Ï and critics Q Ï1 and Q Ï2 . The parameters Ï i are learned by minimizing the Bellman error:</p><formula xml:id="formula_1">L(Ï i , B) = E tâ¼B (Q Ïi (o, a) â (r + Î³(1 â d)T )) 2<label>(1)</label></formula><p>where t = (o, a, o , r, d) is a tuple with observation o, action a, reward r and done signal d, B is the replay buffer, and T is the target, defined as:</p><formula xml:id="formula_2">T = min i=1,2 Q * Ïi (o , a ) â Î± log Ï Ï (a |o )<label>(2)</label></formula><p>In the target equation <ref type="formula" target="#formula_2">(2)</ref>, Q * Ïi denotes the exponential moving average (EMA) of the parameters of Q Ïi . Using the EMA has empirically shown to improve training stability in off-policy RL algorithms. The parameter Î± is a positive entropy coefficient that determines the priority of the entropy maximization over value function optimization.</p><p>While the critic is given by Q Ïi , the actor samples actions from policy Ï Ï and is trained by maximizing the expected return of its actions as in:</p><formula xml:id="formula_3">L(Ï) = E aâ¼Ï [Q Ï (o, a) â Î± log Ï Ï (a|o)]<label>(3)</label></formula><p>where actions are sampled stochastically from the policy a Ï (o, Î¾) â¼ tanh (Âµ Ï (o) + Ï Ï (o) Î¾) and Î¾ â¼ N (0, I) is a standard normalized noise vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Rainbow</head><p>Rainbow <ref type="bibr">DQN (Hessel et al., 2017)</ref> is best summarized as multiple improvements on top of the original Nature DQN <ref type="bibr" target="#b32">(Mnih et al., 2015)</ref> applied together. Specifically, Deep Q Network (DQN) <ref type="bibr" target="#b32">(Mnih et al., 2015)</ref> combines the off-policy algorithm Q-Learning with a convolutional neural network as the function approximator to map raw pixels to action value functions. Since then, multiple improvements have been proposed such as Double Q Learning <ref type="bibr">(Van Hasselt et al., 2016)</ref>, Dueling Network Architectures <ref type="bibr">(Wang et al., 2015)</ref>, Prioritized Experience Replay <ref type="bibr" target="#b33">(Schaul et al., 2015)</ref>, and Noisy Networks <ref type="bibr" target="#b10">(Fortunato et al., 2017)</ref>. Additionally, distributional reinforcement learning <ref type="bibr" target="#b3">(Bellemare et al., 2017)</ref> proposed the technique of predicting a distribution over possible value function bins through the C51 Algorithm. Rainbow DQN combines all of the above techniques into a single off-policy algorithm for state-of-the-art sample efficiency on Atari benchmarks. Additionally, Rainbow also makes use of multi-step returns <ref type="bibr">(Sutton et al., 1998)</ref>. <ref type="bibr">van Hasselt et al. (2019)</ref> propose a data-efficient version of the Rainbow which can be summarized as an improved configuration of hyperparameters that is optimized for performance benchmarked at 100K interaction steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Contrastive Learning</head><p>A key component of CURL is the ability to learn rich representations of high dimensional data using contrastive unsupervised learning. Contrastive learning <ref type="bibr" target="#b28">LeCun et al., 2006;</ref><ref type="bibr">van den Oord et al., 2018;</ref><ref type="bibr">Wu et al., 2018;</ref><ref type="bibr">He et al., 2019a)</ref> can be understood as learning a differentiable dictionary look-up task. Given a query q and keys K = {k 0 , k 1 , . . . } and an explicitly known partition of K (with respect to q) P (K) = ({k + }, K \ {k + }), the goal of contrastive learning is to ensure that q matches with k + relatively more than any of the keys in K \ {k + }. q, K, k + , and K \ {k + } are also referred to as anchor, targets, positive, negatives respectively in the parlance of contrastive learning (van den <ref type="bibr">Oord et al., 2018;</ref><ref type="bibr">He et al., 2019a)</ref>. Similarities between the anchor and targets are best modeled with dot products (q T k) <ref type="bibr">(Wu et al., 2018;</ref><ref type="bibr">He et al., 2019a)</ref> or bilinear products (q T W k) (van den <ref type="bibr">Oord et al., 2018;</ref><ref type="bibr">HÃ©naff et al., 2019)</ref> though other forms like euclidean distances are also common <ref type="bibr" target="#b36">(Schroff et al., 2015;</ref><ref type="bibr">Wang &amp; Gupta, 2015)</ref>.</p><p>To learn embeddings that respect these similarity relations, van den <ref type="bibr">Oord et al. (2018)</ref> propose the InfoNCE loss:</p><formula xml:id="formula_4">L q = log exp(q T W k + ) exp(q T W k + ) + Kâ1 i=0 exp(q T W k i )<label>(4)</label></formula><p>The loss 4 can be interpreted as the log-loss of a K-way softmax classifier whose label is k + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CURL Implementation</head><p>CURL minimally modifies a base RL algorithm by training the contrastive objective as an auxiliary loss during the batch update. In our experiments, we train CURL alongside two model-free RL algorithms -SAC for DMControl experiments and Rainbow DQN (data-efficient version) for Atari experiments. To specify a contrastive learning objective, we need to define (i) the discrimination objective (ii) the transformation for generating query-key observations (iii) the embedding procedure for transforming observations into queries and keys and (iv) the inner product used as a similarity measure between the query-key pairs in the contrastive loss. The exact specification these aspects largely determine the quality of the learned representations.</p><p>We first summarize the CURL architecture, and then cover each architectural choice in detail.  <ref type="bibr">, 2017)</ref>. Therefore, instance discrimination is performed across the frame stacks as opposed to single image instances. We use a momentum encoding procedure for targets similar to MoCo (He et al., 2019b) which we found to be better performing for RL. Finally, for the InfoNCE score function, we use a bi-linear inner product similar to CPC (van den Oord et al., 2018) which we found to work better than unit norm vector products used in MoCo and SimCLR. Ablations for both the encoder and the similarity measure choices are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. The contrastive representation is trained jointly with the RL algorithm, and the latent code receives gradients from both the contrastive ob-jective and the Q-function. An overview of the architecture is shown in in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architectural Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Discrimination Objective</head><p>A key component of contrastive representation learning is the choice of positives and negative samples relative to an anchor <ref type="bibr" target="#b1">(Bachman et al., 2019;</ref><ref type="bibr">Tian et al., 2019;</ref><ref type="bibr">HÃ©naff et al., 2019;</ref><ref type="bibr">He et al., 2019a;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref>. Contrastive Predictive Coding (CPC) based pipelines <ref type="bibr">(HÃ©naff et al., 2019;</ref><ref type="bibr">van den Oord et al., 2018)</ref> use groups of image patches separated by a carefully chosen spatial offset for anchors and positives while the negatives come from other patches within the image and from other images.</p><p>While patches are a powerful way to incorporate spatial and instance discrimination together, they introduce extra hyperparameters and architectural design choices which may be hard to adapt for a new problem. SimCLR <ref type="bibr" target="#b4">(Chen et al., 2020)</ref> and MoCo (He et al., 2019a) opt for a simpler design where there is no patch extraction.</p><p>Discriminating transformed image instances as opposed to image-patches within the same image optimizes a simpler instance discrimination objective (Wu et al., 2018) with the InfoNCE loss and requires minimal architectural adjustments <ref type="bibr">(He et al., 2019b;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref>. It is preferable to pick a simpler discrimination objective in the RL setting for two reasons. First, considering the brittleness of reinforcement learning algorithms <ref type="bibr">(Henderson et al., 2018)</ref>, complex discrimination may destabilize the RL objective. Second, since RL algorithms are trained on dynamically generated datasets, a complex discrimination objective may significantly increase the wall-clock training time. CURL therefore uses instance discrimination rather than patch discrimination. One could view contrastive instance discrimination setups like SimCLR and MoCo as maximizing mutual information between an image and its augmented version. The reader is encouraged to refer to van den Oord et al. <ref type="formula" target="#formula_1">(2018)</ref>; <ref type="bibr" target="#b17">Hjelm et al. (2018);</ref><ref type="bibr">Tschannen et al. (2019)</ref> for connections between contrastive learning and mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Query-Key Pair Generation</head><p>Similar to instance discrimination in the image setting <ref type="bibr">(He et al., 2019b;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref>, the anchor and positive observations are two different augmentations of the same image while negatives come from other images. CURL primarily relies on the random crop data augmentation, where a random square patch is cropped from the original rendering.</p><p>A significant difference between RL and computer vision settings is that an instance ingested by a model-free RL algorithm that operates from pixels is not just a single image but a stack of frames <ref type="bibr" target="#b32">(Mnih et al., 2015)</ref>. For example, one typically feeds in a stack of 4 frames in Atari experiments and a stack of 3 frames in DMControl. This way, performing instance discrimination on frame stacks allows CURL to learn both spatial and temporal discriminative features. For details regarding the extent to which CURL captures temporal features, see Appendix E.</p><p>We apply the random augmentations across the batch but consistently across each stack of frames to retain information about the temporal structure of the observation. The augmentation procedure is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. For more details, refer to Appendix A. <ref type="figure" target="#fig_1">Figure 3</ref>. Visually illustrating the process of generating an anchor and its positive using stochastic random crops. Our aspect ratio for cropping is 0.84, i.e, we crop a 84 Ã 84 image from a 100 Ã 100 simulation-rendered image. Applying the same random crop coordinates across all frames in the stack ensures time-consistent spatial jittering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Similarity Measure</head><p>Another determining factor in the discrimination objective is the inner product used to measure agreement between query-key pairs. CURL employs the bi-linear inner-product sim(q, k) = q T W k, where W is a learned parameter matrix. We found this similarity measure to outperform the normalized dot-product (see <ref type="figure" target="#fig_3">Figure 5</ref> in Appendix A) used in recent state-of-the-art contrastive learning methods in computer vision like MoCo and SimCLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Target Encoding with Momentum</head><p>The motivation for using contrastive learning in CURL is to train encoders that map from high dimensional pixels to more semantic latents. InfoNCE is an unsupervised loss that learns encoders f q and f k mapping the raw anchors (query) x q and targets (keys) x k into latents q = f q (x q ) and k = f k (x k ), on which we apply the similarity dot products. It is common to share the same encoder between the anchor and target mappings, that is, to have f q = f k (van den <ref type="bibr">Oord et al., 2018;</ref><ref type="bibr">HÃ©naff et al., 2019)</ref>.</p><p>From the perspective of viewing contrastive learning as building differentiable dictionary lookups over high dimensional entities, increasing the size of the dictionary and enriching the set of negatives is helpful in learning rich We measure the data-efficiency and performance of our method and baselines at 100k and 500k environment steps on DMControl and 100k interaction steps (400k environment steps with action repeat of 4) on Atari, which we will henceforth refer to as DMControl100k, DMControl500k and Atari100k for clarity. While Atari100k benchmark has been common practice when investigating data-efficiency on Atari <ref type="bibr" target="#b20">(Kaiser et al., 2019;</ref><ref type="bibr">van Hasselt et al., 2019;</ref><ref type="bibr" target="#b22">Kielak, 2020)</ref>, the DMControl benchmark was set at 500k environment steps because state-based RL approaches asymptotic performance on many environments at this point, and 100k steps to measure the speed of initial learning. A broader motivation is that while RL algorithms can achieve superhuman performance on Atari games, they are still far less efficient than a human learner. Training for 100-500k environment steps corresponds to a few hours of human time.</p><p>We evaluate (i) sample-efficiency by measuring how many steps it takes the best performing baselines to match CURL performance at a fixed T (100k or 500k) steps and (ii) performance by measuring the ratio of the episode returns achieved by CURL versus the best performing baseline at T steps. To be explicit, when we say data or sample-efficiency we're referring to (i) and when we say performance we're referring to (ii).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Environments</head><p>Our primary goal for CURL is sample-efficient control from pixels that is broadly applicable across a range of environments. We benchmark the performance of CURL for both discrete and continuous control environments. Specifically, we focus on DMControl suite for continuous control tasks and the Atari Games benchmark for discrete control tasks with inputs being raw pixels rendered by the environments.</p><p>DeepMind Control: Recently, there have been a number of papers that have benchmarked for sample efficiency on challenging visual continuous control tasks belonging to the DMControl suite <ref type="bibr">(Tassa et al., 2018)</ref> where the agent operates purely from pixels. The reason for operating in these environments is multi fold: (i) they present a reasonably challenging and diverse set of tasks; (ii) sample-efficiency of pure model-free RL algorithms operating from pixels on these benchmarks is poor; (iii) multiple recent efforts to improve the sample efficiency of both model-free and model-based methods on these benchmarks thereby giving us sufficient baselines to compare against; (iv) performance on the DM control suite is relevant to robot learning in real world benchmarks.</p><p>We run experiments on sixteen environments from DM-Control to examine the performance of CURL on pixels relative to SAC with access to the ground truth state, shown in <ref type="figure">Figure 7</ref>. For more extensive benchmarking, we compare CURL to five leading pixel-based methods across the the six environments presented in Yarats et al. <ref type="formula" target="#formula_1">(2019)</ref>: ball-incup, finger-spin, reacher-easy, cheetah-run, walker-walk, cartpole-swingup for benchmarking.</p><p>Atari: Similar to DMControl sample-efficiency benchmarks, there have been a number of recent papers that have benchmarked for sample-efficiency on the Atari 2600 Games. <ref type="bibr" target="#b20">Kaiser et al. (2019)</ref> proposed comparing various algorithms in terms of performance achieved within 100K timesteps (400K frames, frame skip of 4) of interaction with the environments (games). The method proposed by <ref type="bibr" target="#b20">Kaiser et al. (2019)</ref> called SimPLe is a model-based RL algorithm.</p><p>SimPLe is compared to a random agent, model-free Rainbow <ref type="bibr">DQN (Hessel et al., 2017)</ref> and human performance for the same amount of interaction time. <ref type="bibr">Recently, van Hasselt et al. (2019)</ref> and <ref type="bibr" target="#b22">Kielak (2020)</ref> proposed data-efficient versions of Rainbow DQN which are competitive with SimPLe on the same benchmark. Given that the same benchmark has been established in multiple recent papers and that there is a human baseline to compare to, we benchmark CURL on all the 26 Atari Games <ref type="table" target="#tab_3">(Table 2)</ref>.  <ref type="bibr" target="#b13">(Haarnoja et al., 2018)</ref>. These baselines are competitive methods for benchmarking control from pixels. In addition to these, we also present the baseline State-SAC where the assumption is that the agent has access to low level state based features and does not operate from pixels. This baseline acts as an oracle in that it approximates the upper bound of how sample-efficient a pixel-based agent can get in these environments.</p><p>Atari baselines: For benchmarking performance on Atari, we compare CURL to (i) SimPLe <ref type="bibr" target="#b20">(Kaiser et al., 2019)</ref>, the top performing model-based method in terms of dataefficiency on Atari and (ii) Rainbow DQN (Hessel et al., 2017), a top-performing model-free baseline for Atari, (iii) OTRainbow <ref type="bibr" target="#b22">(Kielak, 2020)</ref> which is an OverTrained version of Rainbow for data-efficiency, (iv) Efficient Rainbow <ref type="bibr">(van Hasselt et al., 2019)</ref> which is a modification of Rainbow hyperparameters for data-efficiency, (v) Random Agent <ref type="bibr" target="#b20">(Kaiser et al., 2019)</ref>, (vi) Human Performance <ref type="bibr" target="#b20">(Kaiser et al., 2019;</ref><ref type="bibr">van Hasselt et al., 2019)</ref>. All the baselines and our method are evaluated for performance after 100K interaction steps (400K frames with a frame skip of 4) which corresponds to roughly two hours of gameplay. These benchmarks help us understand how the state-of-the-art pixel based RL algorithms compare in terms of sample efficiency and also to human efficiency. Note: Scores for SimPLe <ref type="table" target="#tab_10">Table 1</ref>. Scores achieved by CURL (mean &amp; standard deviation for 10 seeds) and baselines on DMControl500k and 1DMControl100k. CURL achieves state-of-the-art performance on the majority (5 out of 6) environments benchmarked on DMControl500k. These environments were selected based on availability of data from baseline methods (we run CURL experiments on 16 environments in total and show results in <ref type="figure">Figure 7</ref>). The baselines are PlaNet <ref type="bibr" target="#b15">(Hafner et al., 2018)</ref>, Dreamer <ref type="bibr" target="#b16">(Hafner et al., 2019)</ref>, <ref type="bibr">SAC+AE (Yarats et al., 2019)</ref>, SLAC <ref type="bibr" target="#b29">(Lee et al., 2019)</ref>, pixel-based SAC and state-based SAC <ref type="bibr" target="#b13">(Haarnoja et al., 2018)</ref>. SLAC results were reported with one and three gradient updates per agent step, which we refer to as SLACv1 and SLACv2 respectively. We compare to SLACv1 since all other baselines and CURL only make one gradient update per agent step. We also ran CURL with three gradient updates per step and compare results to SLACv2 in  and Human baselines have been reported differently in prior work <ref type="bibr" target="#b22">(Kielak, 2020;</ref><ref type="bibr">van Hasselt et al., 2019)</ref>. To be rigorous, we take the best reported score for each individual game reported in prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">DMControl</head><p>Sample-efficiency results for DMControl experiments are shown in <ref type="table" target="#tab_10">Table 1</ref> and in <ref type="figure">Figures 4, 6, and 7</ref>. Below are the key findings:</p><p>(i) CURL is the state-of-the-art image-based RL algorithm on the majority (5 out of 6) DMControl environments that we benchmark on for sample-efficiency against existing pixel-based baselines. On DMControl100k, CURL achieves 1.9x higher median performance than Dreamer <ref type="bibr" target="#b16">(Hafner et al., 2019</ref>), a leading model-based method, and is 4.5x more data-efficient shown in <ref type="figure">Figure 6</ref>.</p><p>(ii) CURL operating purely from pixels nearly matches (and sometimes surpasses) the sample efficiency of SAC operating from state on the majority of 16 DMControl environments tested shown in <ref type="figure">Figure 7</ref> and matches the median state-based score on DMControl500k shown in <ref type="figure">Figure  4</ref>. This is a first for any image-based RL algorithm, be it model-based, model-free, with or without auxiliary tasks.</p><p>(iii) CURL solves (converges close to optimal score of 1000) on the majority of 16 DMControl experiments within 500k steps. It also matches the state-based median score across the 6 extensively benchmarked environments in this regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Atari</head><p>Results for Atari100k are shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Ablation Studies</head><p>In Appendix E, we present the results of ablation studies carried out to answer the following questions: (i) Does  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this work, we proposed CURL, a contrastive unsupervised representation learning method for RL, that achieves stateof-the-art data-efficiency on pixel-based RL tasks across a diverse set of benchmark environments. CURL is the first model-free RL pipeline accelerated by contrastive learning with minimal architectural changes to demonstrate state-ofthe-art performance on complex tasks so far dominated by approaches that have relied on learning world models and (or) decoder-based objectives. We hope that progress like CURL enables avenues for real-world deployment of RL in areas like robotics where data-efficiency is paramount. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Below, we explain the implementation details for CURL in the DMControl setting. Specifically, we use the SAC algorithm as the RL objective coupled with CURL and build on top of the publicly released implementation from <ref type="bibr">Yarats et al. (2019)</ref>. We present in detail the hyperparameters for the architecture and optimization. We do not use any extra hyperparameter for balancing the contrastive loss and the reinforcement learning losses. Both the objectives are weighed equally in the gradient updates. Architecture: We use an encoder architecture that is similar to <ref type="bibr">(Yarats et al., 2019)</ref>, which we sketch in PyTorch-like pseuodocode below. The actor and critic both use the same encoder to embed image observations. A full list of hyperparameters is displayed in <ref type="table" target="#tab_7">Table 3</ref>.</p><p>For contrastive learning, CURL utilizes momentum for the key encoder <ref type="figure">(He et al., 2019b)</ref> and a bi-linear inner product as the similarity measure (van den <ref type="bibr">Oord et al., 2018)</ref>. Performance curves ablating these two architectural choices are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. Terminology: A common point of confusion is the meaning "training steps." We use the term environment steps to denote the amount of times the simulator environment is stepped through and interaction steps to denote the number of times the agent steps through its policy. The terms action repeat or frame skip refer to the number of times an action is repeated when it's drawn from the agent's policy. For example, if action repeat is set to 4, then 100k interaction steps is equivalent to 400k environment steps.</p><p>Batch Updates: After initializing the replay buffer with observations extracted by a random agent, we sample a batch of observations, compute the CURL objectives, and step through the optimizer. Note that since queries and keys are generated by data-augmenting an observation, we can generate arbitrarily many keys to increase the contrastive batch size without sampling any additional observations.</p><p>Shared Representations: The objective of performing contrastive learning together with RL is to ensure that the shared encoder learns rich features that facilitate sample efficient control. There is a subtle coincidental connection between MoCo and off-policy RL. Both the frameworks adopt the usage of a momentum averaged (EMA) version of the underlying model. In MoCo, the EMA encoder is used for encoding the keys (targets) while in off-policy RL, the EMA version of the Q-networks are used as targets in the Bellman error <ref type="bibr" target="#b32">(Mnih et al., 2015;</ref><ref type="bibr" target="#b13">Haarnoja et al., 2018)</ref>. Thanks to this connection, CURL shares the convolutional encoder, momentum coefficient and EMA update between contrastive and reinforcement learning updates for the shared parameters. The MLP part of the critic that operates on top of these convolutional features has a separate momentum coefficient and update decoupled from the image encoder parameters.</p><p>Balancing Contrastive and RL Updates: While past work has learned hyperparameters to balance the auxiliary loss coefficient or learning rate relative to the RL objective <ref type="bibr" target="#b18">(Jaderberg et al., 2016;</ref><ref type="bibr">Yarats et al., 2019)</ref>, CURL does not need any such adjustments. We use both the contrastive and RL objectives together with equal weight and learning rate. This simplifies the training process compared to other methods, such as training a VAE jointly <ref type="bibr" target="#b15">(Hafner et al., 2018;</ref><ref type="bibr" target="#b29">Lee et al., 2019)</ref>, that require careful tuning of coefficients for representation learning.</p><p>Differences in Data Collection between Computer Vision and RL Settings: There are two key differences between contrastive learning in the computer vision and RL settings because of their different goals. Unsupervised feature learning methods built for downstream vision tasks like image classification assume a setting where there is a large static dataset of unlabeled images. On the other hand, in RL, the dataset changes over time to account for the agent's new experiences. Secondly, the size of the memory bank of labeled images and dataset of unlabeled ones in vision-based settings are 65K and 1M (or 1B) respectively. The goal in vision-based methods is to learn from millions of unlabeled images. On the other hand, the goal in CURL is to develop sample-efficient RL algorithms. For example, to be able to solve a task within 100K timesteps (approximately 2 hours in real-time), an agent can only ingest 100K image frames.</p><p>Therefore, unlike MoCo, CURL does not use a memory bank for contrastive learning. Instead, the negatives are constructed on the fly for every minibatch sampled from the agent's replay buffer for an RL update similar to SimCLR. The exact implementation is provided as a PyTorch-like code snippet in 4.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation:</head><p>Random crop data augmentation has been crucial for the performance of deep learning based computer vision systems in object recognition, detection and segmentation <ref type="bibr" target="#b25">(Krizhevsky et al., 2012;</ref><ref type="bibr">Szegedy et al., 2015;</ref><ref type="bibr" target="#b6">Cubuk et al., 2019;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref>. However, similar augmentation methods have not seen much adoption in the field of RL even though several benchmarks use raw pixels as inputs to the model.</p><p>CURL adopts the random crop data augmentation as the stochastic data augmentation applied to a frame stack. To make it easier for the model to correlate spatio-temporal patterns in the input, we apply the same random crop (in terms of box coordinates) across all four frames in the stack as opposed to extracting different random crop positions from each frame in the stack. Further, unlike in computer vision systems where the aspect ratio for random crop is allowed to be as low as 0.08, we preserve much of the spatial information as possible and use a constant aspect ratio of 0.84 between the original and cropped. In our experiments, data augmented samples for CURL are formed by cropping 84 Ã 84 frames from an input frame of 100 Ã 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DMControl:</head><p>We render observations at 100 Ã 100 and randomly crop 84 Ã 84 frames. For evaluation, we render observations at 100 Ã 100 and center crop to 84 Ã 84 pixels. We found that implementing random crop efficiently was extremely important to the success of the algorithm. We provide pseudocode below:</p><p>from skimage import view_as_windows import numpy as np def random_crop(imgs, out): """ Vectorized random crop args: imgs: shape (B,C,H,W) out: output size (e.g. 84) """ # n: batch size. n = imgs.shape[0] img_size = imgs.shape[-1] # e.g. 100 crop_max = img_size -out imgs = np.transpose(imgs, (0, 2, 3, 1)) w1 = np.random.randint(0, crop_max, n) h1 = np.random.randint(0, crop_max, n) # creates all sliding window # combinations of size (out) windows = view_as_windows( imgs, (1, out, out, 1))[..., 0,:,:, 0] # selects a random window # for each batch element cropped = windows[np.arange(n), w1, h1] return cropped</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Atari100k Implementation Details</head><p>The flexibility of CURL allows us to apply it to discrete control setting with minimal modifications. Similar to our rationale for picking SAC as the baseline RL algorithm to couple CURL with (for continuous control), we pick the data-efficient version of Rainbow DQN (Efficient Rainbow) <ref type="bibr">(van Hasselt et al., 2019)</ref> for Atari100K which performs competitively with an older version of SimPLe (most recent version has improved numbers). In order to understand specifically what the gains from CURL are without any other changes, we adopt the exact same hyperparameters specified in the paper <ref type="bibr">(van Hasselt et al., 2019)</ref> (including a modified convolutional encoder that uses larger kernel size and stride of 5). We present the details in <ref type="table" target="#tab_9">Table 4</ref>. Similar to DMControl, the contrastive objective and the RL objective are weighted equally for learning (except for Pong, Freeway, Boxing and PrivateEye for which we used a coefficient of 0.05 for the momentum contastive loss. On a large majority (22 out of 26) of the games, we do not use this adjustment. While it is standard practice to use the same hyperparameters for all games in Atari, papers proposing auxiliary losses have adopted a different practice of using game specific coefficients <ref type="bibr" target="#b18">(Jaderberg et al., 2016)</ref>.). We use the Efficient Rainbow codebase from https: //github.com/Kaixhin/Rainbow which has a reproduced version of <ref type="bibr">van Hasselt et al. (2019)</ref>. We evaluate with 20 random seeds and report the mean score for each game given the high variance nature of the Atari100k steps benchmark. We restrict ourselves to using grayscale renderings of image observations and use random crop of frame stack as data augmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Benchmarking Data Efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Further Investigation of Data-Efficiency in Contrastive RL</head><p>To further benchmark CURL's sample-efficiency, we compare it to state-based SAC on a total of 16 DMControl environments. Shown in <ref type="figure">Figure 7</ref>, CURL matches state-based  <ref type="figure">Figure 6</ref>. The number of steps it takes a prior leading pixel-based method, Dreamer, to achieve the same score that CURL achieves at 100k training steps (clipped at 1M steps). On average, CURL is 4.5x more data-efficient. We chose Dreamer because the authors <ref type="bibr" target="#b16">(Hafner et al., 2019)</ref> report performance for all of the above environments while other baselines like SLAC and SAC+AE only benchmark on 4 and 6 environments, respectively. For further comparison of CURL with these methods, the reader is referred to Encoding each frame indiviudally ensures that the contrastive objective only has access to visual discriminants.</p><p>Comparing the visual and spatiotemporal variants of CURL in <ref type="figure">Figure 8</ref> shows that the variant trained on stacked frames outperforms the visual-only version in most environments. The only exceptions are reacher and ball-in-cup environments. Indeed, in those environments the visual signal is strong enough to solve the task optimally, whereas in other environments, such as walker and cheetah, where balance or coordination is required, visual information alone is insufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Environment Steps (Millions)</head><p>Episode Score State SAC CURL <ref type="figure">Figure 7</ref>. CURL compared to state-based SAC run for 3 seeds on each of 16 selected DMControl environments. For the 6 environments in 4, CURL performance is averaged over 10 seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Increasing Gradient Updates per Agent Step</head><p>Although most baselines we benchmark against use one gradient update per agent step, it was recently empirically shown that increasing the ratio of gradients per step improves data-efficiency in RL <ref type="bibr" target="#b22">(Kielak, 2020)</ref>. This finding is also supported by SLAC <ref type="bibr" target="#b29">(Lee et al., 2019)</ref>, where results are shown with a ratio of 1:1 (SLACv1) and 3:1 (SLACv2). We  <ref type="figure">Figure 8</ref>. CURL with temporal and visual discrimination (red) compared to CURL with only visual discrimination (green). In most settings, the variant with temporal variant outperforms the purely visual variant of CURL. The two exceptions are reacher and ball in cup environments, suggesting that learning dynamics is not necessary for those two environments. Note that the walker environment was run with action repeat of 4, whereas walker walk in the main results <ref type="table" target="#tab_10">Table 1</ref> and <ref type="figure">Figure 7</ref> was run with action repeat of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Decoupling Representation Learning from Reinforcement Learning</head><p>Typically, Deep RL representations depend almost entirely on the reward function specific to a task. However, handcrafted representations such as the proprioceptive state are independent of the reward function. It is much more desirable to learn reward-agnostic representations, so that the same representation can be re-used across different RL tasks. We test whether CURL can learn such representations by comparing CURL to a variant where the critic gradients are backpropagated through the critic and contrastive dense feedforward networks but stopped before reaching the convolutional neural network (CNN) part of the encoder.</p><p>Scores displayed in <ref type="figure">Figure 9</ref> show that for many environments, the detached CNN representations are sufficient to learn an optimal policy. The major exception is the cheetah environment, where the detached representation significantly under-performs. Though promising, we leave further exploration of task-agnostic representations for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Removing Data Augmentation for the Actor Critic</head><p>Our main results involve the use of data augmentations to regularize both the contrastive and SAC objectives. Here, we investigate whether the contrastive representations alone are sufficient for learning effective policies. In these experiments, we only augment the data for the contrastive  <ref type="figure">Figure 9</ref>. CURL where the CNN part of the encoder receives gradients from both the contrastive loss and critic (red) compared to CURL with the convolutional part of the encoder trained only with the contrastive objective (green). The detached encoder variant is able to learn representations that enable near-optimal learning on most environments, except for cheetah. As in <ref type="figure">Figure 8</ref>, the walker environment was run with action repeat of 4.</p><p>objective but not for the SAC agent. As a result, data augmentation is used only to learn features but does not influence the control policy. The pseudocode is shown below: . CURL with no data augmentations passed to the SAC agent improves the performance of the baseline pixel SAC by a mean of 2.0x / median of 1.7x on DMControl500k. For these runs we use a smaller batch size of 128 than the 512 batch size used for results in <ref type="table" target="#tab_9">Table 4</ref>. While the constastive loss alone improves over the pixel SAC baseline, most environments benefit from data augmentation also being passed to the SAC agent.</p><p>DMControl500k results plotted in <ref type="figure">Figure 10</ref> show that, on average, features learned through the contrastive loss alone improve the pixel SAC baseline by 2x. Augmenting the input passed to the SAC algorithm further improves performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5. Predicting State from Pixels</head><p>Despite improved sample-efficiency on most DMControl tasks, there is still a visible gap between the performance of SAC on state and SAC with CURL in some environments. Since CURL learns representations by performing instance discrimination across stacks of three frames, it's possible that the reason for degraded sample-efficiency on more challenging tasks is due to partial-observability of the ground truth state.</p><p>To test this hypothesis, we perform supervised regression (X, Y ) from pixels X to the proprioceptive state Y , where each data point x â X is a stack of three consecutive frames and y â Y is the corresponding state extracted from the simulator. We find that the error in predicting the state from pixels correlates with the policy performance of pixelbased methods. Test-time error rates displayed in <ref type="figure">Figure  11</ref> show that environments that CURL solves as efficiently as state-based SAC have low error-rates in predicting the state from stacks of pixels. The prediction error increases for more challenging environments, such as cheetah-run and walker-walk. Finally, the error is highest for environments where current pixel-based methods, CURL included, make no progress at all <ref type="bibr">(Tassa et al., 2018)</ref>, such as humanoid and swimmer.</p><p>This investigation suggests that degraded policy performance on challenging tasks may result from the lack of requisite information about the underlying state in the pixel data used for learning representations. We leave further investigation for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6. CURL + Efficient Rainbow Atari runs</head><p>We report the scores <ref type="table">(Tables 6 and 7)</ref> for 20 seeds across the 26 Atari games in the Atari100k benchmark for CURL coupled with Efficient Rainbow. The variance across multiple seeds is considerably high in this benchmark. Therefore, we report the scores for each of the seeds along with the mean and standard deviation for each game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Document changelog</head><p>This document tracks the progress and changes of CURL. In order to help readers be aware of and understand the changes, here is a brief summary:</p><p>v1 Initial version. <ref type="figure">Figure 11</ref>. Test-time mean squared error for predicting the proprioceptive state from pixels on a number of DMControl environments. In DMControl, environments fall into two groups -where the state corresponds to either (a) positions and velocities of the robot joints or (b) the joint angles and angular velocities.</p><p>v2 Minor changes to DMControl to account for frame skip factor when evaluating data-efficiency of CURL and baselines. Changed action repeat for the Walker-walk task from 4 to 2 to match baseline implementations.</p><p>v3 ICML 2020 Camera Ready. For our Atari experiments, we moved to the https://github.com/Kaixhin/ Rainbow codebase for easy and clean benchmarking that directly builds on top of Efficient Rainbow without other changes. We also run 20 seeds as opposed to 3 seeds earlier given the high variance nature of the benchmark.</p><p>v4 Added in Section E.4 -an ablation investigating whether contrastive representations alone, with no augmentations passed to the policy during training, improve the baseline SAC policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Connection to work on data augmentations</head><p>Recently, there have been two papers published on using data augmentations for reinforcement learning, RAD <ref type="bibr" target="#b27">(Laskin et al., 2020)</ref> and DrQ <ref type="bibr" target="#b24">(Kostrikov et al., 2020)</ref>. These two papers present the version of CURL without an auxiliary contrastive loss but rather directly feeding in the augmented views of the image observations to the underlying value / policy network(s). Both RAD and DrQ present results on both continuous and discrete control environments, surpassing the results presented in CURL on both the DMControl and Atari benchmarks. Plenty of researchers have opined in public forums whether the results in RAD and DrQ make CURL irrelevant if the objective is to use data augmentations for data-efficient reinforcement learning. We believe that answering this question needs more nuance and present our opinions below:</p><p>1. If one has access to a rich stream of rewards from the underlying environment and is interested in optimzing the performance in terms of average reward, RAD and DrQ are likely to work better than CURL. The reason for this is simply that RAD and DrQ directly optimize for the objective one cares about, while CURL introduces an additional auxiliary consistency objective.</p><p>2. If one does not have access to a rich stream of rewards and is interested in learning good latent spaces in a task agnostic manner that can allow for data-efficient controllers across multiple tasks, CURL is the only option since the contrastive objective in CURL is reward independent. Our ablation on the detached encoder with the CURL objective present evidence that one could build simple MLPs on top of the CURL features without fine-tuning the underlying encoder and still be data-efficient on many of the DMControl tasks.</p><p>3. Future work in data-efficient reinforcement learning, particularly for real world settings, is likely to require approaches that do not rely on reward functions. In such scenarios, CURL is likely to be the more preferred approach. Further, one could potentially use CURL in a scenario where unsupervised pre-training without reward functions is initially performed before fine-tuning to the RL objective across multiple tasks.</p><p>Given the above reasons, there isn't a straightforward answer as to which is the better algorithm and the answer really depends on what the researcher / practioner wants to solve. We also emphasize that CURL was the first approach that used data augmentations effectively to significantly improve the data-efficiency of model-free reinforcement learning methods with very simple changes and showed improvement over relatively more complex model-based methods. The augmentations and results in CURL inspired future work in the form of RAD and DrQ. We hope that the analysis and results presented in CURL encourage researchers to employ data augmentations, contrastive losses and unsupervised pre-training for future reinforcement learning research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>CURL uses instance discrimination with similarities to Sim-CLR<ref type="bibr" target="#b4">(Chen et al., 2020)</ref>, MoCo (He et al., 2019a) and CPC (HÃ©naff et al., 2019). Most Deep RL architectures operate with a stack of temporally consecutive frames as input (Hessel et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5. 3 .</head><label>3</label><figDesc>Baselines for benchmarking sample efficiency DMControl baselines: We present a number of baselines for continuous control within the DMControl suite: (i) SAC-AE (Yarats et al., 2019) where the authors attempt to use a Î²-VAE (Higgins et al., 2017), VAE (Kingma &amp; Welling, 2013) and a regualrized autoencoder Vincent et al. (2008); Ghosh et al. (2019) jointly with SAC; (ii) SLAC (Lee et al., 2019) which learns a latent space world model on top of VAE features<ref type="bibr" target="#b12">Ha &amp; Schmidhuber (2018)</ref> and builds value functions on top; (iii) PlaNet and (iv) Dreamer<ref type="bibr" target="#b15">(Hafner et al., 2018;</ref> both of which learn a latent space world model and explicitly plan through it; (v) Pixel SAC: Vanilla SAC operating purely from pixels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure</head><label></label><figDesc>Figure 4. Performance of CURL coupled to SAC averaged across 10 seeds relative to SLACv1, PlaNet, Pixel SAC and State SAC baselines. At the 500k benchmark CURL matches the median score of state-based SAC. At 100k environment steps CURL achieves a 1.9x higher median score than Dreamer. For a direct comparison, we only compute the median across the 6 environments in 1 (4 for SLAC) and show learning curves for CURL across 16 DMControl experiments in 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Performance on cheetah-run environment ablated twoways: (left) using the query encoder or exponentially moving average of the query encoder for encoding keys (right) using the bi-linear inner product as in(van den Oord et al., 2018)  or the cosine inner product as inHe et al. (2019b);<ref type="bibr" target="#b4">Chen et al. (2020)</ref> Pseudo-code for the architecture is provided below: channels, f: filters # k: kernel, s: stride z = Conv2d(c=x.shape[1], f=32, k=3, s=2)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>loss o_anchor, o_target = aug(o), aug(o) curl_loss = contrastive(o_anchor, o_target) sac_loss = critic_loss(o) + actor_loss(o) loss = curl_loss + sac_loss params = update(params, grad(loss, params))Figure 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>representations.He et al. (2019a)  propose momentum contrast (MoCo), which uses the exponentially moving average (momentum averaged) version of the query encoder f q for encoding the keys in K. Given f q parametrized by Î¸ q and f k parametrized by Î¸ k , MoCo performs the update Î¸ k = mÎ¸ k + (1 â m)Î¸ q and encodes any target x k using SG(f k (x k )) [SG : Stop Gradient].CURL couples frame-stack instance discrimination with momentum encoding for the targets during contrastive learning, and RL is performed on top of the encoder features.</figDesc><table><row><cell>4.6. Differences Between CURL and Prior Contrastive</cell></row><row><cell>Methods in RL</cell></row><row><cell>van den Oord et al. (2018) use Contastive Predictive Coding</cell></row><row><cell>(CPC) as an auxiliary task wherein an LSTM operates on</cell></row><row><cell>a latent space of a convolutional encoder; and both the</cell></row><row><cell>CPC and A2C (Mnih et al., 2015) objectives are jointly</cell></row><row><cell>optimized. CURL avoids using pipelines that predict the</cell></row><row><cell>future in a latent space such as van den Oord et al. (2018);</cell></row><row><cell>Hafner et al. (2019). In CURL, we opt for a simple instance</cell></row><row><cell>discrimination style contrastive auxiliary task.</cell></row><row><cell>4.7. CURL Contrastive Learning Pseudocode</cell></row><row><cell>(PyTorch-like)</cell></row><row><cell># f_q, f_k: encoder networks for anchor</cell></row><row><cell># (query) and target (keys) respectively.</cell></row><row><cell># loader: minibatch sampler from ReplayBuffer</cell></row><row><cell># B-batch_size, C-channels, H,W-spatial_dims</cell></row><row><cell># x : shape : [B, C, H, W]</cell></row><row><cell># C = c * num_frames; c=3 (R/G/B) or 1 (gray)</cell></row><row><cell># m: momentum, e.g. 0.95</cell></row><row><cell># z_dim: latent dimension</cell></row><row><cell>f_k.params = f_q.params</cell></row><row><cell>W = rand(z_dim, z_dim) # bilinear product.</cell></row><row><cell>for x in loader: # load minibatch from buffer</cell></row><row><cell>x_q = aug(x) # random augmentation</cell></row><row><cell>x_k = aug(x) # different random augmentation</cell></row><row><cell>z_q = f_q.forward(x_q)</cell></row><row><cell>z_k = f_k.forward(x_k)</cell></row><row><cell>z_k = z_k.detach() # stop gradient</cell></row><row><cell>proj_k = matmul(W, z_k.T) # bilinear product</cell></row><row><cell>logits = matmul(z_q, proj_k) # B x B</cell></row><row><cell># subtract max from logits for stability</cell></row><row><cell>logits = logits -max(logits, axis=1)</cell></row><row><cell>labels = arange(logits.shape[0])</cell></row><row><cell>loss = CrossEntropyLoss(logits, labels)</cell></row><row><cell>loss.backward()</cell></row><row><cell>update(f_q.params) # Adam</cell></row><row><cell>update(W) # Adam</cell></row><row><cell>f_k.params = m * f_k.params+(1-m) * f_q.params</cell></row><row><cell>5. Experiments</cell></row><row><cell>5.1. Evaluation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>500K STEP SCORES</cell><cell>CURL</cell><cell>PLANET</cell><cell>DREAMER</cell><cell>SAC+AE</cell><cell>SLACV1</cell><cell cols="2">PIXEL SAC STATE SAC</cell></row><row><cell>FINGER, SPIN</cell><cell cols="4">926 Â± 45 561 Â± 284 796 Â± 183 884 Â± 128</cell><cell>673 Â± 92</cell><cell>179 Â± 166</cell><cell>923 Â± 21</cell></row><row><cell>CARTPOLE, SWINGUP</cell><cell>841 Â± 45</cell><cell>475 Â± 71</cell><cell>762 Â± 27</cell><cell>735 Â± 63</cell><cell>-</cell><cell>419 Â± 40</cell><cell>848 Â± 15</cell></row><row><cell>REACHER, EASY</cell><cell>929 Â± 44</cell><cell cols="2">210 Â± 390 793 Â± 164</cell><cell>627 Â± 58</cell><cell>-</cell><cell>145 Â± 30</cell><cell>923 Â± 24</cell></row><row><cell>CHEETAH, RUN</cell><cell>518 Â± 28</cell><cell cols="2">305 Â± 131 570 Â± 253</cell><cell>550 Â± 34</cell><cell>640 Â± 19</cell><cell>197 Â± 15</cell><cell>795 Â± 30</cell></row><row><cell>WALKER, WALK</cell><cell>902 Â± 43</cell><cell>351 Â± 58</cell><cell>897 Â± 49</cell><cell>847 Â± 48</cell><cell>842 Â± 51</cell><cell>42 Â± 12</cell><cell>948 Â± 54</cell></row><row><cell>BALL IN CUP, CATCH</cell><cell>959 Â± 27</cell><cell>460 Â± 380</cell><cell>879 Â± 87</cell><cell>794Â± 58</cell><cell>852 Â± 71</cell><cell>312Â± 63</cell><cell>974 Â± 33</cell></row><row><cell>100K STEP SCORES</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FINGER, SPIN</cell><cell>767 Â± 56</cell><cell>136 Â± 216</cell><cell>341 Â± 70</cell><cell>740 Â± 64</cell><cell>693 Â± 141</cell><cell>179 Â± 66</cell><cell>811Â±46</cell></row><row><cell cols="2">CARTPOLE, SWINGUP 582Â±146</cell><cell>297Â±39</cell><cell>326Â±27</cell><cell>311Â±11</cell><cell>-</cell><cell>419Â±40</cell><cell>835Â±22</cell></row><row><cell>REACHER, EASY</cell><cell>538Â±233</cell><cell>20Â±50</cell><cell>314Â±155</cell><cell>274Â±14</cell><cell>-</cell><cell>145Â±30</cell><cell>746Â±25</cell></row><row><cell>CHEETAH, RUN</cell><cell>299 Â±48</cell><cell>138Â±88</cell><cell>235Â± 137</cell><cell>267Â±24</cell><cell>319Â±56</cell><cell>197Â±15</cell><cell>616Â±18</cell></row><row><cell>WALKER, WALK</cell><cell>403Â±24</cell><cell>224Â±48</cell><cell>277Â±12</cell><cell>394Â±22</cell><cell>361Â±73</cell><cell>42Â±12</cell><cell>891Â±82</cell></row><row><cell>BALL IN CUP, CATCH</cell><cell>769 Â± 43</cell><cell>0 Â± 0</cell><cell>246 Â± 174</cell><cell>391Â± 82</cell><cell>512 Â± 110</cell><cell>312Â± 63</cell><cell>746Â±91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>GAME</cell><cell cols="6">HUMAN RANDOM RAINBOW SIMPLE OTRAINBOW EFF. RAINBOW</cell><cell>CURL</cell></row><row><cell>ALIEN</cell><cell>7127.7</cell><cell>227.8</cell><cell>318.7</cell><cell>616.9</cell><cell>824.7</cell><cell>739.9</cell><cell>558.2</cell></row><row><cell>AMIDAR</cell><cell>1719.5</cell><cell>5.8</cell><cell>32.5</cell><cell>88.0</cell><cell>82.8</cell><cell>188.6</cell><cell>142.1</cell></row><row><cell>ASSAULT</cell><cell>742.0</cell><cell>222.4</cell><cell>231</cell><cell>527.2</cell><cell>351.9</cell><cell>431.2</cell><cell>600.6</cell></row><row><cell>ASTERIX</cell><cell>8503.3</cell><cell>210.0</cell><cell>243.6</cell><cell>1128.3</cell><cell>628.5</cell><cell>470.8</cell><cell>734.5</cell></row><row><cell>BANK HEIST</cell><cell>753.1</cell><cell>14.2</cell><cell>15.55</cell><cell>34.2</cell><cell>182.1</cell><cell>51.0</cell><cell>131.6</cell></row><row><cell>BATTLE ZONE</cell><cell>37187.5</cell><cell>2360.0</cell><cell>2360.0</cell><cell>5184.4</cell><cell>4060.6</cell><cell>10124.6</cell><cell>14870.0</cell></row><row><cell>BOXING</cell><cell>12.1</cell><cell>0.1</cell><cell>-24.8</cell><cell>9.1</cell><cell>2.5</cell><cell>0.2</cell><cell>1.2</cell></row><row><cell>BREAKOUT</cell><cell>30.5</cell><cell>1.7</cell><cell>1.2</cell><cell>16.4</cell><cell>9.84</cell><cell>1.9</cell><cell>4.9</cell></row><row><cell>CHOPPER COMMAND</cell><cell>7387.8</cell><cell>811.0</cell><cell>120.0</cell><cell>1246.9</cell><cell>1033.33</cell><cell>861.8</cell><cell>1058.5</cell></row><row><cell>CRAZY CLIMBER</cell><cell>35829.4</cell><cell>10780.5</cell><cell>2254.5</cell><cell>62583.6</cell><cell>21327.8</cell><cell>16185.3</cell><cell>12146.5</cell></row><row><cell>DEMON ATTACK</cell><cell>1971.0</cell><cell>152.1</cell><cell>163.6</cell><cell>208.1</cell><cell>711.8</cell><cell>508.0</cell><cell>817.6</cell></row><row><cell>FREEWAY</cell><cell>29.6</cell><cell>0.0</cell><cell>0.0</cell><cell>20.3</cell><cell>25.0</cell><cell>27.9</cell><cell>26.7</cell></row><row><cell>FROSTBITE</cell><cell>4334.7</cell><cell>65.2</cell><cell>60.2</cell><cell>254.7</cell><cell>231.6</cell><cell>866.8</cell><cell>1181.3</cell></row><row><cell>GOPHER</cell><cell>2412.5</cell><cell>257.6</cell><cell>431.2</cell><cell>771.0</cell><cell>778.0</cell><cell>349.5</cell><cell>669.3</cell></row><row><cell>HERO</cell><cell>30826.4</cell><cell>1027.0</cell><cell>487</cell><cell>2656.6</cell><cell>6458.8</cell><cell>6857.0</cell><cell>6279.3</cell></row><row><cell>JAMESBOND</cell><cell>302.8</cell><cell>29.0</cell><cell>47.4</cell><cell>125.3</cell><cell>112.3</cell><cell>301.6</cell><cell>471.0</cell></row><row><cell>KANGAROO</cell><cell>3035.0</cell><cell>52.0</cell><cell>0.0</cell><cell>323.1</cell><cell>605.4</cell><cell>779.3</cell><cell>872.5</cell></row><row><cell>KRULL</cell><cell>2665.5</cell><cell>1598.0</cell><cell>1468</cell><cell>4539.9</cell><cell>3277.9</cell><cell>2851.5</cell><cell>4229.6</cell></row><row><cell>KUNG FU MASTER</cell><cell>22736.3</cell><cell>258.5</cell><cell>0.</cell><cell>17257.2</cell><cell>5722.2</cell><cell>14346.1</cell><cell>14307.8</cell></row><row><cell>MS PACMAN</cell><cell>6951.6</cell><cell>307.3</cell><cell>67</cell><cell>1480.0</cell><cell>941.9</cell><cell>1204.1</cell><cell>1465.5</cell></row><row><cell>PONG</cell><cell>14.6</cell><cell>-20.7</cell><cell>-20.6</cell><cell>12.8</cell><cell>1.3</cell><cell>-19.3</cell><cell>-16.5</cell></row><row><cell>PRIVATE EYE</cell><cell>69571.3</cell><cell>24.9</cell><cell>0</cell><cell>58.3</cell><cell>100.0</cell><cell>97.8</cell><cell>218.4</cell></row><row><cell>QBERT</cell><cell>13455.0</cell><cell>163.9</cell><cell>123.46</cell><cell>1288.8</cell><cell>509.3</cell><cell>1152.9</cell><cell>1042.4</cell></row><row><cell>ROAD RUNNER</cell><cell>7845.0</cell><cell>11.5</cell><cell>1588.46</cell><cell>5640.6</cell><cell>2696.7</cell><cell>9600.0</cell><cell>5661.0</cell></row><row><cell>SEAQUEST</cell><cell>42054.7</cell><cell>68.4</cell><cell>131.69</cell><cell>683.3</cell><cell>286.92</cell><cell>354.1</cell><cell>384.5</cell></row><row><cell>UP N DOWN</cell><cell>11693.2</cell><cell>533.4</cell><cell>504.6</cell><cell>3350.3</cell><cell>2847.6</cell><cell>2877.4</cell><cell>2955.2</cell></row></table><note>Scores achieved by CURL (coupled with Eff. Rainbow) and baselines on Atari benchmarked at 100k time-steps (Atari100k). CURL achieves state-of-the-art performance on 7 out of 26 environments. Our baselines are SimPLe (Kaiser et al., 2019), OverTrained Rainbow (OTRainbow) (Kielak, 2020), Data-Efficient Rainbow (Eff. Rainbow) (van Hasselt et al., 2019), Rainbow (Hessel et al., 2017), Random Agent and Human Performance (Human). We see that CURL implemented on top of Eff. Rainbow improves over Eff. Rainbow on 19 out of 26 games. We also run CURL with 20 random seeds given that this benchmark is susceptible to high variance across multiple runs. We also see that CURL achieves superhuman performance on JamesBond and Krull.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>. Below are the</cell></row><row><cell>key findings:</cell></row><row><cell>(i) CURL achieves a median human-normalized score</cell></row><row><cell>(HNS) of 17.5% while SimPLe and Efficient Rainbow DQN</cell></row><row><cell>achieve 14.4% and 16.1% respectively. The mean HNS is</cell></row><row><cell>38.1%, 44.3%, and 28.5% for CURL, SimPLe, and Efficient</cell></row><row><cell>Rainbow DQN respectively.</cell></row><row><cell>(ii) CURL improves on top of Efficient Rainbow on 19</cell></row><row><cell>out of 26 Atari games. Averaged across 26 games, CURL</cell></row><row><cell>improves on top of Efficient Rainbow by 1.3x, while the me-</cell></row><row><cell>dian performance improvement over SimPLE and Efficient</cell></row><row><cell>Rainbow are 1.2x and 1.1x respectively.</cell></row><row><cell>(iii) CURL surpasses human performance on two games</cell></row><row><cell>JamesBond (1.6 HNS), Krull (2.5 HNS).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Sutton, R. S. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Machine learning proceedings 1990, pp. 216-224. Elsevier, 1990. Sutton, R. S. et al. Introduction to reinforcement learning, volume 135. 1998. Wu, Z., Xiong, Y., Yu, S., and Lin, D. Unsupervised feature learning via non-parametric instance-level discrimination. arXiv preprint arXiv:1805.01978, 2018. Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R. Improving sample efficiency in modelfree reinforcement learning from images. arXiv preprint arXiv:1910.01741, 2019.</figDesc><table><row><cell>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,</cell><cell></cell></row><row><cell>Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,</cell><cell></cell></row><row><cell>A. Going deeper with convolutions. In Computer</cell><cell></cell></row><row><cell>Vision and Pattern Recognition (CVPR), 2015. URL</cell><cell></cell></row><row><cell>http://arxiv.org/abs/1409.4842.</cell><cell></cell></row><row><cell>Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D.</cell><cell></cell></row><row><cell>d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq,</cell><cell></cell></row><row><cell>A., et al. Deepmind control suite. arXiv preprint</cell><cell></cell></row><row><cell>arXiv:1801.00690, 2018.</cell><cell></cell></row><row><cell>Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview</cell><cell></cell></row><row><cell>coding. arXiv preprint arXiv:1906.05849, 2019.</cell><cell></cell></row><row><cell>Tschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S.,</cell><cell></cell></row><row><cell>and Lucic, M. On mutual information maximization for</cell><cell></cell></row><row><cell>representation learning. arXiv preprint arXiv:1907.13625,</cell><cell></cell></row><row><cell>2019.</cell><cell></cell></row><row><cell>van den Oord, A., Li, Y., and Vinyals, O. Representa-</cell><cell></cell></row><row><cell>tion learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.</cell><cell>He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-mentum contrast for unsupervised visual representation</cell></row><row><cell>Van Hasselt, H., Guez, A., and Silver, D. Deep reinforce-</cell><cell>learning. arXiv preprint arXiv:1911.05722, 2019a.</cell></row><row><cell>ment learning with double q-learning. In Thirtieth AAAI conference on artificial intelligence, 2016.</cell><cell>He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-mentum contrast for unsupervised visual representation</cell></row><row><cell>van Hasselt, H. P., Hessel, M., and Aslanides, J. When to</cell><cell>learning. 2019b.</cell></row><row><cell>use parametric models in reinforcement learning? In</cell><cell></cell></row><row><cell>Advances in Neural Information Processing Systems, pp.</cell><cell>HÃ©naff, O. J., Srinivas, A., De Fauw, J., Razavi, A., Doersch,</cell></row><row><cell>14322-14333, 2019.</cell><cell>C., Eslami, S., and Oord, A. v. d. Data-efficient image</cell></row><row><cell>Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. Extracting and composing robust features with denoising</cell><cell>recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.</cell></row><row><cell>autoencoders. In Proceedings of the 25th international</cell><cell>Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup,</cell></row><row><cell>conference on Machine learning, pp. 1096-1103, 2008.</cell><cell>D., and Meger, D. Deep reinforcement learning that</cell></row><row><cell>Wang, X. and Gupta, A. Unsupervised learning of visual representations using videos. In Proceedings of the IEEE</cell><cell>This research is supported in part by DARPA through the matters. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</cell></row><row><cell>International Conference on Computer Vision, pp. 2794-2802, 2015. Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanc-tot, M., and De Freitas, N. Dueling network architec-tures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.</cell><cell>Learning with Less Labels (LwLL) Program and by ONR through PECASE N000141612723. We also thank Wendy Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostro-vski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Shang for her help with Section E.4; Zak Stone and Google Silver, D. Rainbow: Combining improvements in deep TFRC for cloud credits; Danijar Hafner, Alex Lee, and De-reinforcement learning, 2017. nis Yarats for sharing data for baselines; and Lerrel Pinto, Adam Stooke, Will Whitney, and Ankesh Anand for insight-ful discussions. Higgins, I., Pal, A., Rusu, A., Matthey, L., Burgess, C., Pritzel, A., Botvinick, M., Blundell, C., and Lerchner,</cell></row><row><cell></cell><cell>A. Darla: Improving zero-shot transfer in reinforcement</cell></row><row><cell></cell><cell>learning. In Proceedings of the 34th International Con-</cell></row><row><cell></cell><cell>ference on Machine Learning-Volume 70, pp. 1480-1490.</cell></row><row><cell></cell><cell>JMLR. org, 2017.</cell></row></table><note>Warde-Farley, D., Van de Wiele, T., Kulkarni, T., Ionescu, C., Hansen, S., and Mnih, V. Unsupervised control through non-parametric discriminative rewards. arXiv preprint arXiv:1811.11359, 2018.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Hyperparameters used for DMControl CURL experiments.Most hyperparameters values are unchanged across environments with the exception for action repeat, learning rate, and batch size.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Random crop</cell><cell>True</cell></row><row><cell>Observation rendering</cell><cell>(100, 100)</cell></row><row><cell cols="2">Observation downsampling (84, 84)</cell></row><row><cell>Replay buffer size</cell><cell>100000</cell></row><row><cell>Initial steps</cell><cell>1000</cell></row><row><cell>Stacked frames</cell><cell>3</cell></row><row><cell>Action repeat</cell><cell>2 finger, spin; walker, walk</cell></row><row><cell></cell><cell>8 cartpole, swingup</cell></row><row><cell></cell><cell>4 otherwise</cell></row><row><cell>Hidden units (MLP)</cell><cell>1024</cell></row><row><cell>Evaluation episodes</cell><cell>10</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>(Î²1, Î²2) â (f Î¸ , Ï Ï , Q Ï )</cell><cell>(.9, .999)</cell></row><row><cell>(Î²1, Î²2) â (Î±)</cell><cell>(.5, .999)</cell></row><row><cell cols="2">Learning rate (f Î¸ , Ï Ï , Q Ï ) 2e â 4 cheetah, run</cell></row><row><cell></cell><cell>1e â 3 otherwise</cell></row><row><cell>Learning rate (Î±)</cell><cell>1e â 4</cell></row><row><cell>Batch Size</cell><cell>512</cell></row><row><cell>Q function EMA Ï</cell><cell>0.01</cell></row><row><cell>Critic target update freq</cell><cell>2</cell></row><row><cell>Convolutional layers</cell><cell>4</cell></row><row><cell>Number of filters</cell><cell>32</cell></row><row><cell>Non-linearity</cell><cell>ReLU</cell></row><row><cell>Encoder EMA Ï</cell><cell>0.05</cell></row><row><cell>Latent dimension</cell><cell>50</cell></row><row><cell>Discount Î³</cell><cell>.99</cell></row><row><cell>Initial temperature</cell><cell>0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>Hyperparameters used for Atari100K CURL experiments. Hyperparameters are unchanged across games.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Random crop</cell><cell>True</cell></row><row><cell>Image size</cell><cell>(84, 84)</cell></row><row><cell>Data Augmentation</cell><cell>Random Crop (Train)</cell></row><row><cell>Replay buffer size</cell><cell>100000</cell></row><row><cell>Training frames</cell><cell>400000</cell></row><row><cell>Training steps</cell><cell>100000</cell></row><row><cell>Frame skip</cell><cell>4</cell></row><row><cell>Stacked frames</cell><cell>4</cell></row><row><cell>Action repeat</cell><cell>4</cell></row><row><cell>Replay period every</cell><cell>1</cell></row><row><cell>Q network: channels</cell><cell>32, 64</cell></row><row><cell>Q network: filter size</cell><cell>5 Ã 5, 5 Ã 5</cell></row><row><cell>Q network: stride</cell><cell>5, 5</cell></row><row><cell>Q network: hidden units</cell><cell>256</cell></row><row><cell>Momentum (EMA for CURL) Ï</cell><cell>0.001</cell></row><row><cell>Non-linearity</cell><cell>ReLU</cell></row><row><cell>Reward Clipping</cell><cell>[â1, 1]</cell></row><row><cell>Multi step return</cell><cell>20</cell></row><row><cell cols="2">Minimum replay size for sampling 1600</cell></row><row><cell>Max frames per episode</cell><cell>108K</cell></row><row><cell>Update</cell><cell>Distributional Double Q</cell></row><row><cell>Target Network Update Period</cell><cell>every 2000 updates</cell></row><row><cell>Support-of-Q-distribution</cell><cell>51 bins</cell></row><row><cell>Discount Î³</cell><cell>0.99</cell></row><row><cell>Batch Size</cell><cell>32</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>Optimizer: learning rate</cell><cell>0.0001</cell></row><row><cell>Optimizer: Î²1</cell><cell>0.9</cell></row><row><cell>Optimizer: Î²2</cell><cell>0.999</cell></row><row><cell>Optimizer</cell><cell>0.000015</cell></row><row><cell>Max gradient norm</cell><cell>10</cell></row><row><cell>Exploration</cell><cell>Noisy Nets</cell></row><row><cell>Noisy nets parameter</cell><cell>0.1</cell></row><row><cell>Priority exponent</cell><cell>0.5</cell></row><row><cell>Priority correction</cell><cell>0.4 â 1</cell></row><row><cell>Hardware</cell><cell>CPU</cell></row><row><cell cols="2">data-efficiency on most of the environments, but lags behind</cell></row><row><cell cols="2">state-based SAC on more challenging environments.</cell></row><row><cell>E. Ablations</cell><cell></cell></row><row><cell cols="2">E.1. Learning Temporal Dynamics</cell></row><row><cell cols="2">To gain insight as to whether CURL learns temporal dy-</cell></row><row><cell cols="2">namics across the stacked frames, we also train a variant</cell></row><row><cell cols="2">of CURL where the discriminants are individual frames as</cell></row><row><cell cols="2">opposed to stacked ones. This can be done by sampling</cell></row><row><cell cols="2">stacked frames from the replay buffer but only using the</cell></row><row><cell cols="2">first frame to update the contrastive loss:</cell></row><row><cell cols="2">f_q = x_q[:,:3,...] # (B,C,H,W), C=9.</cell></row><row><cell>f_k = x_k[:,:3,...]</cell><cell></cell></row><row><cell cols="2">During the actor-critic update, frames in the batch are en-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>and Figure 4.</cell></row><row><cell>coded individually into latent codes, which are then concate-</cell></row><row><cell>nated before being passed to a dense network.</cell></row><row><cell># x: (B,C,H,W), C=9.</cell></row><row><cell>z1 = encode(x[:,:3,...])</cell></row><row><cell>z2 = encode(x[:,3:6,...])</cell></row><row><cell>z3 = encode(x[:,6:9,...])</cell></row><row><cell>z = torch.cat([z1,z2,z3],-1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 .</head><label>5</label><figDesc>Scores achieved by CURL and SLAC when run with a 3:1 ratio of gradient updates per agent step on DMControl500k and DMControl100k. CURL achieves state-of-the-art performance on the majority (3 out of 4) environments on DMControl500k. Performance of both algorithms is improved relative to the 1:1 ratio reported for all baselines inTable 1but at the cost of significant compute and wall-clock time overhead.</figDesc><table><row><cell>DMCONTROL500K</cell><cell>CURL</cell><cell>SLACV2</cell></row><row><cell>FINGER, SPIN</cell><cell>923 Â± 50</cell><cell>884 Â± 98</cell></row><row><cell>WALKER, WALK</cell><cell>911 Â± 35</cell><cell>891 Â± 60</cell></row><row><cell>CHEETAH, RUN</cell><cell>545 Â± 39</cell><cell>791 Â± 37</cell></row><row><cell>BALL IN CUP, CATCH</cell><cell>948 Â± 21</cell><cell>885 Â± 154</cell></row><row><cell>DMCONTROL100K</cell><cell>CURL</cell><cell>SLACV2</cell></row><row><cell>FINGER, SPIN</cell><cell>741Â± 118</cell><cell>728 Â±212</cell></row><row><cell>WALKER, WALK</cell><cell>428 Â± 59</cell><cell>513 Â± 41</cell></row><row><cell>CHEETAH, RUN</cell><cell>314 Â± 46</cell><cell>438 Â± 76</cell></row><row><cell>BALL IN CUP, CATCH</cell><cell>899 Â± 47</cell><cell>837 Â± 147</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised state representation learning in atari</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Racah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>CÃ´tÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8766" to="8779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15509" to="15519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning actionable representations from visual observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1577" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01561</idno>
		<title level="m">Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.10295</idno>
		<title level="m">Noisy networks for exploration</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">From variational to deterministic autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10122</idno>
	</analytic>
	<monogr>
		<title level="j">J. World models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05905</idno>
		<title level="m">Soft actor-critic algorithms and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning latent dynamics for planning from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davidson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04551</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dream to control: Learning behaviors by latent imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01603</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05397</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human-level performance in 3d multiplayer games with populationbased reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Castaneda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ruderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">6443</biblScope>
			<biblScope unit="page" from="859" to="865" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Model-based reinforcement learning for atari</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Osinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czechowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kozakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00374</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Holly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10293</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Do recent advancements in model-based deep reinforcement learning really improve data efficiency?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kielak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Image augmentation is all you need: Regularizing deep reinforcement learning from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13649</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14990</idno>
		<title level="m">Reinforcement learning with augmented data</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stochastic latent actor-critic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagabandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00953</idno>
	</analytic>
	<monogr>
		<title level="m">Deep reinforcement learning with a latent variable model</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning to navigate in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03673</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05952</idno>
		<title level="m">Prioritized experience replay</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Making the world differentiable: On using fully recurrent self-supervised neural networks for dynamic reinforcement learning and planning in nonstationary environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>FKI-126-90</idno>
		<imprint>
			<date type="published" when="1990" />
			<publisher>TUM</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mastering atari, go, chess and shogi by planning with a learned model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Loss is its own reward: Self-supervision for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mahmoudieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07307</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Universal planning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00645</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">Table 6. CURL implemented on top of Efficient Rainbow -Scores reported for 20 random seeds for each of the above games</title>
		<imprint/>
	</monogr>
	<note>with the last two rows being the mean and standard deviation across the runs</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">DemonAttack Amidar Alien BankHeist Breakout Freeway Pong PrivateEye Boxing 3529</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upndown</forename><surname>Hero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Crazyclimber</forename><surname>Choppercomm</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<title level="m">Table 7. CURL implemented on top of Efficient Rainbow -Scores reported for 20 random seeds for each of the above games</title>
		<imprint/>
	</monogr>
	<note>with the last two rows being the mean and standard deviation across the runs</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
