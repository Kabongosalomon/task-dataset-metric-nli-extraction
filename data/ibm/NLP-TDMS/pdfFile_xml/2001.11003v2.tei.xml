<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><forename type="middle">F R</forename><surname>Ribeiro</surname></persName>
							<email>ribeiro@aiphes.tu-darmstadt.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<email>yue.zhang@wias.org.cnclaire.gardent@loria.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">CNRS/LORIA</orgName>
								<address>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Training Group AIPHES and UKP Lab</orgName>
								<orgName type="institution" key="instit1">Technische Universität Darmstadt ‡ School of Engineering</orgName>
								<orgName type="institution" key="instit2">Westlake University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent graph-to-text models generate text from graph-based data using either global or local aggregation to learn node representations. Global node encoding allows explicit communication between two distant nodes, thereby neglecting graph topology as all nodes are directly connected. In contrast, local node encoding considers the relations between neighbor nodes capturing the graph structure, but it can fail to capture long-range relations. In this work, we gather both encoding strategies, proposing novel neural models which encode an input graph combining both global and local node contexts, in order to learn better contextualized node embeddings. In our experiments, we demonstrate that our approaches lead to significant improvements on two graph-to-text datasets achieving BLEU scores of 18.01 on AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph-to-text generation refers to the task of generating natural language text from input graph structures, which can be semantic representations <ref type="bibr" target="#b17">(Konstas et al., 2017)</ref> or knowledge graphs (KG) <ref type="bibr" target="#b11">(Gardent et al., 2017;</ref><ref type="bibr" target="#b16">Koncel-Kedziorski et al., 2019)</ref>. While most recent work <ref type="bibr" target="#b25">Ribeiro et al., 2019;</ref><ref type="bibr" target="#b12">Guo et al., 2019)</ref> focuses on generating sentences, a more challenging and interesting scenario emerges when the goal is to generate multi-sentence texts. In this context, in addition to sentence generation, document planning needs to be handled: the input needs to be mapped into several sentences; sentences need to be ordered and connected using appropriate discourse markers; and inter-sentential anaphora and 1 Code is available at https://github.com/UKPLab/kg2text DistMult … For the link prediction task, first we learn node embeddings using DistMult method. … Further, we also experiment with GAT, a GNN model, in order to generate node embeddings to predict edges between nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Node</head><p>Encoder ellipsis may need to be generated to avoid repetition. In this paper, we focus on generating texts rather than sentences where the output are short texts <ref type="bibr" target="#b11">(Gardent et al., 2017)</ref> or paragraphs <ref type="bibr" target="#b16">(Koncel-Kedziorski et al., 2019)</ref>.</p><p>A key issue in neural graph-to-text generation is how to encode the input graphs. The basic idea is to incrementally compute node representations by aggregating structural context information. To this end, two main approaches have been proposed: (i) models based on local node aggregation, usually built upon Graph Neural Networks (GNN) <ref type="bibr" target="#b14">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b13">Hamilton et al., 2017)</ref> and (ii) models that leverage global node aggregation. Systems that adopt global encoding strategies are typically based on Transformers <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>, using self-attention to compute a node representation based on all nodes in the graph. This approach enjoys the advantage of a large node context range, but neglects the graph topology by effectively treating every node as being connected to all the others in the graph. In contrast, models based on local aggregation learn the representation of each node based on its adjacent nodes as defined in the input graph. This approach effectively exploits the graph topology, and the graph structure has a strong impact on the node representation <ref type="bibr" target="#b36">(Xu et al., 2018)</ref>. However, encoding relations between distant nodes can be challenging by requiring more graph encoding layers, which can also propagate noise .</p><p>For example, <ref type="figure" target="#fig_1">Figure 1a</ref> presents a KG, for which a corresponding text is shown in <ref type="figure" target="#fig_1">Figure 1b</ref>. Note that there is a mismatch between how entities are connected in the graph and how their natural language descriptions are related in the text. Some entities syntactically related in the text are not connected in the graph. For instance, in the sentence "For the link prediction task, first we learn node embeddings using DistMult method.", while the entity mentions are dependent of the same verb, in the graph, the node embeddings node has no explicit connection with link prediction and Dist-Mult nodes, which are in a different connected component. This example illustrates the importance of encoding distant information in the input graph. As shown in <ref type="figure" target="#fig_1">Figure 1c</ref>, a global encoder is able to learn a node representation for node embeddings which captures information from nonconnected entities such as DistMult. By modeling distant connections between all nodes, we allow for these missing links to be captured, as KGs are known to be highly incomplete <ref type="bibr" target="#b8">(Dong et al., 2014;</ref><ref type="bibr" target="#b26">Schlichtkrull et al., 2018)</ref>.</p><p>In contrast, the local strategy refines the node representation with richer neighborhood information, as nodes that share the same neighborhood exhibit a strong homophily: two similar entities are much more likely to be connected than at random. Consequently, the local context enriches the node representation with local information from KG triples. For example, in <ref type="figure" target="#fig_1">Figure 1a</ref>, GAT reaches node embeddings through the GNN. This transi-tive relation can be captured by a local encoder, as shown in <ref type="figure" target="#fig_1">Figure 1d</ref>. Capturing this form of relationship also can support text generation at the sentence level.</p><p>In this paper, we investigate novel graph-totext architectures that combine both global and local node aggregations, gathering the benefits from both strategies. In particular, we propose a unified graph-to-text framework based on Graph Attention Networks (GAT) <ref type="bibr" target="#b32">(Veličković et al., 2018)</ref>. As part of this framework, we empirically compare two main architectures: a cascaded architecture that performs global node aggregation before performing local node aggregation, and a parallel architecture that performs global and local aggregations simultaneously. While the cascaded architecture allows the local encoder to leverage global encoding features, the parallel architecture allows more independent features to complement each other. To further consider fine-grained integration, we additionally consider layer-wise integration of the global and local encoders.</p><p>Extensive experiments show that our approaches consistently outperform recent models on two benchmarks for text generation from KGs. To the best of our knowledge, we are the first to consider integrating global and local context aggregation in graph-to-text generation, and the first to propose a unified GAT structure for combining global and local node contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early efforts for graph-to-text generation employ statistical methods <ref type="bibr" target="#b10">(Flanigan et al., 2016;</ref><ref type="bibr" target="#b24">Pourdamghani et al., 2016;</ref><ref type="bibr" target="#b28">Song et al., 2017)</ref>. Recently, several neural graph-to-text models have exhibited success by leveraging encoder mechanisms based on LSTMs, GNNs and Transformers.</p><p>AMR-to-Text Generation. Various neural models have been proposed to generate sentences from Abstract Meaning Representation (AMR) graphs. <ref type="bibr" target="#b17">Konstas et al. (2017)</ref> provide the first neural approach for this task, by linearising the input graph as a sequence of nodes and edges.  propose the graph recurrent network (GRN) to directly encode the AMR nodes, whereas <ref type="bibr" target="#b0">Beck et al. (2018)</ref> develop a model based on gated GNNs. However, both approaches only employ local node aggregation strategies. <ref type="bibr" target="#b5">Damonte and Cohen (2019)</ref>   <ref type="bibr" target="#b34">Wang et al. (2020)</ref> propose a local graph encoder based on Transformers using separated attentions for incoming and outgoing neighbors. Recent methods <ref type="bibr" target="#b38">(Zhu et al., 2019;</ref><ref type="bibr" target="#b1">Cai and Lam, 2020)</ref> also employ Transformers, but learn globalized node representations, modeling graph paths in order to capture structural relations.</p><p>KG-to-Text Generation. In this work, we focus on generating text from KGs. In comparison to AMRs, which are rooted and connected graphs, KGs do not have a defined topology, which may vary widely among different datasets, making the generation process more demanding. KGs are sparse structures that potentially contain a large number of relations. Moreover, we are typically interested in generating multi-sentence texts from KGs, which involves solving document planning issues <ref type="bibr" target="#b18">(Konstas and Lapata, 2013)</ref>.</p><p>Recent neural approaches for KG-to-text generation simply linearise the KG triples thereby loosing graph structure information. For instance, <ref type="bibr" target="#b4">Colin and Gardent (2018)</ref>, <ref type="bibr" target="#b21">Moryossef et al. (2019)</ref> and Adapt <ref type="bibr" target="#b11">(Gardent et al., 2017)</ref> employ LSTM/GRU to encode WebNLG graphs. <ref type="bibr" target="#b2">Castro Ferreira et al. (2019)</ref> systematically compare pipeline and end-to-end models for text generation from WebNLG graphs. <ref type="bibr" target="#b30">Trisedya et al. (2018)</ref> develop a graph encoder based on LSTMs that captures relationships within and between triples. Previous work has also studied how to explicitly encode the graph structure using GNNs or Transformers. <ref type="bibr" target="#b20">Marcheggiani and Perez Beltrachini (2018)</ref> propose an encoder based on GCNs, which consider explicitly local node contexts, and show superior performance compared to LSTMs. Recently, <ref type="bibr" target="#b16">Koncel-Kedziorski et al. (2019)</ref> propose a Transformer-based approach which computes the node representations by attending over node neighborhoods following a self-attention strategy. In contrast, our models focus on distinct global and local message passing mechanisms, capturing complementary graph contexts.</p><p>Integrating Global Information. There has been recent work that attempts to integrate global context in order to learn better node representations in graph-to-text generation. To this end, existing methods employ an artificial global node for message exchange with the other nodes. This strategy can be regarded as extending the graph structure but using similar message passing mechanisms. In particular, <ref type="bibr" target="#b16">Koncel-Kedziorski et al. (2019)</ref> add a global node to the graph and use its representation to initialize the decoder. Recently, <ref type="bibr" target="#b12">Guo et al. (2019)</ref> and <ref type="bibr" target="#b1">Cai and Lam (2020)</ref> also employ an artificial global node with direct edges to all other nodes to allow global message exchange for AMR-to-text generation. Similarly,  use a global node to a GRN model for sentence representation. Different from the above methods, we consider integrating global and local contexts at the node level, rather than the graph level, by investigating model alternatives rather than graph structure changes. In addition, we integrate GAT and Transformer architectures into a unified global-local model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph-to-Text Model</head><p>This section first describes (i) the graph transformation adopted to create a relational graph from the input (Section 3.1), and (ii) the graph encoders of our framework based on Graph Attention Networks (GAT) <ref type="bibr" target="#b32">(Veličković et al., 2018)</ref>, for dealing with both global (Section 3.3) and local (Section 3.4) node contexts. We adopt GAT because it is closely related to the Transformer architecture <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>, which provides a convenient prototype for modeling global node context. Then, (iii) we proposed strategies to combined the global and local graph encoders (Section 3.5). Finally, (iv) we describe the decoding and training procedures (Section 3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Preparation</head><p>We represent a KG as a multi-relational graph 2 G e = (V e , E e , R) with entity nodes e ∈ V e and labeled edges (e h , r, e t ) ∈ E e , where r ∈ R denotes the relation existing from the entity e h to e t . 3</p><p>Unlike other current approaches <ref type="bibr" target="#b16">(Koncel-Kedziorski et al., 2019;</ref><ref type="bibr" target="#b21">Moryossef et al., 2019)</ref>, we represent an entity as a set of nodes. For instance, the KG node "node embedding" in <ref type="figure" target="#fig_1">Figure 1</ref> will be represented by two nodes, one for the token "node" and the other for the token "embedding". Formally, we transform each G e into a new graph G = (V, E, R), where each token of an entity e ∈ V e becomes a node v ∈ V. We convert each edge (e h , r, e t ) ∈ E e into a set of edges (with the same relation r) and connect every token of e h to every token of e t . That is, an edge (u, r, v) will belong to E if and only if there exists an edge (e h , r, e t ) ∈ E e such that u ∈ e h and v ∈ e t , where e h and e t are seen as sets of tokens. We represent each node v ∈ V with an embedding h 0 v ∈ R dv , generated from its corresponding token.</p><p>The new graph G increases the representational power of the models because it allows learning node embeddings at a token level, instead of entity level. This is particularly important for text generation as it permits the model to be more flexible, capturing richer relationships between entity tokens. This also allows the model to learn relations and attention functions between source and target tokens. However, it has the side effect of removing the natural sequential order of multi-word entities. To preserve this information, we employ position embeddings <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>, i.e., h 0 v becomes the sum of the corresponding token embedding and the positional embedding for v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Neural Networks (GNN)</head><p>Multi-layer GNNs work by iteratively learning a representation vector h v of a node v based on both its context node neighbors and edge features, through an information propagation scheme. More formally, the l-th layer aggregates the representations of v's context nodes:</p><formula xml:id="formula_0">h (l) N (v) = AGGR (l) h (l−1) u , r uv : u ∈ N (v) ,</formula><p>where AGGR (l) (.) is an aggregation function, shared by all nodes on the l-th layer. r uv represents the relation between u and v. N (v) is a set of context nodes for v. In most GNNs, the context nodes are those adjacent to v. h</p><formula xml:id="formula_1">(l) N (v) is the ag- gregated context representation of N (v) at layer l. h (l) N (v) is used to update the representation of v: h (l) v = COMBINE (l) h (l−1) v , h (l) N (v) .</formula><p>After L iterations, a node's representation encodes the structural information within its Lhop neighborhood. The choices of AGGR (l) (.) and COMBINE (l) (.) differ by the specific GNN model. An example of AGGR (l) (.) is the sum of the representations of N (v). An example of COMBINE (l) (.) is a concatenation after the feature transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Global Graph Encoder</head><p>A global graph encoder aggregates the global context for updating each node based on all nodes of the graph (see <ref type="figure" target="#fig_1">Figure 1c</ref>). We use the attention mechanism as the message passing scheme, extending the self-attention network structure of Transformer to a GAT structure. In particular, we compute a layer of the global convolution for a node v ∈ V, which takes the input feature representations h v as input, adopting AGGR (l) (.) as:</p><formula xml:id="formula_2">h N (v) = u∈V α vu W g h u ,<label>(1)</label></formula><p>where W g ∈ R dv×dz is a model parameter. The attention weight α vu is calculated as:</p><formula xml:id="formula_3">α vu = exp(e vu ) k∈V exp(e vk ) ,<label>(2)</label></formula><p>where,</p><formula xml:id="formula_4">e vu = W q h v W k h u /d z<label>(3)</label></formula><p>is the attention function which measures the global importance of node u's features to node v. W q , W k ∈ R dv×dz are model parameters and d z is a scaling factor. To capture distinct relations between nodes, K independent global convolutions are calculated and concatenated:</p><formula xml:id="formula_5">h N (v) = K k=1 h (k) N (v) .<label>(4)</label></formula><p>Finally, we define COMBINE (l) (.) employing layer normalization (LayerNorm) and a fully connected feed-forward network (FFN), in a similar way as the transformer architecture:</p><formula xml:id="formula_6">h v = LayerNorm(ĥ N (v) + h v ) ,<label>(5)</label></formula><formula xml:id="formula_7">h global v = FFN(ĥ v ) +ĥ N (v) + h v .<label>(6)</label></formula><p>Note that the global encoder creates an artificial complete graph with O(n 2 ) edges and does not consider the edge relations. In particular, if the labelled edges were considered, the self-attention space complexity would increase to Θ(|R| n 2 ).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Local Graph Encoder</head><p>The representation h global v captures macro relationships from v to all other nodes in the graph. However, this representation lacks both structural information regarding the local neighborhood of v and the graph topology. Also, it does not capture labelled edges (relations) between nodes (see Equations 1 and 3). In order to capture these crucial graph properties and impose a strong relational inductive bias, we build a graph encoder to aggregate the local context by employing a modified version of GAT augmented with relational weights. In particular, we compute a layer of the local convolution for a node v ∈ V, adopting AGGR (l) (.) as:</p><formula xml:id="formula_8">h N (v) = u ∈ N (v) α vu W r h u ,<label>(7)</label></formula><p>where W r ∈ R dv×dz encodes the relation r ∈ R between u and v. N (v) is a set of nodes adjacent to v and v itself. The attention coefficient α vu is computed as:</p><formula xml:id="formula_9">α vu = exp(e vu ) k ∈ N (v) exp(e vk ) ,<label>(8)</label></formula><p>where,</p><formula xml:id="formula_10">e vu = σ a [W v h v W r h u ]<label>(9)</label></formula><p>is the attention function which calculates the local importance of adjacent nodes, considering the edge labels. σ is an activation function, denotes concatenation and W v ∈ R dv×dz and a ∈ R 2dz are model parameters.</p><p>We employ multi-head attentions to learn local relations in different perspectives, as in <ref type="bibr">Equation 4,</ref><ref type="bibr">generatingĥ N (v)</ref> . Finally, we define COMBINE (l) (.) as:</p><formula xml:id="formula_11">h local v = RNN(h v ,ĥ N (v) ) ,<label>(10)</label></formula><p>where we employ as RNN a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>. GRU facilitates information propagation between local layers. This choice is motivated by recent works <ref type="bibr" target="#b36">(Xu et al., 2018;</ref><ref type="bibr" target="#b6">Dehmamy et al., 2019)</ref> that theoretically demonstrate that sharing information between layers helps the structural signals propagate. In a similar direction, AMR-to-text generation models employ LSTMs <ref type="bibr" target="#b28">(Song et al., 2017)</ref> and dense connections <ref type="bibr" target="#b12">(Guo et al., 2019)</ref> between GNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Combining Global and Local Encodings</head><p>Our goal is to implement a graph encoder capable of encoding global and local aspects of the input graph. We hypothesize that these two sources of information are complementary, and a combination of both enriches node representations for text generation. In order to test this hypothesis, we investigate different combined architectures. Intuitively, there are two general methods for integrating two types of representation. The first is to concatenate vectors of global and local contexts, which we call a parallel representation. The second is to form a pipeline, where a global representation is first obtained, which is then used as a input for calculating refined representations based on the local node context. We call this approach a cascaded representation.</p><p>Parallel and cascaded integration can be performed at the model level, considering the global and local graph encoders as two representation learning units disregarding internal structures. However, because our model takes a multi-layer architecture, where each layer makes a level of abstraction in representation, we can alternatively consider integration on the layer level, so that more interaction between global and local contexts may be captured. As a result, we present four architectures for integration, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. All models serve the same purpose, and their relative strengths should be evaluated empirically. Parallel Graph Encoding (PGE). In this setup, we compose global and local graph encoders in a fully parallel structure <ref type="figure" target="#fig_2">(Figure 2a</ref>). Note that each graph encoder can have different numbers of layers and attention heads. The final node representation is the concatenation of the local and global node representations of the last layers of both graph encoders:</p><formula xml:id="formula_12">h global v = GE(h 0 v , {h 0 u : u ∈ V}) h local v = LE(h 0 v , {h 0 u : u ∈ N (v)}) h v = [ h global v h local v ] ,<label>(11)</label></formula><p>where GE and LE denote the global and local graph encoders, respectively. h 0 v is the initial node embedding used in the first layer of both encoders.</p><p>Cascaded Graph Encoding (CGE). We cascade local and global graph encoders as shown in <ref type="figure" target="#fig_2">Figure 2b</ref>. We first compute a globally contextualized node embedding, and then refining it with the local node context. h 0 v is the initial input for the global encoder and h global v is the initial input for the local encoder. In particular, the final node representation is calculated as follows:</p><formula xml:id="formula_13">h global v = GE(h 0 v , {h 0 u : u ∈ V}) h v = LE(h global v ,{h global u : u ∈ N (v)}). (12)</formula><p>Layer-wise Parallel Graph Encoding. To allow fine-grained interaction between the two types of graph contextual information, we also combine the encoders in a layer-wise (LW) fashion. As shown in <ref type="figure" target="#fig_2">Figure 2c</ref>, for each graph layer, we employ both global and local encoders in a parallel structure (PGE-LW). More precisely, each encoder layer is calculates as follows:</p><formula xml:id="formula_14">h global v = GE l (h l−1 v , {h l−1 u : u ∈ V}) h local v = LE l (h l−1 v , {h l−1 u : u ∈ N (v)}) h l v = [ h global v h local v ] ,<label>(13)</label></formula><p>where GE l and LE l refer to the l-th layers of the global and local graph encoders, respectively.</p><p>Layer-wise Cascaded Graph Encoding. We also propose cascading the graph encoders layerwise (CGE-LW, <ref type="figure" target="#fig_2">Figure 2d</ref>). In particular, we compute each encoder layer as follows:</p><formula xml:id="formula_15">h global v = GE l (h l−1 v , {h l−1 u : u ∈ V}) h l v = LE l (h global v ,{h global u : u ∈ N (v)}). (14)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Decoder and Training</head><p>Our decoder follows the core architecture of a Transformer decoder <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>. Each time step t is updated by performing multi-head attentions over the output of the encoder (node embeddings h v ) and over previously-generated tokens (token embeddings). An additional challenge in our setup is to generate multi-sentence outputs.</p><p>In order to encourage the model to generate longer texts, we employ a length penalty <ref type="bibr" target="#b35">(Wu et al., 2016)</ref> to refine the pure max-probability beam search. The model is trained to optimize the negative log-likelihood of each gold-standard output text. We employ label smoothing regularization to prevent the model from predicting the tokens too confidently during training and generalizing poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data and Preprocessing</head><p>We attest the effectiveness of our models on two datasets: AGENDA <ref type="bibr" target="#b16">(Koncel-Kedziorski et al., 2019)</ref> and WebNLG <ref type="bibr" target="#b11">(Gardent et al., 2017)</ref>. <ref type="table" target="#tab_2">Table  1</ref> shows the statistics for both datasets.</p><p>AGENDA. In this dataset, KGs are paired with scientific abstracts extracted from proceedings of 12 top AI conferences. Each instance consists of the paper title, a KG and the paper abstract. Entities correspond to scientific terms which are often multi-word expressions (co-referential entities are merged). We treat each token in the title as a node, creating a unique graph with title and KG tokens as nodes. As shown in <ref type="table" target="#tab_2">Table 1</ref>, the average output length is considerably large, as the target outputs are multi-sentence abstracts.</p><p>WebNLG. In this dataset, each instance contains a KG extracted from DBPedia. The target text consists of sentences that verbalise the graph.  We evaluate the models on the test set with seen categories. Note that this dataset has a considerable number of edge relations (see <ref type="table" target="#tab_2">Table 1</ref>). In order to avoid parameter explosion, we use regularization based on the basis function decomposition to define the model relation weights <ref type="bibr" target="#b26">(Schlichtkrull et al., 2018)</ref>. Also, as an alternative, we employ the Levi Transformation to create nodes from relational edges between entities <ref type="bibr" target="#b0">(Beck et al., 2018)</ref>. That is, we create a new relation node for each edge relation between two nodes. The new relation node is connected to the subject and object token entities by two binary relations, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We implemented all our models using PyTorch Geometric (PyG) <ref type="bibr" target="#b9">(Fey and Lenssen, 2019)</ref> and</p><p>OpenNMT-py <ref type="bibr" target="#b15">(Klein et al., 2017)</ref>. We employ the Adam optimizer with β 1 = 0.9 and β 2 = 0.98. Our learning rate schedule follows <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref> with 8000 and 16000 warming-up steps for WebNLG and AGENDA, respectively. The vocabulary is shared between the node and target tokens. In order to mitigate the effects of random seeds, for the test sets, we report the averages over 4 training runs along with their standard deviation. We employ byte pair encoding (BPE, Sennrich et al., 2016) to split entity words into smaller more frequent pieces. So some nodes in the graph can be sub-words. We also obtain sub-words on the target side. Following previous works, we evaluate the results with BLEU <ref type="bibr" target="#b22">(Papineni et al., 2002)</ref>, METEOR <ref type="bibr" target="#b7">(Denkowski and Lavie, 2014)</ref> and CHRF++ <ref type="bibr" target="#b23">(Popović, 2015)</ref> automatic metrics and also perform a human evaluation (Section 5.6). For layer-wise models, the number of encoder layers are chosen from {2, 4, 6}, and for PGE and CGE, the global and local layers are chosen from and {2, 4, 6} and {1, 2, 3}, respectively.</p><p>The hidden encoder dimensions are chosen from {256, 384, 448} (see <ref type="figure" target="#fig_3">Figure 3</ref>). Hyperparameters are tuned on the development set of both datasets. We report the test results when the BLEU score on dev set is optimal. <ref type="table" target="#tab_4">Table 2</ref> shows the results, where we report the number of layers and attention heads employed. We train models with only global or local encoders as baselines. Each model has the respective parameter size that gives the best results on the dev set. First, the local encoder, which requires fewer encoder layers and parameters, has a better performance compared to the global encoder. This shows that explicitly encoding the graph structure is important to improve the node representations. Second, our approaches substantially outperform both baselines. CGE-LW outperforms Koncel-Kedziorski et al. <ref type="formula" target="#formula_2">(2019)</ref>, a transformer model that focuses on the relations between adjacent nodes, by a large margin, achieving the new state-of-the-art BLEU score of 18.01, 25.9% higher. We also note that KGs are highly incomplete in this dataset, with an average number of connected components of 19.1 (see <ref type="table" target="#tab_2">Table 1</ref>). For this reason, the global encoder plays an important role in our models as it enables learning node representations based on all connected components. The results indicate that combining the local node context, leveraging the graph topology, and the global node context, capturing macro-level node relations, leads to better performance. We find that, even though CGE has a small number of parameters compared to CGE-LW, it achieves comparable performance. PGE-LW has the worse performance among the proposed models. Finally note that cascaded architectures are more effective according to different metrics. Model BLEU METEOR CHRF++ #P UPF-FORGe <ref type="bibr" target="#b11">(Gardent et al., 2017)</ref> 40.88 40.00 --Melbourne <ref type="bibr" target="#b11">(Gardent et al., 2017)</ref> 54.52 41.00 70.72 -Adapt <ref type="bibr" target="#b11">(Gardent et al., 2017)</ref> 60  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on AGENDA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on WebNLG</head><p>We compare the performance of our more effective models (CGE, CGE-LW) with six state-of-theart results reported on this dataset. Three systems are the best competitors in the WebNLG challenge for seen categories: UPF-FORGe, Melbourne and Adapt. UPF-FORGe follows a rule-based approach, whereas the others use neural encoderdecoder models with linearized triple sets as input. <ref type="table" target="#tab_6">Table 3</ref> presents the results. CGE achieves a BLEU score of 62.30, 8.9% better than the best model of Castro <ref type="bibr" target="#b2">Ferreira et al. (2019)</ref>, who employ an end-to-end architecture based on GRUs. CGE using Levi graphs outperforms <ref type="bibr" target="#b30">Trisedya et al. (2018)</ref>, an approach that encodes both intratriple and inter-triple relationships, by 4.5 BLEU points. Interestingly, their intra-triple and intertriple mechanisms are closely related with the local and global encodings. However, they rely on encoding entities based on sequences generated by traversal graph algorithms, whereas we explicitly exploit the graph structure, throughout the local neighborhood aggregation.</p><p>CGE-LW with Levi graphs as inputs has the best performance, achieving 63.69 BLEU points, even thought it uses fewer parameters. Note that this approach allows the model to handle new relations, as they are treated as nodes. Moreover, the rela-tions become part of the shared vocabulary, making this information directly usable during the decoding phase. We outperform an approach based on GNNs <ref type="bibr" target="#b20">(Marcheggiani and Perez Beltrachini, 2018)</ref> by a large margin of 7.7 BLEU points, showing that our combined graph encoding strategies lead to better text generation. We also outperform Adapt, a strong competitor that employs subword encodings, by 3.1 BLEU points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Development Experiments</head><p>We report several development experiments in <ref type="figure" target="#fig_3">Figure 3</ref>. <ref type="figure" target="#fig_3">Figure 3a</ref> shows the effect of the number of encoder layers in the four encoding methods. <ref type="bibr">4</ref> In general, the performance increases when we gradually enlarge the number of layers, achieving the best performance with 6 encoder layers. <ref type="figure" target="#fig_3">Figure 3b</ref> shows the choices of hidden sizes for the encoders. The best performances for global and PGE are achieved with 384 dimensions, whereas the other models have the better performance with 448 dimensions. In <ref type="figure" target="#fig_3">Figure 3c</ref> lel encoders obtain better results than the cascaded ones. When the models are larger, cascaded models perform better. We speculate that for some models, the performance can be further improved with more parameters and layers. However, we do not attempt this owing to hardware limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>In <ref type="table" target="#tab_8">Table 4</ref>, we report an ablation study on the impact of each module used in CGE model on the dev set of AGENDA. We also report the number of parameters used in each configuration.</p><p>Global Graph Encoder. We start by an ablation on the global encoder. After removing the global attention coefficients, the performance of the model drops by 1.79 BLEU and 1.97 CHRF++ scores. Results also show that using FFN in the global COMBINE(.) function is important to the model but less effective than the global attention. However, when we remove FNN, the number of parameters drops considerably (around 18%) from 61.5 to 50.4 million. Finally, without the entire global encoder, the result drops substantially by 2.21 BLEU points. This indicates that enriching node embeddings with a global context allows learning more expressive graph representations.</p><p>Local Graph Encoder. We first remove the local graph attention and the BLEU score drops to 16.92, showing that the neighborhood attention improves the performance. After removing the relation types, encoded as model weights, the performance drops by 0.5 BLEU points. However, the number of parameters is reduced by around 7.9 million. This indicates that we can have a more efficient model, in terms of the number of parameters, with a slight drop in performance. Removing the GRU used on the COMBINE(.) function decreases the performance considerably. The worse performance occurs if we remove the entire local  Finally, we find that vocabulary sharing improves the performance, and the length penalty is beneficial as we generate multi-sentence outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Impact of the Graph Structure and Output Length</head><p>The overall performance on both datasets suggests the strength of combining global and local node representations. However, we are also interested in estimating the models' performance concerning different data properties.</p><p>Graph Size. <ref type="figure" target="#fig_4">Figure 4a</ref> shows the effect of the graph size, measured in number of nodes, on the performance, measured using CHRF++ scores, 6 for the AGENDA. We evaluate global and local graph encoders, PGE-LW and CGE-LW. We find that the score increases as the graph size increases. Interesting, the gap between the local and global encoders increases when the graph size increases. This suggests that, because larger graphs may have very different topologies, modeling the relations  between nodes based on the graph structure is more beneficial than allowing direct communication between nodes, overlooking the graph structure. Also note that the the cascaded model (CGE-LW) is consistently better than the parallel model (PGE-LW) over all graph sizes. <ref type="table" target="#tab_10">Table 5</ref> shows the effect of the graph size, measured in number of triples, on the performance for the WebNLG. Our model obtains better scores over all partitions. In contrast to AGENDA, the performance decreases as the graph size increases. This behavior highlights a crucial difference between AGENDA and AMR and WebNLG datasets, in which the models' general performance decreases as the graph size increases <ref type="bibr" target="#b11">(Gardent et al., 2017;</ref><ref type="bibr" target="#b1">Cai and Lam, 2020)</ref>. In WebNLG, the graph and sentence sizes are correlated, and longer sentences are more challenging to generate than the smaller ones. Differently, AGENDA contains similar text lengths 7 and when the input is a larger graph, the model has more information to be leveraged during the generation.</p><p>Graph Diameter. <ref type="figure" target="#fig_4">Figure 4b</ref> shows the impact of the graph diameter 8 on the performance for the AGENDA. Similarly to the graph size, the score increases as the diameter increases. As the global encoder is not aware of the graph structure, this module has the worst scores, even though it en-7 As shown on <ref type="figure" target="#fig_4">Figure 4c</ref>, 82% of the reference abstracts have more than 100 words. <ref type="bibr">8</ref> The diameter of a graph is defined as the length of the longest shortest path between two nodes. We convert the graphs into undirected graphs to calculate the diameters. ables direct node communication over long distance. In contrast, the local encoder can propagate precise node information throughout the graph structure for k-hop distances, making the relative performance better. <ref type="table" target="#tab_10">Table 5</ref> shows the models' performances with respect to the graph diameter for WebNLG. Similarly to the graph size, the score decreases as the diameter increases.</p><p>Output Length. One interesting phenomenon to analyze is the length distribution (in number of words) of the generated outputs. We expect that our models generate texts with similar output lengths as the reference texts. As shown in <ref type="figure" target="#fig_4">Figure 4c</ref>, the references usually are bigger than the texts generated by all models for AGENDA. The texts generated by CGE-no-pl, a CGE model without length penalty, are consistently shorter than the texts from the global and CGE models. We increase the length of the texts when we employ the length penalty (see Section 3.6). However, there is still a gap between the reference and the generated text lengths. We leave further investigation of this aspect for future work. <ref type="table" target="#tab_10">Table 5</ref> shows the models' performances with respect to the number of sentences for WebNLG. In general, increasing the number of sentences reduces the performance of all models. Note that when the number of sentences increases, the gap between CGE-LW and the baselines becomes larger. This suggests that our approach is able to better handle complex graph inputs in order to generate multi-sentence texts.</p><p>Effect of the Number of Nodes on the Output Length. <ref type="figure" target="#fig_5">Figure 5</ref> shows the effect of the size of a graph, defined as the number of nodes, on the  , indicating that our model is more effective in capturing semantic signals from graphs with scarce information. Our approach also performs better when the graph size is large (&gt; 55) but the generation output is small (≤ 75), beating the global encoder by 9 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Human Evaluation</head><p>To further assess the quality of the generated text, we conduct a human evaluation on the WebNLG dataset. 9 Following previous works <ref type="bibr" target="#b11">(Gardent et al., 2017;</ref><ref type="bibr" target="#b2">Castro Ferreira et al., 2019)</ref>, we assess two quality criteria: (i) Fluency (i.e., does the text flow in a natural, easy to read manner?) and (ii) Adequacy (i.e., does the text clearly express the data?). We divide the datapoints into seven different sets by the number of triples. For each set, we randomly select 20 texts generated by Adapt, CGE with Levi graphs and their corresponding human reference (420 texts in total). Since the number of datapoints for each set is not balanced (see <ref type="table" target="#tab_10">Table 5</ref>), this sampling strategy assures us to have the same amount of samples for 9 Because AGENDA is scientific in nature, we choose to crowd source human evaluations only for WebNLG.  <ref type="table">Table 7</ref>: Fluency (F) and Adequacy (A) obtained in the human evaluation. #T refers to the number of input triples and #D to graph diameters. The ranking was determined by pair-wise Mann-Whitney tests with p &lt; 0.05, and the difference between systems which have a letter in common is not statistically significant.</p><p>the different triple sets. Moreover, having human references may serve as an indicator of the sanity of the human evaluation experiment. We recruited human workers from Amazon Mechanical Turk to rate the text outputs on a 1-5 Likert scale. For each text, we collect scores from 4 workers and average them. <ref type="table">Table 7</ref> shows the results. We first note a similar trend as in the automatic evaluation, with CGE outperforming Adapt on both fluency and adequacy. In sets with the number of triples smaller than 5, CGE was the highest rated system in fluency. Similarly to the automatic evaluation, both systems are better in generating text from graphs with smaller diameters. Note that bigger diameters pose difficulties to the models, which achieve their worst performance for diameters ≥ 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Additional Experiments</head><p>Impact of the Vocabulary Sharing and Length Penalty. During the ablation studies, we note that the vocabulary sharing and length penalty are beneficial for the performance. To better estimate their impact, we evaluate CGE-LW model with its variations without employing vocabulary sharing, length penalty and without both mechanisms, on the test set of both datasets. <ref type="table" target="#tab_12">Table 6</ref> shows the results. We observe that sharing vocabulary is more important to WebNLG than AGENDA. This suggests that sharing vocabulary is beneficial when the training data is small, as in WebNLG. On the other hand, length penalty is more effective for AGENDA, as it has longer texts than WebNLG 10 , improving the BLEU score by 0.71 points.  Attention Distance <ref type="figure">Figure 7</ref>: The average distance between nodes for the maximum attention for each head. ∞ indicates no path between two nodes, that is, they belong to distinct connected components.</p><p>How Far Does the Global Attention Look At. Following previous works <ref type="bibr" target="#b33">(Voita et al., 2019;</ref><ref type="bibr" target="#b1">Cai and Lam, 2020)</ref>, we investigate the attention distribution of each graph encoder global layer of CGE-LW on the AGENDA dev set. In particular, for each node, we verify its global neighbor that receives the maximum attention weight and record the distance between them. 11 <ref type="figure">Figure 7</ref> shows the averaged distances for each global layer. We observe that the global encoder mainly focuses on distant nodes, instead of the neighbours and closest nodes. This is very interesting and agrees with our intuition: whereas the local encoder is concerned about the local neighborhood, the global encoder is focusing on the information from longdistance nodes.</p><p>Case Study. <ref type="figure" target="#fig_7">Figure 6</ref> shows examples of generated texts when the WebNLG graph is complex (7 triples). While CGE generates a factually correct text (it correctly verbalises all triples), the Adapt's output is repetitive. The example also illustrates how the text generated by CGE closely follows the graph structure whereby the first sentence verbalises the right-most subgraph, the sec- <ref type="bibr">11</ref> The distance between two nodes is defined as the number of edges in a shortest path connecting them. ond the left-most one and the linking node Turkey makes the transition (using hyperonymy and a definite description, i.e., The country). The text created by CGE is also more coherent than the reference. As noted above, the input graph includes two subgraphs linked by Turkey. In natural language, such a meaning representation corresponds to a topic shift with the first part of the text describing an entity from one subgraph, the second part an entity from the other subgraph, and the linking entity (Turkey) marking the topic shift. Typically, in English, a topic shift is marked by a definite noun phrase in the subject position. While this is precisely the discourse structure generated by CGE (Turkey is realised in the second sentence by the definite description The country in subject position), the reference fails to mark the topic shift, resulting in a text with weaker discourse coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we introduced a unified graph attention network structure for investigating graph-totext models that combine global and local graph encoders in order to improve text generation. An extensive evaluation of our models demonstrated that the global and local contexts are empirically complementary, and a combination can achieve state-of-the-art results on two datasets. In addition, cascaded architectures give better results compared to parallel ones.</p><p>We point out some directions for future work. First, it is interesting to study different fusion strategies to assemble the global and local encodings; Second, a promising direction is incorporating pre-trained contextualized word embeddings in graphs; Third, as discussed in Section 5.5, it is worth studying ways to diminish the gap between the reference and the generated text lengths.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>A graphical representation (a) of a scientific text (b). (c) A global encoder directly captures longer dependencies between any pair of nodes (blue and red arrows), but fails in capturing the graph structure. (d) A local encoder explicitly accesses information from the adjacent nodes (blue arrows) and implicitly captures distant information (dashed red arrows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed encoder architectures. (a) Parallel Graph Encoder (PGE) with separated parallel global and local node encoders. (b) Cascaded Graph Encoder (CGE) with separated cascaded encoders. c) PGE-LW: global and local node representations are concatenated layer-wise. d) CGE-LW: Both node representations are cascaded layer-wise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>BLEU scores for AGENDA dev set, with respect to (a) the encoder layers, (b) the encoder hidden dimensions and (c) the number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>, we evaluate the performance employing different number of parameters. 5 When the models are smaller, paral-CHRF++ scores for AGENDA test set, with respect to (a) the number of nodes, and (b) the graph diameter. (c) Distribution of length of the gold references and models' outputs for the AGENDA test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Relation between the number of nodes and the length of the generated text, in number of words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Ahmet Davutoglu is the president of Turkey. The capital city is Ankara, but it is in Izmir that the bronze Ataturk monument designed by Pietro Canonica and inaugurated on 27 july 1932 is located. The monument was designed in bronze by Pietro Canonica and inaugurated on 27 July 1932. President Ahmet Davutoglu is the leader of Turkey where the capital city is Ankara. The country is the location of the bronze Ataturk monument designed by Pietro Canonica and inaugurated on 27 July 1932 in Izmir.d) Reference: President Ahmet Davutoglu is the Turkish leaderwhere the capital city is Ankara. The bronze Ataturk Monument which was designed by Pietro Canonica is located in Izmir and was inaugurated on 27 July 1932.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>(a) A WebNLG input graph and the outputs for (b) Adapt and (c) CGE. The color face indicates repetition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>combine graph convolutional networks (GCN) and LSTMs in order to learn complementary node contexts. However, differently from Transformers and GNNs, LSTMs generate node representations that are influenced by the node order. Ribeiro et al. (2019) develop a model based on different GNNs which learns node representations which simultaneously encode a top-down and a bottom-up views of the AMR graphs, whereas Guo et al. (2019) leverage dense connectivity in GNNs. Recently,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>#train #dev #test #relations avg #entities avg #nodes avg #edges avg #CC avg length Data statistics. Nodes, edges and CC values are calculated after the graph transformation. The average values are calculated for all splits(training, dev and test sets). CC refers to the number of connected components.</figDesc><table><row><cell>AGENDA 38,720 1,000 1,000</cell><cell>7</cell><cell>12.4</cell><cell>44.3</cell><cell>68.6</cell><cell>19.1</cell><cell>140.3</cell></row><row><cell>WebNLG 18,102 872 971</cell><cell>373</cell><cell>4.0</cell><cell>34.9</cell><cell>101.0</cell><cell>1.5</cell><cell>24.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>±0.25 20.76 ±0.19 43.95 ±0.40 54.4 ±0.19 21.12 ±0.32 44.70 ±0.29 54.0 PGE 6, 3 8, 8 17.55 ±0.15 22.02 ±0.07 46.41 ±0.07 56.1 CGE 6, 3 8, 8 17.82 ±0.13 22.23 ±0.09 46.47 ±0.10 61.5 18.01 ±0.14 22.34 ±0.07 46.69 ±0.17 69.8</figDesc><table><row><cell>Model</cell><cell cols="2">#L #H</cell><cell>BLEU</cell><cell>METEOR</cell><cell>CHRF++</cell><cell>#P</cell></row><row><cell>Koncel-Kedziorski et al. (2019)</cell><cell>6</cell><cell>8</cell><cell cols="2">14.30 ±1.01 18.80 ±0.28</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">Global Encoder 15.44 Local Encoder 6 8 3 8 16.03 PGE-LW 6 8, 8 17.42 ±0.25 21.78 ±0.20 45.79 ±0.32 69.0</cell></row><row><cell>CGE-LW</cell><cell>6</cell><cell>8, 8</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results on AGENDA test set. #L and #H are the numbers of layers and the attention heads in each layer, respectively. When more than one, the values are for the global and local encoders, respectively. #P stands for the number of parameters in millions (node embeddings included).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>±0.27 43.51 ±0.18 75.49 ±0.34 13.9 CGE (Levi Graph) 63.10 ±0.13 44.11 ±0.09 76.33 ±0.10 12.8 CGE-LW 62.85 ±0.07 43.75 ±0.21 75.73 ±0.31 11.2 CGE-LW (Levi Graph) 63.69 ±0.10 44.47 ±0.12 76.66 ±0.10 10.4</figDesc><table><row><cell></cell><cell>.59</cell><cell>44.00</cell><cell>76.01</cell><cell>-</cell></row><row><cell cols="2">Marcheggiani and Perez Beltrachini (2018) 55.90</cell><cell>39.00</cell><cell>-</cell><cell>4.9</cell></row><row><cell>Trisedya et al. (2018)</cell><cell>58.60</cell><cell>40.60</cell><cell>-</cell><cell>-</cell></row><row><cell>Castro Ferreira et al. (2019)</cell><cell>57.20</cell><cell>41.00</cell><cell>-</cell><cell>-</cell></row><row><cell>CGE</cell><cell>62.30</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Results on WebNLG test set with seen categories.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for modules used in the encoder and decoder of the CGE model. encoder, with a BLEU score of 14.68, essentially making the encoder similar to the global baseline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: CHRF++ scores with respect to the number</cell></row><row><cell>of triples (#T), graph diameters (#D) and number of</cell></row><row><cell>sentences (#S) on the WebNLG test set. #DP refers to</cell></row><row><cell>the number of datapoints.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Effects of the vocabulary sharing and length penalty on the test sets of AGENDA and WebNLG.</figDesc><table><row><cell>quality (measured in CHRF++ scores) and length</cell></row><row><cell>of the generated text (in number of words) in the</cell></row><row><cell>AGENDA dev set. We bin both the graph size and</cell></row><row><cell>the output length in 4 classes. CGE consistently</cell></row><row><cell>outperforms the global model, in some cases by a</cell></row><row><cell>large margin. When handling smaller graphs (with</cell></row><row><cell>≤ 35 nodes), both models have difficulties gener-</cell></row><row><cell>ating good summaries. However, for these smaller</cell></row><row><cell>graphs, our model achieves a score 12.2% better</cell></row><row><cell>when generating texts with length ≤ 75. Interest-</cell></row><row><cell>ingly, when generating longer texts (&gt;140) from</cell></row><row><cell>smaller graphs, our model outperforms the global</cell></row><row><cell>encoder by an impressive 21.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>All 3.96 C 4.44 C 4.12 B 4.54 B 4.24 A 4.63 A 1-2 3.94 C 4.59 B 4.18 B 4.72 A 4.30 A 4.69 A 3-4 3.79 C 4.45 B 3.96 B 4.50 AB 4.14 A 4.66 A 5-7 4.08 B 4.35 B 4.18 B 4.45 B 4.28 A 4.59 A 3.98 C 4.50 B 4.16 B 4.61 A 4.28 A 4.66 A ≥ 3 3.91 C 4.33 B 4.03 B 4.43 B 4.17 A 4.60 A</figDesc><table><row><cell>#T</cell><cell cols="2">Adapt</cell><cell>CGE</cell><cell></cell><cell cols="2">Reference</cell></row><row><cell></cell><cell>F</cell><cell>A</cell><cell>F</cell><cell>A</cell><cell>F</cell><cell>A</cell></row><row><cell>#D</cell><cell>Adapt</cell><cell></cell><cell>CGE</cell><cell></cell><cell cols="2">Reference</cell></row><row><cell></cell><cell>F</cell><cell>A</cell><cell>F</cell><cell>A</cell><cell>F</cell><cell>A</cell></row><row><cell>1-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In this paper, multi-relational graphs refer to directed graphs with labelled edges.3 R contains relations both in canonical direction (e.g. used-for) and in inverse direction (e.g. used-for-inv), so that the models consider the differences in the incoming and outgoing relations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For CGE and PGE the values refer to the global layers and the number of local layers is fixed to 3. 5 It was not possible to execute the local model with larger number of parameters due to memory limitations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">CHRF++ score is used as it is a sentence-level metric.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">As shown inTable 1, AGENDA has texts 5.8 times longer than WebNLG on average.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Pedro Savarese, Markus Zopf, Mohsen Mesgar, Prasetya Ajie Utama, Ji-Ung Lee and Kevin Stowe for their feedback on this work, as well as the anonymous reviewers for detailed comments that improved this paper. This work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph-to-sequence learning using gated graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="273" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph transformer for graph-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural data-to-text generation: A comparison between pipeline and end-to-end architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Thiago Castro Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Emiel Van Miltenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1052</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="552" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating syntactic paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Colin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1113</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="937" to="943" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structural neural encoders for AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1366</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3649" to="3658" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the representation power of graph neural networks in learning graph topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Dehmamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-Laszlo</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15387" to="15397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-3348</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Xin Luna Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<editor>Evgeniy Gabrilovich Wilko Horn Ni Lao Kevin Murphy Thomas Strohmann Shaohua Sun Wei Zhang Geremy Heitz</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-24" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generation from abstract meaning representation using tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1087</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="731" to="739" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The WebNLG challenge: Generating text from RDF data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected graph convolutional networks for graph-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00269</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="297" to="312" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open-NMT: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Text Generation from Knowledge Graphs with Graph Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhanush</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1238</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2284" to="2293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural amr: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inducing document plans for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1503" to="1514" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep graph convolutional encoders for structured data to text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">Perez</forename><surname>Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation<address><addrLine>Tilburg University, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Step-by-step: Separating planning from realization in neural data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Moryossef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1236</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2267" to="2277" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">chrF: character n-gram fscore for automatic MT evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popović</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-3049</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="392" to="395" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating English from abstract meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Pourdamghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-6603</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Natural Language Generation conference</title>
		<meeting>the 9th International Natural Language Generation conference<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhancing AMR-to-text generation with dual graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1314</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3181" to="3192" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93417-4_38</idno>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-03" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">AMRto-text generation with synchronous node replacement grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7" to="13" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A graph-to-sequence model for AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1616" to="1626" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GTR-LSTM: A triple encoder for sentence generation from RDF data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Bayu Distiawan Trisedya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1151</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1627" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1580</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5797" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Amr-to-text generation with graph transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00297</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="19" to="33" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Ichi Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sentence-state LSTM for text representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="317" to="327" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling graph structure in transformer for better AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1548</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5458" to="5467" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
