<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analysis of Convolutional Neural Networks for Document Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tensmeyer</surname></persName>
							<email>tensmeyer@byu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Brigham Young University</orgName>
								<address>
									<settlement>Provo</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Martinez</surname></persName>
							<email>martinez@cs.byu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Brigham Young University</orgName>
								<address>
									<settlement>Provo</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Analysis of Convolutional Neural Networks for Document Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Document Image Classification</term>
					<term>Convolutional Neural Networks</term>
					<term>Deep Learning</term>
					<term>Preprocessing</term>
					<term>Data Aug- mentation</term>
					<term>Network Architecture</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNNs) are stateof-the-art models for document image classification tasks. However, many of these approaches rely on parameters and architectures designed for classifying natural images, which differ from document images. We question whether this is appropriate and conduct a large empirical study to find what aspects of CNNs most affect performance on document images. Among other results, we exceed the state-of-the-art on the RVL-CDIP dataset by using shear transform data augmentation and an architecture designed for a larger input image. Additionally, we analyze the learned features and find evidence that CNNs trained on RVL-CDIP learn region-specific layout features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, Convolutional Neural Networks (CNNs) have been proposed for document image classification and have enjoyed great success. These impressive results include:</p><p>• 100% accuracy on NIST tax forms dataset <ref type="bibr" target="#b0">[1]</ref> using only one training sample per class <ref type="bibr" target="#b1">[2]</ref>. • 89.8% accuracy on the RVL-CDIP genre classification dataset. The Bag of Words baseline was 49.3% <ref type="bibr" target="#b2">[3]</ref>. • 96.5% accuracy in fine-grained classification of identity documents compared to 92.7% accuracy using Histogram of Oriented Gradients (HOG) features <ref type="bibr" target="#b3">[4]</ref>. However, many such applications of CNNs to whole document image classification <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b3">[4]</ref> use a CNN pre-trained on the ImageNet dataset of natural images <ref type="bibr" target="#b5">[6]</ref> as a starting point for features. Through this transfer learning approach is effective due to the generality of CNN features <ref type="bibr" target="#b6">[7]</ref>, there exist large domain differences between natural images and document images. For example, in ImageNet, the object of interest can appear in any region of the image in a variety of 3D poses. In contrast, many document images are 2D entities that occupy the whole image. Due to such domain differences, we question whether the same architectures and techniques that are effective for natural images are also optimal for document images.</p><p>To answer this query, we conducted a large empirical study utilizing two large document image datasets. To our knowledge, we are the first to conduct such a study for the domain of document images. We examine many factors that contribute to CNN classification accuracy, which can be broadly categorized as data preprocessing and network architecture. All CNNs in this work are randomly initialized and not pretrained on natural images.</p><p>For data preprocessing, we examine what representation(s) of the image (e.g. binary, RGB, HSV), are most effective as input to the network. We also test 10 different types of label-preserving image transformations (e.g. crop, rotation, shear) to artificially expand the training data. While cropping is typically applied in CNNs trained on ImageNet, we find that shear transforms yield best performance for document image tasks. While not all document images are the same aspect ratio (AR), CNNs typically only accept inputs of a fixed size (e.g. 227x227) and hence a fixed AR. We investigate this issue and find that CNNs trained with stochastic shearing are not adversely affected by AR warping of input images.</p><p>For network architectures, we examine factors such as network depth, width, and input size under various amounts of training data. Critically, we achieve 90.8% accuracy on RVL-CDIP without pretraining on natural images by using a larger input size This surpasses the previous best result of 89.8% <ref type="bibr" target="#b2">[3]</ref> on this dataset. By incorporating multi-scale images into training and inference, we reach 91.03%. We also examine non-linear network operations in the domain of document images.</p><p>Lastly, we analyze what intermediate features CNNs learn from document image tasks. Though CNNs are often considered black-box models, individual neurons can be characterized by their maximal-exciting inputs. We find evidence that CNNs learn a wide variety of region-specific layout features. Several intermediate neurons fire on page elements of specifics shapes and types (e.g. typed text, handwritten text, graphics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>CNNs have been used in document image analysis for two decades for tasks such as character recognition <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, but only more recently were applied to large image classification tasks. In 2012, Krizhevsky et al. showed the effectiveness of CNNs in large image classification by winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) by a large margin <ref type="bibr" target="#b8">[9]</ref>. Since then, all top entries of ILSVRC have been based on CNNs, rather than on handcrafted features, with the 2015 winner surpassing human level performance by utilizing residual connections and batch normalization in a CNN with 152 layers <ref type="bibr" target="#b11">[12]</ref>.</p><p>In 2014, one of the first applications of CNNs to whole document image classification used a 4-layer CNN to classify tax forms and the Small Tobacco datasets <ref type="bibr" target="#b1">[2]</ref>. This CNN acheived 65.37% accuracy on Small Tobacco compared to ∼42% by the previous state-of-the-art HVP-RF classifier <ref type="bibr" target="#b12">[13]</ref>, which uses SURF local descriptors.</p><p>The following year, two works explored transferring the parameters of a CNN learned on ILSVRC to document classification tasks. The DeepDocClassifier system <ref type="bibr" target="#b4">[5]</ref> retrained the top classification layer from scratch while finetuning the other layers. They report an accuracy of 77.3% on Small Tobacco. Harley et al. <ref type="bibr" target="#b2">[3]</ref> also transferred parameters from ILSVRC, but also introduced a larger dataset, called RVL-CDIP, that can be used to train CNNs from scratch.They also found that a single holistic CNN outperformed an ensemble of region-specific CNNs on RVL-CDIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CONVOLUTIONAL NEURAL NETWORKS</head><p>The CNNs we consider in this work are models that map input images x ∈ R H×W ×D into the probability vectors y ∈ R C , where D is the input image depth (e.g. 3 for RGB) and C is the number of classes. Each layer, of the CNN performs an affine transformation with learnable parameters followed by non-linear operation(s):</p><formula xml:id="formula_0">x = g (W x −1 + b )<label>(1)</label></formula><p>where 1 ≤ ≤ L is the layer index, x 0 is the input image, W , b are learnable parameters, is either 2D convolution (with multi-channel kernels) for convolution layers or matrix multiplication for fully connected layers, and g is a layer specific non-linearity, composed of ReLU(x) = max(0, x), and optionally max-pooling, local response normalization, or dropout <ref type="bibr" target="#b8">[9]</ref>. The output of the last layer, x L , is input to a softmax function, which outputs a probability vector over the target classes. For more details, consult <ref type="bibr" target="#b8">[9]</ref>. Many of our experiments are based on the standard AlexNet architecture <ref type="bibr" target="#b8">[9]</ref> (5 conv layers, 3 fully connected layers), but without the original sparse connections in the convolutional layers. We also test many architectural variations, which are noted in the relevant experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. TRAINING DETAILS A. Datasets</head><p>For this work, we utilize 2 datasets. The first is the publicly avaialable RVL-CDIP dataset <ref type="bibr" target="#b2">[3]</ref> which is composed of 400,000 grayscale images split into 320,000/40,000/40,000 train/val/test sets. They are scanned office documents with 16 conceptual categories such as Letter, Memo, Email, Form.</p><p>The second dataset, denoted ANDOC, is composed of genealogical records sampled from 974 collections owned by Ancestry.com. The target class of each image is its collection of origin (e.g. 1940 US Census, Pennsylvania death certificates 1880-1940). In total, there are 880,000 images partitioned into a randomized 800,000/40,000/40,000 train/val/test split. 481,000 of these images are color, while the rest are grayscale.</p><p>For data preprocessing, pixel intensities are scaled to [0, 1] and then the channel mean is subtracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training hyperparameters</head><p>We empirically determined good training schemes for each dataset. For RVL-CDIP, CNNs are trained with Stochastic Gradient Descent (SGD) with mini-batches of 32 for 500,000 weight updates. The initial learning rate (LR) is 0.003 and is decayed by a factor of 10 every 150,000 minibatches. For ANDOC, we used SGD with mini-batches of 128 for 250,000 weight updates. The initial LR was 0.005 and is decayed by a factor of 10 every 100,000 mini-batches. We believe the larger mini-batch does better for ANDOC because it has more output classes.</p><p>All networks are trained on the training split and progress is monitored on the validation set. The set of network parameters that performed best on the validation set is then evaluated on the test images. In this work, we use test set accuracy as our evaluation metric.</p><p>CNNs require days to train on high-end GPUs, so training many CNNs for statistical significance testing is often too time-consuming. For this reason, CNN literature almost always reports numbers from only a best single trained model. Thus we typically report average performance of 1-2 CNNs. However, in Section V-A, we trained 10 CNNs for each set of hyperparameters in order to measure the variance in model accuracy, which we estimate to be σ ≈ 0.1 for RVL-CDIP and σ ≈ 0.05 for ANDOC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PREPROCESSING EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Representation</head><p>Here we examine whether the best input representation is RGB, HSV, grayscale (G), binary (B), dense-SURF (S) <ref type="bibr" target="#b13">[14]</ref>, or a combination. For example, when combining RGB and G, we treat the input as a 4-channel image where channels 1-3 are RGB and channel 4 is G. For D-SURF inputs, we compute SURF descriptors at fixed orientation on a 227x227 grid on the originally sized images. This results in a 64dimensional descriptor for each grid point, which we treat as an image with 64 channels. Binary images are computed with Otsu's method <ref type="bibr" target="#b14">[15]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows average accuracy of 10 CNNs. Overall, combining RGB or G with S leads to the best accuracy. Though S by itself does not perform well, we believe it combines well with RGB/G because the SURF descriptors are computed using the originally sized images. As we later show, using larger input images lead to significant gains in performance. Using B leads to worst performance and augmenting RGB/G with B leads to lower performance. In ANDOC, the HSV colorspace performed equally as well as RGB, though combining both leads to a marginal increase.</p><p>For simplicity, the rest of the experiments use G input for RVL-CDIP and RGB input for ANDOC. Additional experiments (results not shown) suggest that with optimal data augmentation (Section V-B), augmenting RGB/G with S does not lead to significant gains in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Augmentation</head><p>It is common practice to stochastically transform each input during SGD training to artificially enlarge the training set to improve performance <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>. We experimented with 10 types of transformations (e.g. crop, blur, rotation) and 2-4 parameter settings for each transformation type for a total of 38 CNNs trained with different data augmentation. We report results for the best CNNs per transform type in <ref type="table">Table I</ref>. Following <ref type="bibr" target="#b15">[16]</ref>, we also report multiple-view test performance where the overall CNN prediction is the average of the predictions made on 10 transforms of each test image. This increases computation for prediction by 10x, but can increase accuracy <ref type="bibr" target="#b8">[9]</ref>, which makes it appropriate for some applications. The transforms used at test time are the same type as those used during training and include the untransformed image. While some of these transforms are commonly used, shear, perspective, and elastic transforms have not been.</p><p>Shear transforms perform best for single-view testing and are comparable to the best multiple-view transforms.   When images are sheared (either horizontally or vertically), the relative locations of layout components are perturbed, but unlike rotation or perspective transforms, either the horizontal or vertical structure is preserved. We believe that compared to other transform types, shearing best models the types of intra-class variations in document datasets. The CNNs in the remaining experiments were trained using shearing with θ ∈ [−10 • , 10 • ].</p><p>We then attempted to combine the two or three best performing types of transformations. However, this did not improve performance over using a single transform for either single or multi-view testing (results omitted for brevity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Aspect Ratio</head><p>One potential drawback of the standard CNN is fixed spatial input dimensions (e.g. 227x227). This means all inputs must have the same aspect ratio (AR), though the original images may have various ARs. For example, some document images are in portrait or landscape orientation or are printed on different paper sizes. <ref type="figure" target="#fig_1">Figure 2</ref> shows histograms of AR's in both datasets, which is more varied for ANDOC. These images of various AR must be transformed to match the same expected CNN input dimensions. A common technique resizes the image to the input size without maintaining AR. We refer to this as inconsistent AR warping.</p><p>In this experiment, we compare some alternatives to inconsistent AR warping in the context of document images. One method pads images to the correct AR before resizing, however this wastes some precious input content. Another method resizes (preserving AR) the image until the smallest dimension is the correct size. Next, crops of the correct size are taken as input images. At test time, predictions are averaged over 3 crops so the CNN sees all parts of the image. A third way is to modify the CNN architecture to accept variable AR inputs. While the convolution layers can operate on any input size, the fully connected layers expect a fixed sized vector input. One way around the input size requirement is to replace the fixed 2x2 pooling regions of the last convolution layer with a Spatial Pyramid Pooling (SPP) operation <ref type="bibr" target="#b16">[17]</ref>. This way the size of the pooling regions vary with the input size and always yield the same sized vector output. Inspired by <ref type="bibr" target="#b12">[13]</ref>, we experimented with pooling regions arranged in horizontal and vertical partitions (HVP), but it did not outperform SPP pooling, likely due to the small input size (13x13) at this layer.</p><p>We experimented with two different input sizes, 227x227 and 384x384. The 384x384 CNN has the same overall structure as AlexNet, but is wider and has a greater amount of downsampling. The variable AR inputs for the SPP-CNN were resized to have the same or fewer number of pixels as the fixed sized inputs. In <ref type="table" target="#tab_2">Table II</ref> we report the average test accuracy of 2 CNNs on each type of input. Overall, no alternative outperformed inconsistent AR warping even with the SPP which accepts inputs of various AR. Our hypothesis is that stochastically shearing input images (Section V-B) makes the CNN more invariant to inconsistent AR warping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. NETWORK ARCHITECTURE EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Depth</head><p>The computational time of a CNN for both training and inference is influenced by the number of layers (depth) and the number of neurons in each layer (width).</p><p>To modify depth, we removed or added convolution layers to the AlexNet architecture, always keeping the first two layers and maintaining the same downsampling. For shallower nets, we removed layers in this order: conv3, conv4, conv5. For deeper nets, we inserted layers identical to conv3 between conv3 and conv4, where no pooling occurs. We also ran these experiments where the size of the training set was reduced to 50%, 10%, 1% for RVL-CDIP and 50%, 10%, 2% for ANDOC. <ref type="figure" target="#fig_2">Figure 3</ref> gives the results for varying network depth. Here we observe an overall decay in performance as depth increases, even for large amounts of training data. This decay especially pronounced when less less than 10% of the data  is used (&lt;32K or &lt;79K instances), as increasing depth past 2 layers leads to sharp decreases in accuracy. This validates the architectural choice of a 2-conv layer network of <ref type="bibr" target="#b1">[2]</ref>. These results suggest that network depth should adapt to the size of the training dataset.</p><p>We also observe the diminishing returns of additional data for CNN approaches, with only a 1-2% decrease in performance with 50% of the training data. This suggests that gathering more data (e.g. millions of training examples) will only marginally improve CNN performance and more data efficient methods should be sought for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Width</head><p>To modify width, we multiply the number of neurons in each layer by a constant factor. We also experiment with changing just the width of the convolution layers or just the width of the fully connected layers. Results for RVL-CDIP (ANDOC results were similar) are presented in <ref type="table" target="#tab_2">Table III.</ref> With the full training set, network performance saturates at 100% of the AlexNet width. For smaller datasets, smaller widths are optimal. We also see that reducing the width of only the convolutional layers leads to lower performance than just reducing the width of the fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Input Size</head><p>Standard CNN architectures <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b11">[12]</ref> accept inputs that are 224x224, but to our knowledge, this design choice has not been thoroughly explored before. At this resolution, large text is generally legible, but smaller text is not. We empirically test square input sizes {n × n|n = 32, 64, 100, 150, 227, 256, 320, 384, 512}. Of necessity, we modify the architecture of the CNN for each input size with the following principles.</p><p>1) Same number of layers 2) Increase kernel size and network width with input size 3) Spatial output size of final convolution layer is 6x6 An exhaustive grid search over possible architectures was not possible with our computational resources and we acknowledge that our chosen architectures may not be optimal. <ref type="figure" target="#fig_3">Figure 4</ref> shows a distinct trend with larger inputs leading to increased performance. In fact, increasing the input size to 384x384 for RVL-CDIP yields an accuracy of 90.8%, which exceeds the previous published best result of 89.9% <ref type="bibr" target="#b2">[3]</ref>.</p><p>Given that input size significantly impacts performance, we went further to evaluate multi-scale training and testing of CNNs on document images. We did this by using the architecture of the largest input size and replacing the last fixed-sized pooling regions of the CNN with SPP pooling regions <ref type="bibr" target="#b16">[17]</ref> (see Section V-C). During training, all images are randomly resized within a prespecified range (spanning 3 sizes in <ref type="figure" target="#fig_3">Figure 4</ref>). At test time, we average predictions across 3 sizes and report the results in <ref type="figure" target="#fig_3">Figure 4</ref>. This may be considered another type of data augmentation (Section V-B), but only works for architectures that can process multiple sizes. This further improves performance on RVL-CDIP to 91.03% for a CNN trained on images of size 320-512 and predictions averaged across sizes 320,384,512. Smaller datasets also have higher relative improvement using multiscale training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Non-linearities</head><p>Here we test whether LRN or Dropout non-linear components contribute to the overall CNN performance. We also examine a relative new technique termed Batch Normalization (BN) that has been shown to both improve performance and increase training speed <ref type="bibr" target="#b7">[8]</ref>. BN works by first linearly scaling and shifting each neuron's activations to have zero mean and unit variance. These statistics are calculated from the activations caused by just the images in each mini-batch. This is then followed by a learnable shifting and scaling of the activation values so that the mean and variance of the neuron's activation values is learned. We insert BN after each convolution or matrix multiplication and before ReLU. <ref type="table" target="#tab_5">Category  conv1 conv2 conv3  conv4 conv5  Edge  10  2  0  0  0  Parallel lines  46  9  10  9  2  Checker  5  1  2  2  0  Stroke(s)  10  10  2  0  0  Shape Pattern(s)  15  32  24  14  16  Corner  5  25  13  5  3  Small Text  3  4  8  23  42  Large Text  0  16  20  23  12  Column/Margin  0  0  11  5  9  Table Cells  1</ref>    Results are shown in <ref type="table" target="#tab_5">Table IV</ref>. For ANDOC, BN improves performance by a large margin and can replace dropout. This is likely due to the visual variety of the documents (due to diverse set of classes) in ANDOC. In contrast, Dropout is better than BN in RVL-CDIP, likely due to the visual uniformity of office-style documents. We also observe that LRN also does not increase performance for either dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ANALYZING WHAT IS LEARNED</head><p>One critique of CNNs is that they lack interpretation, making it difficult to know what features they use for classification decisions. We analyzed a CNN trained on RVL-CDIP by examining the top-9 input patches that excite neurons and the deconv visualization of each of these patches <ref type="bibr" target="#b17">[18]</ref>. The deconv visualization runs the network backwards from the neuron to the input in the context of some input image or patch. It highlights the salient parts of the image that led to the neuron firing. We found this technique critical for deciphering patterns recognized by neurons in later layers because the effective input size (i.e. receptive field) is very large. For this analysis, we utilized the visualization tool proposed in <ref type="bibr" target="#b18">[19]</ref>.</p><p>In our analysis we manually categorized the first 100 neurons in each convolutional layer (see <ref type="table" target="#tab_5">Table V</ref>). Although the category of some neurons is subjective, our labeling captures the trends in the features learned by each layer. As expected, the first two layers detect simple elements which are gradually abstracted into complex detectors for text and arrangement of elements. While our category labels were the same for all layers, the complexity of each category increased for deeper layers. It is noteworthy that almost all neurons were interpretable, with the majority of the Ambiguous category being neurons that fired on two distinct types of features. This was not the case for the 4-layer network of <ref type="bibr" target="#b1">[2]</ref>, whose first layer filters appear random.</p><p>Neurons in conv5 tended to find particular configurations of text, such as those shown in <ref type="figure" target="#fig_5">Figure 5</ref>. One interpretation is that the CNN is performing a loose form of layout analysis as an intermediary step to classification. The fully connected layers can then reason about the spatial correlation of these elements to form a classification decision. We also observed that by conv5, the CNN has learned to distinguish between handwritten text and type-set text, though no explicit information about the type of text was provided to the CNN.</p><p>We also examined the spatial specificity of features by averaging the intermediate output images for each filter across all images of RVL-CDIP. As seen in <ref type="figure" target="#fig_6">Figure 6</ref>, conv1 filter responses are generally not confined to any portion of the image. However, the other layers exhibit many filters that only respond to certain regions, which is consistent with the hypothesis that CNNs learn layout features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>We examined several factors that influence CNN performance on document images. Overall, applying shear transforms during training and using large input images lead to the biggest gains in performance, acheiving state-of-the-art performance on RVL-CDIP at 90.8% accuracy. Multi-scale training and testing also improve performance, specifically for smaller training sets. As well, BN is a useful alternative to Dropout in datasets that have large visual variety.</p><p>We also examined a CNN trained on RVL-CDIP and found evidence that the CNN is learning intermediate layout features. Neurons fire based on type of layout component (graphic, text, handwriting, noise, etc) and tend to fire on specific locations on the image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Mean accuracy of 10 CNNs as we vary the image representation. Error bars indicate the standard deviation of 10 trials. G=Grayscale, C=RGB, H=HSV, S=dense-SURF, B=Binary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Histogram of Aspect Ratios. Note y-axes are log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Accuracy vs network depth. Each sub-graph is for a different training set size, which is indicated to the right of the graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Accuracy vs input size. Solid blue lines are for CNNs trained with a single input size. Dashed green lines are for CNNs trained with variable sized input images. The x-axis location corresponds to the largest input size used to train the CNN. For example, the performance of the CNNs trained with images of size 256-384 is plotted at x=384.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Top 4 activating patches and deconv visualizations for 2 text detection neurons in conv5. The top neuron detects left justified headers followed by wide paragraphs of text. The bottom neuron detects right justified text below paragraphs of text. Both features are class discriminative (e.g. Letter class).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Average of filter responses for 36 filters for convolution layers. Layers conv2-5 have filters that are specific to spatial regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II COMPARISON</head><label>II</label><figDesc></figDesc><table /><note>OF WAYS TO ADDRESS ASPECT RATIO WARPING</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table V CATEGORIZATION</head><label>V</label><figDesc>OF 100 NEURONS FOR EACH CONVOLUTION LAYER.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nist structured forms reference set of binary images (sfrs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Dimmick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Garris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C L</forename></persName>
		</author>
		<ptr target="http://www.nist.gov/srd/nistsd2.cfm" />
	</analytic>
	<monogr>
		<title level="j">Online</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for document image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3168" to="3172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluation of deep convolutional nets for document image classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ufkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="991" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fine-grained classification of identity document types with only one example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MVA 2015</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="126" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepdocclassifier: Document classification with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Capobianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marinai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1111" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cnn features offthe-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR 2003</title>
		<meeting>ICDAR 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised classification of structurally similar document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR 2013</title>
		<meeting>ICDAR 2013</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1225" to="1229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speeded-up robust features (surf)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="27" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop, ICML 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
