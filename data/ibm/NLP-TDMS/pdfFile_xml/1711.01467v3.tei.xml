<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attentional Pooling for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attentional Pooling for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a simple yet surprisingly powerful model to incorporate attention in action recognition and human object interaction tasks. Our proposed attention module can be trained with or without extra supervision, and gives a sizable boost in accuracy while keeping the network size and computational cost nearly the same. It leads to significant improvements over state of the art base architecture on three standard action recognition benchmarks across still images and videos, and establishes new state of the art on MPII dataset with 12.5% relative improvement. We also perform an extensive analysis of our attention module both empirically and analytically. In terms of the latter, we introduce a novel derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods (typically used for fine-grained classification). From this perspective, our attention formulation suggests a novel characterization of action recognition as a fine-grained recognition problem.</p><p>Attention: While using standard deep networks over the full image have shown great promise for the task <ref type="bibr" target="#b52">[54]</ref>, it raises the question of whether action recognition can be considered as a general classification problem. Some recent works have tried to generate more fine-grained representations by extracting features around human pose keypoints [8] or on object/person bounding boxes <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30]</ref>. This form of 'hard-coded attention' helps improve performance, but requires labeling (or detecting) objects or human pose. Moreover, these methods assume that focusing on the human or its parts is always useful for discriminating actions. This might not necessarily be true for all actions; some actions might be easier to distinguish using the background and context, like a 'basketball shoot' vs a 'throw'; while others might require paying close attention to objects being interacted by the human, like in case of 'drinking from mug' vs 'drinking from water bottle'.</p><p>Our work: In this work, we propose a simple yet surprisingly powerful network modification that learns attention maps which focus computation on specific parts of the input relevant to the task at hand. Our attention maps can be learned without any additional supervision and automatically lead to significant improvements over the baseline architecture. Our formulation is simple-to-implement, and can be seen as a natural extension of average pooling into a "weighted" average pooling with image-specific weights. Our formulation also provides a novel factorization of attentional processing 31st</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action recognition is a fundamental and well studied problem in computer vision. Traditional approaches to action recognition relied on object detection <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b55">57]</ref>, articulated pose <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b55">57]</ref>, dense trajectories <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b51">53]</ref> and part-based/structured models <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b56">58]</ref>. However, more recently these methods have been surpassed by deep CNN-based representations <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b45">47]</ref>. Interestingly, even video based action recognition has benefited greatly from advancements in imagebased CNN models <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b44">46]</ref>. With the exception of a few 3D-conv-based methods <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b47">49]</ref>, most approaches <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b52">54]</ref>, including the current state of the art <ref type="bibr" target="#b52">[54]</ref>, use a variant of discriminatively trained 2D-CNN <ref type="bibr" target="#b20">[22]</ref> over the appearance (frames) and in some cases motion (optical flow) modalities of the input video.</p><p>into bottom-up saliency combined with top-down attention. We further experiment with adding human pose as an intermediate supervision to train our attention module, which encourages the network to look for human object interactions. While this makes little difference to the performance of image-based recognition models, it leads to a larger improvement on video datasets as videos consist of large number of 'non-iconic' frames where the subject of object of actions may not be at the center of focus.</p><p>Our contributions: (1) An easy-to-use extension of state-of-the-art base architectures that incorporates attention to give significant improvement in action recognition performance at virtually negligible increase in computation cost; <ref type="bibr" target="#b0">(2)</ref> Extensive analysis of its performance on three action recognition datasets across still images and videos, obtaining state of the art on MPII and HMDB-51 (RGB, single-frame models) and competitive on HICO; (3) Analysis of different base architectures for applicability of our attention module; and (4) Mathematical analysis of our proposed attention module and showing its equivalence to a rank-1 approximation of second order or bilinear pooling (typically used in fine grained recognition methods <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b26">28]</ref>) suggesting a novel characterization of action recognition as a fine grained recognition problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Human action recognition is a well studied problem with various standard benchmarks spanning across still images <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b56">58]</ref> and videos <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b43">45]</ref>. The newer image based datasets such as HICO <ref type="bibr" target="#b5">[7]</ref> and MPII <ref type="bibr" target="#b32">[34]</ref> are large and highly diverse, containing 600 and 393 classes respectively. In contrast, collecting such diverse video based action datasets is hard, and hence existing popular benchmarks like UCF101 <ref type="bibr" target="#b43">[45]</ref> or HMDB51 <ref type="bibr" target="#b25">[27]</ref> contain only 101 and 51 categories each. This in turn has lead to much higher baseline performance on videos, eg. ∼ 94% <ref type="bibr" target="#b52">[54]</ref> classification accuracy on UCF101, compared to images, eg. ∼ 32% <ref type="bibr" target="#b28">[30]</ref> mean average precision (mAP) on MPII.</p><p>Features: Video based action recognition methods focus on two main problems: action classification and (spatio-)temporal detection. While image based recognition problems, including action recognition, have seen a large boost with the recent advancements in deep learning (e.g., MPII performance went up from 5% mAP <ref type="bibr" target="#b32">[34]</ref> to 27% mAP <ref type="bibr" target="#b16">[18]</ref>), video based recognition still relies on hand crafted features such as iDT <ref type="bibr" target="#b51">[53]</ref> to obtain competitive performance. These features are computed by extracting appearance and motion features along densely sampled point trajectories in the video, aggregated into a fixed length representation by using fisher vectors <ref type="bibr" target="#b30">[32]</ref>. Convolutional neural network (CNN) based approaches to video action recognition have broadly followed two main paradigms: (1) Multi-stream networks <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b52">54]</ref> which split the input video into multiple modalities such as RGB, optical flow, warped flow etc, train standard image based CNNs on top of those, and late-fuse the predictions from each of the CNNs; and (2) 3D Conv Networks <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b47">49]</ref> which represent the video as a spatio-temporal blob and train a 3D convolutional model for action prediction. In terms of performance, 3D conv based methods have been harder to scale and multi-stream methods <ref type="bibr" target="#b52">[54]</ref> currently hold state of the art performance on standard benchmarks. Our approach is complementary to these paradigms and the attention module can be applied on top of either. We show results on improving action classification over state of the art multi-stream model <ref type="bibr" target="#b52">[54]</ref> in experiments. Pose: There have also been previous works in incorporating human pose into action recognition <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b58">60]</ref>. In particular, P-CNN <ref type="bibr" target="#b6">[8]</ref> computes local appearance and motion features along the pose keypoints and aggregates those over the video for action prediction, but is not end-to-end trainable. More recent work <ref type="bibr" target="#b58">[60]</ref> adds pose as an additional stream in chained multi-stream fashion and shows significant improvements. Our approach is complementary to these approaches as we use pose as a regularizer in learning spatial attention maps to weight regions of the RGB frame. Moreover, our method is not constrained by pose labels, and as we show in experiments, can show effective performance with pose predicted by existing methods <ref type="bibr" target="#b2">[4]</ref> or even without using pose.</p><p>Hard attention: Previous works in image based action recognition have shown impressive performance by incorporating evidence from the human, context and pose keypoint bounding boxes <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b28">30]</ref>. Gkioxari el al. <ref type="bibr" target="#b16">[18]</ref> modified R-CNN pipeline to propose R*CNN, where they choose an auxiliary box to encode context apart from the human bounding box. Mallya and Lazebnik <ref type="bibr" target="#b28">[30]</ref> improve upon it by using the full image as the context and using multiple instance learning (MIL) to reason over all humans present in the image to predict an action label for the image. Our approach gets rid of the bounding box detection step and improves over both these methods by automatically learning to attend to the most informative parts of the image for the task.</p><p>Soft attention: There has been relatively little work that explores unconstrained 'soft' attention for action recognition, with the exception of <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b42">44]</ref> for spatio-temporal and <ref type="bibr" target="#b38">[40]</ref> for temporal attention. Importantly, all these consider a video setting, where a LSTM network predicts a spatial attention map for the current frame. Our method, however, uses a single frame to both predict and apply spatial attention, making it amenable to both single image and video based use cases. <ref type="bibr" target="#b42">[44]</ref> also uses pose keypoints labeled in 3D videos to drive attention to parts of the body. In contrast, we learn an unconstrained attention model that frequently learns to look around the human body for objects that make it easier to classify the action.</p><p>Second-order pooling: Because our model uses a single set of appearance features to both predict and apply an attention map, this makes the output quadratic in the features (Sec. 3.1). This observation allows us to implement attention through second-order or bilinear pooling operations <ref type="bibr" target="#b26">[28]</ref>, made efficient through low-rank approximations <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b24">26]</ref>. Our work is most related to <ref type="bibr" target="#b24">[26]</ref>, who point out when efficiently implemented, low-rank approximations avoid explicitly computing second-order features. We point out that a rank-1 approximation of second-order features is equivalent to an attentional model sometimes denoted as "self attention" <ref type="bibr" target="#b48">[50]</ref>. Exposing this connection allows us to explore several extensions, including variations of bottom-up and top-down attention, as well as regularized attention maps that make use of additional supervised pose labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our attentional pooling module is a trainable layer that plugs in as a replacement for a pooling operation in any standard CNN. As most contemporary architectures <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b44">46]</ref> are fully convolutional with an average pooling operation at the end, our module can be used to replace that operation with an attention-weighted pooling. We now derive the pooling layer as an efficient low-rank approximation to second order pooling (Sec. 3.1). Then, we describe our network architecture that incorporates this attention module and explore a pose-regularized variant of the same (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attentional pooling as low-rank approximation of second-order pooling</head><p>Let us write the layer to be pooled as X ∈ R n×f , where n is the number of spatial locations (e.g., n = 16 × 16 = 256) and f is the number of channels (e.g., 2048). Standard sum (or max) pooling would reduce this to vector in R f ×1 , which could then be processed by a "fully-connected" weight vector w ∈ R f ×1 to generate a classification score. We will denote matrices with upper case letters, and vectors with lower-case bold letters. For the moment, assume we are training a binary classifier (we generalize to more classes later in the derivation). We can formalize this pipeline with the following notation:</p><formula xml:id="formula_0">score pool (X) = 1 T Xw, where X ∈ R n×f , 1 ∈ R n×1 , w ∈ R f ×1 (1)</formula><p>where 1 is a vector of all ones and x = 1 T X ∈ R 1×f is the (transposed) sum-pooled feature.</p><p>Second-order pooling: Following past work on second-order pooling <ref type="bibr" target="#b3">[5]</ref>, let us construct the feature</p><formula xml:id="formula_1">X T X ∈ R f ×f .</formula><p>Prior work has demonstrated that such second-order statistics can be useful for fine-grained classification <ref type="bibr" target="#b26">[28]</ref>. Typically, one then "vectorizes" this feature, and learns a f 2 vector of weights to generate a score. If we write the vector of weights as a f × f matrix, the inner product between the two vectorized quantities can be succinctly written using the trace operator 1 . The key identity, T r(AB T ) = dot(A(:), B(:)) (using matlab notation), can easily be verified by plugging in the definition of a trace operator. This allows us to write the classification score as follows:</p><formula xml:id="formula_2">score order2 (X) = T r(X T XW T ), where X ∈ R n×f , W ∈ R f ×f<label>(2)</label></formula><p>Low-rank second-order pooling: Let us approximate matrix W with a rank-1 approximation, W = ab T where a, b ∈ R f ×1 . Plugging this into the above yields a novel formulation of attentional pooling:</p><formula xml:id="formula_3">score attention (X) = T r(X T Xba T ), where X ∈ R n×f , a, b ∈ R f ×1 (3) = T r(a T X T Xb) (4) = a T X T Xb (5) = a T X T (Xb)<label>(6)</label></formula><p>where (4) makes use of the trace identity that T r(ABC) = T r(CAB) and <ref type="formula">(5)</ref> uses the fact that the trace of a scalar is simply the scalar. The last line <ref type="bibr" target="#b4">(6)</ref> gives efficient implementation of attentional pooling: given a feature map X, compute an attention map over all n spatial locations with h = Xb ∈ R n×1 , that is then used to compute a weighted average of features x = X T h ∈ R f ×1 . This weighted-average feature is then pushed through a linear model a T x to produce the final score.</p><p>Interestingly, <ref type="bibr" target="#b4">(6)</ref> can also be written as the following:</p><formula xml:id="formula_4">score attention (X) = (Xa) T X b (7) = (Xa) T (Xb)<label>(8)</label></formula><p>The first line illustrates that the attentional heatmap can also be seen as Xa ∈ R n×1 , with b being the classifier of the attentionally-pooled feature. The second line illustrates that our formulation is in fact symmetric, where the final score can be seen as the inner product between two attentional heatmaps defined over all n spatial locations. <ref type="figure" target="#fig_1">Fig. 1a</ref> illustrates our approach.</p><p>Top-down attention: To generate prediction for multiple classes, we replace the weight matrix from (2) with class-specific weights:</p><formula xml:id="formula_5">score order2 (X, k) = T r(X T XW T k ), where X ∈ R n×f , W k ∈ R f ×f<label>(9)</label></formula><p>One could apply a similar derivation to produce class-specific vectors a k and b k , each of them generating a class-specific attention map. Instead, we choose to distinctly model class-specific "top-down" attention <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b57">59]</ref> from bottom-up visual saliency that is class-agnostic <ref type="bibr" target="#b35">[37]</ref>. We do so by forcing one of the attention parameter vectors to be class-agnostic -e.g., b k = b. This makes our final low-rank attentional model</p><formula xml:id="formula_6">score attention (X, k) = t T k h, where t k = Xa k , h = Xb<label>(10)</label></formula><p>equivalent to an inner product between top-down (class-specific) t k and bottom-up (saliency-based) h attention maps. Our approach of combining top-down and botom-up attentional maps is reminiscent of biologically-motivated schemes that modulate saliency maps with top-down cues <ref type="bibr" target="#b29">[31]</ref>. This suggests that our attentional model can also be implemented using a single, combined attention map defined over all n spatial locations:</p><formula xml:id="formula_7">score attention (X, k) = 1 T c k , where c k = t k • h,<label>(11)</label></formula><p>where • denotes element-wise multiplication and 1 is defined as before. We visualize the combined, top-down, and bottom-up attention maps c k , t k , h ∈ R n×1 in our experimental results.</p><p>Average pooling (revisited): The above derivation allows us to revisit our average pooling formulation from (1), replacing weights w with class-specific weights w k as follows:</p><formula xml:id="formula_8">score top−down (X, k) = 1 T Xw k = 1 T t k where t k = Xw k<label>(12)</label></formula><p>From this perspective, the above derivation gives the ability to generate top-down attentional maps from existing average-pooling networks. While similar observations have been pointed out before <ref type="bibr" target="#b57">[59]</ref>, it naturally emerges as a special case of our bottom-up and top-down formulation of attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>We now describe our network architecture to implement the attentional pooling described above. We start from a state of the art base architecture, ResNet-101 <ref type="bibr" target="#b18">[20]</ref>. It consists of a stack of 'modules', each of which contains multiple convolutional, pooling or identity mapping streams. It finally generates a n 1 × n 2 × f spatial feature map, which is average pooled to get a f -dimensional vector and is then classified using a linear classifier.  Our attention module plugs in at the last layer, after the spatial feature map. As shown in <ref type="figure" target="#fig_1">Fig. 1b</ref> (Method 1), we predict a single channel bottom-up saliency map of same spatial resolution as the last feature map, using a linear classifier on top of it (Xb). Similarly, we also generate the n 1 × n 2 × K dimensional top-down attention map Xa, where K is number of classes. The two attention maps are multiplied and spatially averaged to generate the K-dimensional output predictions ((Xa) T (Xb)). These operations are equivalent to first multiplying the features with saliency (X T (Xb)) and then passing through a classifier (a(X T (Xb))).</p><p>Pose: While this unconstrained attention module automatically learns to focus on relevant parts and gives a sizable boost in accuracy, we take inspiration from previous work <ref type="bibr" target="#b6">[8]</ref> and use human pose keypoints to guide the attention. As shown in <ref type="figure" target="#fig_1">Fig. 1b (Method 2)</ref>, we use a two-layer MLP on top of the last layer to predict a 17 channel heatmap. The first 16 channels correspond to human pose keypoints and incur a l 2 loss against labeled (or detected, using <ref type="bibr" target="#b2">[4]</ref>) pose) The final channel is used as an unconstrained bottom-up attention map, as before. We refer to this method as pose-regularized attention, and it can be thought of as a non-linear extension of previous attention map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets: We experiment with three recent, large scale action recognition datasets, across still images and videos, namely MPII, HICO and HMDB51. MPII Human Pose Dataset <ref type="bibr" target="#b32">[34]</ref> contains 15205 images labeled with up to 16 human body keypoints, and classified into one of 393 action classes. It is split into train, val (from authors of <ref type="bibr" target="#b16">[18]</ref>) and test sets, with 8218, 6987 and 5708 images each. We use the val set to compare with <ref type="bibr" target="#b16">[18]</ref> and for ablative analysis while the final test results are obtained by emailing our results to authors of <ref type="bibr" target="#b32">[34]</ref>. The dataset is highly imbalanced and the evaluation is performed using mean average precision (mAP) to equally weight all classes. HICO <ref type="bibr" target="#b5">[7]</ref> is a recently introduced dataset with labels for 600 human object interactions (HOI) combining 117 actions with 80 objects. It contains 38116 training and 9658 test images, with each image labeled with all the HOIs active for that image (multi-label setting). Like MPII, this dataset is also highly unbalanced and evaluation is performed using mAP over classes. Finally, to verify our method's applicability to video based action recognition, we experiment with a challenging trimmed action classification dataset, HMDB51 <ref type="bibr" target="#b25">[27]</ref>. It contains 6766 realistic and varied video clips from 51 action classes. Evaluation is performed using average classification accuracy over three train/test splits from <ref type="bibr" target="#b21">[23]</ref>, each with 3570 train and 1530 test videos. Baselines: Throughout the following sections, we compare our approach first to the standard base architecture, mostly ResNet-101 <ref type="bibr" target="#b18">[20]</ref>, without the attention-weighted pooling. Then we compare to other reported methods and previous state of the art on the respective datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPII:</head><p>We train our models for 393-way action classification on MPII with softmax cross-entropy loss for both the baseline ResNet and our attentional model. We compare our performance in Tab. 1. Our unconstrained attention model clearly out-performs the base ResNet model, as well as previous state of the art methods involving detection of multiple contextual bounding boxes <ref type="bibr" target="#b16">[18]</ref> and fusion of full image with human bounding box features <ref type="bibr" target="#b28">[30]</ref>. Our pose-regularized model performs best, though the improvement is small. We visualize the attention maps learned in <ref type="figure" target="#fig_2">Fig. 2</ref>  ) attention on validation images in MPII, that see largest improvement in softmax score for correct class when trained with attention. Since the top-down/combined maps are class specific, we mention the class name for which they are generated for on top left of those heatmaps. We consider 2 classes, the ground truth (GT) for the image, and the class on which it gets lowest softmax score. The attention maps for GT class focus on the objects most useful for distinguishing the class. Though the top-down and combined maps look similar in many cases, they do capture different information. For example, for a garbage collector action (second row), top-down also focuses on the vehicles in background, while the combined map narrows focus down to the garbage bags.  <ref type="figure">Figure 3</ref>: We crop a 100px patch around the attention peak for all images containing an HOI involving a given object, and show 5 randomly picked patches for 6 object classes here. This suggests our attention model learns to look for objects to improve HOI detection.  <ref type="bibr" target="#b16">[18]</ref>. Test performance obtained from training on complete train set and submitting our output file to authors of <ref type="bibr" target="#b32">[34]</ref>. Note that even though our pose regularized model uses pose labels at training time for regularizing attention, it does not require any pose input at test time. The top-half corresponds to a diagnostic analysis of our approach with different base networks. Attention provides a strong 4% improvement for baseline networks with larger spatial resolution (e.g., ResNet). Please see text for additional discussion. The bottom-half reports prior work that makes use of object bounding boxes/pose. Our method performs slightly better with pose annotations (on training data), but even without any pose or detection annotations, we outperform all prior work.   <ref type="bibr" target="#b28">[30]</ref>, when that is trained with a specialized weighted loss for this dataset. It is also worth noting that the full image-only performance of VGG and ResNet were comparable in our experiments (29.4% and 30.2%), suggesting that our approach shows larger relative improvement over a similar starting baseline. Though we did not experiment with the same optimization setting as <ref type="bibr" target="#b28">[30]</ref>, we believe it will give similar improvements there as well. Since this dataset also comes with labels decomposed into actions and objects, we visualize what our attention model looks for, given images containing interactions with a specific object. As <ref type="figure">Fig. 3</ref> shows, the attention peak is typically close to the object of interest, showing the importance of detecting objects in HOI detection tasks. Moreover, this suggests that our attention maps can also function as weak-supervision for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>HMDB51: Next, we apply our attentional method to the RGB stream of the current state of the art single-frame deep model on this dataset, TSN <ref type="bibr" target="#b52">[54]</ref>. TSN extends the standard two-stream <ref type="bibr" target="#b40">[42]</ref> architecture by using a much deeper base architecture <ref type="bibr" target="#b20">[22]</ref> along with enforcing consensus over multiple frames from the video at training time. For the purpose of this work, we focus on the RGB stream only but our method is applicable to flow/warped-flow streams as well. We first train a TSN model using ResNet-101 as base architecture after re-sizing input frames to 450px. This ensures larger spatial dimensions of the output <ref type="figure" target="#fig_1">(14 × 14)</ref>, hence ensuring the last-layer features are amenable to attention. Though our base ResNet model does worse than BN-inception TSN model, as Tab. 3 shows, using our attention module improves the base model to do comparably well. Interestingly, on this dataset regularizing the attention through pose gives a significant boost in  performance, out-performing TSN and establishing new state of the art on the RGB-only single-frame model for HMDB. We visualize the attention maps with normal and pose-regularized attention in <ref type="figure" target="#fig_4">Fig. 4</ref>. The pose regularized attention are more peaky near the human than their linear counterparts. This potentially explains the improvement using pose on HMDB while it does not help as much on HICO or MPII; HICO and MPII, being image based datasets typically have 'iconic' images, with the subjects and objects of action typically in the center and focus of the image. Video frames in HMDB, on the other hand, may have the subject move all across the frame throughout the video, and hence additional supervision through pose at training time helps focus the attention at the right spot.</p><p>Full-rank pooling: Given our formulation of attention as low-rank second-order pooling, a natural question is what would be the performance of a full-rank model? Explicitly computing the secondorder features of size f × f for f = 2048 (and learning the associated classifier) is cumbersome. Instead, we make use of the compact bilinear approach (CBP) of <ref type="bibr" target="#b14">[16]</ref>, which generates a lowdimensional approximation of full bilinear pooling <ref type="bibr" target="#b26">[28]</ref> using the TensorSketch algorithm. To keep the final output comparable to our attentional-pooled model, we project to f = 2048 dimensions. We find it performs slightly worse than simple average pooling in <ref type="table" target="#tab_3">Table 2</ref>. Note that we use an existing implementation [1] with minimal hyper-parameter optimization, and leave a more rigorous comparison to future work. Rank-P approximation: While a full-rank model is cumbersome, we can still explore the effect of using a higher, P -rank approximation. Essentially, a rank-P approximation generates P (1-channel) bottom-up and (C channel) top-down attention maps, and the final prediction is the product of corresponding heatmaps, summed over P . On MPII, we obtain mAP of 30.3, 29.9, 30.0 for P =1, 2 and 5 respectively, showing that the validation performance is relatively stable with P . We do observe a drop in training loss with a higher P , indicating that a higher-rank approximation could be useful for harder datasets and tasks. Per-class attention maps: As we described in Sec. 3.1, our inspiration for combining class-specific and class-agnostic classifiers (i.e. top-down and bottom-up attention respectively), came from the Neuroscience literature on integrating top-down and bottom-up attention <ref type="bibr" target="#b29">[31]</ref>. However, our model can also be extended to learn completely class-specific attention maps, by predicting C bottom-up attention maps, and combining each map with the corresponding softmax classifier for that class. We experiment with this idea on MPII and obtain a mAP of 27.9 with 393 (=num-classes) attention maps, compared to 30.3% with 1 map, and 26.2% without attention.</p><p>On further analysis we observe that both models achieve near perfect mAP on training data, implying that adding more parameters with multiple attention maps leads to over-fitting on the relatively small MPII trainset. However, this may be a viable approach for larger datasets.</p><p>Diagnostics: It is natural to consider variants of our model that only consider the bottom-up or top-down attentional map. As derived in <ref type="bibr" target="#b10">(12)</ref>, baseline models with average pooling are equivalent to "top-down-only" attention models, which are resoundingly outperformed by our joint bottom-up and top-down model. It is not clear how to construct a bottom-up only model, since it is class-agnostic, making it difficult to produce class-specific scores. Rather, a reasonable approximation might be applying an off-the-shelf (bottom-up) saliency method used to limit the spatial region that features are averaged over. Our initial experiments with existing saliency-based methods <ref type="bibr" target="#b19">[21]</ref> were not promising.</p><p>Base Network: Finally, we analyze the choice of base architecture for the effectiveness of our proposed attentional pooling module. In Tab. 1, we compare the improvement using attention over ResNet-101 (R-101) <ref type="bibr" target="#b18">[20]</ref> and an BN-Inception (I-V2) <ref type="bibr" target="#b20">[22]</ref>. Both models perform comparably when trained for full image, however, while we see a 4% improvement on R-101 on using attention, we do not see similar improvements for I-V2. This points to an important distinction in the two architectures, i.e., Inception-style models are designed to be faster in inference and training by rapidly down sampling input images in initial layers through max-pooling. While this reduces the computational cost for later layers, it leads to most layers having very large receptive fields, and hence later neurons have effective access to all of the image pixels. This suggests that all the spatial features at the last layer could be highly similar. In contrast, R-101 downscales the spatial resolution gradually, allowing the last layer features to specialize to different parts of the image, hence benefiting more from attentional pooling. This effect was further corroborated by our experiments on HMDB, where using the standard 224px input resolution showed no improvement with attention, while the same image resized to 450px at input time did. This initial resize ensures the last-layer features are sufficiently distinct to benefit from attentional pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>An important distinction of our model from some previous works <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30]</ref> is that it does not explicitly model action at an instance or bounding-box level. This, in fact, is a strength of our model; making it capable of attending to objects outside of any person-instance bounding box (such as bags of garbage for "garbage collecting", in <ref type="figure" target="#fig_2">Fig 2)</ref>. In theory, our model can also be applied to instance-level action recognition by applying attentional pooling over an instance's RoI features. Such a model would learn to look at different parts of human body and its interactions with nearby objects. However, it's notable that most existing action datasets, including <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b43">45]</ref>, come with only frame or video level labels; and though <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30]</ref> are designed for instance-level recognition, they are not applied as such. They either copy image level labels to instances or use multiple-instance learning, either of which can be used in conjunction with our model. Another interesting connection that emerges from our work is the relation between second-order pooling and attention. The two communities are traditionally seen as distinct, and our work strongly suggests that they should mix: as newer action datasets become more fine-grained, we should explore second-order pooling techniques for action recognition. Similarly, second-order pooling can serve as a simple but strong baseline for the attention community, which tends to focus on more complex sequential attention networks (based on RNNs or LSTMs). It is also worth noting that similar ideas involving self attention and bilinear models have recently also shown significant improvements in other tasks like image classification <ref type="bibr" target="#b49">[51]</ref>, language translation <ref type="bibr" target="#b48">[50]</ref> and visual question answering <ref type="bibr" target="#b36">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion:</head><p>We have introduced a simple formulation of attention as low-rank second-order pooling, and illustrate it on the task of action classification from single (RGB) images. Our formulation allows for explicit integration of bottom-up saliency and top-down attention, and can take advantage of additional supervision when needed (through pose labels). Our model produces competitive or state-of-the-art results on widely benchmarked datasets, by learning where to look when pooling features across an image. Finally, it is easy to implement and requires few additional parameters, making it an attractive alternative to standard pooling, which is a ubiquitous operation in nearly all contemporary deep networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Visualization of our approach to attentional pooling as a rank-1 approximation of 2 nd order pooling. By judicious ordering of the matrix multiplications, one can avoid computing the second order feature X T X and instead compute the product of two attention maps. The top-down attentional map is computed using class-specific weights a k , while the bottom-up map is computed using class-agnostic weights b. We visualize the top-down and bottom-up attention maps learned by our approach inFig. 2. * We explore two architectures in our work, explained in Sec. 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of our derivation and final network architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Auto-generated (not hand-picked) visualization of bottom-up (Xb), top-down (Xa k ) and combined ((Xa k ) • (Xb)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>Best viewed zoomed-in on screen)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Attention maps with linear attention and pose regularized attention on a video from HMDB. Note the pose-guided attention is better able to focus on regions of interest in the non-iconic frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Action classification performance on MPII dataset. Validation (Val) performance is reported on train set split shared by authors of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Full Im. Bbox/Pose MIL Wtd Loss mAP</cell></row><row><cell>AlexNet+SVM [7]</cell><cell>19.4</cell></row><row><cell>VGG16, full image [30]</cell><cell>29.4</cell></row><row><cell>ResNet101, full image (ours)</cell><cell>30.2</cell></row><row><cell>ResNet101 with CBP [16] (impl. from [1])</cell><cell>26.8</cell></row><row><cell>Attentional Pooling (R-101) (ours)</cell><cell>35.0</cell></row><row><cell>R*CNN [18] (reported in [30])</cell><cell>28.5</cell></row><row><cell>Scene-RCNN [18] (reported in [30])</cell><cell>29.0</cell></row><row><cell>Fusion (best reported) [30]</cell><cell>33.8</cell></row><row><cell>Pose Regularized Attentional Pooling (R101) (ours)</cell><cell>34.6</cell></row><row><cell>Fusion, weighted loss (best reported) [30]</cell><cell>36.1</cell></row></table><note>Multi-label HOI classification performance on HICO dataset. The top-half compares our performance to other full image-based methods. The bottom-half reports methods that use object bounding boxes/pose. Our model out-performs various approaches that need bounding boxes, multi-instance learning (MIL) or specialized losses, and achieves performance competitive to state of the art. Note that even though our pose regularized model uses computed pose labels at training time, it does not require any pose input at test time.previous methods, including ones that use detection bounding boxes at test time except one</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Action classification performance on HMDB51 dataset using only the RGB stream of a two-stream model. Our base ResNet stream training is done over 480px rescaled images, same as used in our attention model for comparison purposes. Our pose based attention model out-performs the base network by large margin, as well as the previous RGB stream (single-frame) state-of-the-art, TSN<ref type="bibr" target="#b52">[54]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Split 1 Split 2 Split 3 Avg</cell></row><row><cell>TSN, BN-inception (RGB) [54] (Via email with authors)</cell><cell>54.4</cell><cell>49.5</cell><cell>49.2 51.0</cell></row><row><cell>ActionVLAD [17]</cell><cell>51.2</cell><cell>-</cell><cell>-49.8</cell></row><row><cell>RGB Stream, ResNet50 (RGB) [14] (reported at [2])</cell><cell>-</cell><cell>-</cell><cell>-48.9</cell></row><row><cell>RGB Stream, ResNet152 (RGB) [14] (reported at [2])</cell><cell>-</cell><cell>-</cell><cell>-46.7</cell></row><row><cell>TSN, ResNet101 (RGB) (ours)</cell><cell>48.2</cell><cell>46.5</cell><cell>46.7 47.1</cell></row><row><cell>Linear Attentional Pooling (ours)</cell><cell>51.1</cell><cell>51.6</cell><cell>49.7 50.8</cell></row><row><cell>Pose regularized Attentional Pooling (ours)</cell><cell>54.4</cell><cell>51.1</cell><cell>50.9 52.2</cell></row><row><cell>Attention</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pose Reg. Attention</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://en.wikipedia.org/wiki/Trace_(linear_algebra)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: Authors would like to thank Olga Russakovsky for initial review. This research was supported in part by the National Science Foundation (NSF) under grant numbers CNS-1518865 and IIS-1618903, and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001117C0051. Additional support was provided by the Intel Science and Technology Center for Visual Cloud Systems (ISTC-VCS). Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view(s) of their employers or the above-mentioned funding sources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<ptr target="http://www.robots.ox.ac.uk/~vgg/software/two_stream_action/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mechanisms of top-down attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baluch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Neurosciences</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>P-Cnn</surname></persName>
		</author>
		<title level="m">Pose-based CNN Features for Action Recognition. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognizing human actions in still images: a study of bag-of-features and part-based representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning person-object interactions for action recognition in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative models for static human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR-Workshops</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ActionVLAD: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contextual action recognition with R*CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Observing human-object interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SALICON: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://www.thumos.info/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hadamard Product for Low-rank Bilinear Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning models for actions and person-object interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An integrated model of top-down and bottom-up attention for optimizing detection speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Navalpakkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning latent sub-events in activity videos using temporal attention filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fine-grained activity recognition with holistic and pose based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic annotation of everyday movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Describing common human visual actions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Is bottom-up attention useful for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rutishauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01427</idno>
		<title level="m">A simple neural network module for relational reasoning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR-Workshops</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Joint network based attention for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05215</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<title level="m">Visual routines. Cognition</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>abs/1604.04494</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action Recognition by Dense Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng-Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recognizing human actions from still images with latent poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Grouplet: A structured image representation for recognizing human and object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
