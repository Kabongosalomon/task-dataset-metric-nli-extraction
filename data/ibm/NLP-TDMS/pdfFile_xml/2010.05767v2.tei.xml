<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Smaller World Models for Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Robine</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Uelwer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
						</author>
						<title level="a" type="main">Smaller World Models for Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sample efficiency remains a fundamental issue of reinforcement learning. Model-based algorithms try to make better use of data by simulating the environment with a model. We propose a new neural network architecture for world models based on a vector quantized-variational autoencoder (VQ-VAE) to encode observations and a convolutional LSTM to predict the next embedding indices. A model-free PPO agent is trained purely on simulated experience from the world model. We adopt the setup introduced by Kaiser et al. <ref type="formula">(2020)</ref>, which only allows 100K interactions with the real environment. We apply our method on 36 Atari environments and show that we reach comparable performance to their SimPLe algorithm, while our model is significantly smaller.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reinforcement learning is a generally applicable framework for finding actions in an environment that maximizes the sum of rewards. From a probabilistic perspective, the following probabilities lay the foundations of all reinforcement learning problems:</p><p>The dynamics p(r t , s t+1 | s t , a t ), i.e., the conditional joint probability of the next reward and state given the current state and action. This probability defines the environment and is usually unknown. In most real-world applications the underlying states s t are not observable, but instead the environment produces observations x t , which might not contain all state information. In this case we can only observe p(r t , x t+1 | x t , a t ).</p><p>The policy p θ (a t |x t ), i.e., the conditional probability of choosing an action given the current observation, where θ denotes the parameters of some model, indicating that this probability is not provided by the environment, but learned. Reinforcement learning methods provide means to optimize the policy in the sense that the actions that maximize the future sum of rewards have the highest probability.</p><p>Model-free algorithms try to optimize the policy p θ (a t |x t ) based on real experience from the environment, without any knowledge of the underlying dynamics. They have shown great success in a wide range of environments, but usually require a large amount of training data, i.e., many interactions with the environment. This low sample efficiency makes them inappropriate for real-world applications in which data collection is expensive.</p><p>Model-based algorithms approximate the dynamics p φ (r t , x t+1 | x t , a t ) ≈ p(r t , x t+1 | x t , a t ), where φ denotes some learned parameters, thus building a model of the environment, which we will call world model to differentiate it from other models. The process of improving the policy using the world model is called planning, but there are two types: first, we can generate training data by sampling from p φ (r t , x t+1 | x t , a t ) and apply a model-free algorithm to this simulated experience. Second, we can try to look ahead into the future using the world model, starting from the current observation, in order to dynamically improve the action selection during runtime.</p><p>Contributions. In this work we follow a model-based approach and consider the first type of planning. The ability to generate new experience without acting in the real environment does effectively increase the sample efficiency. Our main contributions and insights can be summarized as follows:</p><p>• VQ-VAE based world models require fewer parameters than previous approaches (see <ref type="table" target="#tab_0">Table 2</ref> and <ref type="table" target="#tab_1">Table 3</ref>).</p><p>• Learning a latent space world model and training an agent on it is possible with only 100K interactions (see <ref type="table" target="#tab_2">Table 4</ref>).</p><p>• A two-dimensional discrete latent representation combined with a dynamics network built from convolutional LSTMs (see <ref type="figure" target="#fig_1">Fig. 3</ref>) is sufficient for model-based training in Atari games.</p><p>arXiv:2010.05767v2 [cs.</p><p>LG] 2 Mar 2021</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>World Models <ref type="bibr" target="#b2">(Ha &amp; Schmidhuber, 2018)</ref>. Modeling environments with complex visual observations is a hard task, but luckily predicting observations on pixel level is not necessary. The authors introduce latent variables by encoding the high-dimensional observations x t into lowerdimensional, latent representations z t . For this purpose they use a variational autoencoder <ref type="bibr" target="#b6">(Kingma &amp; Welling, 2014)</ref>, which they call the "vision" component, that extracts information from the observation at the current time step. They use an LSTM combined with a mixture density network <ref type="bibr" target="#b1">(Bishop, 1994)</ref> to predict the next latent variables stochastically. They call this component the "memory", that can accumulate information over multiple time steps.</p><p>They also condition the policy on the latent variable, which enables them to stay in latent space, so that decoding back into pixels is not required (except for learning the representations). This makes simulation more efficient and can reduce the effect of accumulating errors. In Section 3.1 we describe in more detail how to integrate latent variables into the dynamics.</p><p>They successfully evaluate their architectures on two environments, but it involves some manual fine-tuning of the policy. They use an evolution strategy to optimize the policy, which is not suitable for bigger networks. They also use a non-iterative training procedure, i.e., they randomly collect real experience only once and then train the world model and the policy. This implies that the improved policies cannot be used to obtain new experience, and a random policy has to ensure sufficient exploration, which makes the approach inappropriate for more complex environments.</p><p>Simulated Policy Learning <ref type="bibr" target="#b5">(Kaiser et al., 2020)</ref>. The authors introduce a new model-based algorithm (SimPLe) and successfully apply it to Atari games. They use an iterative training procedure, that alternates between collecting real experience, training the world model, and improving the policy using the world model.</p><p>Another novelty is that they restrict the algorithm to about 100K interactions with the real environment, which is considerably less than the usual 10M to 50M interactions.</p><p>They train the policy using the model-free PPO algorithm <ref type="bibr" target="#b8">(Schulman et al., 2017)</ref> instead of an evolution strategy. They use a video prediction model similar to SV2P <ref type="bibr" target="#b0">(Babaeizadeh et al., 2017)</ref> and incorporate the input action in the decoding process to predict the next frame. The latent variable is discretized into a bit vector, that is predicted autoregressively using an LSTM during inference time. The policy gets frames as input, which means that decoding the latent variables back into pixel-level is required.</p><formula xml:id="formula_0">x t x t+1 x t+2 z t z t+1 z t+2 r t r t+1 y t y t+1 a t a t+1 h t h t+1 h t+2</formula><p>··· ··· <ref type="figure">Figure 1</ref>. Graphical model of the world model and the policy, which arises from inserting the latent variables and from our independence assumptions.</p><p>They get very good results on a lot of Atari environments, considering the low number of interactions.</p><p>Dreamer <ref type="bibr" target="#b3">(Hafner et al., 2020a)</ref> and DreamerV2 <ref type="bibr" target="#b4">(Hafner et al., 2020b)</ref>. The architecture of DreamerV2 is closely related to ours. The authors train a world model with latent representations, such that the agent can operate directly on the latent variables. One of the main improvements of DreamerV2 over the model of Dreamer is the discretization of the latent space, as it uses categorical instead of Gaussian latent variables.</p><p>They show that their agent beats model-free algorithms in many Atari games after 50M interactions. This is a quite different goal from our work, where we attempt to learn as much as possible from only 100K interactions. Furthermore, we base our discrete latent world model on a VQ-VAE and thus discretize the latent variables using vector quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Discrete Latent Space World Models</head><p>The goal of this work is to extend the idea of <ref type="bibr" target="#b2">Ha &amp; Schmidhuber (2018)</ref> by using more sophisticated neural network architectures and evaluating them on Atari environments with the model-free PPO algorithm instead of evolution strategies. In particular, we discretize the latent space, but have a fundamentally different architecture compared to DreamerV2 <ref type="bibr" target="#b4">(Hafner et al., 2020b)</ref>, since our latent space is two-dimensional. Moreover, we adopt the limitation to 100K interactions, the iterative training scheme and some other crucial ideas from <ref type="bibr" target="#b5">Kaiser et al. (2020)</ref>, which will be explained in later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Latent Variables</head><p>Similar to <ref type="bibr" target="#b2">Ha &amp; Schmidhuber (2018)</ref> we approximate the dynamics with the help of latent variables. The sum rule of probability allows us to introduce a latent variable z t ,</p><formula xml:id="formula_1">p(r t , x t+1 | x t , a t ) ≈ E zt∼p(zt|xt) E zt+1∼p(zt+1|zt,at) p(r t |z t , a t )p(x t+1 |z t+1 )<label>(1)</label></formula><p>where we have made multiple independence assumptions. Especially, we want an observation encoding model, p φ (z t |x t ), that does not depend on the action (analogous to the "vision" component of <ref type="bibr" target="#b2">Ha &amp; Schmidhuber (2018)</ref>). Second, we want to predict the next latent variable based on the previous latent variable and action, i.e., p φ (z t+1 |z t , a t ), independent of the observation x t . Furthermore, the reward and next latent variable should not depend on each other, which allows to predict them using two heads and compute them in a single neural network pass.</p><p>In contrast to <ref type="bibr" target="#b5">Kaiser et al. (2020)</ref>, the policy is conditioned on the latent variables, so that no decoding into high-dimensional observations is necessary,</p><formula xml:id="formula_2">p(a t |x t ) ≈ E zt∼p(zt|xt) p(a t |z t ) .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Recurrent Dynamics</head><p>Predicting the next latent variable and reward can be improved by introducing a recurrent variable h t to the dynamics model, similar to <ref type="bibr" target="#b2">Ha &amp; Schmidhuber (2018)</ref>,</p><formula xml:id="formula_3">p(x t+1 , r t , h t |x t , a t , h t−1 ) ≈ E zt∼p(zt|xt) p(h t |z t , a t , h t−1 ) E zt+1∼p(zt+1|zt,at,ht) p(r t |z t , a t , h t ) p(x t+1 |z t+1 ) = E zt∼p(zt|xt) p(h t |z t , a t , h t−1 ) E zt+1∼p(zt+1|yt) p(r t |y t ) p(x t+1 |z t+1 ) .<label>(3)</label></formula><p>We have made independence assumptions analogous to Eq. (1). In particular, the observation encoder and decoder do not depend on the recurrent variable, but the latent and reward dynamics do. For notation purposes, we introduced an intermediate representation</p><formula xml:id="formula_4">y t = f (z t , a t , h t ) in Eq. (3),</formula><p>which is a deterministic function of z t , a t , and h t . The resulting graphical model can be seen in <ref type="figure">Fig. 1</ref>.</p><p>We can also condition the policy on the recurrent variable, which adds a dependency from a t to h t−1 ,</p><formula xml:id="formula_5">p(a t |x t , h t−1 ) ≈ E zt∼p(zt|xt) p(a t |z t , h t−1 ) . (4) From Eq. (3), Eq.</formula><p>(2), and Eq. (4) it becomes clear that several models have to be learned. We denote the parameters of the world model by φ, and the parameters of the policy by θ. <ref type="table">Table 1</ref> provides an overview of all models that need to be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture</head><p>Latent Representations. We use a vector quantizedvariational autoencoder (van den Oord et al., 2017) for the observation encoder and decoder, so each latent variable z t is a matrix filled with discrete embedding indices. The <ref type="table">Table 1</ref>. Summary of the learned models. Note that yt is a deterministic function of zt, at, and ht.</p><formula xml:id="formula_6">Observation encoder p φ (z t |x t ) Recurrent dynamics p φ (h t |z t , a t , h t−1 ) World model Reward dynamics p φ (r t |y t ) Latent dynamics p φ (z t+1 |y t ) Observation decoder p φ (x t+1 |z t+1 ) Agent Policy p θ (a t |z t ) or p θ (a t |z t , h t−1 )</formula><p>observations x t are the last four frames of the Atari game, stacked, scaled down to 96 × 96 and converted from RGB to grayscale. Frame stacking allows the observation encoder to incorporate short-time information, e.g., the velocity of objects, into the otherwise stationary latent representations.</p><p>The encoder uses convolutions with batch normalization, while the decoder uses deconvolutions without batch normalization. All convolutions and deconvolutions are followed by leaky ReLU nonlinearities (after the batch normalization). We model the outputs of the decoder with continuous Bernoulli distributions (Loaiza-Ganem &amp; Cunningham, 2019) with independence among the stacked frames and pixels, so the last deconvolution outputs the logits of 96×96×4 distributions. See <ref type="figure" target="#fig_0">Fig. 2</ref> for a visualization of the model.</p><p>We employ a two-dimensional representation because it can also express local spatial correlations. Thus, it is better suited to predict the representation at the next time step, especially when combined with convolutional operations.</p><p>Dynamics. For the recurrent dynamics we use a two-cell convolutional LSTM <ref type="bibr" target="#b9">(Shi et al., 2015)</ref> with layer normalization. The input consists of a 6 × 6 × 48 tensor, where the first 32 channels are the embedding vectors looked up in the codebook of the VQ-VAE using the indices of the 6 × 6 state representation. The last 16 channels contain one-hot encodings of the actions, repeated along the spatial dimensions. By doing this, we condition the dynamics on the action. The action encodings are also concatenated to the output of each convolutional LSTM cell, since the action information might get lost during the forward pass. After the last convolutional LSTM cell, this corresponds to the intermediate representation y t from <ref type="figure">Fig. 1</ref>, which means that we actually drop its direct dependence on z t . Then, there are two prediction heads, one for the next latent variable, f φ (y t ) consisting of one convolutional layer, and one for the reward, g φ (y t ) consisting of a convolutional layer and two fully-connected layers. The convolutional layers are followed by layer normalization and leaky ReLU nonlinearities. For a detailed depiction of the model see <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>The output of f φ (y t ) is a H × W × K tensor (6 × 6 × 128) that contains the unnormalized scores for the embedding indices that get normalized via the softmax function,</p><formula xml:id="formula_7">p φ z (j,k) t+1 y t = Cat K, Softmax f (j,k) φ (y t ) . (5)</formula><p>We suppose that the discretization of the latent space stabilizes the dynamics model, since it has to predict scores for a predefined set of categories instead of real values, especially considering that the target is moving, i.e., the latent representations change during training.</p><p>The rewards are discretized into three categories {−1, 0, 1} by clipping them into the interval [−1, 1] and rounding them to the nearest integer. The output of g φ (y t ) is a 3dimensional vector containing the scores for each reward category, which are also normalized via the softmax function, p φ (r t |y t ) = Cat(3, Softmax(g φ (y t ))).</p><p>The support of this distribution is r ∈ {1, 2, 3}, so we have to map the rewards accordingly (r = r orig + 2) when we compute the likelihood.</p><p>Policy. The input of the policy network are the embedding vectors and it processes them using two convolutional layers with layer normalization, followed by a fully connected layer. Unlike Eq. (4) the policy does not depend on the recurrent variable h t−1 from the world model in our experiments, but this could be useful for more complex environments. The output of the network f θ (z t ) is again a vector of unnormalized scores of a categorical distribution for the M possible actions, p θ (a t |z t ) = Cat(M, Softmax(f θ (z t ))). Number of Parameters. <ref type="table" target="#tab_0">Table 2</ref> shows the number of parameters of our model compared with the model by <ref type="bibr" target="#b5">Kaiser et al. (2020)</ref>, which uses about seven times as many parameters. <ref type="table" target="#tab_1">Table 3</ref> shows the number of parameters of our models in detail. At training time all models are used, but at test time only the encoder and the policy network are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>The distributions in Eq. (5) and Eq. (6) are trained using maximum likelihood. The policy is optimized on simulated experience using proximal policy optimization <ref type="bibr" target="#b8">(Schulman et al., 2017)</ref>, a model-free reinforcement learning algorithm. We approximate the expectations in Eq. (1) and Eq.</p><p>(2) with single Monte Carlo samples. While we simulate experience, we use the same sample z t ∼ p(z t |x t ) for both the dynamics, Eq. (3), and the policy, Eq. (2). At inference time, when the policy is applied to a real environment, Eq.</p><p>(2) still needs access to the observation encoder p φ (z t |x t ) in order to sample the latent variables.</p><p>Episodic Environments. Atari games are episodic, therefore the world model needs to predict terminal states, for instance by predicting a binary variable that indicates the end of the episode. This prediction has to be reliable, since an incorrect prediction of "true" can have a severe impact on the simulated experience and thus on the policy.  We follow <ref type="bibr" target="#b5">Kaiser et al. (2020)</ref> and end all episodes after a fixed number of steps (e.g., 50), so the world model does not have to terminate episodes.</p><p>Furthermore, we adopt the idea of randomly selecting the initial observation of a simulated episode from the collected real data. This enables the policy to learn from experience from any stage of the environment, although the number of time steps is limited. On the downside, this prevents the policy to learn from effects that are longer than the fixed number of steps <ref type="bibr" target="#b5">(Kaiser et al., 2020)</ref>.</p><p>Iterative Training. We adopt the iterative training procedure from <ref type="bibr" target="#b5">Kaiser et al. (2020)</ref> and alternate between interacting with the real environment, training the world model, and training the policy.</p><p>We also adopt the number of interactions per iteration, 6400, and the number of iterations, 15. The authors state that they perform additional 6400 interactions prior to the first iteration. Thus, we perform 12,800 interactions in the first iteration, resulting in the same number of total interactions, 12,800 + 6400 × 14 = 102,400. In the first iteration we use a random uniform policy.</p><p>Warming Up Latent Representations. After collecting the first batch of data, we train the VQ-VAE separately for 50 epochs with a higher learning rate. We want to give the dynamics model a better starting point, with representations that already contain useful information. This cannot be done in later training stages, as the dynamics model would not be able to keep up with the representations.</p><p>Fixed Representations. After warming up, we even slow down the state representation training by updating the parameters of the VQ-VAE only in every second training step, so the targets of the dynamics model are moving slower.</p><p>Reward Loss. If the gradients coming from the reward prediction head have a high magnitude, they have a degrading effect on the performance of the next latent prediction head. We solve this issue by scaling down the cross-entropy loss of the rewards to reduce its influence on the entire model, but using a higher learning rate for the reward prediction head to compensate for the smaller gradients.</p><p>Constant KL Term. <ref type="bibr" target="#b5">Kaiser et al. (2020)</ref> state that the weight of the KL divergence loss of a variational autoencoder is game dependent, which makes VAEs impractical to apply on all Atari games without fine-tuning. The VQ-VAE does not suffer from this problem, since the KL term is constant and depends on the number of embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>We compare our method with the variant of Kaiser et al.  . Mean episode reward across five training runs for three Atari environments. The x-axis shows the number of interactions with the real environment, and does not reflect the number of parameter updates that were performed in between. For SimPLe <ref type="bibr" target="#b5">(Kaiser et al., 2020)</ref> we only know the final score which is depicted by a straight line. This implies that the results of <ref type="bibr" target="#b5">Kaiser et al. (2020)</ref> that are shown are not necessarily the best across all of their variants, but the best for a fairer comparison with our method. We do not know the performance difference to <ref type="bibr" target="#b5">Kaiser et al. (2020)</ref> in terms of training time and test time, but considering the model size our method should be a lot faster.</p><p>We restrict our agent to 100K interactions with the environment and average the results over five training runs. For every run we evaluate the latest policy in each iteration by rolling out 32 episodes in the real environment and computing the mean of the (cumulative) episode rewards. In <ref type="table" target="#tab_2">Table 4</ref> we report the mean final episode rewards (i.e., the mean episode reward after the final iteration; averaged over five runs) for 36 Atari environments. Our method achieves a higher value than SimPLe <ref type="bibr" target="#b5">(Kaiser et al., 2020)</ref> in 20 out of 36 environments (when also considering no frame stacking, as explained below).</p><p>Learning Curves. <ref type="figure" target="#fig_3">Fig. 4</ref> shows three cases of learning curves that were typical for our model. First, <ref type="figure" target="#fig_3">Fig. 4a</ref> shows an example of an environment in which the agent's perfor-mance successfully increases over the course of training.</p><p>Secondly, <ref type="figure" target="#fig_3">Fig. 4b</ref> shows an example of an environment in which the agent's performance decreases over the course of training. This can have various reasons, e.g., when the agent reaches a new area of the state space and the environment dynamics change drastically. Finally, <ref type="figure" target="#fig_3">Fig. 4c</ref> shows an example of an environment in which the agent has comparably high performance starting from the first iteration, which is most likely due to model bias.</p><p>Latent Representations. In <ref type="figure" target="#fig_4">Fig. 5</ref> we can see that the state representation model is able to encode almost all information into the latent representation. However, the embedding indices are tuned for the decoder, so the dynamics model still has a hard task. We picked these two environments to show that changes in the scene <ref type="figure" target="#fig_4">(Fig. 5a</ref>) or even switching scenes <ref type="figure" target="#fig_4">(Fig. 5b)</ref> can be represented, although this can cause some loss of details (e.g., see the left reconstruction in <ref type="figure" target="#fig_4">Fig. 5b)</ref>.</p><p>No Frame Stacking. Our default model stacks the last four frames to incorporate short-time information into the state representations. On the downside, this introduces complexity, since the same frame can get a different representation, depending on the three other frames in the stack. This in turn will make it harder for the dynamics model to predict the next representation. The results show that no frame stacking improves the performance in some environments, as can be seen in <ref type="table" target="#tab_2">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploration.</head><p>In our experiments we observe the same phenomenon as <ref type="bibr" target="#b5">Kaiser et al. (2020)</ref>, namely that the results can vary drastically for different runs with the same hyperparameters but different random seeds. The main reasons might be that the world model cannot infer dynamics of regions of the environment's state space that it has never seen before, and that the algorithm is very sensitive to the exploration-exploitation trade-off, as the number of interactions is low. LSTM architecture. Instead of a convolutional LSTM, we also tried follow-up architectures like the spatio-temporal LSTM <ref type="bibr" target="#b11">(Wang et al., 2017)</ref> or causal LSTM <ref type="bibr" target="#b12">(Wang et al., 2018)</ref>, but for our problem the performance gain was not significant enough, compared with the imposed additional training time and number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and Future Work</head><p>Currently, our dynamics model samples the indices in the latent representation independently. This might be disadvantageous because conditional dependencies between the indices, which correspond to certain areas of the video frames, are ignored. So in the future it would be interesting to predict them autoregressively, e.g., with a conditional Pix-elCNN (van den Oord et al., 2016) conditioned on y t , to see whether this solves prediction errors like duplication or incoherent movement of objects. Nevertheless, an autoregressive model might have a negative impact on the training times (when training the dynamics model and when simulating experience), and it has to be seen whether the resulting training times are acceptable.</p><p>Another line of research should improve exploration in order to enable even higher sample efficiency. Furthermore, we would like the world model to predict terminal states and move away from terminating episodes after a fixed number of steps, since this introduces a new hyperparameter that needs to be tuned, and prevents the agent from learning beyond this time horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we demonstrate that a generative model with a discrete latent space can be used to strongly decrease the size of world models. We employ a VQ-VAE with a discrete two-dimensional latent space. We show that this powerful model is able to effectively encode the complex visual observations of Atari games. The chosen model and latent space have produced representations that are more stable and expressive at the same time. Our experiments show that acting entirely in latent space is possible, which speeds up training since no decoding into high-dimensional frames is required.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Visualization of the observation encoder architecture (top) and decoder architecture (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>A visualization of the architecture of the dynamics network. After the second convolutional LSTM cell the network splits into the reward prediction head g φ (yt) at the top and the next latent prediction head f φ (yt) at the bottom. The recurrent states ht, ht−1 of the LSTM are not visualized for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(stochastic discrete, 50 steps, γ = 0.99) that comes closest to our model in terms of hyperparameters (discount rate, batch size etc.) and number of parameter updates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4. Mean episode reward across five training runs for three Atari environments. The x-axis shows the number of interactions with the real environment, and does not reflect the number of parameter updates that were performed in between. For SimPLe (Kaiser et al., 2020) we only know the final score which is depicted by a straight line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of original game frames (top row), the encoded 6 × 6 discrete latent variables (middle row), and the reconstructions from the VQ-VAE (bottom row). We assign colors to the embedding indices and draw a colored square for each entry of the latent matrix. A square does not necessarily correspond to the same area in the frame as they have larger receptive fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Number of parameters of the world model compared with Kaiser et al. (2020) (their number is approximate).</figDesc><table><row><cell>Model</cell><cell># parameters</cell></row><row><cell>Ours</cell><cell>10,332,740</cell></row><row><cell>SimPLe</cell><cell>74,000,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Number of parameters of our models in detail.</figDesc><table><row><cell>The en-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison of our method (with and without frame stacking) with SimPLe<ref type="bibr" target="#b5">(Kaiser et al., 2020)</ref> and model-free PPO<ref type="bibr" target="#b8">(Schulman et al., 2017)</ref> trained with 100K steps. Our scores are the mean final episode reward, averaged over five runs ± standard deviation. The PPO scores are taken from<ref type="bibr" target="#b5">Kaiser et al. (2020)</ref>.</figDesc><table><row><cell></cell><cell>Ours (4 frames)</cell><cell>Ours (1 frame)</cell><cell>SimPLe (SD, γ = 0.99)</cell><cell>PPO 100K</cell></row><row><cell>Game</cell><cell>Mean Std. Dev.</cell><cell>Mean Std. Dev.</cell><cell>Mean Std. Dev.</cell><cell>Mean Std. Dev.</cell></row><row><cell>Alien</cell><cell>409.9 (± 73.0)</cell><cell>423.3 (± 48.1)</cell><cell>405.2 (± 130.8)</cell><cell>291.0 (± 40.3)</cell></row><row><cell>Amidar</cell><cell>37.6 (± 13.6)</cell><cell>30.5 (± 10.1)</cell><cell>88.0 (± 23.8)</cell><cell>56.5 (± 20.8)</cell></row><row><cell>Assault</cell><cell>375.4 (± 111.9)</cell><cell>408.3 (± 27.8)</cell><cell>369.3 (± 107.8)</cell><cell>424.2 (± 55.8)</cell></row><row><cell>Asterix</cell><cell>504.4 (± 53.3)</cell><cell>456.5 (± 146.4)</cell><cell>1089.5 (± 335.3)</cell><cell>385.0 (± 104.4)</cell></row><row><cell>Asteroids</cell><cell>862.9 (± 85.4)</cell><cell>989.9 (± 88.7)</cell><cell>731.0 (± 165.3)</cell><cell>1134.0 (± 326.9)</cell></row><row><cell>Atlantis</cell><cell cols="2">9413.1 (± 3349.8) 15463.7 (± 5478.7)</cell><cell>14481.6 (± 2436.9)</cell><cell>34316.7 (± 5703.8)</cell></row><row><cell>BankHeist</cell><cell>101.2 (± 17.4)</cell><cell>249.3 (± 49.8)</cell><cell>8.2 (± 4.4)</cell><cell>16.0 (± 12.4)</cell></row><row><cell>BattleZone</cell><cell>5631.2 (± 1179.1)</cell><cell>5531.3 (± 2515.4)</cell><cell>5184.4 (± 1347.5)</cell><cell>5300.0 (± 3655.1)</cell></row><row><cell>BeamRider</cell><cell>410.4 (± 55.4)</cell><cell>527.6 (± 61.8)</cell><cell>422.7 (± 103.6)</cell><cell>563.6 (± 189.4)</cell></row><row><cell>Bowling</cell><cell>27.9 (± 4.8)</cell><cell>24.5 (± 5.1)</cell><cell>34.4 (± 16.3)</cell><cell>17.7 11.2</cell></row><row><cell>Boxing Breakout</cell><cell>−2.8 (± 5.7) 8.8 (± 1.5)</cell><cell>−9.3 (± 12.6) 8.4 (± 1.5)</cell><cell>9.1 (± 8.8) 12.7 (± 3.8)</cell><cell>−3.9 (± 6.4) 5.9 (± 3.3)</cell></row><row><cell>ChopperCommand</cell><cell>766.2 (± 195.3)</cell><cell>590.6 (± 335.0)</cell><cell>1246.9 (± 392.0)</cell><cell>730.0 (± 199.0)</cell></row><row><cell>CrazyClimber</cell><cell cols="2">47536.9 (± 6114.9) 36923.8 (± 2780.6)</cell><cell>39827.8 (± 22582.6)</cell><cell>18400.0 (± 5275.1)</cell></row><row><cell>DemonAttack</cell><cell>195.0 (± 76.4)</cell><cell>211.3 (± 86.2)</cell><cell>169.5 (± 41.8)</cell><cell>192.5 (± 83.1)</cell></row><row><cell>FishingDerby Freeway</cell><cell>−89.6 (± 4.5) 24.6 (± 3.4)</cell><cell>−87.9 (± 4.1) 11.3 (± 9.6)</cell><cell>−91.5 (± 2.8) 20.3 (± 18.5)</cell><cell>−95.6 (± 4.3) 8.0 (± 9.8)</cell></row><row><cell>Frostbite</cell><cell>214.4 (± 10.2)</cell><cell>219.1 (± 45.6)</cell><cell>254.7 (± 4.9)</cell><cell>174.0 (± 40.7)</cell></row><row><cell>Gopher</cell><cell>687.2 (± 91.1)</cell><cell>1398.4 (± 166.5)</cell><cell>771.0 (± 160.2)</cell><cell>246.0 (± 103.3)</cell></row><row><cell>Gravitar</cell><cell>87.2 (± 60.9)</cell><cell>82.2 (± 64.5)</cell><cell>198.3 (± 39.9)</cell><cell>235.0 (± 197.2)</cell></row><row><cell>Hero</cell><cell>3453.6 (± 594.7)</cell><cell>3911.6 (± 1259.3)</cell><cell>1295.1 (± 1600.1)</cell><cell>569.0 (± 1100.9)</cell></row><row><cell>IceHockey Jamesbond</cell><cell>−13.6 (± 2.5) 66.6 (± 7.8)</cell><cell>−12.0 (± 2.1) 46.6 (± 24.2)</cell><cell>−10.5 (± 2.2) 125.3 (± 112.5)</cell><cell>−10.0 (± 2.1) 65.0 (± 46.4)</cell></row><row><cell>Kangaroo</cell><cell>245.0 (± 99.6)</cell><cell>276.3 (± 136.7)</cell><cell>323.1 (± 359.8)</cell><cell>140.0 (± 102.0)</cell></row><row><cell>Krull</cell><cell>3520.2 (± 211.4)</cell><cell>3241.0 (± 448.4)</cell><cell>4539.9 (± 2470.4)</cell><cell>3750.4 (± 3071.9)</cell></row><row><cell>KungFuMaster</cell><cell>11903.1 (± 4399.5)</cell><cell cols="2">8521.2 (± 1330.9) 17257.2 (± 5502.6)</cell><cell>4820.0 (± 983.2)</cell></row><row><cell>MsPacman</cell><cell>652.2 (± 92.6)</cell><cell>668.3 (± 86.4)</cell><cell>762.8 (± 331.5)</cell><cell>496.0 (± 379.8)</cell></row><row><cell>NameThisGame</cell><cell>2448.4 (± 179.5)</cell><cell>2119.4 (± 217.9)</cell><cell>1990.4 (± 284.7)</cell><cell>2225.0 (± 423.7)</cell></row><row><cell>Pong PrivateEye</cell><cell>11.8 (± 6.9) 99.4 (± 1.2)</cell><cell>−3.9 (± 7.6) 96.9 (± 5.4)</cell><cell>5.2 (± 9.7) 58.3 (± 45.4)</cell><cell>−20.5 (± 0.6) 10.0 (± 20.0)</cell></row><row><cell>Qbert</cell><cell>480.9 (± 143.5)</cell><cell>617.5 (± 149.5)</cell><cell>559.8 (± 183.8)</cell><cell>362.5 (± 117.8)</cell></row><row><cell>Riverraid</cell><cell>2100.2 (± 50.4)</cell><cell>2273.3 (± 188.5)</cell><cell>1587.0 (± 818.0)</cell><cell>1398.0 (± 513.8)</cell></row><row><cell>RoadRunner</cell><cell>1562.5 (± 440.2)</cell><cell>1723.8 (± 688.2)</cell><cell>5169.4 (± 3939.0)</cell><cell>1430.0 (± 760.0)</cell></row><row><cell>Seaquest</cell><cell>458.1 (± 155.7)</cell><cell>531.5 (± 105.1)</cell><cell>370.9 (± 128.2)</cell><cell>370.0 (± 103.3)</cell></row><row><cell>UpNDown</cell><cell>1128.2 (± 247.6)</cell><cell>1354.6 (± 741.4)</cell><cell>2152.6 (± 1192.4)</cell><cell>2874.0 (± 1105.8)</cell></row><row><cell>YarsRevenge</cell><cell>4096.0 (± 520.9)</cell><cell>4360.3 (± 1156.9)</cell><cell>2980.2 (± 778.6)</cell><cell>5182.0 (± 1209.3)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Department of Computer Science, Heinrich Heine University Düsseldorf, Düsseldorf, Germany. Correspondence to: Jan Robine &lt;jan.robine@hhu.de&gt;.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11252</idno>
		<title level="m">Stochastic variational video prediction</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Birmingham</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Aston University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent world models facilitate policy evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gar</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>nett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2450" to="2462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dream to control: Learning behaviors by latent imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01603</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mastering atari with discrete world models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02193</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model based reinforcement learning for atari</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miłos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Osiński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czechowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kozakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<editor>Bengio, Y. and LeCun, Y.</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The continuous bernoulli: fixing a pervasive error in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Loaiza-Ganem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alché-Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13287" to="13297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">;</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Fergus, R., Vishwanathan, S., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6306" to="6315" />
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PredRNN++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5123" to="5132" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
