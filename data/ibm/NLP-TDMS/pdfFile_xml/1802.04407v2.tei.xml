<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarially Regularized Graph Autoencoder for Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
							<email>shirui.pan@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Hu</surname></persName>
							<email>ruiqi.hu@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
							<email>guodong.long@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<email>jing.jiang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
							<email>lina.yao@unsw.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
							<email>chengqi.zhang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarially Regularized Graph Autoencoder for Graph Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph embedding is an effective method to represent graph data in a low dimensional space for graph analytics. Most existing embedding algorithms typically focus on preserving the topological structure or minimizing the reconstruction errors of graph data, but they have mostly ignored the data distribution of the latent codes from the graphs, which often results in inferior embedding in realworld graph data. In this paper, we propose a novel adversarial graph embedding framework for graph data. The framework encodes the topological structure and node content in a graph to a compact representation, on which a decoder is trained to reconstruct the graph structure. Furthermore, the latent representation is enforced to match a prior distribution via an adversarial training scheme. To learn a robust embedding, two variants of adversarial approaches, adversarially regularized graph autoencoder (ARGA) and adversarially regularized variational graph autoencoder (ARVGA), are developed. Experimental studies on real-world graphs validate our design and demonstrate that our algorithms outperform baselines by a wide margin in link prediction, graph clustering, and graph visualization tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs are essential tools to capture and model complicated relationships among data. In a variety of graph applications, including protein-protein interaction networks, social media, and citation networks, analyzing graph data plays an important role in various data mining tasks including node or graph classification <ref type="bibr" target="#b1">[Kipf and Welling, 2016a;</ref><ref type="bibr" target="#b3">Pan et al., 2016a]</ref>, link prediction <ref type="bibr" target="#b7">[Wang et al., 2017c]</ref>, and node clustering . However, the high computational complexity, low parallelizability, and inapplicability of machine learning methods to graph data have made these graph analytic tasks profoundly challenging . Recently graph embedding has emerged as a general approach to these problems. * These authors contribute equally to this work.</p><p>Graph embedding converts graph data into a low dimensional, compact, and continuous feature space. The key idea is to preserve the topological structure, vertex content, and other side information <ref type="bibr" target="#b7">[Zhang et al., 2017a]</ref>. This new learning paradigm has shifted the tasks of seeking complex models for classification, clustering, and link prediction to learning a robust representation of the graph data, so that any graph analytic task can be easily performed by employing simple traditional models (e.g., a linear SVM for the classification task). This merit has motivated a number of studies in this area <ref type="bibr" target="#b0">[Cai et al., 2017;</ref><ref type="bibr" target="#b0">Goyal and Ferrara, 2017]</ref>.</p><p>Graph embedding algorithms can be classified into three categories: probabilistic models, matrix factorization-based algorithms, and deep learning-based algorithms. Probabilistic models like DeepWalk <ref type="bibr" target="#b3">[Perozzi et al., 2014]</ref>, node2vec <ref type="bibr">[Grover and Leskovec, 2016]</ref> and LINE  attempt to learn graph embedding by extracting different patterns from the graph. The captured patterns or walks include global structural equivalence, local neighborhood connectivities, and other various order proximities. Compared with classical methods such as Spectral Clustering <ref type="bibr" target="#b4">[Tang and Liu, 2011]</ref>, these graph embedding algorithms perform more effectively and are scalable to large graphs.</p><p>Matrix factorization-based algorithms, such as <ref type="bibr">GraRep [Cao et al., 2015]</ref>, HOPE <ref type="bibr" target="#b2">[Ou et al., 2016]</ref>, M-NMF <ref type="bibr" target="#b6">[Wang et al., 2017b]</ref> pre-process the graph structure into an adjacency matrix and get the embedding by decomposing the adjacency matrix. Recently it has been shown that many probabilistic algorithms are equivalent to matrix factorization approaches <ref type="bibr" target="#b3">[Qiu et al., 2017]</ref>. Deep learning approaches, especially autoencoder-based methods, are also widely studied for graph embedding. SDNE  and DNGR <ref type="bibr" target="#b0">[Cao et al., 2016]</ref> employ deep autoencoders to preserve the graph proximities and model positive pointwise mutual information (PPMI). The MGAE algorithm utilizes a marginalized single layer autoencoder to learn representation for clustering .</p><p>The approaches above are typically unregularized approaches which mainly focus on preserving the structure relationship (probabilistic approaches), or minimizing the reconstruction error (matrix factorization or deep learning methods). They have mostly ignored the data distribution of the latent codes. In practice unregularized embedding approaches often learn a degenerate identity mapping where the latent code space is free of any structure <ref type="bibr" target="#b1">[Makhzani et al., 2015]</ref>, and can easily result in poor representation in dealing with real-world sparse and noisy graph data. One common way to handle this problem is to introduce some regularization to the latent codes and enforce them to follow some prior data distribution <ref type="bibr" target="#b1">[Makhzani et al., 2015]</ref>. Recently generative adversarial based frameworks <ref type="bibr" target="#b0">[Donahue et al., 2016;</ref><ref type="bibr" target="#b3">Radford et al., 2015]</ref> have also been developed for learning robust latent representation. However, none of these frameworks is specifically for graph data, where both topological structure and content information are required to embed to a latent space.</p><p>In this paper, we propose a novel adversarial framework with two variants, namely adversarially regularized graph autoencoder (ARGA) and adversarially regularized variational graph autoencoder (ARVGA), for graph embedding. The theme of our framework is to not only minimize the reconstruction errors of the graph structure but also to enforce the latent codes to match a prior distribution. By exploiting both graph structure and node content with a graph convolutional network, our algorithms encodes the graph data in the latent space. With a decoder aiming at reconstructing the topological graph information, we further incorporate an adversarial training scheme to regularize the latent codes to learn a robust graph representation. The adversarial training module aims to discriminate if the latent codes are from a real prior distribution or from the graph encoder. The graph encoder learning and adversarial regularization are jointly optimized in a unified framework so that each can be beneficial to the other and finally lead to a better graph embedding. The experimental results on benchmark datasets demonstrate the superb performance of our algorithms on three unsupervised graph analytic tasks, namely link prediction, node clustering, and graph visualization. Our contributions can be summarized below:</p><p>• We propose a novel adversarially regularized framework for graph embedding, which represent topological structure and node content in a continuous vector space. Our framework learns the embedding to minimize the reconstruction error while enforcing the latent codes to match a prior distribution.</p><p>• We develop two variants of adversarial approaches, adversarially regularized graph autoencoder (ARGA) and adversarially regularized variational graph autoencoder (ARVGA) to learn the graph embedding.</p><p>• Experiments on benchmark graph datasets demonstrate that our graph embedding approaches outperform the others on three unsupervised tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph Embedding Models. From the perspective of information exploration, graph embedding algorithms can be also separated into two groups: topological embedding approaches and content enhanced embedding methods. Topological embedding approaches assume that there is only topological structure information available, and the learning objective is to preserve the topological information maximumly. Perozzi et al. propose a DeepWalk model to learn the node embedding from a collection of random walks <ref type="bibr" target="#b3">[Perozzi et al., 2014]</ref>. Since then, a number of probabilistic models such as node2vec <ref type="bibr">[Grover and Leskovec, 2016]</ref> and LINE  have been developed. As a graph can be mathematically represented as an adjacency matrix, many matrix factorization approaches such as <ref type="bibr">GraRep [Cao et al., 2015]</ref>, HOPE <ref type="bibr" target="#b2">[Ou et al., 2016]</ref>, M-NMF <ref type="bibr" target="#b6">[Wang et al., 2017b]</ref> are proposed to learn the latent representation for a graph. Recently deep learning models have been widely exploited to learn the graph embedding. These algorithms preserve the first and second order of proximities , or reconstruct the positive pointwise mutual information (PPMI) <ref type="bibr" target="#b0">[Cao et al., 2016]</ref> via different variants of autoencoders.</p><p>Content enhanced embedding methods assume node content information is available and exploit both topological information and content features simultaneously. TADW <ref type="bibr" target="#b7">[Yang et al., 2015]</ref> presents a matrix factorization approach to explore node features. TriDNR <ref type="bibr" target="#b3">[Pan et al., 2016b]</ref> captures structure, node content, and label information via a tri-party neural network architecture. UPP-SNE employs an approximated kernel mapping scheme to exploit user profile features to enhance the embedding learning of users in social networks <ref type="bibr" target="#b8">[Zhang et al., 2017b]</ref>.</p><p>Unfortunately the above algorithms largely ignore the latent distribution of the embedding, which may result in poor representation in practice. In this paper, we explore adversarial training methods to address this issue. Adversarial Models. Our method is motivated by the generative adversarial network (GAN) <ref type="bibr" target="#b0">[Goodfellow et al., 2014]</ref>. GAN plays an adversarial game with two linked models: the generator G and the discriminator D. The discriminator can be a multi-layer perceptron which discriminates if an input sample comes from the data distribution or from the generator we built. Simultaneously, the generator is trained to generate the samples to convince the discriminator that the generated samples come from the prior data distribution. Due to its effectiveness in many unsupervised tasks, recently a number of adversarial training algorithms have been proposed <ref type="bibr" target="#b0">[Donahue et al., 2016;</ref><ref type="bibr" target="#b3">Radford et al., 2015]</ref>.</p><p>Recently Makhzani et al. proposed an adversarial autoencoder (AAE) to learn the latent embedding by merging the adversarial mechanism into the autoencoder <ref type="bibr" target="#b1">[Makhzani et al., 2015]</ref>. However, it is designed for general data rather than graph data. Dai et al. applied the adversarial mechanism to graphs. However, their approach can only exploit the topological information <ref type="bibr" target="#b0">[Dai et al., 2017]</ref>. In contrast, our algorithm is more flexible and can handle both topological and content information for graph data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition and Framework</head><p>A graph is represented as G = {V, E, X}, where V = {v i } i = 1, · · · , n consists of a set of nodes in a graph and e i,j =&lt; v i , v j &gt;∈ E represents a linkage encoding the citation edge between the nodes. The topological structure of graph G can be represented by an adjacency matrix A, where  <ref type="figure">Figure 1</ref>: The architecture of the adversarially regularized graph autoencoder (ARGA). The upper tier is a graph convolutional autoencoder that reconstructs a graph A from an embedding Z which is generated by the encoder which exploits graph structure A and the node content matrix X. The lower tier is an adversarial network trained to discriminate if a sample is generated from the embedding or from a prior distribution. The adversarially regularized variational graph autoencoder (ARVGA) is similar to ARGA except that it employs a variational graph autoencoder in the upper tier (See Algorithm 1 for details).</p><formula xml:id="formula_0">A i,j = 1 if e i,j ∈ E, otherwise A i,j = 0. x i ∈ X indicates</formula><p>the content features associated with each node v i . Given a graph G, our purpose is to map the nodes</p><formula xml:id="formula_1">v i ∈ V to low-dimensional vectors z i ∈ R d with the formal format as follows: f : (A, X) Z, where z i is the i-th row of the matrix Z ∈ R n×d .</formula><p>n is the number of nodes and d is the dimension of embedding. We take Z as the embedding matrix and the embeddings should well preserve the topological structure A as well as content information X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Framework</head><p>Our objective is to learn a robust embedding given a graph G = {V, E, X}. To this end, we leverage an adversarial architecture with a graph autoencoder to directly process the entire graph and learn a robust embedding. <ref type="figure">Figure 1</ref> demonstrates the workflow of ARGA which consists of two modules: the graph autoencoder and the adversarial network.</p><p>• Graph Convolutional Autoencoder. The autoencoder takes in the structure of graph A and the node content X as inputs to learn a latent representation Z, and then reconstructs the graph structure A from Z.</p><p>• Adversarial Regularization. The adversarial network forces the latent codes to match a prior distribution by an adversarial training module, which discriminates whether the current latent code z i ∈ Z comes from the encoder or from the prior distribution.</p><p>4 Proposed Algorithm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Convolutional Autoencoder</head><p>The graph convolutional autoencoder aims to embed a graph G = {V, E, X} in a low-dimensional space. Two key questions arise (1) how to integrate both graph structure A and node content X in an encoder, and <ref type="formula" target="#formula_3">(2)</ref> what sort of information should be reconstructed via a decoder?</p><p>Graph Convolutional Encoder Model G(X, A). To represent both graph structure A and node content X in a unified framework, we develop a variant of the graph convolutional network (GCN) <ref type="bibr" target="#b1">[Kipf and Welling, 2016a</ref>] as a graph encoder. Our graph convolutional network (GCN) extends the operation of convolution to graph data in the spectral domain, and learns a layer-wise transformation by a spectral convolution function f (Z (l) , A|W (l) ):</p><formula xml:id="formula_2">Z (l+1) = f (Z (l) , A|W (l) )<label>(1)</label></formula><p>Here, Z l is the input for convolution, and Z (l+1) is the output after convolution. We have Z 0 = X ∈ R n×m (n nodes and m features) for our problem. W (l) is a matrix of filter parameters we need to learn in the neural network. If f (Z (l) , A|W (l) ) is well defined, we can build arbitrary deep convolutional neural networks efficiently. Each layer of our graph convolutional network can be expressed with the function f (Z (l) , A|W (l) ) as follows:</p><formula xml:id="formula_3">f (Z (l) , A|W (l) ) = φ( D − 1 2 A D − 1 2 Z (l) W (l) ),<label>(2)</label></formula><p>where A = A + I and D ii = j A ij . I is the identity matrix of A and φ is an activation function such as Relu(t) = max(0, t) or sigmoid(t) = 1 1+e t . Overall, the graph encoder G(X, A) is constructed with a two-layer GCN. In our paper, we develop two variants of encoder, e.g., Graph Encoder and Variational Graph Encoder.</p><p>The Graph Encoder is constructed as follows:</p><formula xml:id="formula_4">Z (1) = f Relu (X, A|W (0) );<label>(3)</label></formula><formula xml:id="formula_5">Z (2) = f linear (Z (1) , A|W (1) ).<label>(4)</label></formula><p>Relu(·) and linear activation functions are used for the first and second layers. Our graph convolutional encoder G(Z, A) = q(Z|X, A) encodes both graph structure and node content into a representation Z = q(Z|X, A) = Z (2) . A Variational Graph Encoder is defined by an inference model:</p><formula xml:id="formula_6">q(Z|X, A) = n i=1 q(z i |X, A),<label>(5)</label></formula><formula xml:id="formula_7">q(z i |X, A) = N (z i |µ i , diag(σ 2 ))<label>(6)</label></formula><p>Here, µ = Z (2) is the matrix of mean vectors z i ; similarly logσ = f linear (Z (1) , A|W (1) ) which share the weights W (0) with µ in the first layer in Eq. (3). Decoder Model. Our decoder model is used to reconstruct the graph data. We can reconstruct either the graph structure A, content information X, or both. In our paper, we propose to reconstruct graph structure A, which provides more flexibility in the sense that our algorithm will still function properly even if there is no content information X available (e.g., X = I). Our decoder p(Â|Z) predicts whether there is a link between two nodes. More specifically, we train a link prediction layer based on the graph embedding:</p><formula xml:id="formula_8">p(Â|Z) = n i=1 n j=1 p(Â ij |z i , z j ); (7) p(Â ij = 1|z i , z j ) = sigmoid(z i , z j ),<label>(8)</label></formula><p>Graph Autoencoder Model.</p><p>The embedding Z and the reconstructed graphÂ can be presented as follows:</p><formula xml:id="formula_9">A = sigmoid(ZZ ), here Z = q(Z|X, A)<label>(9)</label></formula><p>Optimization.</p><p>For the graph encoder, we minimize the reconstruction error of the graph data by:</p><formula xml:id="formula_10">L 0 = E q(Z|(X,A)) [log p(Â|Z)]<label>(10)</label></formula><p>For the variational graph encoder, we optimize the variational lower bound as follows:</p><formula xml:id="formula_11">L 1 = E q(Z|(X,A)) [log p(Â|Z)] − KL[q(Z|X, A) p(Z)] (11) where KL[q(•)||p(•)]</formula><p>is the Kullback-Leibler divergence between q(•) and p(•). We also take a Gaussian prior p(Z) = i p(z i ) = i N (z i |0, I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Adversarial Model D(Z)</head><p>The key idea of our model is to enforce latent representation Z to match a prior distribution, which is achieved by an adversarial training model. The adversarial model is built on a standard multi-layer perceptron (MLP) where the output layer only has one dimension with a sigmoid function. The adversarial model acts as a discriminator to distinguish whether a latent code is from the prior p z (positive) or from graph encoder G(X, A) (negative). By minimizing the cross-entropy cost for training the binary classifier, the embedding will finally be regularized and improved during the training process. The cost can be computed as follows: <ref type="figure">D(G(X, A)</ref>)), <ref type="formula" target="#formula_2">(12)</ref> Algorithm 1 Adversarially Regularized Graph Embedding</p><formula xml:id="formula_12">− 1 2 E z∼pz logD(Z) − 1 2 E X log(1 −</formula><p>Require: G = {V, E, X}: a Graph with links and features; T : the number of iterations; K: the number of steps for iterating discriminator; d: the dimension of the latent variable Ensure: Z ∈ R n×d 1: for iterator = 1,2,3, · · · · · · , T do 2: Generate latent variables matrix Z through Eq.(4); 3:</p><p>for k = 1,2, · · · · · · , K do 4:</p><p>Sample m entities {z (1) , . . . , z (m) } from latent matrix Z 5:</p><p>Sample m entities {a (1) , . . . , a (m) } from the prior distribution pz 6:</p><p>Update the discriminator with its stochastic gradient:</p><formula xml:id="formula_13">1 m m i=1 [log D(a i ) + log (1 − D(z (i) ))]</formula><p>end for 7:</p><p>Update the graph autoencoder with its stochastic gradient by Eq. (10) for ARGA or Eq. (11) for ARVGA; end for 8: return Z ∈ R n×d</p><p>In our paper, we use simple Gaussian distribution as p z . Adversarial Graph Autoencoder Model. The equation for training the encoder model with Discriminator D(Z) can be written as follows: <ref type="figure">D(G(X, A)</ref>))] <ref type="formula" target="#formula_2">(13)</ref> where G(X, A) and D(Z) indicate the generator and discriminator explained above.</p><formula xml:id="formula_14">min G max D Ez∼p z [logD(Z)]+E x∼p(x) [log(1−</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Algorithm Explanation</head><p>Algorithm 1 is our proposed framework. Given a graph G, the step 2 gets the latent variables matrix Z from the graph convolutional encoder. Then we take the same number of samples from the generated Z and the real data distribution p z in step 4 and 5 respectively, to update the discriminator with the cross-entropy cost computed in step 6. After K runs of training the discriminator, the graph encoder will try to confuse the trained discriminator and update itself with generated gradient in step 7. We can update Eq. (10) to train the adversarially regularized graph autoencoder (ARGA), or Eq. (11) to train the adversarially regularized variational graph autoencoder (ARVGA), respectively. Finally, we will return the graph embedding Z ∈ R n×d in step 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We report our results on three unsupervised graph analytic tasks: link prediction, node clustering, and graph visualization. The benchmark graph datasets used in the paper are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Each data set consists of scientific publications as nodes and citation relationships as edges. The features are unique words in each document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Link Prediction</head><p>Baselines. We compared our algorithms against state-ofthe-art algorithms for the link prediction task:  <ref type="bibr">et al., 2014]</ref>: is a network representation approach which encodes social relations into a continuous vector space. • Spectral Clustering <ref type="bibr" target="#b4">[Tang and Liu, 2011]</ref>: is an effective approach for learning social embedding. • GAE <ref type="bibr" target="#b1">[Kipf and Welling, 2016b]</ref>: is the most recent autoencoder-based unsupervised framework for graph data, which naturally leverages both topological and content information. • VGAE <ref type="bibr" target="#b1">[Kipf and Welling, 2016b]</ref>: is a variational graph autoencoder approach for graph embedding with both topological and content information. • ARGA: Our proposed adversarially regularized autoencoder algorithm which uses graph autoencoder to learn the embedding. • ARVGA: Our proposed algorithm, which uses a variational graph autoencoder to learn the embedding. Metrics. We report the results in terms of AUC score (the area under a receiver operating characteristic curve) and average precision (AP) <ref type="bibr" target="#b1">[Kipf and Welling, 2016b]</ref> score. We conduct each experiment 10 times and report the mean values with the standard errors as the final scores. Each dataset is separated into a training, testing set and validation set. The validation set contains 5% citation edges for hyperparameter optimization, the test set holds 10% citation edges to verify the performance, and the rest are used for training. Parameter Settings. For the Cora and Citeseer data sets, we train all autoencoder-related models for 200 iterations and optimize them with the Adam algorithm. Both learning rate and discriminator learning rate are set as 0.001. As the PubMed data set is relatively large (around 20,000 nodes), we iterate 2,000 times for an adequate training with a 0.008 discriminator learning rate and 0.001 learning rate. We construct encoders with a 32-neuron hidden layer and a 16-neuron embedding layer for all the experiments and all the discriminators are built with two hidden layers(16-neuron, 64-neuron respectively). For the rest of the baselines, we retain to the settings described in the corresponding papers. Experimental Results.</p><p>The details of the experimental results on the link prediction are shown in <ref type="table" target="#tab_3">Table 2</ref>. The results show that by incorporating an effective adversarial training module into our graph convolutional autoencoder, ARGA and ARVGA achieve outstanding performance: all AP and AUC scores are as higher as 92% on all three data sets. Compared with all the baselines, ARGE increased the AP score from around 2.5% compared with VGAE incorporating with node features, 11% compared with VGAE without node features; 15.5% and 10.6% compared with DeepWalk and Spectral Clustering respectively on the large PubMed data set . Parameter Study. We vary the dimension of embedding from 8 neurons to 1024 and report the results in <ref type="figure" target="#fig_1">Fig 2.</ref> The results from both <ref type="figure" target="#fig_1">Fig 2 (A)</ref> and (B) reveal similar trends: when adding the dimension of embedding from 8neuron to 16-neuron, the performance of embedding on link prediction steadily rises; but when we further increase the number of the neurons at the embedding layer to 32-neuron, the performance fluctuates however the results for both the AP score and the AUC score remain good.</p><p>It is worth mentioning that if we continue to set more neurons, for examples, 64-neuron, 128-neuron and 1024-neuron, the performance rises markedly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Node Clustering</head><p>For the node clustering task, we first learn the graph embedding, and then perform K-means clustering algorithm based on the embedding. Baselines. We compare both embedding based approaches as well as approaches directly for graph clustering. Except for the baselines we compared for link prediction, we also include baselines which are designed for clustering:</p><p>1. K-means is a classical method and also the foundation of many clustering algorithms. 2. Graph Encoder <ref type="bibr" target="#b4">[Tian et al., 2014]</ref>  for network representation learning. Here the first three algorithms only exploit the graph structures, while the last three algorithms use both graph structure and node content for the graph clustering task. Metrics. Following <ref type="bibr" target="#b7">[Xia et al., 2014]</ref>, we employ five metrics to validate the clustering results: accuracy (Acc), normalized mutual information (NMI), precision, F-score (F1) and average rand index (ARI). Experimental Results. The clustering results on the Cora and Citeseer data sets are given in <ref type="table" target="#tab_4">Table 3 and Table 4</ref>. The results show that ARGA and ARVGA have achieved a dramatic   improvement on all five metrics compared with all the other baselines. For instance, on Citeseer, ARGA has increased the accuracy from 6.1% compared with K-means to 154.7% compared with GraphEncoder; increased the F1 score from 31.9% compared with TADW to 102.2% compared with DeepWalk; and increased NMI from 14.8% compared with K-means to 124.4% compared with VGAE. The wide margin in the results between ARGE and GAE (and the others) has further proved the superiority of our adversarially regularized graph autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Graph Visualization</head><p>We visualize the Cora data in a two-dimensional space by applying the t-SNE algorithm <ref type="bibr" target="#b1">[Maaten, 2014]</ref> on the learned embedding. The results in <ref type="figure">Fig 3 validate</ref> that by applying adversarial training to the graph data, we can obtained a more meaningful layout of the graph data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a novel adversarial graph embedding framework for graph data. We argue that most existing graph embedding algorithms are unregularized methods that ignore the data distributions of the latent representation and suffer from inferior embedding in real-world graph data. We proposed an adversarial training scheme to regularize the latent codes and enforce the latent codes to match a prior distribution. The adversarial module is jointly learned with a graph convolutional autoencoder to produce a robust representation. Experiment results demonstrated that our algorithms ARGA and ARVGA outperform baselines in link prediction, node clustering, and graph visualization tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Average performance on different dimensions of the embedding. (A) Average Precision score; (B) AUC score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Real-world Graph Datasets Used in the Paper</figDesc><table><row><cell cols="5">Data Set # Nodes # Links # Content Words # Features</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>3,880,564</cell><cell>1,433</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>12,274,336</cell><cell>3,703</cell></row><row><cell>PubMed</cell><cell>19,717</cell><cell>44,338</cell><cell>9,858,500</cell><cell>500</cell></row></table><note>• DeepWalk [Perozzi</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>RTM [Chang and Blei, 2009]  learns the topic distributions of each document from both text and citation. 5. RMSC [Xia et al., 2014] employs a multi-view learning approach for graph clustering. 6. TADW [Yang et al., 2015] applies matrix factorization</figDesc><table><row><cell>learns graph embed-</cell></row><row><cell>ding for spectral graph clustering.</cell></row><row><cell>3. DNGR [Cao et al., 2016] trains a stacked denoising au-</cell></row><row><cell>toencoder for graph embedding.</cell></row><row><cell>4.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results for Link Prediction. GAE * and VGAE * are variants of GAE, which only explore topological structure, i.e., X = I.Figure 3: The Cora data visualization comparison. From left to right: embeddings from our ARGA, VGAE, GAE, DeepWalk, and Spectral Clustering. The different colors represent different groups.</figDesc><table><row><cell>Cora</cell><cell>Acc</cell><cell>NMI</cell><cell>F1</cell><cell>Precision</cell><cell>ARI</cell></row><row><cell>K-means</cell><cell cols="3">0.492 0.321 0.368</cell><cell>0.369</cell><cell>0.230</cell></row><row><cell>Spectral</cell><cell cols="3">0.367 0.127 0.318</cell><cell>0.193</cell><cell>0.031</cell></row><row><cell cols="4">GraphEncoder 0.325 0.109 0.298</cell><cell>0.182</cell><cell>0.006</cell></row><row><cell>DeepWalk</cell><cell cols="3">0.484 0.327 0.392</cell><cell>0.361</cell><cell>0.243</cell></row><row><cell>DNGR</cell><cell cols="3">0.419 0.318 0.340</cell><cell>0.266</cell><cell>0.142</cell></row><row><cell>RTM</cell><cell cols="3">0.440 0.230 0.307</cell><cell>0.332</cell><cell>0.169</cell></row><row><cell>RMSC</cell><cell cols="3">0.407 0.255 0.331</cell><cell>0.227</cell><cell>0.090</cell></row><row><cell>TADW</cell><cell cols="3">0.560 0.441 0.481</cell><cell>0.396</cell><cell>0.332</cell></row><row><cell>GAE</cell><cell cols="3">0.596 0.429 0.595</cell><cell>0.596</cell><cell>0.347</cell></row><row><cell>VGAE</cell><cell cols="3">0.609 0.436 0.609</cell><cell>0.609</cell><cell>0.346</cell></row><row><cell>ARGE</cell><cell cols="3">0.640 0.449 0.619</cell><cell>0.646</cell><cell>0.352</cell></row><row><cell>ARVGE</cell><cell cols="3">0.638 0.450 0.627</cell><cell>0.624</cell><cell>0.374</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Clustering Results on Cora</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Clustering Results on Citeseer</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was funded by the Australian Government through the Australian Research Council (ARC) under grants 1) LP160100630 partnership with Australia Government Department of Health and 2) LP150100671 partnership with Australia Research Alliance for Children and Youth (ARACY) and Global Business College Australia (GBCA). We acknowledge the support of NVIDIA Corporation and MakeMagic Australia with the donation of GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07604</idno>
		<idno>arXiv:1705.02801</idno>
	</analytic>
	<monogr>
		<title level="m">A comprehensive survey of graph embedding: Problems, techniques and applications</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>SIGKDD</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">T</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling ; T. N Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">V D</forename><surname>Welling ; L</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maaten ; Makhzani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<idno>arXiv:1511.05644</idno>
	</analytic>
	<monogr>
		<title level="m">Semisupervised classification with graph convolutional networks</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial autoencoders. arXiv preprint</note>
	<note>Variational graph auto-encoders. NIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint structure feature exploration and regularization for multitask graph classification</title>
		<idno type="arXiv">arXiv:1710.02971</idno>
		<idno>arXiv:1511.06434</idno>
	</analytic>
	<monogr>
		<title level="m">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Network embedding as matrix factorization: Unifyingdeepwalk, line, pte, and node2vec</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Leveraging social media networks for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
	<note>WWW</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mgae: Marginalized graph autoencoder for graph clustering</title>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="203" to="209" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust multi-view spectral clustering via low-rank and sparse decomposition</title>
		<idno type="arXiv">arXiv:1801.05852</idno>
	</analytic>
	<monogr>
		<title level="m">Network representation learning: A survey</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2111" to="2117" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>SIGIR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">User profile preserving social network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJ-CAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3378" to="3384" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
