<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Unsupervised Image Clustering With Robust Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Data Science Group</orgName>
								<orgName type="department" key="dep2">Institute for Basic Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Data Science Group</orgName>
								<orgName type="department" key="dep2">Institute for Basic Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundong</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Data Science Group</orgName>
								<orgName type="department" key="dep2">Institute for Basic Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danu</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Data Science Group</orgName>
								<orgName type="department" key="dep2">Institute for Basic Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungkyu</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Data Science Group</orgName>
								<orgName type="department" key="dep2">Institute for Basic Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meeyoung</forename><surname>Cha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Data Science Group</orgName>
								<orgName type="department" key="dep2">Institute for Basic Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Unsupervised Image Clustering With Robust Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised image clustering methods often introduce alternative objectives to indirectly train the model and are subject to faulty predictions and overconfident results. To overcome these challenges, the current research proposes an innovative model RUC that is inspired by robust learning. RUC's novelty is at utilizing pseudo-labels of existing image clustering models as a noisy dataset that may include misclassified samples. Its retraining process can revise misaligned knowledge and alleviate the overconfidence problem in predictions. The model's flexible structure makes it possible to be used as an add-on module to other clustering methods and helps them achieve better performance on multiple datasets. Extensive experiments show that the proposed model can adjust the model confidence with better calibration and gain additional robustness against adversarial noise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unsupervised clustering is a core task in computer vision that aims to identify each image's class membership without using any labels. Here, a class represents the group membership of images that share similar visual characteristics. Many studies have proposed deep learning-based algorithms that utilize distance in a feature space as the similarity metric to assign data points into classes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>Training without ground-truth guidance, however, is prone to finding trivial solutions that are learned from lowlevel visual traits like colors and textures <ref type="bibr" target="#b21">[22]</ref>. Several studies have introduced innovative ways to guide the model's training indirectly by setting alternative objectives. For example, Hu et al. <ref type="bibr" target="#b19">[20]</ref> proposed to maximize the mutual information between input and its hidden representations, and Ji et al. <ref type="bibr" target="#b21">[22]</ref> proposed to learn invariant features against data augmentation. Entropy-based balancing has often been adopted to prevent degenerate solutions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Nevertheless, these alternative objectives are bound to * Equal contribution to this work. <ref type="figure">Figure 1</ref>: Illustration for this work's basic concept: robust learning is used to separate clean data from unclean data using pseudo-labels from off-the-shelf unsupervised clustering algorithm.</p><p>producing overconfident results, i.e., low-entropy predictions, due to the dense grouping among clusters. When uncertain samples are added to a wrong cluster at an early stage of training, the model gradually becomes overconfident in its later predictions as the noise from misclassification accumulates and degrades the overall performance. This paper introduces a novel robust learning training method, RUC (Robust learning for Unsupervised Clustering), that runs in conjunction with existing clustering models to alleviate the noise discussed above. Utilizing and treating the existing clustering model's results as a noisy dataset that may include wrong labels, RUC updates the model's misaligned knowledge. Bringing insights from the literature, we filter out unclean samples and apply loss correction as in <ref type="figure">Fig. 1</ref>. This process is assisted by label smoothing and co-training to reduce any wrong gradient signals from unclean labels. This retraining process with revised pseudo-labels further regularizes the model and prevents overconfident results.</p><p>RUC comprises two key components: (1) extracting clean samples and <ref type="bibr" target="#b1">(2)</ref> retraining with the refined dataset. We propose confidence-based, metric-based, and hybrid strategies to filter out misclassified pseudo-labels. The first strategy considers samples of high prediction confidence from the original clustering model as a clean set; it filters out low confidence samples. This strategy relies on the model's calibration performance. The second strategy utilizes similarity metrics from unsupervised embedding models to detect clean samples with non-parametric classifiers by checking whether the given instance shares the same labels with top k-nearest samples. The third strategy combines the above two and selects samples that are credible according to both strategies. The next step is to retrain the clustering model with the sampled dataset. We use MixMatch <ref type="bibr" target="#b4">[5]</ref>, a semi-supervised learning technique; which uses clean samples as labeled data and unclean samples as unlabeled data. We then adopt label smoothing to leverage strong denoising effects on the label noise <ref type="bibr" target="#b28">[29]</ref> and block learning from overconfident samples <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42]</ref>. Finally, a co-training architecture with two networks is used to mitigate noise accumulation from the unclean samples during training and increase performance.</p><p>We evaluate RUC with rigorous experiments on datasets, including CIFAR-10, CIFAR-20, STL-10, and ImageNet-50. Combining RUC to an existing clustering model outperforms the state-of-the-art results with the accuracy of 90.3% in CIFAR-10, 54.3% in CIFAR-20, 86.7% in STL-10, and 78.5% in ImageNet-50 dataset. RUC also enhances the baseline model to be robust against adversarial noise. Our contributions are as follows:</p><p>• The proposed algorithm RUC aids existing unsupervised clustering models via retraining and avoiding overconfident predictions. • The unique retraining process of RUC helps existing models boost performance. It achieves a 5.3pp increase for the STL-10 dataset when added to the stateof-the-art model (81.4% to 86.7%). • The ablation study shows every component in RUC is critical, including the three proposed strategies (i.e., confidence-based, metric-based, and hybrid) that excel in extracting clean samples from noisy pseudo-labels. • The proposed training process is robust against adversarial noise and can adjust the model confidence with better calibrations. Implementation details of the model and codes are available at https://github.com/deu30303/RUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Unsupervised Image Clustering</head><p>The main objective of clustering is to group the data points into distinct classes of similar traits <ref type="bibr" target="#b20">[21]</ref>. Most realworld problems deal with high dimensional data (e.g., images), and thereby, setting a concrete notion of similarity while extracting low-dimensional features becomes key components for setting appropriate standards for grouping <ref type="bibr" target="#b48">[49]</ref>. Likewise, unsupervised clustering is a line of research aiming to tackle both dimensionality reduction and boundary identification over the learned similarity met-ric <ref type="bibr" target="#b16">[17]</ref>. Existing research can be categorized into sequential, joint, and multi-step refinement approach.</p><p>Sequential approach. Sequential approach extracts features, then sequentially applies the conventional distance or density-based clustering algorithm for class assignments. For example, Ding et al. <ref type="bibr" target="#b10">[11]</ref> use principal component analysis to extract low-dimensional features and then apply k-means clustering to assign classes. For feature extraction, autoencoder structures are often used to extract latent features before grouping, types of autoencoder include stacked <ref type="bibr" target="#b43">[44]</ref>, boolean <ref type="bibr" target="#b2">[3]</ref>, or variational autoencoder <ref type="bibr" target="#b23">[24]</ref>. However, these models tend to produce features with little separation among clusters due to the lack of knowledge on subsequent assignment processes.</p><p>Joint approach. The joint approach's characteristic is to use an end-to-end pipeline that concurrently performs feature extraction and class assignment. An example is Yang et al. <ref type="bibr" target="#b50">[51]</ref>, which adopt the concept of clustering loss to guarantee enough separations among clusters. End-to-end CNN pipelines are used widely to iteratively identify clusters while refining extracted features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b48">49]</ref>. Recent studies have shown that a mutual information-based objective is an effective measure to improve classification accuracy <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>. Nonetheless, those models still bear the problem of generating unintended solutions that depend on trivial low-level features from random initialization <ref type="bibr" target="#b16">[17]</ref>.</p><p>Multi-step refinement approach. To mitigate the unintended trivial solutions, recent approaches leverage the power of unsupervised embedding learning models to provide better initialization for downstream clustering tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>. These methods generate feature representations to gather data points with similar visual traits and push away the rest in an embedding space. With the initialization, clustering results are elaborated in a refinement step, bringing significant gain in its class assignment quality <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref>. In particular, SCAN <ref type="bibr" target="#b41">[42]</ref> first obtains high-level feature representations by feature similarity then clusters those representations by nearest neighbors, and this model has shown remarkable performance on unsupervised clustering.</p><p>Add-on modules to improve unsupervised clustering. The proposed retraining process with sample selection strategy improves off-the-shelf unsupervised clustering algorithms (e.g., sequential, joint, multi-step refinement) by acting as an add-on module. Our module's main objective is to revise the misaligned knowledge of trained clustering models via label cleansing and retraining with the refined labels. This method has not been well investigated before but has begun to be proposed recently. Gupta et al. <ref type="bibr" target="#b13">[14]</ref> show that semi-supervised retraining improves unsupervised clustering. They draw a graph where data samples are nodes, and the confidence from ensemble models <ref type="figure">Figure 2</ref>: Illustration of the proposed model. Our model first selects clean samples as a labeled dataset X and considers the remaining samples as an unlabeled dataset U (Section 3.1). Next, we train two networks f θ <ref type="bibr" target="#b0">(1)</ref> and f θ <ref type="bibr" target="#b1">(2)</ref> in a semi-supervised fashion (Section 3.2). In each epoch, the MixMatch algorithm, along with co-training and label smoothing, is applied for training. The clean set is updated via co-refurbishing for the next epoch.</p><p>between the samples is an edge. Then, a dense sub-graph is considered as a clean set.</p><p>The main difference between Gupta et al. and ours is in how we treat the pseudo-labels obtained by the clustering. Gupta et al. treats the pseudo-label as a ground-truth for semi-supervised learning, which produces sub-optimal result if the pseudo-label is noisy (i.e., memorization). In contrast, we introduce the robust learning concept of label smoothing and co-training to mitigate the memorization of noisy samples, which leads to substantial improvements in the calibration and clustering performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Robust Learning With Label Noise</head><p>A widely used setting for robust learning is where an adversary has deliberately corrupted the labels, which otherwise arise from some clean distribution <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref>. According to the literature, deep networks easily overfit to the label noise during training and get a low generalization power <ref type="bibr" target="#b27">[28]</ref>. In this light, models that prevent overfitting in a noise label environment have been studied.</p><p>Loss correction. The first representative line of work is a loss correction, which relabels unclean samples explicitly or implicitly. For example, Patrini et al. <ref type="bibr" target="#b34">[35]</ref> estimate the label transition probability matrix to correct the loss and retrain the model. To estimate the transition matrix more accurately, the gold loss correction approach <ref type="bibr" target="#b18">[19]</ref> is proposed to utilize trusted labels as additional information.</p><p>Loss reweighting. The next line of work is loss reweighting, which aims to give a smaller weight to the loss of unclean samples so that model can reduce the negative effect of label noise during training. One work computes the importance as an approximated ratio of two data distributions; clean and unclean <ref type="bibr" target="#b45">[46]</ref>. On the other hand, the active bias approach <ref type="bibr" target="#b6">[7]</ref> calculates the inconsistency of predictions during training and assigns a weight to penalize unclean data.</p><p>Sample selection. Relabeling the misclassified samples may cause a false correction. In this context, recent works introduce a sample selection approach that filters out misclassified samples and only selects clean data for training <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>. Notably, the small loss trick, which regards the sample with small training loss as clean, effectively separates true-and false-labeled data points <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>. Also, recent studies suggest diverse ways to lead additional performance by maintaining two networks to avoid accumulating sampling bias <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b53">54]</ref>, adopting refurbishment of falselabeled samples <ref type="bibr" target="#b38">[39]</ref>, or using a semi-supervised approach to utilize false-labeled sample maximally <ref type="bibr" target="#b26">[27]</ref>. Our model advances some of these sample selection approaches to filter out unclean samples out of clustering results and utilize clean samples only during retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>RUC is an add-on method that can be used in conjunction with the existing unsupervised clustering methods to refine mispredictions. Its key idea is at utilizing the initial clustering results as noisy pseudo-labels and learning to refine them with a mild clustering assumption <ref type="bibr" target="#b40">[41]</ref> and techniques from the robust learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>. <ref type="figure">Figure 2</ref> and Algorithm 1 illustrate the overall pipeline of the proposed algorithm. Given the initial pseudo-labels, we first divide the training data into the two disjoint sets: clean and unclean (Section 3.1). Then treating these sets each as labeled and unlabeled data, we train a classifier in a semi-supervised manner while refurbishing the labeled and unlabeled data (Section 3.2). We guide the semi-supervised class assignment with robust learning techniques, such as co-training and label smoothing, to account for inherent label noises. These techniques are useful in handling label noises and calibrating the model's prediction score. Below we describe the model in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Extracting Clean Samples</head><formula xml:id="formula_0">Let D = {(x i , y i )} N i=1</formula><p>denote the training data, where x i is an image and y i = g φ (x i ) is a pseudo-label from Algorithm 1 Robust learning algorithm using unsupervised clustering pseudo-label. Input: Sampling strategy S, training dataset with pseudo-labels D, two networks f θ <ref type="bibr" target="#b0">(1)</ref> , f θ <ref type="bibr" target="#b1">(2)</ref> , sharpening temperature T , number of augmentations M , unsupervised loss weight λ U , refurbish threshold τ 2 , weak-and strong augmentation φ a , φ A / * Divide the dataset D into clean and noisy set using a sampling strategy * /</p><formula xml:id="formula_1">X , U = S(D) (i.e. X = {(x b , y b ) : b ∈ (1, ..., B)}, U = {u b : b ∈ (1, ..., B)}) for k ∈ {1, 2} do / * Train the two networks f θ (1) and f θ (2) iteratively * / for b ∈ {1, .., B} dõ y b = (1 − ) · y b + (C−1) · (1 − y b ) // Inject uniform noise into all classes (label smoothing) for m ∈ {1, .., M } do x b,m , u b,m = φa(x b ), φa(u b ) // Perform weak augmentation M times end y b = (1 − w (c) b ) · y b + w (c) b · f θ (c) (x b )</formula><p>// Refine the labels ((c) denotes the counter network)</p><formula xml:id="formula_2">y b = Sharpen(ȳ b , T )</formula><p>// Apply sharpening to the refined label</p><formula xml:id="formula_3">q b = 1 2M m (f θ (1) (u b,m ) + f θ (2) (u b,m ))</formula><p>// Ensemble both networks' predictions to guess labels q b = Sharpen(q b , T ) // Apply sharpening to the guessed labels end</p><formula xml:id="formula_4">X s = {(φA(x b ),ỹ b ); b ∈ {1, ..., B}} // Strongly augmented samples with smoothed labels X (k) = {(x b ,ȳ b ); b ∈ {1, ..., B}} // Co-refined labeled samples U (k) = {(u b ,q b ); b ∈ {1, ..., B}} // Co-refined unlabeled sampleŝ X (k) ,Û (k) = MixMatch(X (k) ,Ū (k) ) // Apply MixMatch Lt = LXs + LX + λU LÛ // Calculate the total loss X ← X ∪ Co-Refurbish(U, f θ (k) , τ2) // Refurbish noisy samples to clean samples (Eq. (18), (19)) θ (k) ← SGD(Lt, θ (k) )</formula><p>// Update network parameters end an unsupervised classifier g φ . The model first divides the pseudo-labels into two disjoint sets as D = X ∪ U with a specified sampling strategy. We consider X as clean, whose pseudo-labels are moderately credible and thus can be used as a labeled dataset (x, y) ∈ X for refinement. In contrast, we consider U as unclean, whose labels we discard u ∈ U. Designing an accurate sampling strategy is not straightforward, as there is no ground-truth to validate the pseudo-labels directly. Inspired by robust learning's clean set selection strategy, we explore three different approaches: (1) confidence-based, (2) metric-based, and (3) hybrid.</p><p>Confidence-based strategy. This approach selects clean samples based on the confidence score of the unsupervised classifier. Given a training sample (x, y) ∈ D, we consider the pseudo-label y is credible if max(y) &gt; τ 1 , and add it to the clean set X . Otherwise, it is assigned to U. This is motivated by the observation that the unsupervised classifier tends to generate overconfident predictions; thus, we trust only the most typical examples from each class while ignoring the rest. The threshold τ 1 is set substantially high to eliminate as many uncertain samples as possible.</p><p>Metric-based strategy. The limitation of the above approach is that the selection strategy still entirely depends on the unsupervised classifier. The metric-based approach leverages an additional embedding network h ψ learned in an unsupervised manner (e.g., SimCLR <ref type="bibr" target="#b8">[9]</ref>) and measures the credibility of the pseudo-label based on how well it coincides to the classification results using h ψ . For each (x, y) ∈ D, we compute its embedding h ψ (x), and apply the non-parameteric classifier based on k-Nearest Neighbor (k-NN) by y = k-NN(h ψ (x)). We consider that the pseudo-label is credible if argmax(y) = argmax(y ) and add it to the clean set X . Otherwise, it is assigned to the unclean set U.</p><p>Hybrid strategy. This approach will add a sample to the clean set X only if it is considered credible by both the confidence-based and metric-based strategies. All other samples are added to U.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Retraining via Robust Learning</head><p>Given the clean set X and the unclean set U, our next step aims to train the refined classifier f θ that revises incorrect predictions of the initial unsupervised classifier.</p><p>Vanilla semi-supervised learning. A naive baseline is to consider X as labeled data and U as unlabeled data each to train a classifier f θ using semi-supervised learning techniques. We utilize MixMatch <ref type="bibr" target="#b4">[5]</ref> as such a baseline, which is a semi-supervised algorithm that estimates low-entropy mixed labels from unlabeled examples using MixUp augmentation <ref type="bibr" target="#b55">[56]</ref>. <ref type="bibr" target="#b0">1</ref> For unsupervised clustering, MixUp can bring additional resistance against noisy labels since a large amount of extra virtual examples from MixUp interpolation makes memorization hard to achieve <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b55">56]</ref>. Specifically, given a two paired data (x 1 , y 1 ) and (x 2 , y 2 ) sampled from either labeled or unlabeled data, it augments the data using the following operations.</p><formula xml:id="formula_5">λ ∼ Beta(α, α) (1) λ = max(λ, 1 − λ) (2) x = λ x 1 + (1 − λ )x 2 (3) y = λ y 1 + (1 − λ )y 2 .<label>(4)</label></formula><p>In the case of unlabeled data u ∈ U, MixMatch is employed such that a surrogate label y = q is obtained by averaging the model's predictions over multiple augmentations after sharpening <ref type="bibr" target="#b4">[5]</ref>. Later, we will show that using the labels y and q directly in semi-supervised learning leads to a suboptimal solution and discuss how to improve its robustness. ForX andÛ after MixMatch (Eq. <ref type="formula">(5)</ref>), a vanilla semisupervised learning model trains with two separate losses: the cross-entropy loss for the labeled setX (Eq. <ref type="formula">(6)</ref>), and the consistency regularization for the unlabeled setÛ (Eq. <ref type="formula" target="#formula_6">(7)</ref>). H(p, q) denotes the cross-entropy between p and q.</p><formula xml:id="formula_6">X ,Û = MixMatch(X , U) (5) LX = 1 |X | x,ŷ∈X H(ŷ, f θ (x)) (6) LÛ = 1 |Û| û,q∈Û ||q − f θ (û)|| 2 2<label>(7)</label></formula><p>Label smoothing. To regularize our model from being overconfident to noisy predictions, we apply label smoothing along with vanilla semi-supervised learning. Label smoothing prescribes soft labels by adding uniform noise and improves the calibration in predictions <ref type="bibr" target="#b28">[29]</ref>. Given a labeled sample with its corresponding label (x, y) ∈ X , we inject uniform noise into all classes as follows:</p><formula xml:id="formula_7">y = (1 − ) · y + (C − 1) · (1 − y)<label>(8)</label></formula><p>where C is the number of class and ∼ Uniform(0, 1) is the noise. We compute cross-entropy using the soft labelỹ and the predicted label of the strongly augmented sample φ A (x) via RandAugment <ref type="bibr" target="#b9">[10]</ref>. We find that strong augmentations minimize the memorization from noise samples.</p><formula xml:id="formula_8">L X s = 1 |X | x,ỹ∈X H(ỹ, f θ (φ A (x)))<label>(9)</label></formula><p>Our final objective for training can be written as:</p><formula xml:id="formula_9">L(θ; X ,X ,Û) = L X s + LX + λ U LÛ ,<label>(10)</label></formula><p>where λ U is a hyper-parameter to control the effect of the unsupervised loss in MixMatch.</p><p>Co-training. Maintaining a single network for learning has a vulnerability of overfitting to incorrect pseudo-labels since the initial error from the network is transferred back again, and thereby, accumulated <ref type="bibr" target="#b15">[16]</ref>. To avoid this fallacy, we additionally introduce a co-training module where the two networks f θ <ref type="bibr" target="#b0">(1)</ref> , f θ <ref type="bibr" target="#b1">(2)</ref> are trained in parallel and exchange their guesses for teaching each other by adding a co-refinement step on top of MixMatch. Co-refinement is a label refinement process that aims to produce reliable labels by incorporating both networks' predictions. Following the previous literature <ref type="bibr" target="#b26">[27]</ref>, we apply co-refinement both on the label set X and the unlabeled set U for each network. Here, we explain the co-refinement process from the perspective of f θ <ref type="bibr" target="#b0">(1)</ref> . For the labeled data point x, we calculate the linear sum between the original label y in X and the prediction from the counter network f θ <ref type="bibr" target="#b1">(2)</ref> (Eq. <ref type="formula">(11)</ref>) and apply sharpening on the result to generate the refined labelȳ (Eq. <ref type="formula" target="#formula_10">(12)</ref>).</p><formula xml:id="formula_10">y = (1 − w (2) ) · y + w (2) · f θ (2) (x) (11) y = Sharpen(ȳ, T ),<label>(12)</label></formula><p>where w <ref type="bibr" target="#b1">(2)</ref> is the counter network's confidence value of x, and T is the sharpening temperature. For the unlabeled set U, we apply an ensemble of both networks' predictions to guess the pseudo-labelq of data sample u as follows:</p><formula xml:id="formula_11">q = 1 2M m (f θ (1) (u m ) + f θ (2) (u m )) (13) q = Sharpen(q, T ),<label>(14)</label></formula><p>where u m is m-th weak augmentation of u.</p><p>In place of X and U, co-refinement produces the refined dataset (x,ȳ) ∈X <ref type="bibr" target="#b0">(1)</ref> , and (u,q) ∈Ū (1) through Eq. (11) to <ref type="bibr" target="#b13">(14)</ref>. We utilize those datasets as an input for MixMatch, and the model is eventually optimized as follows:</p><formula xml:id="formula_12">X (1) ,Ū (1) = Co-refinement(X , U, θ (1) , θ (2) )<label>(15)</label></formula><formula xml:id="formula_13">X (1) ,Û (1) = MixMatch(X (1) ,Ū (1) )<label>(16)</label></formula><formula xml:id="formula_14">θ (1) ← arg min θ (1) L(θ (1) ; X ,X (1) ,Û (1) ),<label>(17)</label></formula><p>where L is the loss defined in Eq. (10). This process is also conducted for f θ <ref type="bibr" target="#b1">(2)</ref> in the same manner. Co-refurbishing. Lastly, we refurbish the noise samples at the end of every epoch to deliver the extra clean samples across the training process. If at least one of the networks' confidence on the given unclean sample u ∈ U is over the threshold τ 2 , the corresponding sample's label is updated with the network's prediction p. The updated sample is then regarded as clean and appended to the labeled set X .</p><formula xml:id="formula_15">p = f θ (k) (u), where k = arg max k (max(f θ (k ) (u))) (18) X ← X ∪ {(u, 1 p )| max(p) &gt; τ 2 },<label>(19)</label></formula><p>where 1 p is a one-hot vector of p whose i-th element is 1, considering i = arg max(p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>For evaluation, we first compared the performance of our model against other baselines over multiple datasets. Then, we examined each component's contribution to performance improvement. Lastly, we investigated how RUC helps improve existing clustering models in terms of their confidence calibration and robustness against adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Unsupervised Image Clustering Task</head><p>Settings. Four benchmark datasets were used. The first two are CIFAR-10 and CIFAR-100, which contain 60,000 images of 32x32 pixels. For CIFAR-100, we utilized 20 superclasses following previous works <ref type="bibr" target="#b41">[42]</ref>. The next is STL-10, containing 100,000 unlabeled images and 13,000 labeled images of 96x96 pixels. For the clustering problem, only 13,000 labeled images were used. Lastly, we test the model with the large-scale ImageNet-50 dataset, containing 65,000 images of 256x256 pixels.</p><p>Our model employed the ResNet18 <ref type="bibr" target="#b17">[18]</ref> architecture following other baselines <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42]</ref> and the model was trained for 200 epochs. Initial confidence threshold τ 1 was set as 0.99, and the number of neighbors k to divide the clean and noise samples was set to 100. The threshold τ 2 for refurbishing started from 0.9 and increased by 0.02 in every 40 epochs. The label smoothing parameter was set to 0.5. For evaluating the class assignment, the Hungarian method <ref type="bibr" target="#b24">[25]</ref> was used to map the best bijection permutation between the predictions and ground-truth.</p><p>Result. <ref type="table" target="#tab_1">Table 1</ref> shows the overall performance of clustering algorithms over three datasets: CIFAR-10, CIFAR-20, and STL-10. For these datasets, the proposed model RUC, when applied to the SCAN <ref type="bibr" target="#b41">[42]</ref> algorithm, outperforms all other baselines. Particularly for STL-10, the combined model shows a substantial improvement of 5.3 pp. <ref type="table" target="#tab_2">Table 2</ref> reports ImageNet-50 result on the confidence based sampling strategy, which demonstrates RUC's applicability to large-scale dataset. Furthermore, RUC achieves consistent performance gain over another clustering model, TSUC <ref type="bibr" target="#b16">[17]</ref>. These results confirm that our model can be successfully applied to existing clustering algorithms and improve them. We also confirm that all three selection strategies (i.e., confidence-based, metric-based, and hybrid) bring considerable performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Component Analyses</head><p>To evaluate the model's efficacy, we conduct an ablation study by repeatedly assessing its performance after removing each component. We also evaluate the accuracy of dif-    <ref type="table">Table 3</ref>: Ablation results of the SCAN+RUC on STL-10 ferent selection and refurbishing strategies based on precision, recall, and the F1 score.</p><p>Ablation study. The proposed model utilizes two robust learning techniques to cope with unclean samples: cotraining and label smoothing. We remove each component from the full model to assess its efficacy. <ref type="table">Table 3</ref> shows the classification accuracy of each ablation on the STL-10 dataset. RUC with all components performs the best, implying that dropping any component results in performance degradation. We also compare a variant, which drops both label smoothing and co-training (i.e., MixMatch only). The effect of co-training is not evident in <ref type="table">Table 3</ref>. Nevertheless, it improved the performance from 36.3% to 39.6% for the lowest noise ratio CIFAR-20 pseudo-labels when we set   the base model as TSUC. This finding may suggest that cotraining is more effective for pseudo-labels with high noise ratios. The co-training structure showed additional stability in training. Due to space limitation, we report these findings in the supplementary material.</p><p>Comparison with other possible add-on modules As an alternative of RUC, one may combine the extant robust learning algorithms (e.g., M-correction <ref type="bibr" target="#b0">[1]</ref>, Pcorrection <ref type="bibr" target="#b52">[53]</ref>, and DivideMix <ref type="bibr" target="#b26">[27]</ref>) or another previously proposed add-on module (e.g., Gupta et al. <ref type="bibr" target="#b13">[14]</ref>) on top of SCAN <ref type="bibr" target="#b41">[42]</ref>. <ref type="table" target="#tab_1">Table 1</ref> summarizes the comparisons to four baselines. For a fair comparison, we employed SCAN as the initial clustering method and applied each baseline on top of it. <ref type="bibr" target="#b1">2</ref> As shown in the results, improving noisy clustering is non-trivial as some baselines show even worse results after the refinement (e.g., DivideMix, M-correction, P-correction). While Gupta et al. effectively refines the results, its improvement is limited when the initial clustering is reasonably accurate (e.g., CIFAR-10). In contrast, RUC achieves consistent improvements in all datasets with non-trivial margins, showing that a carefully designed robust learning strategy is critical to the performance gain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">In-Depth Performance Analysis</head><p>So far, we showed that RUC improves existing baselines substantially, and its components contribute to the performance gain. We now examine RUC's calibration effect and present a qualitative analysis by applying it to the state-ofthe-art base model, SCAN <ref type="bibr" target="#b41">[42]</ref>. We will also demonstrate the role of RUC in handling adversarially crafted noise.</p><p>Confidence calibration. Many unsupervised clustering algorithms are subject to overconfident results due to their entropy-based balancing <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref>. If a model is overconfident to noisy samples, separating the clean and unclean set becomes challenging, and this can induce overall performance degradation. <ref type="figure" target="#fig_1">Figure 4</ref> shows the calibration effect of RUC. SCAN's confidence is highly concentrated near 1, while our model's confidence is widely distributed over [0.6, 0.8]. We also report the degree of calibration quality using Expected Calibration Error (ECE) <ref type="bibr" target="#b12">[13]</ref>:  where n is the number of data, B m is the m-th group from equally spaced buckets based on the model confidence over the data points; acc(B m ) and conf (B m ) are the average accuracy and confidence over B m . Lower ECE of RUC in <ref type="figure" target="#fig_1">Figure 4</ref> implies that our approach led to better calibrations.</p><formula xml:id="formula_16">ECE = M m=1 |B m | n |acc(B m ) − conf (B m )|,<label>(20)</label></formula><p>To observe this effect more clearly, we visualize the clustering confidence result at different training epochs in <ref type="figure" target="#fig_0">Figure 3</ref>. Unlike the result of SCAN in which the overly clustered sample and the uncertain sample are mixed, the result of SCAN+RUC shows that the sample's class distribution has become smoother, and uncertain samples disappeared quickly as training continues.</p><p>Qualitative analysis. We conducted a qualitative analysis to examine how well RUC corrects the initial misclassification in pseudo-labels. <ref type="figure" target="#fig_2">Figure 5</ref> compares the confusion matrices of SCAN and the SCAN+RUC for STL-10. A high concentration of items on the diagonal line confirms the advanced correction effect of RUC for every class. <ref type="figure" target="#fig_3">Figure 6</ref> compares how the two models interpreted class traits based on the Grad-CAM <ref type="bibr" target="#b37">[38]</ref> visualization on example images. The proposed model shows a more sophisticated prediction for similar images.</p><p>Robustness to adversarial noise. Clustering algorithms like SCAN introduce pseudo-labels to train the model via the Empirical Risk Minimization (ERM) method <ref type="bibr" target="#b42">[43]</ref>. ERM is a learning principle that minimizes the averaged error over the sampled training data (i.e., empirical risk) to find the model with a small population risk (i.e., true risk). However, ERM is known to be vulnerable to adversarial examples, which are crafted by adding visually imperceptible perturbations to the input images <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>Here, we show that RUC improves robustness against the adversarial noise. We conduct an experiment on STL-10 using adversarial perturbations of FGSM <ref type="bibr" target="#b11">[12]</ref> and BIM <ref type="bibr" target="#b25">[26]</ref> attacks, whose directions are aligned with the gradient of the loss surface of given samples. <ref type="figure" target="#fig_4">Figure 7</ref> compares the model's ability to handle the adversarial noise. Models based on MixMatch (Gupta et al., DivideMix, RUC) out-perform the rest, probably because the calibration effect of MixUp prevents overconfident predictions. Among them, RUC achieves superior improvement, demonstrating that robust learning components, such as careful filtering, label smoothing, and co-training, can also handle the adversarial noise (see the supplementary material for further details). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This study presented RUC, an add-on approach for improving existing unsupervised image clustering models via robust learning. Retraining via robust training helps avoid overconfidence and produces more calibrated clustering results. As a result, our approach achieved a meaningful gain on top of two state-of-the-art clustering methods. Finally, RUC helps the clustering models to be robust against adversarial attacks. We expect robust learning will be a critical building block to advance real-world clustering solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Release</head><p>Codes, training details, and the downloadable link for trained models are available at https://github.com/deu30303/ RUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Training Details</head><p>Our model employed the ResNet18 <ref type="bibr" target="#b17">[18]</ref> architecture following other baselines <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42]</ref>. Before retraining, note that we randomly initialize the final fully connected layer and replace the backbone network with a newly pretrained one from an unsupervised embedding learning algorithm as done in SimCLR <ref type="bibr" target="#b8">[9]</ref>. This random re-initialization process helps avoid the model from falling into the same local optimum. The initial confidence threshold τ1 was set as 0.99, and the number of neighbors k to divide the clean and noise samples was set to 100. The threshold τ2 for refurbishing started from 0.9 and increased by 0.02 in every 40 epochs. The label smoothing parameter was set to 0.5. The initial learning rate was set as 0.01, which decays smoothly by cosine annealing. The model was trained for 200 epochs using SGD with a momentum of 0.9, a weight decay of 0.0005. The batch size was 100 for STL-10 and 200 for CIFAR-10 and CIFAR-20. We chose λu as 25, 50, 100 for CIFAR-10, STL-10 and CIFAR-20. The w b value was calculated by applying min-max normalization to the confidence value of the counter network f θ (c) . Random crop and horizontal flip were used as a weak augmentation, which does not deform images' original forms. RandAugment <ref type="bibr" target="#b9">[10]</ref> was used as a strong augmentation. We report all transformation operations for strong augmentation strategies in <ref type="table" target="#tab_6">Table 5</ref>. The number of transformations and magnitude for all the transformations in RandAugment was set to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformation Parameter Range</head><p>AutoContrast  To evaluate class assignment, the Hungarian method <ref type="bibr" target="#b24">[25]</ref> was used to map the best bijection permutation between the predictions and ground-truth. We also note that the computational cost of RUC is not a huge burden. It took less than 12 hours to run 200 epochs with 4 TITAN Xp processors for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Sampling strategy analysis.</head><p>We evaluate the quality of the clean set generated from three sampling strategies (See <ref type="table" target="#tab_7">Table 6</ref>). Overall, precision was the high-est for the hybrid strategy, whereas recall was the highest for the metric-based strategy. We also tested the co-refurbish accuracy over the epochs. <ref type="figure" target="#fig_5">Figure 8</ref> displays the change of precision, recall, and the F1-score using confidence-based sampling on the STL-10 dataset. The model's precision drops slightly as the number of epochs increases, but the recall increases significantly. The F1score, which shows the overall sampling accuracy of the clean set, increased about 5% over 200 epochs. It can be interpreted a higher rate of true-positive cases than the false-positive cases in the refurbished samples, which means that the model could successfully correct the misclassified unclean samples. Overall, we find the current hybrid selection strategy can distinguish clean sets relatively well since the selected samples benefit from both strategies' merits. This strategy, however, cannot always achieve the best performance. Further development of the selection strategy will help increase the proposed RUC model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Hyper-parameters of Sampling Strategies</head><p>We investigate the effect of hyper-parameters from two sampling strategies: τ1 and k. τ1 is the threshold for selecting clean samples in the confidence-based strategy, and k is the number of neighbors for the kNN classifier in the metric-based strategy. <ref type="figure">Figure 9</ref> summarizes the effect of each hyper-parameter. In the case of τ1, the final accuracy reaches the highest at τ1 = 0.99 and starts to decrease. Small τ1 extracts clean samples with higher recall and lower precision, while large τ1 extracts clean samples with higher precision and lower recall. Hence, balancing between the precision and recall through appropriate τ1 can lead to better performance. Meanwhile, the number of nearest neighbors k does not significantly affect the final accuracy. Given k within the reasonable range, our model consistently produces results of high performance.</p><p>(a) Effect of τ1 (b) Effect of k <ref type="figure">Figure 9</ref>: Analysis of the accuracy on the STL-10 dataset across two hyper-parameters: (a) τ 1 from the confidencebased strategy and (b) k from the metric-based strategy.</p><p>For remaining hyper-parameters (λu, , τ2), our model resorted to a standard hyper-parameter setting that is commonly used in practice. For example, we set λU following earlier works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref>, choose = 0.5 as the mean of Uniform(0, 1), and choose τ2 to be a reasonably high value, similar to τ1. Empirically, we find the model is oblivious to these parameters (see <ref type="table" target="#tab_9">Table 7</ref>).  A.5. Additional Analysis for RUC on TSUC Many recent unsupervised clustering algorithms are subject to overconfident results because of their entropy-based balancing <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref>. If a model is overconfident to noisy samples, separating the clean set and the unclean set becomes challenging, which can induce the overall performance degradation. We evaluated the calibration effect of RUC on top of TSUC in <ref type="figure" target="#fig_6">Figure 10</ref>. TSUC's confidence is highly concentrated near 1, while our model's confidence is widely distributed. We also report the degree of calibration quality using the Expected Calibration Error (ECE) <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_17">ECE = M m=1 |Bm| n |acc(Bm) − conf (Bm)|,<label>(21)</label></formula><p>where n is the number of data points, Bm is the m-th group from equally spaced buckets based on the model confidence over the data points; acc(Bm) and conf (Bm) are the average accuracy and confidence over Bm respectively. TSUC's high ECE implies that TSUC is more overconfident than SCAN. Lower ECE of TSUC + RUC case in <ref type="figure" target="#fig_6">Figure 10</ref> implies that our add-on process led to better calibrations.</p><p>We also evaluated quality of the clean set from TSUC under three sampling strategies. The results are shown in <ref type="table" target="#tab_11">Table 8</ref>. Overall, the precision is the highest for the hybrid strategy, whereas the recall is the highest for the metric-based strategy, as same as the SCAN's results. Meanwhile, confidence-based strategies in TSUC showed low precision, which implies that TSUC is not wellcalibrated and highly overconfident.   Our model architecture introduces a co-training module where the two networks exchange their guesses for teaching each other via co-refinement. Due to the different learning abilities in two networks, disagreements from networks help filter out corrupted labels, which contributes to a substantial performance increase in unsupervised classification. Besides, the co-training structure provides extra stability in the training process. <ref type="figure" target="#fig_7">Figure 11</ref> compares our model and the same model without cotraining based on classification accuracy across the training epoch. The model without co-training shows large fluctuations in accuracy; in contrast, the full model's accuracy remains stable and consistent throughout epochs. We speculate this extra stability comes from our model's ensemble architecture and the effect of loss correction. Corrected labels via ensemble predictions bring additional label smoothing. Therefore, it may reduce the negative training signals from unclean samples, which can lead to abrupt updates on the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Further Details on Adversarial Robustness</head><p>Empirical risk minimization (ERM), a learning principle which aims to minimize the averaged error over the sampled training data (i.e., empirical risk), has shown remarkable success in finding models with small population risk (i.e., true risk) in the supervised setting <ref type="bibr" target="#b42">[43]</ref>. However, ERM-based training is also known to lead the model to memorize the entire training data and often does not guarantee to be robust on adversarial noise <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b55">56]</ref>. This weakness can also be inherited from several unsupervised clustering algorithms that introduce the ERM principle with their pseudo-labels, like SCAN <ref type="bibr" target="#b41">[42]</ref>.</p><p>Adding RUC to the existing clustering models improves robustness against adversarial noise. To demonstrate this, we conducted an experiment using adversarial perturbations of the FGSM <ref type="bibr" target="#b11">[12]</ref> and BIM <ref type="bibr" target="#b25">[26]</ref> attacks, whose directions are aligned with the gradient of the loss surface of given samples. The details of each attack are as follows:</p><p>Fast Gradient Sign Method (FGSM) FGSM crafts adversarial perturbations by calculating the gradients of the loss function J(θ, x, y) with respect to the input variables. The input image is perturbed by magnitude with the direction aligned with the computed gradients (Eq. <ref type="formula">(22)</ref>).</p><p>x adv = x + · sgn(∇xJ(θ, x, y)) <ref type="bibr" target="#b21">(22)</ref> Basic Iterative Method (BIM) BIM is an iterative version of FGSM attack, which generates FGSM based adversarial noise with small and applies the noise many times in a recursive way (Eq. <ref type="formula" target="#formula_5">(24)</ref>).</p><p>x adv 0 = x (23)</p><p>x adv i = clip x, (x adv i−1 + · sgn(∇ x adv i−1 J(θ, x adv i−1 , y)))</p><p>Clip function maintains the magnitude of noise below by clipping. For BIM attack experiments, we use five iterations with an equal step size. <ref type="figure" target="#fig_4">Figure 7</ref> in our main manuscript compares the model's ability to handle adversarial attacks, which confirms that adding RUC helps maintain the model accuracy better for both attack types. An investigation could guide us that this improved robustness is mainly due to the label smoothing techniques, which regularize the model to avoid overconfident results and reduce the amplitude of adversarial gradients with smoothed labels <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47]</ref>. <ref type="figure" target="#fig_8">Figure 12</ref> shows additional examples for the visual interpretation from RUC on top of SCAN via the Grad-CAM algorithm <ref type="bibr" target="#b37">[38]</ref>. Blue framed images are the randomly chosen success cases from STL-10, and the red-framed images are example failure cases. Overall, the network trained with our model can extract key features from the images. Even though the model sometimes fails, most of the failures occurred between visually similar classes (e.g., horse-deer, cat-dog, truck-car, dog-deer). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8. Additional Examples for Qualitative Analysis</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of clustering results on STL-10. The leftmost figure shows the results from the SCAN, while the right three figures display the intermediate results of our model on top of SCAN at different training epochs. This result demonstrates that RUC effectively alleviates the overconfident predictions while enhancing the clustering quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Confidence distribution for noise samples from the STL-10 dataset. RUC shows more widely distributed confidence and produces better calibration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Confusion matrices of the SCAN and SCAN+RUC results on STL-10. The row names are predicted class labels, and the columns are the ground-truths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Class activation maps and the model's confidence on STL-10. The highlighted area indicates where the model focused to classify the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>RUC's robustness to adversarial attacks, experimented with different perturbation rate . Robust learning components are important in handling the adversarial noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Changes of sampling accuracy across each epoch on .2 89.3 83.1 88.5 77.4 79.4 94.4 78.3 F1 Score 93.0 92.4 91.4 69.3 70.8 69.8 85.7 90.9 85.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Confidence distribution for noise sample on STL-10 with the base model TSUC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Changes of clustering accuracy across each epoch on CIFAR-20 with the base model TSUC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Additional example of successes and failures from STL-10 where the highlighted part indicates how the model interprets class traits based on the Grad-CAM method (Blue frame: success case, Red frame: failure case).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>ImageNet-50</cell><cell>76.8</cell><cell>78.5 / 78.5</cell></row></table><note>Performance improvement with RUC (accu- racy presented in percent). Baseline results are excerpted from [17, 42] and we report the last/best accuracy. †Results obtained from our experiments with official code.Method SCAN (Best) SCAN + RUC (Last / Best accuracy)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison over ImageNet-50</figDesc><table><row><cell>Setup</cell><cell cols="2">Last Acc Best Acc</cell></row><row><cell>RUC with all components</cell><cell>86.7</cell><cell>86.8</cell></row><row><cell>without co-training</cell><cell>86.2</cell><cell>86.4</cell></row><row><cell>without label smoothing</cell><cell>85.5</cell><cell>85.8</cell></row><row><cell>with MixMatch only</cell><cell>85.2</cell><cell>85.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Performance comparison with other possible add- on modules (Last / Best accuracy)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>List of transformations used in RandAugment</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Quality of the clean set (C : Confidence, M : Met-</cell></row><row><cell>ric, H : Hybrid)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameter analyses (λ u , , τ 2 )</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Quality of the clean set regards to sampling strate-</figDesc><table><row><cell>gies (C : Confidence, M : Metric, H : Hybrid)</cell></row><row><cell>A.6. Further Discussion on Co-Training</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that our method is not dependent on the particular choice of the semi-supervised learning method and can incorporate the others.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For methods employing an ensemble of clusterings (e.g., Gupta et al.), we employed multiple SCAN models with random initialization. We also applied the same semi-supervised method (i.e., MixMatch).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stanislaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autoencoders, unsupervised learning, and deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning Workshop on Unsupervised and Transfer Learning</title>
		<meeting>of International Conference on Machine Learning Workshop on Unsupervised and Transfer Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MixMatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haw-Shiuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mc-Callum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1002" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shiming Xiang, and Chunhong Pan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Deep adaptive image clustering</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">K-means clustering via principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations</title>
		<meeting>of International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised clustering using pseudosemi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divam</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramachandran</forename><surname>Ramjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nipun</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muthian</forename><surname>Sivathanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations</title>
		<meeting>of International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Associative deep clustering: Training a classification network with no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Plapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elie</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of German Conference on Pattern Recognition</title>
		<meeting>of German Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mitigating embedding and class assignment mismatch in unsupervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungkyu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meeyoung</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10456" to="10465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1558" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anil K Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick J</forename><surname>Narasimha Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations</title>
		<meeting>of International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Does label smoothing mitigate label noise?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Curriculum loss: Robust learning and generalization against label corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations</title>
		<meeting>of International Conference on Learning Representations</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations</title>
		<meeting>of International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations</title>
		<meeting>of International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning a distance metric from relative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Computer Vision</title>
		<meeting>of IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08199,2020.3</idno>
		<title level="m">Learning from noisy labels with deep neural networks: A survey</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jesper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Engelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="373" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision</title>
		<meeting>of European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Springer science &amp; business media</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Optimized cartesian k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Shun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Heng Tao Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="180" to="192" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiclass learning with partially corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2568" to="2580" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">11 adversarial perturbations of deep neural networks. Perturbations, Optimization, and Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">311</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A Comprehensive and Adversarial Approach to Unsupervised Embedding Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meeyoung</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Te</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Big Data</title>
		<meeting>of IEEE International Conference on Big Data</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning</title>
		<meeting>of International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7164" to="7173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations</title>
		<meeting>of International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
