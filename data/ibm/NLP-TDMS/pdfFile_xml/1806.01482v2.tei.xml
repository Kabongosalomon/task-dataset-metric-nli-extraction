<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
							<email>amirabs@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriaki</forename><surname>Hirose</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Hamid</forename><surname>Rezatofighi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of path prediction for multiple interacting agents in a scene, which is a crucial step for many autonomous platforms such as self-driving cars and social robots. We present SoPhie; an interpretable framework based on Generative Adversarial Network (GAN), which leverages two sources of information, the path history of all the agents in a scene, and the scene context information, using images of the scene. To predict a future path for an agent, both physical and social information must be leveraged. Previous work has not been successful to jointly model physical and social interactions. Our approach blends a social attention mechanism with a physical attention that helps the model to learn where to look in a large scene and extract the most salient parts of the image relevant to the path. Whereas, the social attention component aggregates information across the different agent interactions and extracts the most important trajectory information from the surrounding neighbors. SoPhie also takes advantage of GAN to generates more realistic samples and to capture the uncertain nature of the future paths by modeling its distribution. All these mechanisms enable our approach to predict socially and physically plausible paths for the agents and to achieve state-of-the-art performance on several different trajectory forecasting benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When people navigate through a park or crowded mall, they follow common sense rules in view of social decorum to adjust their paths. At the same time, they are able to adapt to the physical space and obstacles in their way. Interacting with the physical terrain as well as humans around them is by no means an easy task; because it requires: * indicates equal contribution Social Attention Physical Attention <ref type="figure">Figure 1</ref>. SoPhie predicts trajectories that are socially and physically plausible. To perform this, our approach incorporates the influence of all agents in the scene as well as the scene context.</p><p>• Obeying physical constraints of the environment. In order to be able to walk on a feasible terrain and avoid obstacles or similar physical constraints, we have to process the local and global spatial information of our surroundings and pay attention to important elements around us. For example, when reaching a curved path, we focus more on the curve rather than other constraints in the environment, we call this physical attention.</p><p>• Anticipating movements and social behavior of other people. To avoid collisions with other people, disturbing their personal space, or interrupting some social interactions (e.g. a handshake), we must have a good understanding of others' movements and the social norms of an environment and adjust our path accordingly. We should take into account that some agents have more influence in our decision. For example, when walking in a corridor, we pay more attention to people in front of us rather than the ones behind us, we call this social attention. Modeling these social interactions is a non-trivial task.</p><p>• Finding more than a single feasible path. To get to our destination, there often exists more than a single choice for our path, which is the fuzzy nature of human motion. Indeed, there is a range for our traversable paths toward our destinations <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>In this paper, we aim to tackle the problem of future path prediction for a set of agents. The existing approaches follow different strategies to solve this problem. Some methods solely rely on the scene context to predict a feasible path for each agent. For example, the approach in <ref type="bibr" target="#b2">[3]</ref> learns a dynamic pattern for all agents from patch-specific descriptors using previously created navigation maps that encode scene-specific observed motion patterns. In <ref type="bibr" target="#b13">[14]</ref>, the approach learns the scene context from top-view images in order to predict future paths for each agent. <ref type="bibr" target="#b22">[23]</ref> applies an attention mechanism to input images in order to highlight the important regions for each agent's future path. However, all above approaches ignore the influence of the other agents' state on predicting the future path for a targeted agent.</p><p>Parallel to path prediction using scene context information, several approaches have recently proposed to model interactions between all agents in the scene in order to predict the future trajectory for each targeted agent <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Although these methods have shown promising progress in addressing this challenging problem, they still ignore the scene contexts as crucial information. In addition, these methods fall short as instead of treating pedestrian's future movements as a distribution of locations, they only predict a single path, which generally ends up optimizing "average behavior" rather than learning difficult constraints.. To address the second problem, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref> have introduced models that are able to generate multiple feasible paths. However, most of these models only incorporate the influence of few adjacent agents in a very limited search space. Recently, <ref type="bibr" target="#b7">[8]</ref> proposed a GAN model that takes into account the influence of all agents in the scene.</p><p>In this work, we propose SoPhie an attentive GAN-based approach that can take into account the information from both scene context and social interactions of the agents in order to predict future paths for each agent. Influenced by the recent success of attention networks <ref type="bibr" target="#b24">[25]</ref> and also GANs <ref type="bibr" target="#b6">[7]</ref> in different real-world problems, our proposed framework simultaneously uses both mechanisms to tackle the challenging problem of trajectory prediction. We use a visual attention model to process the static scene context alongside a novel attentive model that observes the dynamic trajectory of other agents. Then, an LSTM based GAN module is applied to learn a reliable generative model representing a distribution over a sequence of plausible and realistic paths for each agent in future.</p><p>To the best of our knowledge, no other work has previously tackled all the above problems together. SoPhie generates multiple socially-sensitive and physically-plausible trajectories and achieves state-of-the-art results on multiple trajectory forecasting benchmarks. To summarize the main contribution of the paper are as follows:</p><p>• Our model uses scene context information jointly with social interactions between the agents in order to predict future paths for each agent.</p><p>• We propose a more reliable feature extraction strategy to encode the interactions among the agents.</p><p>• We introduce two attention mechanisms in conjunction with an LSTM based GAN to generate more accurate and interpretable socially and physically feasible paths.</p><p>• State-of-the-art results on multiple trajectory forecasting benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In recent years, there have been many advances in the task of trajectory prediction. Many of the previous studies on trajectory prediction either focus on the effect of physical environment on the agents paths (agent-space interactions) and learn scene-specific features to predict future paths <ref type="bibr" target="#b22">[23]</ref>, or, focus on the effect of social interactions (dynamic agentagent phenomena) and model the behavior of agents influenced by other agents' actions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>. Few works have been trying to combine both trajectory and scene cues <ref type="bibr" target="#b13">[14]</ref>.</p><p>Agent-Space Models. This models mainly take advantage of the scene information, e.g., cars tend to drive between lanes or humans tend to avoid obstacles like benches. Morris et al. <ref type="bibr" target="#b18">[19]</ref> cluster the spatial-temporal patterns and use hidden Markov models to model each group. Kitani et al. <ref type="bibr" target="#b12">[13]</ref> use hidden variable Markov decision processes to model human-space interactions and infer walkable paths for a pedestrian. Recently, Kim et al. <ref type="bibr" target="#b11">[12]</ref>, train a separate recurrent network, one for each future time step, to predict the location of nearby cars. Ballan et al. <ref type="bibr" target="#b2">[3]</ref> introduce a dynamic Bayesian network to model motion dependencies from previously seen patterns and apply them to unseen scenes by transferring the knowledge between similar settings. In an interesting work, a variational auto-encoders is used by Lee et al. <ref type="bibr" target="#b13">[14]</ref> to learn static scene context (and agents in a small neighborhood) and rank the generated trajectories accordingly. Sadeghian et al. <ref type="bibr" target="#b22">[23]</ref>, also use top-view images and learn to predict trajectories based on the static scene context. Our work is similar to <ref type="bibr" target="#b22">[23]</ref> in the sense that we both use attentive recurrent neural networks to predict trajectories considering the physical surrounding; nonetheless, our model is able to take into account other surrounding agents and is able to generate multiple plausible paths using a GAN module.</p><p>Agent-Agent Models. Traditional models for modeling and predicting human-human interactions used "social forces" to capture human motion patterns <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">21]</ref>. The main disadvantage of these models is the need to hand-craft rules and features, limiting their ability to efficiently learn beyond abstract level and the domain experts. Modern socially-aware trajectory prediction work usually use recurrent neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11]</ref>. Hug et al. <ref type="bibr" target="#b9">[10]</ref> present an experiment-based study the effectiveness of some RNN models in the context socially aware trajectory prediction. This methods are relatively successful, however, most of these methods only take advantage of the local interactions and don't take into account further agents. In a more recent work, Gupta et al. <ref type="bibr" target="#b7">[8]</ref> address this issue as well as the fact that agent's trajectories may have multiple plausible futures, by using GANs. Nevertheless, their method treats the influence of all agents on each other uniformly. In contrast, our method uses a novel attention framework to highlight the most important agents for each targeted agent.</p><p>Few recent approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b3">4]</ref>, to some extent, incorporate both the scene and social factors into their models. However, these models only consider the interaction among the limited adjacent agents and are only able to generate a single plausible path for each agent. We address all these limitations by applying wiser strategies such as 1-using visual attention component to process the scene context and highlight the most salient features of the scene for each agent, 2using a social attention component that estimates the amount of contribution from each agent on the future path prediction of a targeted agent, and 3-using GAN to estimate a distribution over feasible paths for each agent. We support our claims by demonstrating state-of-the-art performance on several standard trajectory prediction datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SoPhie</head><p>Our goal is to develop a model that can successfully predict future trajectories of a set of agents. To this end, the route taken by each agent in future needs to be influenced not only by its own state history, but also the state of other agents and physical terrain around its path. SoPhie takes all these cues into account when predicting each agent's future trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>Trajectory prediction can be formally stated as the problem of estimating the state of all agents in future, given the scene information and their past states. In our case, the scene information is fed as an image I t , e.g. a top-view or angleview image of the scene at time t, into the model. Moreover, the state of each agent i at time t is assumed to be its location, e.g. its 2D coordinate (x t i , y t i ) ∈ R 2 with respect to a reference, e.g. the image corner or the top view's world coordinates. Therefore, the past and current states the N agents are represented by the ordered set of their 2D locations as:</p><formula xml:id="formula_0">X 1:t i = {(x τ i , y τ i )|τ = 1, · · · , t} ∀i ∈ [N ],</formula><p>where [N ] = {1, · · · , N }. Throughout the paper, we use the notations X · 1:N and X · 1:N \i to represent the collection of all N agents' states and all agents' states excluding the target agent i, respectively. We also use the notation Y τ , to represent the future state in t + τ . Therefore, the future ground truth and the predicted states of the agent i, between frames t + 1 and t + T for T &gt; 1, are denoted by Y 1:T i and</p><formula xml:id="formula_1">Y 1:T i respectively, where Y 1:T i = {(x τ i , y τ i )|τ = t + 1, · · · , t + T } ∀i ∈ [N ].</formula><p>Our aim is to learn the parameters of a model W * in order to predict the future states of each agent between t + 1 and t + T , given the input image at time t and all agents' states up to the current frame t, i.e.</p><formula xml:id="formula_2">Y 1:T i = f (I t , X 1:t i , X 1:t 1:N \i ; W * ),</formula><p>where the model parameters W * is the collection of the weights for all deep neural structures used in our model. We train all the weights end-to-end using back-propagation and stochastic gradient descent by minimizing a loss L GAN between the predicted and ground truth future states for all agents. We elaborate the details in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overall Model</head><p>Our model consists of three key components including: 1-A feature extractor module, 2-An attention module, and 3-An LSTM based GAN module ( <ref type="figure" target="#fig_0">Fig. 2)</ref>. First, the feature extractor module extracts proper features from the scene, i.e. the image at the current frame I t , using a convolutional neural network. It also uses an LSTM encoder to encode an index invariant, but temporally dependent, feature between the state of each agent, X 1:t i , and the states of all other agents up to the current frame, X 1:t 1:N \i ( <ref type="figure" target="#fig_0">Fig. 2(a)</ref>). Then, the attention module highlights the most important information of the inputted features for the next module ( <ref type="figure" target="#fig_0">Fig. 2 (b)</ref>). The attention module consists of two attention mechanisms named as social and physical attention components. The physical attention learns the spatial (physical) constraints in the scene from the training data and concentrates on physically feasible future paths for each agent. Similarly, the social attention module learns the interactions between agents and their influence on each agent's future path. Finally, the LSTM based GAN module ( <ref type="figure" target="#fig_0">Fig. 2 (c)</ref>) takes the highlighted features from the attention module to generate a sequence of plausible and realistic future paths for each agent. In more details, an LSTM decoder is used to predict the temporally dependent state of each agent in future, i.e.Ŷ 1:T i . Similar to GAN, a discriminator is also applied to improve the performance of the generator model by forcing it to produce more realistic samples (trajectories). In the following sections, we elaborate each module in detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature extractors</head><p>The feature extractor module has two major components, explained below. To extract the visual features V t P h from the image I t , we use a Convolutional Neural Network (CNN).</p><formula xml:id="formula_3">V t P h = CN N (I t ; W cnn )<label>(1)</label></formula><p>In this paper, we use VGGnet-19 <ref type="bibr" target="#b23">[24]</ref> as CN N (·), where its weights W cnn is initialized by pre-training on ImageNet <ref type="bibr" target="#b21">[22]</ref> and fine-tuning on the task of scene segmentation as described in <ref type="bibr" target="#b15">[16]</ref>.</p><p>To extract joint features from the past trajectory of all agents, we perform the following procedure. Similar to <ref type="bibr" target="#b7">[8]</ref>, first an LSTM is used to capture the temporal dependency between all states of an agent i and encode them into a high dimensional feature representation for time t, i.e.</p><formula xml:id="formula_4">V t en (i) = LST M en (X t i , h t en (i); W en ),<label>(2)</label></formula><p>where h t en (i) represents the hidden state of the encoder LSTM at time t for the agent i. Moreover, to capture the influence of the other agents' state on the prediction of the future trajectory of an agent, we need to extract a joint feature from all agents' encoded features V t en (·). However, this joint feature cannot be simply created by concatenating them as the order of the agents does matter. To make the joint feature permutation invariant with respect to the index of the agents, the existing approaches use a permutation invariant (symmetric) function such as max <ref type="bibr" target="#b7">[8]</ref>. Then, this joint global feature is concatenated by each agent's feature V t en (i) to be fed to the state generator module. However this way, all agents will have an identical joint feature representation. In addition, the permutation invariant functions such as max may discard important information of their inputs as they might loose their uniqueness. To address these two limitations, we instead define a consistent ordering structure, where the joint feature for a target agent i is constructed by sorting the other agents' distances from agent i, i.e.</p><formula xml:id="formula_5">V t So (i) = V t en (π j ) − V t en (i) ∀π j ∈ [N ]\i) ,<label>(3)</label></formula><p>where π j is the index of the other agents sorted according to their distances to the target agent i. In this framework, each agent i has its own unique joint (social) feature vector. We also use sort as the permutation invariant function, where the reference for ordering is the euclidean distance between the target agent i and other agents. Note that sort function is advantageous in comparison with max as it can keep the uniqueness of the input. To deal with variable number of agents, we set a maximum number of agents (N = N max ) and use a dummy value as features if the corresponding agent does not exist in the current frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Attention Modules</head><p>Similar to humans who pays more attention to close obstacles, upcoming turns and people walking towards them, than to the buildings or people behind them, we want the model to focus more on the salient regions of the scene and the more relevant agents in order to predict the future state of each agent. To achieve this, we use two separate soft attention modules similar to <ref type="bibr" target="#b24">[25]</ref> for both physical V t P h and social V t So (i) features. Physical Attention The inputs to this attention module AT T P h (·) are the hidden states of the decoder LSTM in the GAN module, and the visual features extracted from the image V t P h . Note that, the hidden state of the decoder LSTM has the information for predicting the agent's future path. And this module learns the spatial (physical) constraints in the scene from the training data. Therefore, the output would be a context vector C t P h , which concentrates on feasible paths for each agent.</p><formula xml:id="formula_6">C t P h (i) = AT T P h (V t P h , h t dec (i); W P h )<label>(4)</label></formula><p>Here, W P h are the parameters of the physical attention module and h t dec (i) represents the hidden state of the decoder LSTM at time t for the agent i.</p><p>Social Attention Similar to the physical attention module, the joint feature vector V t So (i) together with the hidden state of the decoder LSTM for the i-th agent, are fed to the social attention module AT T So (·) with the parameters W So to obtain a social context vector C t So (i) for the i-th agent. This vector highlights which other agents are most important to focus on when predicting the trajectory of the agent i.</p><formula xml:id="formula_7">C t So (i) = AT T So (V t So (i), h t dec (i); W So )<label>(5)</label></formula><p>We use soft attention similar to <ref type="bibr" target="#b24">[25]</ref> for both AT T P h (·) and AT T So (·), which is differentiable and the whole architecture can be trained end-to-end with back-propagation. Social attention and physical attention aggregate information across all the involved agents and the physical terrain to deal with the complexity of modeling the interactions of all agents in crowded areas while adding interpretability to our predictions. This also suppresses the redundancies of the input data in a helpful fashion, allowing the predictive model to focus on the important features. Our experiments show the contribution of our attention components in <ref type="table" target="#tab_0">Table  1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">LSTM based Generative Adversarial Network</head><p>In this section, we present our LSTM based Generative Adversarial Network (GAN) module that takes the social and physical context vectors for each agent i, C t So (i) and C t P h (i), as input and outputs candidate future states which are compliant to social and physical constraints. Most existing trajectory prediction approaches use the L2 norm loss between the ground truth and the predictions to estimate the future states <ref type="bibr" target="#b22">[23]</ref>. By using L2 loss, the network only learns to predict one future path for each agent, which is intuitively the average of all feasible future paths for each agent. Instead, in our model, we use GAN to learn and predict a distribution over all the feasible future paths.</p><p>GANs consist of two networks, a generator and a discriminator that compete with each other. The generator is trained to learn the distribution of the paths and to generate a sample of the possible future path for an agent while the discriminator learns to distinguish the feasibility or infeasibility of the generated path. These networks are simultaneously trained in a two player min-max game framework. In this paper similar to <ref type="bibr" target="#b7">[8]</ref>, we use two LSTMs, a decoder LSTM as the generator and a classifier LSTM as the discriminator, to estimate the temporally dependent future states.</p><p>Generator (G) Our generator is a decoder LSTM, LST M dec (·). Similar to the conditional GAN <ref type="bibr" target="#b17">[18]</ref>, the input to our generator is a white noise vector z sampled from a multivariate normal distribution while the physical and social context vectors are its conditions. We simply concatenate the noise vector z and these context vectors as the input, i.e. C t G (i) = [C t So (i), C t P h (i), z]. Thus, the generated τ th future state's sample for each agent is attained by:</p><formula xml:id="formula_8">Y τ i = LST M dec C t G (i), h τ dec (i); W dec ,<label>(6)</label></formula><p>Discriminator (D) The discriminator in our case is another LSTM, LST M dis (·), which its input is a randomly chosen trajectory sample from the either ground truth or predicted future paths for each agent up to τ th future time frame, i.e. T 1:τ</p><formula xml:id="formula_9">i ∼ p(Ŷ 1:τ i , Y 1:τ i ) L τ i = LST M dis (T τ i , h τ dis (i); W dis ),<label>(7)</label></formula><p>whereL τ i is the predicted label from the discriminator for the chosen trajectory sample to be a ground truth (real) Y 1:τ i or predicted (fake)Ŷ 1:τ i with the truth label L τ i = 1 and L τ i = 0, respectively. The discriminator forces the generator to generate more realistic (plausible) states.</p><p>Losses To train SoPhie, we use the following losses:</p><formula xml:id="formula_10">W * = argmin W E i,τ [L GAN L τ i , L τ i + λL L2 (Ŷ 1:τ i , Y 1:τ i )],<label>(8)</label></formula><p>where W is the collection of the weights of all networks used in our model and λ is a regularizer between two losses. The adversarial loss L GAN (·, ·) and L2 loss L L2 (·, ·) are shown as follows:</p><formula xml:id="formula_11">L GAN L τ i , L τ i = min G max D E T 1:τ i ∼p(Y 1:τ i ) [L τ i logL τ i ] + E T 1:τ i ∼p(Ŷ 1:τ i ) [(1 − L τ i )log(1 −L τ i )],<label>(9)</label></formula><formula xml:id="formula_12">L L2 (Ŷ τ i , Y τ i ) = ||Ŷ τ i − Y τ i || 2 2 .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first evaluate our method on the commonly used datasets such as ETH <ref type="bibr" target="#b19">[20]</ref> and UCY <ref type="bibr" target="#b14">[15]</ref>, and on a recent and larger dataset, i.e. Stanford drone dataset <ref type="bibr" target="#b20">[21]</ref>. We also compare its performance against the various baselines on these datasets. Next, we present a qualitative analysis of our model on the effectiveness of the attention mechanisms. Finally, we finish the section by demonstrating some qualitative results on how our GAN based approach provides a good indication of path traversability for agents.</p><p>Datasets We perform baseline comparisons and ablation experiments on three core datasets. First, we explore the publicly available ETH <ref type="bibr" target="#b19">[20]</ref> and UCY <ref type="bibr" target="#b14">[15]</ref> datasets, which both contain annotated trajectories of real world pedestrians interacting in a variety of social situations. These datasets include nontrivial movements including pedestrian collisions, collision avoidance behavior, and group movement. Both of the datasets consists of a total of five unique scenes, Zara1, Zara2, and Univ (from UCY), and ETH and Hotel (from ETH). Each scene includes top-view images and 2D locations of each person with respect to the world coordinates. One image is used per scene as the cameras remain static. Each scene occurs in a relatively unconstrained outdoor environment, reducing the impact of physical constraints. We also explore the Stanford Drone Dataset (SDD) <ref type="bibr" target="#b20">[21]</ref>, a benchmark dataset for trajectory prediction problems. The dataset consists of a bird's-eye view of 20 unique scenes in which pedestrians, bikes, and cars navigate on a university campus. Similar to the previous datasets, images are provided from a top-view angle, but coordinates are provided in pixels. These scenes are outdoors and contain physical landmarks such as buildings and roundabouts that pedestrians avoid. Implementation details We iteratively trained the generator and discriminator models with the Adam optimizer, using a mini-batch size of 64 and a learning rate of 0.001 for both the generator and the discriminator. Models were trained for 200 epochs. The encoder encodes trajectories using a single layer MLP with an embedding dimension of 16. In the generator this is fed into a LSTM with a hidden dimension of 32; in the discriminator, the same occurs but with a dimension of 64. The decoder of the generator uses a single layer MLP with an embedding dimension of 16 to encoder agent positions and uses a LSTM with a hidden dimension of 32. In the social attention module, attention weights are retrieved by passing the encoder output and decoder context through multiple MLP layers of sizes 64, 128, 64, and 1, with interspersed ReLu activations. The final layer is passed through a Softmax layer. The interactions of the surrounding N max = 32 agents are considered; this value was chosen as no scenes in either dataset exceeded this number of total active agents in any given timestep. If there are less than N max agents, the dummy value of 0 is used. The physical attention module takes raw VGG features (512 channels), projects those using a convolutional layer, and embeds those using a single MLP to an embedding dimension of 16. The discriminator does not use the attention modules or the decoder network. When training we assume we have observed eight timesteps of an agent and are attempting to predict the next T = 12 timesteps. We weight our loss function by setting λ = 1.</p><p>/ In addition, to make our model more robust to scene orientation, we augmented the training data by flipping and rotating the scene and also normalization of agents' coordinates. We observed that these augmentations are conducive to make the trained model general enough in order to perform well on the unseen cases in the test examples and different scene geometries such as roundabouts.</p><p>Baselines &amp; Evaluation For the first two datasets, a few simple, but strong, baselines are used. These include Lin, a linear regressor that estimates linear parameters by minimizing the least square error; S-LSTM, a prediction model that combines LSTMs with a social pooling layer, as proposed by Alahi et. al. <ref type="bibr" target="#b0">[1]</ref>; S-GAN and S-GAN-P, predictive models that applies generative modeling to social LSTMs <ref type="bibr" target="#b7">[8]</ref>. For the drone dataset, we compare to the same linear and Social LSTM baselines, but also explore several other state-of-the-art methods. These include Social Forces, an implementation of the same Social Forces model from <ref type="bibr" target="#b26">[27]</ref>; DESIRE, an inverse optimal control (IOC) model proposed by Lee et. al. that utilizes generative modeling; and CAR-Net, a physically attentive model from <ref type="bibr" target="#b22">[23]</ref>. For all datasets, we also present results of various versions of our SoPhie model in an ablative setting by 1-T A : Sophie model with social features only and the social attention mechanism, 2-T O + I O Sophie model with both visual and social features without any attention mechanism, 3-T O + I A Sophie model with both visual and social features with only visual attention mechanism, 4-T A + I O Sophie model with both visual and social features with only social attention mechanism, and 5-T A + I A complete Sophie model with all modules.</p><p>All models are evaluated using the average displacement error (ADE) metric defined as the average L2 distance between the ground truth and pedestrian trajectories, over all pedestrians and all time steps, as well as the final displace-ment error metric (FDE). The evaluation task is defined to be performed over 8 seconds, using the past 8 positions consisting of the first 3.2 seconds as input, and predicting the remaining 12 future positions of the last 4.8 seconds. For the first two datasets, we follow a similar evaluation methodology to <ref type="bibr" target="#b7">[8]</ref> by performing a leave-one-out cross-validation policy where we train on four scenes, and test on the remaining one. These two datasets are evaluated in meter space. For the SDD, we utilize the standard split, and for the sake of comparison to baselines we report results in pixel space, after converting from meters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Results</head><p>ETH and UCY We compare our model to various baselines in <ref type="table" target="#tab_0">Table 1</ref>, reporting the average displacement error (ADE) in meter space, as well as the final displacement error (FDE). As expected, we see that in general the linear model performs the worst, as it is unable to model the complex social interactions between different humans and the interactions between humans and their physical space. We also notice that S-LSTM provides an improvement over the linear baseline, due to its use of social pooling, and that S-GAN provides an improvement to this LSTM baseline, by approaching the problem from a generative standpoint.</p><p>Our first model, T A , which solely applies social context to pedestrian trajectories, performs slightly better than the S-GAN on average due to better feature extraction strategy and attention module. As expected, although social context helps the model form better predictions, it alone is not enough to truly understand the interactions in a scene. Similarly, while our second model T O + I O applies both pedestrian trajectories and features from the physical scene (no attention), the lack of any context about these additional features make the model unable to learn which components are most important, giving it a similar accuracy to T A . Our first major gains in model performance come when exploring the T O + I A and T A + I O models. Because the former applies physical context to image features and the latter applies social context to trajectory features, each model is able to learn the important aspects of interactions, allowing them to slightly outperform the previous models. Interestingly, T O + I A performs slightly better than T A + I O potentially suggesting that understanding physical context is slightly more helpful in a prediction task. The final SoPhie model, consisting of social attention on trajectories and physical attention on image features (T A + I A ) outperformed the previous models, suggesting that combining both forms of attention allows for robust model predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stanford Drone Dataset</head><p>We next compare our method to various baselines in <ref type="table">Table 2</ref>, reporting the ADE and FDE in pixel space. Much like the previous datasets, with SDD we see that the linear baseline performs the worst, with S-LSTM and S-GAN providing an improvement in accuracy. The next major improvement in accuracy is made with CAR-Net, due to the use of physical attention. This is likely due to the nature of SDD, where pedestrian movements based on the curvature of the road can be extrapolated from the birds eye view of the scene. The next major improvement in accuracy is made with the DESIRE framework, which explores trajectory prediction from a generative standpoint, making it the best baseline. Note that the DESIRE results are linearly interpolated from the 4.0s result reported in <ref type="bibr" target="#b13">[14]</ref> to 4.8s, as their code is not publicly available. Finally, incorporating social context in T A , as well as both social and physical context in T A + I A allow for significant model improvements, suggesting that both attentive models are crucial to tackling the trajectory prediction problem.</p><p>Impact of social and physical constraints. Since the goal is to produce socially acceptable paths we also used a different evaluation metrics that reflect the percentage of near-collisions (if two pedestrians get closer than the threshold of 0.10m). We have calculated the average percentage of pedestrian near collisions across all frames in each of the BIWI/ETH scenes. These results are presented in <ref type="table">Table  3</ref>. To better understand our model's ability to also produce physically plausible paths, we also split the test set of the Stanford Drone Dataset into two subsets: simple and complex, as previously done in CAR-Net <ref type="bibr" target="#b22">[23]</ref> and report results in <ref type="table" target="#tab_2">Table 4</ref>. We note that the S-GAN baseline achieves decent performance on simple scenes, but is unable to generalize well to physically complex ones. On the other hand, CAR-Net and SoPhie both achieves a slight performance increase on simple scenes over S-GAN and trajectory only LSTM, as well as nearly halving the error on complex scenes, due to this physical context. This experiment demonstrates that Sophie's use of physical attention successfully allows it to predict both physical and socially acceptable paths.  <ref type="table">Table 3</ref>. Average % of colliding pedestrians per frame for each of the scenes in BIWI/ETH. A collision is detected if the euclidean distance between two pedestrians is less than 0.10m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Results</head><p>We further investigate the ability of our architecture to model how social and physical interactions impact future  trajectories. <ref type="figure" target="#fig_1">Fig. 3</ref> demonstrates the affects that attention can have in correcting erroneous predictions. Here we visualize three unique scenarios, comparing a baseline Social GAN prediction to that of our model. In the first scenario (A), physical attention ensures the trajectory of the green pedestrian follows the curve of the road. In the second, scenario B, social attention on the green pedestrian ensures that the main blue pedestrian does not collide with either pedestrian. In the third scenario (C), physical attention is applied to ensure the red pedestrian stays within the road, while social attention ensures that the blue pedestrian does not collide with the red one. As such, the introduction of social and physical attention not only allows for greater model interpretability but also better aligns predictions to scene constraints.</p><formula xml:id="formula_13">SoPhie Social GAN SoPhie Social GAN SoPhie Social GAN (A) (B) (C)</formula><p>An additional benefit of the generative SoPhie architecture is that it can be used to understand which areas in a scene are traversable. To show the effectiveness of our method, we sampled 30 random agents from the test set (i.e., first 8 seconds of each trajectory) and the generator generated sample trajectories using this starting points. These generated trajectories were then validated using the discriminator. The distribution of these trajectories results in an interpretable traversability map, as in <ref type="figure">Fig. 4</ref>. Each image represents a unique scene from SDD, with the overlayed heatmap showing traversable areas and the blue crosses showing the starting samples. With Nexus 6, the model is able to successfully identify the traversable areas as the central road and the path to the side. With Little 1, the model identifies the main sidewalk that pedestrians walk on while correctly ignoring the road that pedestrians avoid. In Huang 1, the model is able to correctly identify the cross section as well as side paths on the image. We thus observe that the generative network can successfully be used to explore regions of traversability in scenes even with a small number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a trajectory prediction framework that outperforms state-of-the-art methods on multiple benchmark datasets. Our method leverages complete scene context and interactions of all agents, while enabling interpretable predictions, using social and physical attention mechanisms. To generate a distribution over the predicted trajectories, we proposed an attentive GAN which can successfully generate multiple physically acceptable paths that respect social constraints of the environment. We showed that by modeling jointly the information about the physical environment and interactions between all agents, our model learns to perform better than when this information is used independently.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>An overview of SoPhie architecture. Sophie consists of three key modules including: (a) A feature extractor module, (b) An attention module, and (c) An LSTM based GAN module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Three sample scenarios where physical and social attention allow correct predictions and fixes the Social GAN errors. In all figures, past and predicted trajectories are plotted as line and distributions, respectively. We display the weight maps of the physical attention mechanism highlighted in white on the image. The white boxes on the agents show the social attention on the agents with respect to the blue agent. The size of the boxes are relative to the attention weights on different agents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>T A T O + I O T O + I A T A + I O T A + I A ETH 1.33 / 2.94 1.09 / 2.41 1.09 / 2.35 0.81 / 1.52 0.87 / 1.62 0.90 / 1.60 0.86 / 1.65 0.71 / 1.47 0.76 / 1.54 0.70 / 1.43 HOTEL 0.39 / 0.72 0.86 / 1.91 0.79 / 1.76 0.72 / 1.61 0.67 / 1.37 0.87 / 1.82 0.84 / 1.80 0.80 / 1.78 0.83 / 1.79 0.76 / 1.67 UNIV 0.82 / 1.59 0.61 / 1.31 0.67 / 1.40 0.60 / 1.26 0.76 / 1.52 0.49 / 1.19 0.58 / 1.27 0.55 / 1.23 0.55 / 1.25 0.54 / 1.24 ZARA1 0.62 / 1.21 0.41 / 0.88 0.47 / 1.00 0.34 / 0.69 0.35 / 0.68 0.38 / 0.72 0.34 / 0.68 0.35 / 0.67 0.32 / 0.64 0.30 / 0.63 ZARA2 0.77 / 1.48 0.52 / 1.11 0.56 / 1.17 0.42 / 0.84 0.42 / 0.84 0.38 / 0.79 0.40 / 0.82 0.43 / 0.87 0.41 / 0.80 0.38 / 0.78 AVG 0.79 / 1.59 0.70 / 1.52 0.72 / 1.54 0.58 / 1.18 0.61 / 1.21 0.61 / 1.22 0.61 / 1.24 0.57 / 1.20 0.58 / 1.20 0.54 / 1.15 Quantitative results of baseline models vs. SoPhie architectures across datasets on the task of predicting 12 future timesteps, given the 8 previous ones. Error metrics reported are ADE / FDE in meters. SoPhie models consistently outperform the baselines, due to the combination of social and physical attention applied in a generative model setting. SDD 37.11 / 63.51 36.48 / 58.14 31.19 / 56.97 27.246 / 41.440 25.72 / 51.8 19.25 / 34.05 17.76 / 32.14 18.40 / 33.78 16.52 / 29.64 17.57 / 33.31 16.27 / 29.38 Table 2. ADE and FDE in pixels of various models on Stanford Drone Dataset. SoPhie's main performance gain comes from the joint introduction of social and physical attention applied in a generative modeling setting.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell cols="2">SoPhie (Ours)</cell><cell></cell><cell></cell></row><row><cell>Dataset Lin</cell><cell></cell><cell>LSTM</cell><cell cols="3">S-LSTM S-GAN-P Baselines S-GAN</cell><cell></cell><cell></cell><cell>SoPhie (Ours)</cell><cell></cell><cell></cell></row><row><cell>Dataset Lin</cell><cell>SF</cell><cell>S-LSTM</cell><cell>S-GAN</cell><cell>CAR-Net</cell><cell>DESIRE</cell><cell>TA</cell><cell>TO + IO</cell><cell>TO + IA</cell><cell>TA + I</cell><cell>TA + IA</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Figure 4. Using the generator to sample trajectories and the discriminator to validate those paths, we present highly accurate traversability maps for SDD scenes. Maps are presented in red, and generated only with 30 starting samples, illustrated as blue crosses. Performance of multiple baselines on the Stanford Drone Dataset, split into physically simple and complex scenes. Error is ADE and is reported in pixels.</figDesc><table><row><cell>Nexus 6</cell><cell></cell><cell>Little 1</cell><cell>Huang 1</cell></row><row><cell>Model</cell><cell cols="2">Complex Simple</cell></row><row><cell>LSTM</cell><cell>31.31</cell><cell>30.48</cell></row><row><cell cols="2">CAR-Net 24.32</cell><cell>30.92</cell></row><row><cell>S-GAN</cell><cell>29.29</cell><cell>22.24</cell></row><row><cell>SoPhie</cell><cell>15.61</cell><cell>21.08</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Socially-aware large-scale crowd forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition, number EPFL-CONF-230284</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2211" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge transfer for scene-specific motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="697" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Contextaware trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02503</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcfadyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04706</idno>
		<title level="m">Tree memory networks for modelling long-term temporal dependencies</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Soft+ hardwired attention: An lstm framework for human trajectory prediction and abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05552</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10892</idno>
		<title level="m">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the reliability of lstm-mdl models for pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hübner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arens</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">VIIth International Workshop on Representation, analysis and recognition of shape and motion FroM Image data</title>
		<imprint/>
	</monogr>
	<note>RFMI 2017</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Particle-based pedestrian path prediction using lstm-mdl models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hübner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05546</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Probabilistic vehicle trajectory prediction over occupancy grid map via recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07049</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="201" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Crowds by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Trajectory learning for activity understanding: Unsupervised, multilevel, and long-term adaptive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2287" to="2301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving data association by joint modeling of pedestrian trajectories and groupings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory understanding in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Legros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Voisin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10061</idno>
		<title level="m">Car-net: Clairvoyant attentive recurrent network</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ss-lstm: A hierarchical lstm model for pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>IEEE Winter Conference on</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Who are you with and where are you going?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
