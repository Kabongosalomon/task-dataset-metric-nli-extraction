<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Group Equivariant Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-06-03">3 Jun 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">T.S.COHEN@UVA.NL University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Irvine Canadian Institute for Advanced Research</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling@uva</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Irvine Canadian Institute for Advanced Research</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nl</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Irvine Canadian Institute for Advanced Research</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Group Equivariant Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-06-03">3 Jun 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CI-FAR10 and rotated MNIST.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks (CNNs, convnets) have proven to be very powerful models of sensory data such as images, video, and audio. Although a strong theory of neural network design is currently lacking, a large amount of empirical evidence supports the notion that both convolutional weight sharing and depth (among other factors) are important for good predictive performance.</p><p>Convolutional weight sharing is effective because there is a translation symmetry in most perception tasks: the label function and data distribution are both approximately invariant to shifts. By using the same weights to analyze or model each part of the image, a convolution layer uses far fewer parameters than a fully connected one, while preserving the capacity to learn many useful transformations. Convolution layers can be used effectively in a deep network because all the layers in such a network are translation equivariant: shifting the image and then feeding it through a number of layers is the same as feeding the original image through the same layers and then shifting the resulting feature maps (at least up to edge-effects). In other words, the symmetry (translation) is preserved by each layer, which makes it possible to exploit it not just in the first, but also in higher layers of the network.</p><p>In this paper we show how convolutional networks can be generalized to exploit larger groups of symmetries, including rotations and reflections. The notion of equivariance is key to this generalization, so in section 2 we will discuss this concept and its role in deep representation learning. After discussing related work in section 3, we recall a number of mathematical concepts in section 4 that allow us to define and analyze the G-convolution in a generic manner.</p><p>In section 5, we analyze the equivariance properties of standard CNNs, and show that they are equivariant to translations but may fail to equivary with more general transformations. Using the mathematical framework from section 4, we can define G-CNNs (section 6) by analogy to standard CNNs (the latter being the G-CNN for the translation group). We show that G-convolutions, as well as various kinds of layers used in modern CNNs, such as pooling, arbitrary pointwise nonlinearities, batch normalization and residual blocks are all equivariant, and thus compatible with G-CNNs. In section 7 we provide concrete implementation details for group convolutions.</p><p>In section 8 we report experimental results on MNIST-rot and CIFAR10, where G-CNNs achieve state of the art results (2.28% error on MNIST-rot, and 4.19% resp. 6.46% on augmented and plain CIFAR10). We show that replacing planar convolutions with G-convolutions consistently improves results without additional tuning. In section 9 we provide a discussion of these results and consider several extensions of the method, before concluding in section 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Structured &amp; Equivariant Representations</head><p>Deep neural networks produce a sequence of progressively more abstract representations by mapping the input through a series of parameterized functions . In the current generation of neural networks, the representation spaces are usually endowed with very minimal internal structure, such as that of a linear space R n .</p><p>In this paper we construct representations that have the structure of a linear G-space, for some chosen group G. This means that each vector in the representation space has a pose associated with it, which can be transformed by the elements of some group of transformations G. This additional structure allows us to model data more efficiently: A filter in a G-CNN detects co-occurrences of features that have the preferred relative pose, and can match such a feature constellation in every global pose through an operation called the G-convolution.</p><p>A representation space can obtain its structure from other representation spaces to which it is connected. For this to work, the network or layer Φ that maps one representation to another should be structure preserving. For G-spaces this means that Φ has to be equivariant:</p><formula xml:id="formula_0">Φ(T g x) = T ′ g Φ(x),<label>(1)</label></formula><p>That is, transforming an input x by a transformation g (forming T g x) and then passing it through the learned map Φ should give the same result as first mapping x through Φ and then transforming the representation.</p><p>Equivariance can be realized in many ways, and in particular the operators T and T ′ need not be the same. The only requirement for T and T ′ is that for any two transformations g and h, we have T (gh) = T (g)T (h) (i.e. T is a linear representation of G).</p><p>From equation 1 we see that the familiar concept of invariance is a special kind of equivariance where T ′ g is the identity transformation for all g. In deep learning, general equivariance is more useful than invariance because it is impossible to determine if features are in the right spatial configuration if they are invariant.</p><p>Besides improving statistical efficiency and facilitating geometrical reasoning, equivariance to symmetry transformations constrains the network in a way that can aid generalization. A network Φ can be non-injective, meaning that non-identical vectors x and y in the input space become identical in the output space (for example, two instances of a face may be mapped onto a single vector indicating the presence of any face). If Φ is equivariant, then the Gtransformed inputs T g x and T g y must also be mapped to the same output. Their "sameness" (as judged by the network) is preserved under symmetry transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>There is a large body of literature on invariant representations. Invariance can be achieved by pose normalization using an equivariant detector <ref type="bibr" target="#b28">(Lowe, 2004;</ref><ref type="bibr" target="#b16">Jaderberg et al., 2015)</ref> or by averaging a possibly nonlinear function over a group <ref type="bibr" target="#b32">(Reisert, 2008;</ref><ref type="bibr" target="#b35">Skibbe, 2013;</ref><ref type="bibr" target="#b29">Manay et al., 2006;</ref><ref type="bibr" target="#b19">Kondor, 2007)</ref>.</p><p>Scattering convolution networks use wavelet convolutions, nonlinearities and group averaging to produce stable invariants <ref type="bibr" target="#b3">(Bruna &amp; Mallat, 2013)</ref>. Scattering networks have been extended to use convolutions on the group of translations, rotations and scalings, and have been applied to object and texture recognition <ref type="bibr" target="#b34">(Sifre &amp; Mallat, 2013;</ref><ref type="bibr" target="#b31">Oyallon &amp; Mallat, 2015)</ref>.</p><p>A number of recent works have addressed the problem of learning or constructing equivariant representations. This includes work on transforming autoencoders <ref type="bibr" target="#b14">(Hinton et al., 2011)</ref>, equivariant Boltzmann machines <ref type="bibr" target="#b18">(Kivinen &amp; Williams, 2011;</ref><ref type="bibr" target="#b36">Sohn &amp; Lee, 2012)</ref>, equivariant descriptors <ref type="bibr" target="#b33">(Schmidt &amp; Roth, 2012)</ref>, and equivariant filtering <ref type="bibr" target="#b35">(Skibbe, 2013)</ref>. <ref type="bibr" target="#b26">Lenc &amp; Vedaldi (2015)</ref> show that the AlexNet CNN <ref type="bibr" target="#b20">(Krizhevsky et al., 2012)</ref> trained on imagenet spontaneously learns representations that are equivariant to flips, scaling and rotation. This supports the idea that equivariance is a good inductive bias for deep convolutional networks. <ref type="bibr" target="#b0">Agrawal et al. (2015)</ref> show that useful representations can be learned in an unsupervised manner by training a convolutional network to be equivariant to ego-motion. <ref type="bibr" target="#b1">Anselmi et al. (2014;</ref> use the theory of locally compact topological groups to develop a theory of statistically efficient learning in sensory cortex. This theory was implemented for the commutative group consisting of time-and vocal tract length shifts for an application to speech recognition by <ref type="bibr" target="#b42">Zhang et al. (2015)</ref>. <ref type="bibr" target="#b9">Gens &amp; Domingos (2014)</ref> proposed an approximately equivariant convolutional architecture that uses sparse, high-dimensional feature maps to deal with highdimensional groups of transformations. <ref type="bibr" target="#b7">Dieleman et al. (2015)</ref> showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation. This work was later extended <ref type="bibr" target="#b8">(Dieleman et al., 2016)</ref> and evaluated on various computer vision problems that have cyclic symmetry. <ref type="bibr" target="#b5">Cohen &amp; Welling (2014)</ref> showed that the concept of disentangling can be understood as a reduction of the operators T g in an equivariant representation, and later related this notion of disentangling to the more familiar statistical notion of decorrelation <ref type="bibr" target="#b6">(Cohen &amp; Welling, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Mathematical Framework</head><p>In this section we present a mathematical framework that enables a simple and generic definition and analysis of G-CNNs for various groups G. We begin by defining symmetry groups, and study in particular two groups that are used in the G-CNNs we have built so far. Then we take a look at functions on groups (used to model feature maps in G-CNNs) and their transformation properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Symmetry Groups</head><p>A symmetry of an object is a transformation that leaves the object invariant. For example, if we take the sampling grid of our image, Z 2 , and flip it over we get −Z 2 = {(−n, −m) | (n, m) ∈ Z 2 } = Z 2 . So the flipping operation is a symmetry of the sampling grid.</p><p>If we have two symmetry transformations g and h and we compose them, the result gh is another symmetry transformation (i.e. it leaves the object invariant as well). Furthermore, the inverse g −1 of any symmetry is also a symmetry, and composing it with g gives the identity transformation e. A set of transformations with these properties is called a symmetry group.</p><p>One simple example of a group is the set of 2D integer translations, Z 2 . Here the group operation ("composition of transformations") is addition: (n, m) + (p, q) = (n + p, m + q). One can verify that the sum of two translations is again a translation, and that the inverse (negative) of a translation is a translation, so this is indeed a group.</p><p>Although it may seem fancy to call 2-tuples of integers a group, this is helpful in our case because as we will see in section 6, a useful notion of convolution can be defined for functions on any group 1 , of which Z 2 is only one example. The important properties of the convolution, such as equivariance, arise primarily from the group structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The group p4</head><p>The group p4 consists of all compositions of translations and rotations by 90 degrees about any center of rotation in a square grid. A convenient parameterization of this group in terms of three integers r, u, v is</p><formula xml:id="formula_1">g(r, u, v) =   cos (rπ/2) − sin(rπ/2) u sin(rπ/2) cos(rπ/2) v 0 0 1   , (2) where 0 ≤ r &lt; 4 and (u, v) ∈ Z 2 .</formula><p>The group operation is given by matrix multiplication.</p><p>The composition and inversion operations could also be represented directly in terms of integers (r, u, v), but the 1 At least, on any locally compact group. equations are cumbersome. Hence, our preferred method of composing two group elements represented by integer tuples is to convert them to matrices, multiply these matrices, and then convert the resulting matrix back to a tuple of integers (using the atan2 function to obtain r).</p><p>The group p4 acts on points in Z 2 (pixel coordinates) by multiplying the matrix g(r, u, v) by the homogeneous co-</p><formula xml:id="formula_2">ordinate vector x(u ′ , v ′ ) of a point (u ′ , v ′ ): gx ≃   cos(rπ/2) − sin(rπ/2) u sin(rπ/2) cos(rπ/2) v 0 0 1     u ′ v ′ 1   (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The group p4m</head><p>The group p4m consists of all compositions of translations, mirror reflections, and rotations by 90 degrees about any center of rotation in the grid. Like p4, we can parameterize this group by integers:</p><formula xml:id="formula_3">g(m, r, u, v) =   (−1) m cos( rπ 2 ) −(−1) m sin( rπ 2 ) u sin( rπ 2 ) cos( rπ 2 ) v 0 0 1   , where m ∈ {0, 1}, 0 ≤ r &lt; 4 and (u, v) ∈ Z 2 .</formula><p>The reader may verify that this is indeed a group.</p><p>Again, composition is most easily performed using the matrix representation. Computing r, u, v from a given matrix g can be done using the same method we use for p4, and for m we have m = 1 2 (1 − det(g)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Functions on groups</head><p>We model images and stacks of feature maps in a conventional CNN as functions f : Z 2 → R K supported on a bounded (typically rectangular) domain. At each pixel coordinate (p, q) ∈ Z 2 , the stack of feature maps returns a K-dimensional vector f (p, q), where K denotes the number of channels.</p><p>Although the feature maps must always be stored in finite arrays, modeling them as functions that extend to infinity (while being non-zero on a finite region only) simplifies the mathematical analysis of CNNs.</p><p>We will be concerned with transformations of the feature maps, so we introduce the following notation for a transformation g acting on a set of feature maps:</p><formula xml:id="formula_4">[L g f ](x) = [f • g −1 ](x) = f (g −1 x)<label>(4)</label></formula><p>Computationally, this says that to get the value of the gtransformed feature map L g f at the point x, we need to do a lookup in the original feature map f at the point g −1 x, which is the unique point that gets mapped to x by g. This operator L g is a concrete instantiation of the transformation operator T g referenced in section 2, and one may verify that</p><formula xml:id="formula_5">L g L h = L gh .<label>(5)</label></formula><p>If g represents a pure translation t = (u, v) ∈ Z 2 then g −1 x simply means x − t. The inverse on g in equation 4 ensures that the function is shifted in the positive direction when using a positive translation, and that L g satisfies the criterion for being a homomorphism (eq. 5) even for transformations g and h that do not commute (i.e. gh = hg).</p><p>As will be explained in section 6.1, feature maps in a G-CNN are functions on the group G, instead of functions on the group Z 2 . For functions on G, the definition of L g is still valid if we simply replace x (an element of Z 2 ) by h (an element of G), and interpret g −1 h as composition.</p><p>It is easy to mentally visualize a planar feature map f : Z 2 → R undergoing a transformation, but we are not used to visualizing functions on groups. To visualize a feature map or filter on p4, we plot the four patches associated with the four pure rotations on a circle, as shown in figure 1 (left). Each pixel in this figure has a rotation coordinate (the patch in which the pixel appears), and two translation coordinates (the pixel position within the patch). When we apply the 90 degree rotation r to a function on p4, each planar patch follows its red r-arrow (thus incrementing the rotation coordinate by 1 (mod 4)), and simultaneously undergoes a 90-degree rotation. The result of this operation is shown on the right of figure 1. As we will see in section 6, a p4 feature map in a p4-CNN undergoes exactly this motion under rotation of the input image.</p><p>For p4m, we can make a similar plot, shown in figure 2. A p4m function has 8 planar patches, each one associated with a mirroring m and rotation r. Besides red rotation arrows, the figure now includes small blue reflection lines (which are undirected, since reflections are self-inverse).</p><p>Upon rotation of a p4m function, each patch again follows its red r-arrows and undergoes a 90 degree rotation. Under a mirroring, the patches connected by a blue line will change places and undergo the mirroring transformation. This rich transformation structure arises from the group operation of p4 or p4m, combined with equation 4 which describes the transformation of a function on a group.</p><p>Finally, we define the involution of a feature map, which will appear in section 6.1 when we study the behavior of the G-convolution, and which also appears in the gradient of the G-convolution. We have:</p><formula xml:id="formula_6">f * (g) = f (g −1 )<label>(6)</label></formula><p>For Z 2 feature maps the involution is just a point reflection, but for G-feature maps the meaning depends on the structure of G. In all cases, f * * = f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Equivariance properties of CNNs</head><p>In this section we recall the definitions of the convolution and correlation operations used in conventional CNNs, and show that these operations are equivariant to translations but not to other transformations such as rotation. This is certainly well known and easy to see by mental visualization, but deriving it explicitly will make it easier to follow the derivation of group equivariance of the group convolution defined in the next section.</p><p>At each layer l, a regular convnet takes as input a stack of feature maps f : Z 2 → R K l and convolves or correlates it with a set of K l+1 filters ψ i : Z 2 → R K l :</p><formula xml:id="formula_7">[f * ψ i ](x) = y∈Z 2 K l k=1 f k (y)ψ i k (x − y) [f ⋆ ψ i ](x) = y∈Z 2 K l k=1 f k (y)ψ i k (y − x)<label>(7)</label></formula><p>If one employs convolution ( * ) in the forward pass, the correlation (⋆) will appear in the backward pass when computing gradients, and vice versa. We will use the correlation in the forward pass, and refer generically to both operations as "convolution".</p><p>Using the substitution y → y + t, and leaving out the summation over feature maps for clarity, we see that a transla-tion followed by a correlation is the same as a correlation followed by a translation:</p><formula xml:id="formula_8">[[L t f ] ⋆ ψ](x) = y f (y − t)ψ(y − x) = y f (y)ψ(y + t − x) = y f (y)ψ(y − (x − t)) = [L t [f ⋆ ψ]](x).<label>(8)</label></formula><p>And so we say that "correlation is an equivariant map for the translation group", or that "correlation and translation commute". Using an analogous computation one can show that also for the convolution,</p><formula xml:id="formula_9">[L t f ] * ψ = L t [f * ψ].</formula><p>Although convolutions are equivariant to translation, they are not equivariant to other isometries of the sampling lattice. For instance, as shown in the supplementary material, rotating the image and then convolving with a fixed filter is not the same as first convolving and then rotating the result:</p><formula xml:id="formula_10">[[L r f ] ⋆ ψ](x) = L r [f ⋆ [L r −1 ψ]](x)<label>(9)</label></formula><p>In words, this says that the correlation of a rotated image L r f with a filter ψ is the same as the rotation by r of the original image f convolved with the inverse-rotated filter L r −1 ψ. Hence, if an ordinary CNN learns rotated copies of the same filter, the stack of feature maps is equivariant, although individual feature maps are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Group Equivariant Networks</head><p>In this section we will define the three layers used in a G-CNN (G-convolution, G-pooling, nonlinearity) and show that each one commutes with G-transformations of the domain of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">G-Equivariant correlation</head><p>The correlation (eq. 7) is computed by shifting a filter and then computing a dot product with the feature maps. By replacing the shift by a more general transformation from some group G, we get the G-correlation used in the first layer of a G-CNN:</p><formula xml:id="formula_11">[f ⋆ ψ](g) = y∈Z 2 k f k (y)ψ k (g −1 y).<label>(10)</label></formula><p>Notice that both the input image f and the filter ψ are functions of the plane Z 2 , but the feature map f ⋆ψ is a function on the discrete group G (which may contain translations as a subgroup). Hence, for all layers after the first, the filters ψ must also be functions on G, and the correlation operation becomes</p><formula xml:id="formula_12">[f ⋆ ψ](g) = h∈G k f k (h)ψ k (g −1 h).<label>(11)</label></formula><p>The equivariance of this operation is derived in complete analogy to eq. 8, now using the substitution h → uh:</p><formula xml:id="formula_13">[[L u f ] ⋆ ψ](g) = h∈G k f k (u −1 h)ψ(g −1 h) = h∈G k f (h)ψ(g −1 uh) = h∈G k f (h)ψ((u −1 g) −1 h) = [L u [f ⋆ ψ]](g)<label>(12)</label></formula><p>The equivariance of eq. 10 is derived similarly. Note that although equivariance is expressed by the same formula</p><formula xml:id="formula_14">[L u f ] ⋆ ψ = L u [f ⋆ ψ]</formula><p>for both first-layer G-correlation (eq. 10) and full G-correlation (11), the meaning of the operator L u is different: for the first layer correlation, the inputs f and ψ are functions on Z 2 , so L u f denotes the transformation of such a function, while L u [f ⋆ ψ] denotes the transformation of the feature map, which is a function on G. For the full G-correlation, both the inputs f and ψ and the output f ⋆ ψ are functions on G.</p><p>Note that if G is not commutative, neither the Gconvolution nor the G-correlation is commutative. However, the feature maps ψ ⋆ f and f ⋆ ψ are related by the involution (eq. 6):</p><formula xml:id="formula_15">f ⋆ ψ = (ψ ⋆ f ) * .<label>(13)</label></formula><p>Since the involution is invertible (it is its own inverse), the information content of f ⋆ψ and ψ⋆f is the same. However, f ⋆ ψ is more efficient to compute when using the method described in section 7, because transforming a small filter is faster than transforming a large feature map.</p><p>It is customary to add a bias term to each feature map in a convolution layer. This can be done for G-conv layers as well, as long as there is only one bias per Gfeature map (instead of one bias per spatial feature plane within a G-feature map). Similarly, batch normalization <ref type="bibr" target="#b15">(Ioffe &amp; Szegedy, 2015)</ref> should be implemented with a single scale and bias parameter per G-feature map in order to preserve equivariance. The sum of two G-equivariant feature maps is also G-equivariant, thus G-conv layers can be used in highway networks and residual networks <ref type="bibr" target="#b38">(Srivastava et al., 2015;</ref><ref type="bibr" target="#b12">He et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Pointwise nonlinearities</head><p>Equation <ref type="formula" target="#formula_0">12</ref> shows that G-correlation preserves the transformation properties of the previous layer. What about nonlinearities and pooling?</p><p>Recall that we think of feature maps as functions on G. In this view, applying a nonlinearity ν : R → R to a feature map amounts to function composition. We introduce the composition operator</p><formula xml:id="formula_16">C ν f (g) = [ν • f ](g) = ν(f (g)).<label>(14)</label></formula><p>which acts on functions by post-composing them with ν.</p><p>Since the left transformation operator L acts by precomposition, C and L commute:</p><formula xml:id="formula_17">C ν L h f = ν • [f • h −1 ] = [ν • f ] • h −1 = L h C ν f,<label>(15)</label></formula><p>so the rectified feature map inherits the transformation properties of the previous layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Subgroup pooling and coset pooling</head><p>In order to simplify the analysis, we split the pooling operation into two steps: the pooling itself (performed without stride), and a subsampling step. The non-strided maxpooling operation applied to a feature map f : G → R can be modeled as an operator P that acts on f as</p><formula xml:id="formula_18">P f (g) = max k∈gU f (k),<label>(16)</label></formula><p>where gU = {gu | u ∈ U } is the g-transformation of some pooling domain U ⊂ G (typically a neighborhood of the identity transformation). In a regular convnet, U is usually a 2 × 2 or 3 × 3 square including the origin (0, 0), and g is a translation.</p><p>As shown in the supplementary material, pooling commutes with L h :</p><formula xml:id="formula_19">P L h = L h P<label>(17)</label></formula><p>Since pooling tends to reduce the variation in a feature map, it makes sense to sub-sample the pooled feature map, or equivalently, to do a "pooling with stride". In a G-CNN, the notion of "stride" is generalized by subsampling on a subgroup H ⊂ G. That is, H is a subset of G that is itself a group (i.e. closed under multiplication and inverses). The subsampled feature map is then equivariant to H but not G.</p><p>In a standard convnet, pooling with stride 2 is the same as pooling and then subsampling on</p><formula xml:id="formula_20">H = {(2i, 2j) |(i, j) ∈ Z 2 } which is a subgroup of G = Z 2 .</formula><p>For the p4-CNN, we may subsample on the subgroup H containing all 4 rotations, as well as shifts by multiples of 2 pixels.</p><p>We can obtain full G-equivariance by choosing our pooling region U to be a subgroup H ⊂ G. The pooling domains gH that result are called cosets in group theory. The cosets partition the group into non-overlapping regions. The feature map that results from pooling over cosets is invariant to the right-action of H, because the cosets are similarly invariant (ghH = gH). Hence, we can arbitrarily choose one coset representative per coset to subsample on. The feature map that results from coset pooling may be thought of as a function on the quotient space G/H, in which two transformations are considered equivalent if they are related by a transformation in H.</p><p>As an example, in a p4 feature map, we can pool over all four rotations at each spatial position (the cosets of the subgroup R of rotations around the origin). The resulting feature map is a function on Z 2 ∼ = p4/R, i.e. it will transform in the same way as the input image. Another example is given by a feature map on Z, where we could pool over the cosets of the subgroup nZ of shifts by multiples of n. This gives a feature map on Z/nZ, which has a cyclic transformation law under translations.</p><p>This concludes our analysis of G-CNNs. Since all layer types are equivariant, we can freely stack them into deep networks and expect G-conv parameter sharing to be effective at arbitrary depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Efficient Implementation</head><p>Computing the G-convolution for involves nothing more than indexing arithmetic and inner products, so it can be implemented straightforwardly.</p><p>Here we present the details for a G-convolution implementation that can leverage recent advances in fast computation of planar convolutions <ref type="bibr" target="#b30">(Mathieu et al., 2014;</ref><ref type="bibr" target="#b39">Vasilache et al., 2015;</ref><ref type="bibr" target="#b22">Lavin &amp; Gray, 2015)</ref>.</p><p>A plane symmetry group G is called split if any transformation g ∈ G can be decomposed into a translation t ∈ Z 2 and a transformation s in the stabilizer of the origin (i.e. s leaves the origin invariant). For the group p4, we can write g = ts for t a translation and s a rotation about the origin, while p4m splits into translations and rotation-flips. Using this split of G and the fact that L g L h = L gh , we can rewrite the G-correlation (eq. 10 and 11) as follows:</p><formula xml:id="formula_21">f ⋆ ψ(ts) = h∈X k f k (h)L t [L s ψ k (h)]<label>(18)</label></formula><p>where X = Z 2 in layer one and X = G in further layers.</p><p>Thus, to compute the p4 (or p4m) correlation f ⋆ ψ we can first compute L s ψ ("filter transformation") for all four rotations (or all eight rotation-flips) and then call a fast planar correlation routine on f and the augmented filter bank.</p><p>The computational cost of the algorithm presented here is roughly equal to that of a planar convolution with a filter bank that is the same size as the augmented filter bank used in the G-convolution, because the cost of the filter transformation is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Filter transformation</head><p>The set of filters at layer l is stored in an array F [·] of shape K l × K l−1 × S l−1 × n × n, where K l is the number of channels at layer l, S l−1 denotes the number of transformations in G that leave the origin invariant (e.g. 1, 4 or 8 for Z 2 , p4 or p4m filters, respectively), and n is the spatial (or translational) extent of the filter. Note that typically, S 1 = 1 for 2D images, while S l = 4 or S l = 8 for l &gt; 1.</p><p>The filter transformation L s amounts to a permutation of the entries of each of the K l × K l−1 scalar-valued filter channels in F . Since we are applying S l transformations to each filter, the output of this operation is an array of shape K l × S l × K l−1 × S l−1 × n × n, which we call F + .</p><p>The permutation can be implemented efficiently by a GPU kernel that does a lookup into F for each output cell of F + , using a precomputed index associated with the output cell. To precompute the indices, we define an invertible map g(s, u, v) that takes an input index (valid for an array of shape S l−1 × n × n) and produces the associated group element g as a matrix (section 4.2 and 4.3). For each input index (s, u, v) and each transformation s ′ , we computē s,ū,v = g −1 (g(s ′ , 0, 0) −1 g(s, u, v)). This index is used</p><formula xml:id="formula_22">to set F + [i, s ′ , j, s, u, v] = F [i, j,s,ū,v] for all i, j.</formula><p>The G-convolution for a new group can be added by simply implementing a map g(·) from indices to matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Planar convolution</head><p>The second part of the G-convolution algorithm is a planar convolution using the expanded filter bank F + . If S l−1 &gt; 1, the sum over X in eq. 18 involves a sum over the stabilizer. This sum can be folded into the sum over feature channels performed by the planar convolution routine by reshaping F + from K l × S l × K l−1 × S l−1 × n × n to S l K l ×S l−1 K l−1 ×n×n. The resulting array can be interpreted as a conventional filter bank with S l−1 K l−1 planar input channels and S l K l planar output channels, which can be correlated with the feature maps f (similarly reshaped).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Rotated MNIST</head><p>The rotated MNIST dataset <ref type="bibr" target="#b21">(Larochelle et al., 2007)</ref> contains 62000 randomly rotated handwritten digits. The dataset is split into a training, validation and test sets of size 10000, 2000 and 50000, respectively.</p><p>We performed model selection using the validation set, yielding a CNN architecture (Z2CNN) with 7 layers of 3 × 3 convolutions (4 × 4 in the final layer), 20 channels in each layer, relu activation functions, batch normalization, dropout, and max-pooling after layer 2. For optimization, we used the Adam algorithm <ref type="bibr" target="#b17">(Kingma &amp; Ba, 2015)</ref>. This baseline architecture outperforms the models tested by <ref type="bibr" target="#b21">Larochelle et al. (2007)</ref> (when trained on 12k and evaluated on 50k), but does not match the previous state of the art, which uses prior knowledge about rotations <ref type="bibr" target="#b33">(Schmidt &amp; Roth, 2012)</ref>  <ref type="table">(see table 1</ref>).</p><p>Next, we replaced each convolution by a p4-convolution (eq. 10 and 11), divided the number of filters by √ 4 = 2 (so as to keep the number of parameters approximately fixed), and added max-pooling over rotations after the last convolution layer. This architecture (P4CNN) was found to perform better without dropout, so we removed it. The P4CNN almost halves the error rate of the previous state of the art (2.28% vs 3.98% error).</p><p>We then tested the hypothesis that premature invariance is undesirable in a deep architecture (section 2). We took the Z2CNN, replaced each convolution layer by a p4convolution (eq. 10) followed by a coset max-pooling over rotations. The resulting feature maps consist of rotationinvariant features, and have the same transformation law as the input image. This network (P4CNNRotationPooling) outperforms the baseline and the previous state of the art, but performs significantly worse than the P4CNN which does not pool over rotations in intermediate layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Test Error (%) <ref type="bibr" target="#b21">Larochelle et al. (2007)</ref> 10.38 ± 0.27 <ref type="bibr" target="#b36">Sohn &amp; Lee (2012)</ref> 4.2 <ref type="bibr" target="#b33">Schmidt &amp; Roth (2012)</ref> 3.98 Z2CNN</p><p>5.03 ± 0.0020 P4CNNRotationPooling 3.21 ± 0.0012 P4CNN 2.28 ± 0.0004 <ref type="table">Table 1</ref>. Error rates on rotated MNIST (with standard deviation under variation of the random seed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">CIFAR-10</head><p>The CIFAR-10 dataset consists of 60k images of size 32 × 32, divided into 10 classes. The dataset is split into 40k training, 10k validation and 10k testing splits.</p><p>We compared the p4-, p4mand standard planar Z 2 convolutions on two kinds of baseline architectures. Our first baseline is the All-CNN-C architecture by <ref type="bibr" target="#b37">Springenberg et al. (2015)</ref>, which consists of a sequence of 9 strided and non-strided convolution layers, interspersed with rectified linear activation units, and nothing else. Our second baseline is a residual network <ref type="bibr" target="#b13">(He et al., 2016)</ref>, which consists of an initial convolution layer, followed by three stages of 2n convolution layers using k i filters at stage i, followed by a final classification layer (6n + 2 layers in total). The first convolution in each stage i &gt; 1 uses a stride of 2, so the feature map sizes are 32, 16, and 8 for the three stages. We use n = 7, k i = 32, 64, 128 yielding a wide 44-layer network called ResNet44.</p><p>To evaluate G-CNNs, we replaced all convolution layers of the baseline architectures by p4 or p4m convolutions. For a constant number of filters, this increases the size of the feature maps 4 or 8-fold, which in turn increases the number of parameters required per filter in the next layer. Hence, we halve the number of filters in each p4-conv layer, and divide it by roughly √ 8 ≈ 3 in each p4m-conv layer. This way, the number of parameters is left approximately invariant, while the size of the internal representation is increased. Specifically, we used k i = 11, 23, 45 for p4m-ResNet44.</p><p>To evaluate the impact of data augmentation, we compare the networks on CIFAR10 and augmented CIFAR10+. The latter denotes moderate data augmentation with horizontal flips and small translations, following <ref type="bibr" target="#b10">Goodfellow et al. (2013)</ref> and many others.</p><p>The training procedure for training the All-CNN was reproduced as closely as possible from <ref type="bibr" target="#b37">Springenberg et al. (2015)</ref>. For the ResNets, we used stochastic gradient descent with initial learning rate of 0.05 and momentum 0.9. The learning rate was divided by 10 at epoch 50, 100 and 150, and training was continued for 300 epochs. To the best of our knowledge, the p4m-CNN outperforms all published results on plain CIFAR10 <ref type="bibr" target="#b40">(Wan et al., 2013;</ref><ref type="bibr" target="#b10">Goodfellow et al., 2013;</ref><ref type="bibr" target="#b27">Lin et al., 2014;</ref><ref type="bibr" target="#b25">Lee et al., 2015b;</ref><ref type="bibr" target="#b38">Srivastava et al., 2015;</ref><ref type="bibr" target="#b4">Clevert et al., 2015;</ref><ref type="bibr" target="#b24">Lee et al., 2015a)</ref>. However, due to radical differences in model sizes and architectures, it is difficult to infer much about the intrinsic merit of the various techniques. It is quite possible that the cited methods would yield better results when deployed in larger networks or in combination with other techniques. Extreme data augmentation and model ensembles can also further improve the numbers <ref type="bibr" target="#b11">(Graham, 2014</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Discussion &amp; Future work</head><p>Our results show that p4 and p4m convolution layers can be used as a drop-in replacement of standard convolutions that consistently improves the results.</p><p>G-CNNs benefit from data augmentation in the same way as convolutional networks, as long as the augmentation comes from a group larger than G. Augmenting with flips and small translations consistently improves the results for the p4 and p4m-CNN.</p><p>The CIFAR dataset is not actually symmetric, since objects typically appear upright. Nevertheless, we see substantial increases in accuracy on this dataset, indicating that there need not be a full symmetry for G-convolutions to be beneficial.</p><p>In future work, we want to implement G-CNNs that work on hexagonal lattices which have an increased number of symmetries relative to square grids, as well as G-CNNs for 3D space groups. All of the theory presented in this paper is directly applicable to these groups, and the G-convolution can be implemented in such a way that new groups can be added by simply specifying the group operation and a bijective map between the group and the set of indices.</p><p>One limitation of the method as presented here is that it only works for discrete groups. Convolution on continuous (locally compact) groups is mathematically welldefined, but may be hard to approximate in an equivariant manner. A further challenge, already identified by <ref type="bibr" target="#b9">Gens &amp; Domingos (2014)</ref>, is that a full enumeration of transformations in a group may not be feasible if the group is large.</p><p>Finally, we hope that the current work can serve as a concrete example of the general philosophy of "structured representations", outlined in section 2. We believe that adding mathematical structure to a representation (making sure that maps between representations preserve this structure), could enhance the ability of neural nets to see abstract similarities between superficially different concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusion</head><p>We have introduced G-CNNs, a generalization of convolutional networks that substantially increases the expressive capacity of a network without increasing the number of parameters. By exploiting symmetries, G-CNNs achieve state of the art results on rotated MNIST and CIFAR10. We have developed the general theory of G-CNNs for discrete groups, showing that all layer types are equivariant to the action of the chosen group G. Our experimental results show that G-convolutions can be used as a drop-in replacement for spatial convolutions in modern network architectures, improving their performance without further tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Equivariance Derivations</head><p>We claim in the paper that planar correlation is not equivariant to rotations. Let f : R 2 → R K be an image with K channels, and let ψ : R 2 → R K be a filter. Take a rotation r about the origin. The ordinary planar correlation ⋆ is not equivariant to rotations, i.e., [L r f ] ⋆ ψ = L r [f ⋆ ψ]. Instead we have:</p><formula xml:id="formula_23">[[L r f ] ⋆ ψ](x) = y∈Z 2 k L r f k (y)ψ k (y − x) = y∈Z 2 k f k (r −1 y)ψ k (y − x) = y∈Z 2 k f k (y)ψ k (ry − x) = y∈Z 2 k f k (y)ψ k (r(y − r −1 x)) = y∈Z 2 k f k (y)L r −1 ψ(y − r −1 x)) = f ⋆ [L r −1 ψ](r −1 x) = L r [f ⋆ [L r −1 ψ]](x)<label>(19)</label></formula><p>Line by line, we used the following definitions, facts and manipulations:</p><p>1. The definition of the correlation ⋆.</p><p>2. The definition of L r , i.e. L r f (x) = f (r −1 x).</p><p>3. The substitution y → ry, which does not change the summation bounds since rotation is a symmetry of the sampling grid Z 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Distributivity.</head><p>5. The definition of L r .</p><p>6. The definition of the correlation ⋆.</p><p>7. The definition of L r .</p><p>A visual proof can be found in <ref type="bibr" target="#b8">(Dieleman et al., 2016)</ref>.</p><p>Using a similar line of reasoning, we can show that pooling commutes with the group action: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Gradients</head><p>To train a G-CNN, we need to compute gradients of a loss function with respect to the parameters of the filters. If we use the fast algorithm explained in section 7 of the main paper, we only have to implement the gradient of the indexing operation (section 7.1, "filter transformation"), because the 2D convolution routine and its gradient are given.</p><p>This gradient is computed as follows. The gradient of the loss with respect to cell i in the input of the indexing operation is the sum of the gradients of the output cells j that index cell i. On current GPU hardware, this can be implemented efficiently using a kernel that is instantiated for each cell j in the output array. The kernel adds the value of the gradient of the loss with respect to cell j to cell i of the array that holds the gradient of the loss with respect to the input of the indexing operation (this array is to be initialized at zero). Since multiple kernels write to the same cell i, the additions must be done using atomic operations to avoid concurrency problems.</p><p>Alternatively, one could implement the filter transformation using a precomputed permutation matrix. This is not as efficient, but the gradient is trivial, and most computation graph / deep learning packages will have implemented the matrix multiplication and its gradient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&amp;CP volume 48. Copyright 2016 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>r</head><label></label><figDesc>Figure 1. A p4 feature map and its rotation by r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2. A p4m feature map and its rotation by r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>=</head><label></label><figDesc>P f (h −1 g) = L h P f (g) (20)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Comparison of conventional (i.e. Z 2 ), p4 and p4m CNNs on CIFAR10 and augmented CIFAR10+. Test set error rates and number of parameters are reported.</figDesc><table><row><cell>Network</cell><cell>G</cell><cell cols="3">CIFAR10 CIFAR10+ Param.</cell></row><row><cell>All-CNN</cell><cell>Z 2</cell><cell>9.44</cell><cell>8.86</cell><cell>1.37M</cell></row><row><cell></cell><cell>p4</cell><cell>8.84</cell><cell>7.67</cell><cell>1.37M</cell></row><row><cell></cell><cell>p4m</cell><cell>7.59</cell><cell>7.04</cell><cell>1.22M</cell></row><row><cell>ResNet44</cell><cell>Z 2</cell><cell>9.45</cell><cell>5.61</cell><cell>2.64M</cell></row><row><cell></cell><cell>p4m</cell><cell>6.46</cell><cell>4.94</cell><cell>2.62M</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Joan Bruna, Sander Dieleman, Robert Gens, Chris Olah, and Stefano Soatto for helpful discussions. This research was supported by NWO (grant number NAI.14.108), Google and Facebook.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C: G-conv calculus</head><p>Although the gradient of the filter transformation operation is all that is needed to do backpropagation in a G-CNN for a split group G, it is instructive to derive the analytical gradients of the G-correlation operation. This leads to an elegant "G-conv calculus", included here for the interested reader.</p><p>Let feature map k at layer l be denoted f l k = f l−1 ⋆ ψ lk , where f l−1 is the stack of feature maps in the previous layer. At some point in the backprop algorithm, we will have computed the derivative ∂L/∂f l k for all k, and we need to compute ∂L/∂f l−1 j (to backpropagate to lower layers) as well as ∂L/∂ψ lk j (to update the parameters). We find that,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group Equivariant Convolutional Networks</head><p>where the superscript * denotes the involution</p><p>and ψ l j is the set of filter components applied to input feature map j at layer l:</p><p>To compute the gradient with respect to component j of filter k, we have to G-convolve the j-th input feature map with the k-th output feature map:</p><p>So we see that both the forward and backward passes involve convolution or correlation operations, as is the case in standard convnets.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to See by Moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Anselmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mutch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<idno>001</idno>
	</analytic>
	<monogr>
		<title level="m">MIT Center for Brains, Minds and Machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On Invariance and Selectivity in Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Anselmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIT Center for Brains, Minds and Machines</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1872" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289v3</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning the Irreducible Representations of Commutative Lie Groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1755" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transformation Properties of Learned Visual Representations. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rotationinvariant convolutional neural networks for galaxy morphology prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Notices of the Royal Astronomical Society</title>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting Cyclic Symmetry in Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Symmetry Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Maxout</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Networks</surname></persName>
		</author>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Max-Pooling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6071</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep Residual Learning for Image Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity Mappings in Deep Residual Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Transforming auto-encoders. ICANN-11: International Conference on Artificial Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Helsinki</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167v3</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28 (NIPS 2015)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transformation equivariant Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyri</forename><forename type="middle">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Christopher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<idno>arXiv:0701127</idno>
		<title level="m">A novel set of rotationally and translationally invariant features for images based on the noncommutative bispectrum</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning (ICML)</title>
		<meeting>the 24th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.09308</idno>
		<title level="m">Fast Algorithms for Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep learning. Nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename></persName>
		</author>
		<idno>ArXiv:1509.08985</idno>
	</analytic>
	<monogr>
		<title level="m">Convolutional Neural Networks: Mixed, Gated, and Tree</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeply-Supervised Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding image representations by measuring their equivariance and equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Network</surname></persName>
		</author>
		<title level="m">Network. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Integral invariants for shape matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">J</forename><surname>Yezzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2006.208</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1602" to="1617" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast Training of Convolutional Networks through FFTs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Roto-Translation Scattering for Object Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2865" to="2873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Group Integration Techniques in Pattern Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Reisert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Albert-Ludwigs-University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning rotation-aware features: From invariant priors to equivariant descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Spherical Tensor Algebra for Biomedical Image Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Skibbe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Albert-Ludwigs-Universitat Freiburg im Breisgau</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Invariant Representations with Local Transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML-12)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML-12)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Striving for Simplicity: The All Convolutional Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Training Very Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast convolutional nets with fbfft: A GPU performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Piantino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<title level="m">Regularization of neural networks using dropconnect. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="109" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide Residual Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Discriminative template learning in group-convolutional networks for invariant speech representations. InterSpeech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Voinea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3229" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
