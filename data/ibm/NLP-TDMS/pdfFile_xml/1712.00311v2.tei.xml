<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Folded Recurrent Neural Networks for Future Video Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Oliu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Oberta de Catalunya Centre de Visio per Computador Rambla del Poblenou</orgName>
								<address>
									<postCode>156, 08018</postCode>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Selva</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Universitat de Barcelona Gran Via de les Corts Catalanes</orgName>
								<address>
									<postCode>585, 08007</postCode>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
							<email>sergio@maia.ub.es</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Universitat de Barcelona Centre de Visio per Computador Gran Via de les Corts Catalanes</orgName>
								<address>
									<postCode>585, 08007</postCode>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Folded Recurrent Neural Networks for Future Video Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>future video prediction</term>
					<term>unsupervised learning</term>
					<term>recurrent neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Main challenges in future video prediction are high variability in videos, temporal propagation of errors, and non-specificity of future frames. This work introduces bijective Gated Recurrent Units (bGRU). Standard GRUs update a state, exposed as output, given an input. We extend them by considering the input as another recurrent state, and update it given the output using an extra set of logic gates. Stacking multiple such layers results in a recurrent auto-encoder: the operators updating the outputs comprise the encoder, while the ones updating the inputs form the decoder. Being the encoder and decoder states shared, the representation is stratified during learning: some information is not passed to the next layers. We show how only the encoder or decoder needs to be applied for encoding or prediction. This reduces the computational cost and avoids re-encoding predictions when generating multiple frames, mitigating error propagation. Furthermore, it is possible to remove layers from a trained model, giving an insight to the role of each layer. Our approach improves state of the art results on MMNIST and UCF101, being competitive on KTH with 2 and 3 times less memory usage and computational cost than the best scored approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Future video prediction is a challenging task that recently received much attention due to its capabilities for learning in an unsupervised manner, making it arXiv:1712.00311v2 [cs.CV] <ref type="bibr" target="#b15">16</ref> Mar 2018 possible to leverage large volumes of unlabelled data for video-related tasks such as action and gesture recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, task planning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, weather prediction <ref type="bibr" target="#b5">[6]</ref>, optical flow estimation <ref type="bibr" target="#b6">[7]</ref> and new view synthesis <ref type="bibr" target="#b2">[3]</ref>.</p><p>One of the main problems in this task is the need of expensive models both in terms of memory and computational power in order to capture the variability present in video data. Another problem is the propagation of errors in recurrent models, which is tied to the inherent uncertainty of video prediction: given a series of previous frames, there are multiple feasible futures. This, left unchecked, results in a blurry prediction averaging the space of possible futures that propagates back into the network when predicting subsequent frames.</p><p>In this work we propose a new approach to recurrent auto-encoders (AE) with state sharing between encoder and decoder. We show how the exposed state in Gated Recurrent Units (GRU) can be used to create a bijective mapping between the input and output of each layer. To do so, the input is treated as a recurrent state, adding another set of logic gates to update it based on the output. Creating a stack of these layers allows for a bidirectional flow of information. Using the forward gates to encode inputs and the backward ones to generate predictions, we obtain a structure similar to an AE 1 , but with many inherent advantages. It reduces memory and computational costs during both training and testing: only the encoder or decoder is executed for input encoding or prediction, respectively. Furthermore, the representation is stratified, encoding only part of the input at each layer: low level information not necessary to capture higher level dynamics is not passed to the next layer. Also, it naturally provides a noisy identity mapping of the input, facilitating the initial stages of training: the input to the first bGRU holds the last encoded frame or, if preceded by convolutional layers, an over-complete representation of the same. During generation, that first untrained bGRU randomly modifies the last input, introducing a noise signal. The approach also mitigates the propagation of errors: while it does not solve the problem of blur, it prevents its magnification in subsequent predictions. Moreover, a trained network can be deconstructed in order to analyse the role of each layer in the final predictions, making the model more explainable. Since the encoder and decoder states are shared, the architecture can be thought of as a recurrent AE folded in half, with encoder and decoder layers overlapping. We call our method Folded Recurrent Neural Network (fRNN). Our main contributions are: 1) A new shared-state recurrent AE with lower memory and computational costs. 2) Mitigation of error propagation through time. 3) It naturally provides an identity function during training. 4) Model explainability and optimisation through layer removal. 5) Demonstration of representation stratification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>While initial proposals focused on prediction on small patches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, future video prediction is nowadays generally approached by building a deep model capable of understanding the input sequence in a manner that allows for the generation of the following frames.</p><p>Building Blocks. Due to the characteristics of the problem, an AE setting has been widely used <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>: the encoder extracts valuable information from the input and the decoder produces new frames. Generally, encoder and decoder are CNNs that tackle the spatial dimension. LSTMs are commonly used to handle the temporal dynamics and project the representations into the future. Some works compute the temporal dynamics at the deep representation bridging the encoder and decoder <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref>. Others jointly handle space and time by using Convolutional LSTMs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> (or GRUs, as in our case), which use convolutional kernels at their gates. For instance, Lotter et al. <ref type="bibr" target="#b1">[2]</ref> use a recurrent residual network with convolutional LSTM where each layer minimises the discrepancies from previous block predictions. Common variations of the AE also include a conditional term to guide the temporal transform, such as a time differential <ref type="bibr" target="#b15">[16]</ref> or prior knowledge of scene events, reducing the space of possible futures. Oh et al. <ref type="bibr" target="#b3">[4]</ref> predict future frames on Atari games conditioning on the action taken by the player. Some works propose such action conditioned models foreseeing an application for autonomous agents learning in an unsupervised fashion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>. Finn et al. <ref type="bibr" target="#b9">[10]</ref> predict a sequence of future frames within a physical system based on both previous frames and actions taken by a robotic arm interacting with the scene. The method was recently applied to task planning <ref type="bibr" target="#b4">[5]</ref> and adapted to perform stochastic future frame prediction <ref type="bibr" target="#b16">[17]</ref>.</p><p>Bridge connections. Introducing bridge connections (connections between equivalent layers of encoder and decoder) is also common <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>. This allows for a stratified representation of the input sequence, reducing the capacity needs of subsequent layers. Video Ladder Networks (VLN) <ref type="bibr" target="#b12">[13]</ref> use a convolutional AE topology implementing skip connections. Pairs of convolutions are grouped into residual blocks, horizontally passing information between corresponding blocks, both by directly and by using a recurrent bridge layer. This topology was further extended with Recurrent Ladder Networks (RLN) <ref type="bibr" target="#b17">[18]</ref>, where the recurrent bridge connections were removed, and the residual blocks replaced by recurrent layers. We propose an alternative to bridge connections by completely sharing the state between encoder and decoder, reducing computational needs while maintaining the stratification ability. Both VLN and RLN share some similarities with our approach: they propose a recurrent AE with bridge connections between encoder and decoder. However, using skip connections instead of state sharing has some disadvantages: higher number of parameters and memory requirements, impossibility to skip the encoding/decoding steps (resulting in a higher computational cost) and reduced explainability due to not allowing layers to be removed after training. Finally, bridge connections do not provide an initial identity function during training. This makes it hard for the model to converge in some cases: when the background is homogeneous the model may not learn a proper initial mapping between input and output, but set the weights to zero and adjust the bias of the last layer, eliminating the gradient in the process.</p><p>Prediction atom. Most of the proposed architectures for future frame generation directly predict at pixel level (as in our case). However, some models have been designed to predict motion and use it to transform the input into future frames. For instance, using the input sequence to anticipate optical flow <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> or convolutional kernels <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Other methods propose mapping the input sequence onto predefined feature spaces, such as affine transforms <ref type="bibr" target="#b20">[21]</ref> or human pose vectors <ref type="bibr" target="#b21">[22]</ref>. These systems use sequences of such features instead of working directly at pixel level. Then, they use the predicted feature vectors to generate the next frames.</p><p>Loss and GANs. Commonly used loss functions such as L2 or MSE tend to average the space of possible futures. For this reason, some works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b14">15]</ref> propose using Generative Adversarial Networks (GAN) <ref type="bibr" target="#b23">[24]</ref> to help traditional losses choose among possible futures by ensuring realistic looking frames and coherent sequences. Mathieu et al. <ref type="bibr" target="#b22">[23]</ref> use a plain multi-scale CNN in an adversarial setting and propose the Gradient Difference Loss to sharpen the predictions.</p><p>Disentangled Motion/Content. Some authors encode content and motion separately. Villegas et al. <ref type="bibr" target="#b10">[11]</ref> use an AE architecture with a two-stream encoder: for motion, a CNN + LSTM encodes difference images; for appearance, a plain CNN encodes the last input frame. In a similar fashion, Denton et al. <ref type="bibr" target="#b11">[12]</ref> use two separate encoders and an adversarial setting to obtain a disentangled representation of content and motion. Alternatively, some works predict motion and content in parallel to benefit from the combined strengths of both tasks. While Sedaghat et al. <ref type="bibr" target="#b24">[25]</ref> propose using an encoding with a dual objective (flow and future frame), Liang et al. <ref type="bibr" target="#b14">[15]</ref> use a dual GAN setting and combine both predicted frame and motion to generate the actual next frame.</p><p>Feedback Predictions. Finally, an important aspect of the recurrent-based models is that they are based on the use of feedback predictions. In general, a model is trained to predict a specific number of time-steps into the future. In order to predict further in time they need to use their own predictions as input. This, if not handled properly, may accentuate small mistakes causing the predictions to quickly deteriorate over time. Our model solves this by enabling encoder and decoder to be executed any number of times independently. This is similar to the proposal by Srivastava et al. <ref type="bibr" target="#b0">[1]</ref>, which uses a recurrent AE approach where an input sequence is encoded and its state copied into the decoder. The decoder is then applied to generate a given number of frames. However, it is limited to a single recurrent layer for each part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed method</head><p>We propose an architecture based on recurrent convolutional AEs to deal with the network capacity and error propagation problems for future video prediction. It consists on a series of bijective GRU layers, which allow for a bidirectional flow of information between input and output: they consider the input as a recurrent state and update it using an extra set of gates. These are then stacked, forming an encoder and decoder using, respectively, the forward and backward functions of the bijective GRUs ( <ref type="figure" target="#fig_0">Fig.1</ref>). We call it Folded Recurrent Neural Network (fRNN). Because of the state sharing between encoder and decoder, the topology allows for: stratification of the encoded information, lower memory and computational requirements compared to regular recurrent AEs, mitigated propagation of errors, and increased explainability through layer removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bijective Gated Recurrent Units</head><p>GRUs have their state fully exposed as output. This allows us to define a bidirectional mapping between input and output by replicating the logic gates of the GRU layer. To do so, we consider the input as a state on itself. Lets define the output of a GRU at layer l and time step t as h l t = f l f (h l−1 t , h l t−1 ) given an input h l−1 t and its state at the previous time step h l t−1 . A second set of weights can be used to define an inverse mapping h l−1</p><formula xml:id="formula_0">t = f l b (h l t , h l−1 t−1 )</formula><p>using the output of the forward function at the current time step to update its input, which is treated as the hidden state of the inverse function. This is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. We will refer to this double mapping as bijective GRU (bGRU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Folded Recurrent Neural Network</head><p>By stacking multiple bGRUs, a recurrent AE is obtained. Given n bGRUs, the encoder is defined by the set of forward functions E = {f 1 f , ..., f n f } and the decoder by the set of backward functions D = {f n b , ..., f 1 b }. This is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, and is equivalent to a recurrent AE, but with shared states, having 3 main advantages: 1) It is not necessary to feed the predictions back into the network in order to generate the following predictions. Because the states are shared, the decoder already updates all the states except for the bridge state between encoder and decoder. The bridge state is updated by applying the last layer of the encoder before generating the next prediction. The shadowed area in <ref type="figure" target="#fig_0">Fig. 1</ref> shows the section of the computational graph that is not required when performing multiple sequential predictions. For the same reason, when considering multiple sequential elements before prediction, only the encoder is required. 2) Because the network updates its states from the higher level representations to the lowest ones during prediction, errors introduced at a given layer during generation are not propagated back into deeper layers, leaving the higher-level dynamics unaffected. 3) The model implicitly provides a noisy identity model during training, as it is shown in <ref type="figure" target="#fig_6">Fig. 7</ref>, when all bGRU layers are removed. The input state of the first bGRU layer is either the input image itself or, when first applying a series of convolutional layers, an over-complete representation of the input. A noise signal is then introduced to the representation by the backward function of the untrained first bGRU layer. Consequently providing the model with an initial identity model. As we show in Section 4.3, this helps the model to converge in some datasets like MMNIST: when the same background is shared across instances, it prevents the model from killing the gradients by adjusting the biases to match the background and setting the weights to zero.</p><p>This approach shares some similarities with VLN and RLN. As with them, part of the information can be passed directly between corresponding layers of the encoder and decoder, not having to encode a full representation of the input into the deepest layer. However, our model implicitly passes the information through the shared recurrent states, making bridge connections unnecessary. When compared against an equivalent recurrent AE with bridge connections, this results in a much lower computational and memory cost. More specifically, the number of weights in a pair of forward and backward functions is equal to 3(h l−1 2 + h l 2 + 2h l−1 h l ) in the case of bGRU, where h l corresponds to the state size of layer l. When using bridge connections, as in the case of VLN and RLN, that value is incremented to 3(h l−1 2 + h l 2 + 4h l−1 h l ). This corresponds to an increase of 44% in the number of parameters when one state has double the size of the other, and of 50% when they have the same size. Furthermore, both the encoder and decoder must be applied at each time step. Thus, memory usage is doubled and computational cost is increased by a factor of between 2.88 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Folded RNNs</head><p>In a regular recurrent AE, a ground truth frame is introduced at each time step by applying both encoder and decoder. The output is used as a supervision point, comparing it to the next ground truth frame in the sequence. This implies all predictions are at a single time step from the last ground truth prediction. Here, we propose a training approach for fRNNs that exploits the ability of the topology of skipping the model encoder or decoder at a given time step. First g ground truth frames are shown to the network by passing them through the encoder.</p><p>The decoder is then applied p times, producing p predictions. This results in only half the memory requirements: either encoder or decoder is applied at each step, never both. This has the same advantage as the approach by Srivastava <ref type="bibr" target="#b0">[1]</ref>, where recurrently applying the decoder without further ground truth inputs encourages the network to learn video dynamics. This also prevents the network from learning an identity model, i.e. copying the last input to the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Here, we first discuss data, evaluation protocol, and methods. Then we provide a detailed quantitative and qualitative evaluation. We finish with a brief analysis on the stratification of sequence representation among bGRU layers 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and evaluation protocol</head><p>We considered 3 datasets of different complexity in order to analyse the performance of the proposed method: Moving MNIST (MMNIST) <ref type="bibr" target="#b0">[1]</ref>, KTH <ref type="bibr" target="#b25">[26]</ref>, and UCF101 <ref type="bibr" target="#b26">[27]</ref>. MMNIST consists of 64 × 64 grayscale sequences of length 20 displaying pairs of digits moving around the image. The sequences are generated by randomly sampling pairs of digits and trajectories. It contains a fixed test partition with 10000 sequences. We generated a million extra samples for training. KTH consists of 600 videos of 15-20 seconds with 25 subjects performing 6 actions in 4 different settings. The videos are grayscale, at a resolution of 120 × 160 pixels and 25 fps. The dataset has been split into subjects 1 to 16 for training, and 17 to 25 for testing, resulting in 383 and 216 sequences, respectively. Frame size is reduced to 64 × 80 using bilinear interpolation, removing 5 pixels from the left and right borders before resizing. UCF101 displays 101 actions, such as playing instruments, weight lifting or sports. It is the most challenging dataset considered, with a high intra-class variability. It contains 9950 training sequences and 3361 test sequences. These are RGB at a resolution of 320 × 240 pixels and 25 fps. To increase motion between consecutive frames, one of every two frames was removed. Following the same procedure as with KTH, the frame size is reduced to 64 × 85.</p><p>All methods are tested using 10 input frames to generate the following 10 frames. We use 3 common metrics for video prediction analysis: Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Dissimilarity (DSSIM). MSE and PSNR are objective measurements of reconstruction quality. DSSIM is a measure of the perceived quality. For DSSIM we use a Gaussian sliding window of size 11 × 11 and σ = 1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Methods</head><p>To train the proposed method we used RMSProp with learning rate of 0.0001 and batch size of 12, sampling a random sub-sequence at each epoch. Weights  <ref type="table">Table 1</ref>. Parameters of the topology used for the experiments. The decoder applies the same topology in reverse, using nearest neighbours interpolation and transposed convolutions to revert the pooling and convolutional layers.</p><p>were orthogonally initialised, with biases set to 0. For testing, we considered all sub-sequences of length 20. Our network topology consists of two convolutional layers followed by 8 convolutional bGRU layers, applying a 2 × 2 max pooling every 2 layers. Topology details are shown in <ref type="table">Table 1</ref>. We use deconvolution and nearest neighbours interpolation to invert the convolutional and max pooling layers, respectively. We train with L1 loss. For evaluation, we include a stub baseline model predicting the last input frame, and design a second baseline (RLadder) to evaluate the advantages of using state sharing. RLadder has the same topology as the fRNN model, but uses bridge connections instead of state sharing. Note that to keep the same state size on GRU layers, using bridge connections doubles the memory size and almost triples the computational cost (Sec.3.2). This is similar to how RLN <ref type="bibr" target="#b17">[18]</ref> works, but using regular conv GRU layers in the decoder. We also compare against Srivastava <ref type="bibr" target="#b0">[1]</ref> and Mathieu <ref type="bibr" target="#b22">[23]</ref>. The former only handles the temporal dimension explicitly with LSTMs, while the latter only treats the spatial dimensions using 3D CNN. Next, we compare against Villegas <ref type="bibr" target="#b10">[11]</ref>, which, contrary to our proposal, uses feedback predictions. Finally, we compare against Lotter et al. <ref type="bibr" target="#b1">[2]</ref> which is based on residual error reduction. All of them were adapted to train using 10 frames as input and predicting the next 10, using the topologies and parameters defined by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative analysis</head><p>The first row of <ref type="figure" target="#fig_1">Fig. 2</ref> displays the results for the MMNIST dataset for the proposed method, baselines, and state of the art alternatives. Mean scores are shown in <ref type="table" target="#tab_1">Table 2</ref>. fRNN performs best on all time steps and metrics, followed by Srivastava et al. <ref type="bibr" target="#b0">[1]</ref>. These two are the only methods to provide valid predictions on this dataset. Most other methods predict a black frame, with Mathieu et al. <ref type="bibr" target="#b22">[23]</ref> progressively blurring the digits. This is caused by a loss of gradient during the first stages of training. On more complex datasets the methods start by learning an identity function, then refining the results. This is possible since in many sequences most of the frame remains unchanged. In the case of MMNIST, where the background is homogeneous, it is much easier for the models to set the weights of the output layer to zero and set the biases to match the background colour. This cuts the gradient and prevents further learning. Srivastava et al. <ref type="bibr" target="#b0">[1]</ref> use an auxiliary decoder to reconstruct the input frames, forcing the model to learn an identity function. This, as discussed at the end of Section 3.2, is implicitly handled in our method, giving an initial solution to improve on and  preventing the models from learning a black image. In order to verify this effect, we pre-trained RLadder on the KTH dataset. While this dataset has completely different dynamics, the initial step to solve the problem remains: providing an identity function. Afterwards the model is fine-tuned on the MMNIST dataset. As it is shown in <ref type="figure" target="#fig_1">Fig. 2 (dashed lines)</ref>, this results in the model converging, with an accuracy comparable to Srivastava et al. <ref type="bibr" target="#b0">[1]</ref> for the 3 evaluation metrics.</p><p>On the KTH dataset, <ref type="table" target="#tab_1">Table 2</ref> shows the best approach is our RLadder baseline followed by fRNN and Villegas et al. <ref type="bibr" target="#b10">[11]</ref>, both having similar results, but with Villegas et al. having slightly lower MSE and higher PSNR, and fRNN a lower DSSIM. While both approaches obtain comparable average results, the error increases faster over time in the case of Villegas et al. (second row in <ref type="figure" target="#fig_1">Fig.2</ref>). Mathieu obtains good scores for MSE and PSNR, but has a much worse DSSIM.</p><p>For the UCF101 dataset, as shown in <ref type="table" target="#tab_1">Table 2</ref>, our fRNN approach is the best performing for all 3 metrics. When looking at the third row of <ref type="figure" target="#fig_4">Fig. 5</ref>, one can see that Villegas et al. starts out with results similar to fRNN on the first frame, but as in the case of KTH and MMNIST, the predictions degrade faster than with the proposed approach. Two methods display low performance in most cases. Lotter et al. works well for the first predicted frame in the case of KTH and UCF101, but the error rapidly increases on the following predictions. This is due to a magnification of artefacts introduced on the first prediction, making the method unable to predict multiple frames without supervision. In the case of Srivastava et al. the problem is about capacity: it uses fully connected LSTM layers, making the number of parameters explode quickly with the state cell size. This severely limits the representation capacity for complex datasets such as KTH and UCF101.</p><p>Overall, for the considered methods, fRNN is the best performing on MMINST and UCF101, the later being the most complex of the 3 datasets. We achieved these results with a simple topology: apart from the proposed bGRU layers, we use conventional max pooling with an L1 loss. There are no normalisation or regularisation mechanisms, specialised activation functions, complex topologies or image transform operators. In the case of MMNIST, fRNN shows the ability to find a good initial representation and converges to good predictions where most other methods fail. In the case of KTH, fRNN has an overall accuracy comparable to that of Villegas et al., being more stable over time. It is only surpassed by the proposed RLadder baseline, a method equivalent to fRNN but with 2 and 3 times more memory and computational requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative analysis</head><p>In this section we evaluate our approach qualitatively on some samples from the three considered datasets. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the last 5 input frames from some MMNIST sequences along with the next 10 ground truth frames and their corresponding fRNN predictions. The 10 predictions are generated sequentially without showing the previous ground truth/prediction to the network, that is, only using the decoder. As it can be seen, the digits maintain their sharpness across the sequence of predictions. Also, the bounces at the edges of the image are done correctly, and the digits do not distort or deform when crossing. This shows the network internally encodes the appearance of each digit, making it possible to reconstruct them after sharing the same region in the image plane.</p><p>Qualitative examples of fRNN predictions on the KTH dataset are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. It shows three actions: hand waving, walking, and boxing. The blur stops increasing after the first three predictions, generating plausible motions for the corresponding actions while background artefacts are not introduced. Although the movement patterns for each type of action have a wide range of variability on its trajectory, bGRU gives relatively sharp predictions for the limbs. The first and third examples also show the ability of the model to recover from blur. The blur slightly increases for the arms while the action is performed, but decreases again as these reach the final position.   <ref type="figure" target="#fig_4">Fig. 5</ref> shows fRNN predictions on the UCF101 dataset. These correspond to two different physical exercises and a girl playing the piano. Common to all predictions, the static parts do not lose sharpness over time, and the background is properly reconstructed after an occlusion. The network correctly predicts actions with low variability, as shown in rows 1-2, where a repetitive movement is performed, and in last row, where the girl recovers a correct body posture. These dynamic regions introduce blur due to the uncertainty of the action, averaging the possible futures. The first row also shows an interesting behaviour: while the woman is standing up, the upper body becomes blurry due to uncertainty, but as the woman finishes her motion and ends up in the expected upright position, the frames sharpen again. Since the model does not propagate errors to deeper layers nor makes use of previous predictions for the following ones, the introduction of blur does not imply this blur will be propagated. In this example, while the middle motion could have multiple predictions depending on the movement pace and the inclination of the body while performing it, the final body pose has a lower uncertainty.</p><p>In <ref type="figure" target="#fig_5">Fig. 6</ref> we compare predictions from the proposed approach against the RLadder baseline and other state of the art methods. For the MMNIST dataset As discussed in Section 4.3, this is due to the use of fully connected recurrent layers, which constrains the state size and prevents the model from encoding relevant information on more complex scenarios. In the case of Lotter, it makes good predictions for the first frame, but rapidly accumulates artefacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Representation stratification analysis</head><p>Here we analyse the stratification of the sequence representation among the bGRU layers. Because bGRU units allow for a bijective mapping between states, it is possible to remove the deepest layers of a trained network, allowing us to check how the predictions are affected and providing an insight on the dynamics captured by each layer. Specifically, the same sequences are predicted multiple  times, removing a layer each time. To our knowledge, this is the first topology allowing for a direct observation of the behaviour encoded on each layer. This is shown in <ref type="figure" target="#fig_6">Fig. 7</ref> for the MMNIST dataset. The analysed model consists of 10 layers: 2 convolutional layers and 8 bGRU layers. Firstly, removing the last 2 bGRU layers has no significant impact on prediction. This shows that, for this simple dataset, the network has a higher capacity than required. Further removing layers does not result in a loss of pixel-level information, but on a progressive loss of behaviours, from more complex to simpler ones. This means information at a given level of abstraction is not encoded into higher level layers. When removing the third deepest bGRU layer, the digits stop bouncing and keep their linear trajectories, exiting the image. This indicates this layer is in charge of encoding information on bouncing dynamics. When removing the next layer, digits stop behaving correctly on the boundaries of the image. Parts of the digit bounce while others keep the previous trajectory. While this also has to do with bouncing dynamics, the layer seems to be in charge of recognising digits as single units following the same movement pattern. When removed, different segments of the digit are allowed to move as separate elements. Finally, with only 3-2 bGRU layers the digits are distorted in various ways. With only two layers left, the general linear dynamics are still captured by the model. By leaving a single bGRU layer, the linear dynamics are lost.</p><p>According to these results, linear movement dynamics are captured at pixel level on the first two bGRU layers. The next two start aggregating these movement patterns into single-trajectory components, preventing their distortion. The collision of these components with image bounds is also detected. The fifth layer aggregates single-motion components into digits, forcing them to follow the same motion. This seems to have the effect of preventing bounces, likely due to only one of the components reaching the edge of the image. It is the sixth bGRU layer that provides a coherent bouncing pattern for the whole digit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented Folded Recurrent Neural Networks, a new recurrent architecture for video prediction with lower computational and memory cost compared to equivalent recurrent AE models. This is achieved by using the proposed bijective GRUs, which horizontally pass information between the encoder and decoder. This eliminates the need for using the entire AE at any given step: only the encoder or decoder needs to be executed for both input encoding and prediction, respectively. It also facilitates the convergence by naturally providing a noisy identity function during training. We evaluated our approach on three video datasets, outperforming state of the art prediction results on MMNIST and UCF101, and obtaining competitive results on KTH with 2 and 3 times less memory usage and computational cost than the best scored approach. Qualitatively, the model can limit and recover from blur by preventing its propagation from low to high level dynamics. We also demonstrated stratification of the representation, topology optimisation, and model explainability through layer removal. Each layer has been shown to modify the state of the previous one by adding more complex behaviours: removing a layer eliminates its behaviours but leaves lower-level ones untouched.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Left: Scheme of a bGRU. Shadowed areas illustrate how multiple bGRU layers are stacked. Right: fRNN topology. The recurrent states of the encoder and decoder are shared, resulting in a bidirectional mapping between states. Shadowed areas represent unnecessary circuitry: re-encoding of the predictions is avoided thanks to the decoder updating all the states. Left-Right: Blue and red correspond to forward and backward gates, respectively. Rectangles represent the recurrent state cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Quantitative results on the considered datasets in terms of the number of time steps since the last input frame. From top to bottom: MMNIST, KTH, and UCF101. From left to right: MSE, PSNR, and DSSIM. For MMNIST, RLadder is pre-trained to learn an initial identity mapping, allowing it to converge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>fRNN predictions on MMNIST. First row for each sequence shows last 5 inputs and target frames. Yellow frames are model predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>fRNN predictions on KTH. First row for each sequence shows last 5 inputs and target frames. Yellow frames are model predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>fRNN predictions on UCF. First row for each sequence shows last 5 inputs and target frames. Yellow frames are model predictions.we did not consider Villegas et al. and Lotter et al. since these methods fail to successfully converge and they predict a sequence of black frames. From the rest of approaches, fRNN obtains the best predictions, with little blur or distortion. The RLadder baseline is the second best approach. It does not introduce blur, but heavily deforms the digits after they cross.Srivastava  et al. and Mathieu et al. both accumulate blur over time, but while the former does so to a smaller degree, the later makes the digits unrecognisable after five frames. For KTH, Villegas et al. obtains outstanding qualitative results. It predicts plausible dynamics and maintains the sharpness of both the individual and background. Both fRNN and RLadder follow closely, predicting plausible dynamics, but not being as good as Villegas et al. at maintaining the sharpness of the individual. On UCF101 the best prediction is obtained by our model, with little blur or distortion compared to the other methods. The second best is Villegas et al., successfully capturing the movement patterns but introducing more blur and important distorsions on the last frame. When looking at the background, fRNN proposes a plausible initial estimation and progressively completes it as the woman moves. On the other hand, Villegas et al. modifies already generated regions as more background is uncovered, generating an unrealistic sequence regarding the background. Srivastava and Lotter fail on both KTH and UCF101. Srivastava et al. heavily distort the frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Predictions at 1, 5, and 10 time steps from the last ground truth frame. RLadder predictions on MMNIST are from the model pre-trained on KTH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Moving MNIST predictions with fRNN layer removal. Removing all bGRU layers (last row) leaves two convolutional layers and their transposed convolutions, providing an identity mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Average results over 10 time steps.</figDesc><table><row><cell>MMNIST</cell><cell>KTH</cell><cell>UCF101</cell></row><row><cell>MSE PSNR DSSIM</cell><cell>MSE PSNR DSSIM</cell><cell>MSE PSNR DSSIM</cell></row><row><cell>Baseline 0.06989 11.745 0.20718</cell><cell>0.00366 29.071 0.07900</cell><cell>0.01294 22.859 0.15043</cell></row><row><cell>RLadder 0.04254 13.857 0.13788</cell><cell>0.00139 31.268 0.05945</cell><cell>0.00918 23.558 0.13395</cell></row><row><cell>Lotter [2] 0.04161 13.968 0.13825</cell><cell>0.00309 28.424 0.09170</cell><cell>0.01550 19.869 0.21389</cell></row><row><cell>Srivastava [1] 0.01737 18.183 0.08164</cell><cell>0.00995 21.220 0.19860</cell><cell>0.14866 10.021 0.42555</cell></row><row><cell>Mathieu [23] 0.02748 15.969 0.29565</cell><cell>0.00180 29.341 0.10410</cell><cell>0.00926 22.781 0.16262</cell></row><row><cell>Villegas [11] 0.04254 13.857 0.13896</cell><cell>0.00165 30.946 0.07657</cell><cell>0.00940 23.457 0.14150</cell></row><row><cell>fRNN 0.00947 21.386 0.04376</cell><cell>0.00175 29.299 0.07251</cell><cell>0.00908 23.872 0.13055</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code available at https://github.com/moliusimon/frnn.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Additional qualitative results, as well as an extended table of quantitative results by time step, are shown in the supplementary material.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02463</idno>
		<title level="m">Video frame synthesis using deep voxel flow</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2863" to="2871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05268</idno>
		<title level="m">Self-supervised visual planning with temporal skip connections</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 28</title>
		<editor>Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06309</idno>
		<title level="m">Spatio-temporal video autoencoder with differentiable memory</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<idno>abs/1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling deep temporal dependencies with recurrent grammar cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1925" to="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">;</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4417" to="4426" />
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cricri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01756</idno>
		<title level="m">Video ladder networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual motion gan for future-flow embedded video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">One-step time-dependent future video frame prediction with a convolutional encoder-decoder neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vukotić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04125</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Prémont-Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09219</idno>
		<title level="m">Recurrent ladder networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Transformation-based models of video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1701.08435</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<editor>Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., Weinberger, K.Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03777</idno>
		<title level="m">Hybrid learning of optical flow and next frame prediction to boost optical flow in the wild</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on</title>
		<meeting>the 17th International Conference on</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
