<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FiLM: Visual Reasoning with a General Conditioning Layer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
							<email>ethanperez@rice.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
							<email>florian.strub@inria.fr</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Centrale Lille</orgName>
								<address>
									<postCode>9189</postCode>
									<settlement>CRIStAL</settlement>
									<region>Inria, UMR</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harm</forename><surname>De Vries</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
							<email>courvila@iro.umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CIFAR Fellow</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FiLM: Visual Reasoning with a General Conditioning Layer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning -answering image-related questions which require a multi-step, high-level process -a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-theart error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.</p><p>1. FiLM models achieve state-of-the-art across a variety of visual reasoning tasks, often by significant margins.</p><p>2. FiLM operates in a coherent manner. It learns a complex, underlying structure and manipulates the conditioned network's features in a selective manner. It also enables the arXiv:1709.07871v2 [cs.CV] 18 Dec 2017</p><p>CNN to properly localize question-referenced objects.</p><p>3. FiLM is robust; many FiLM model ablations still outperform prior state-of-the-art. Notably, we find there is no close link between normalization and the success of a conditioned affine transformation, a previously untouched assumption. Thus, we relax the conditions under which this method can be applied.</p><p>4. FiLM models learn from little data to generalize to more complex and/or substantially different data than seen during training. We also introduce a novel FiLM-based zeroshot generalization method that further improves and validates FiLM's generalization capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our model processes the question-image input using FiLM, illustrated in <ref type="figure">Figure 2</ref>. We start by explaining FiLM and then describe our particular model for visual reasoning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to reason about everyday visual input is a fundamental building block of human intelligence. Some have argued that for artificial agents to learn this complex, structured process, it is necessary to build in aspects of reasoning, such as compositionality <ref type="bibr" target="#b22">Johnson et al. 2017b)</ref> or relational computation <ref type="bibr" target="#b35">(Santoro et al. 2017</ref>). However, if a model made from general-purpose components could learn to visually reason, such an architecture would likely be more widely applicable across domains.</p><p>To understand if such a general-purpose architecture exists, we take advantage of the recently proposed CLEVR dataset <ref type="bibr" target="#b21">(Johnson et al. 2017a</ref>) that tests visual reasoning via question answering. Examples from CLEVR are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Visual question answering, the general task of asking questions about images, has its own line of datasets <ref type="bibr" target="#b28">(Malinowski and Fritz 2014;</ref><ref type="bibr" target="#b10">Geman et al. 2015;</ref><ref type="bibr" target="#b3">Antol et al. 2015)</ref> which generally focus on asking a diverse set of simpler questions on images, often answerable in a single glance. From these datasets, a number of effective, generalpurpose deep learning models have emerged for visual question answering <ref type="bibr" target="#b29">(Malinowski, Rohrbach, and Fritz 2015;</ref><ref type="bibr" target="#b38">Yang et al. 2016;</ref><ref type="bibr" target="#b27">Lu et al. 2016;</ref><ref type="bibr" target="#b0">Anderson et al. 2017)</ref>. However, tests on CLEVR show that these general deep learning approaches struggle to learn structured, multi-step reasoning <ref type="bibr" target="#b21">(Johnson et al. 2017a</ref>). In particular, these methods tend Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>(a) Q: What number of cylinders are small purple things or yellow rubber things? A: 2 (b) Q: What color is the other object that is the same shape as the large brown matte thing? A: Brown to exploit biases in the data rather than capture complex underlying structure behind reasoning <ref type="bibr" target="#b12">(Goyal et al. 2017)</ref>.</p><p>In this work, we show that a general model architecture can achieve strong visual reasoning with a method we introduce as FiLM: Feature-wise Linear Modulation. A FiLM layer carries out a simple, feature-wise affine transformation on a neural network's intermediate features, conditioned on an arbitrary input. In the case of visual reasoning, FiLM layers enable a Recurrent Neural Network (RNN) over an input question to influence Convolutional Neural Network (CNN) computation over an image. This process adaptively and radically alters the CNN's behavior as a function of the input question, allowing the overall model to carry out a variety of reasoning tasks, ranging from counting to comparing, for example. FiLM can be thought of as a generalization of Conditional Normalization, which has proven highly successful for image stylization <ref type="bibr" target="#b11">Ghiasi et al. 2017;</ref><ref type="bibr" target="#b19">Huang and Belongie 2017)</ref>, speech recognition <ref type="bibr" target="#b24">(Kim, Song, and Bengio 2017)</ref>, and visual question answering , demonstrating FiLM's broad applicability.</p><p>In this paper, which expands upon a shorter report <ref type="bibr" target="#b32">(Perez et al. 2017)</ref>, our key contribution is that we show FiLM is a strong conditioning method by showing the following on visual reasoning tasks:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature-wise Linear Modulation</head><p>FiLM learns to adaptively influence the output of a neural network by applying an affine transformation, or FiLM, to the network's intermediate features, based on some input. More formally, FiLM learns functions f and h which output γ i,c and β i,c as a function of input x i :</p><formula xml:id="formula_0">γ i,c = f c (x i ) β i,c = h c (x i ),<label>(1)</label></formula><p>where γ i,c and β i,c modulate a neural network's activations F i,c , whose subscripts refer to the i th input's c th feature or feature map, via a feature-wise affine transformation:</p><formula xml:id="formula_1">F iLM (F i,c |γ i,c , β i,c ) = γ i,c F i,c + β i,c .<label>(2)</label></formula><p>f and h can be arbitrary functions such as neural networks. Modulation of a target neural network's processing can be based on the same input to that neural network or some other input, as in the case of multi-modal or conditional tasks. For CNNs, f and h thus modulate the per-feature-map distribution of activations based on x i , agnostic to spatial location. In practice, it is easier to refer to f and h as a single function that outputs one (γ, β) vector, since, for example, it is often beneficial to share parameters across f and h for more efficient learning. We refer to this single function as the FiLM generator. We also refer to the network to which FiLM layers are applied as the Feature-wise Linearly Modulated network, the FiLM-ed network.</p><p>FiLM layers empower the FiLM generator to manipulate feature maps of a target, FiLM-ed network by scaling them up or down, negating them, shutting them off, selectively thresholding them (when followed by a ReLU), and more. Each feature map is conditioned independently, giving the FiLM generator moderately fine-grained control over activations at each FiLM layer.</p><p>As FiLM only requires two parameters per modulated feature map, it is a scalable and computationally efficient conditioning method. In particular, FiLM has a computational cost that does not scale with the image resolution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model</head><p>Our FiLM model consists of a FiLM-generating linguistic pipeline and a FiLM-ed visual pipeline as depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. The FiLM generator processes a question x i using a Gated Recurrent Unit (GRU) network <ref type="bibr" target="#b5">(Chung et al. 2014)</ref> with 4096 hidden units that takes in learned, 200dimensional word embeddings. The final GRU hidden state is a question embedding, from which the model predicts (γ n i,· , β n i,· ) for each n th residual block via affine projection. The visual pipeline extracts 128 14 × 14 image feature maps from a resized, 224 × 224 image input using either a CNN trained from scratch or a fixed, pre-trained feature extractor with a learned layer of 3 × 3 convolutions. The CNN trained from scratch consists of 4 layers with 128 4 × 4 kernels each, ReLU activations, and batch normalization, similar to prior work on CLEVR <ref type="bibr" target="#b35">(Santoro et al. 2017)</ref>. The fixed feature extractor outputs the conv4 layer of a ResNet-101  pre-trained on ImageNet <ref type="bibr" target="#b34">(Russakovsky et al. 2015)</ref> to match prior work on CLEVR <ref type="bibr" target="#b21">(Johnson et al. 2017a;</ref><ref type="bibr" target="#b22">2017b)</ref>. Image features are processed by several -4 for our model -FiLM-ed residual blocks (ResBlocks) with 128 feature maps and a final classifier. The classifier consists of a 1 × 1 convolution to 512 feature maps, global max-pooling, and a two-layer MLP with 1024 hidden units that outputs a softmax distribution over final answers.</p><p>Each FiLM-ed ResBlock starts with a 1 × 1 convolution followed by one 3 × 3 convolution with an architecture as depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. We turn the parameters of batch normalization layers that immediately precede FiLM layers off. Drawing from prior work on CLEVR <ref type="bibr" target="#b35">Santoro et al. 2017</ref>) and visual reasoning <ref type="bibr" target="#b37">(Watters et al. 2017)</ref>, we concatenate two coordinate feature maps indicating relative x and y spatial position (scaled from −1 to 1) with the image features, each ResBlock's input, and the classifier's input to facilitate spatial reasoning. We train our model end-to-end from scratch with Adam <ref type="bibr" target="#b25">(Kingma and Ba 2015)</ref> (learning rate 3e −4 ), weight decay (1e −5 ), batch size 64, and batch normalization and ReLU throughout FiLM-ed network. Our model uses only image-question-answer triplets from the training set without data augmentation. We employ early stopping based on validation accuracy, training for 80 epochs maximum. Further model details are in the appendix. Empirically, we found FiLM had a large capacity, so many architectural and hyperparameter choices were for added regularization.</p><p>We stress that our model relies solely on feature-wise affine conditioning to use question information influence the visual pipeline behavior to answer questions. This approach differs from classical visual question answering pipelines which fuse image and language information into a single embedding via element-wise product, concatenation, attention, and/or more advanced methods <ref type="bibr" target="#b27">Lu et al. 2016;</ref><ref type="bibr" target="#b0">Anderson et al. 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>FiLM can be viewed as a generalization of Conditional Normalization (CN) methods. CN replaces the parameters of the feature-wise affine transformation typical in normalization layers, as introduced originally (Ioffe and Szegedy 2015), with a learned function of some conditioning information. Various forms of CN have proven highly effective across a number of domains: Conditional Instance Norm <ref type="bibr" target="#b11">Ghiasi et al. 2017)</ref> and Adaptive Instance Norm <ref type="bibr" target="#b19">(Huang and Belongie 2017)</ref> for image stylization, Dynamic Layer Norm for speech recognition <ref type="bibr" target="#b24">(Kim, Song, and Bengio 2017)</ref>, and Conditional Batch Norm for general visual question answering on complex scenes such as VQA and GuessWhat?! ). This work complements our own, as we seek to show that feature-wise affine conditioning is effective for multi-step reasoning and understand the underlying mechanism behind its success.</p><p>Notably, prior work in CN has not examined whether the affine transformation must be placed directly after normalization. Rather, prior work includes normalization in the method name for instructive purposes or due to implementation details. We investigate the connection between FiLM and normalization, finding it not strictly necessary for the affine transformation to occur directly after normalization. Thus, we provide a unified framework for all of these methods through FiLM, as well as a normalization-free relaxation of this approach which can be more broadly applied.</p><p>Beyond CN, there are many connections between FiLM and other conditioning methods. A common approach, used for example in Conditional DCGANs <ref type="bibr" target="#b33">(Radford, Metz, and Chintala 2016)</ref>, is to concatenate constant feature maps of conditioning information with convolutional layer input. Though not as parameter efficient, this method simply results in a feature-wise conditional bias. Likewise, concatenating conditioning information with fully-connected layer input amounts to a feature-wise conditional bias. Other approaches such as WaveNet (van den Oord et al. 2016a) and Conditional PixelCNN (van den Oord et al. 2016b) directly add a conditional feature-wise bias. These approaches are equivalent to FiLM with γ = 1, which we compare FiLM to in the Experiments section. In reinforcement learning, an alternate formulation of FiLM has been used to train one game-conditioned deep Q-network to play ten Atari games <ref type="bibr" target="#b26">(Kirkpatrick et al. 2017)</ref>, though FiLM was neither the focus of this work nor analyzed as a major component.</p><p>Other methods gate an input's features as a function of that same input, rather than a separate conditioning input. These methods include LSTMs for sequence modeling <ref type="bibr" target="#b16">(Hochreiter and Schmidhuber 1997)</ref>, Convolutional Sequence to Sequence for machine translation <ref type="bibr" target="#b9">(Gehring et al. 2017)</ref>, and even the ImageNet 2017 winning model, Squeeze and Excitation Networks <ref type="bibr" target="#b18">(Hu, Shen, and Sun 2017)</ref>. This approach amounts to a feature-wise, conditional scaling, restricted to between 0 and 1, while FiLM consists of both scaling and shifting, each unrestricted. In the Experiments section, we show the effect of restricting FiLM's scaling to between 0 and 1 for visual reasoning. We find it noteworthy that this general approach of feature modulation is effective across a variety of settings and architectures.</p><p>There are even broader links between FiLM and other methods. For example, FiLM can be viewed as using one network to generate parameters of another network, making it a form of hypernetwork <ref type="bibr" target="#b14">(Ha, Dai, and Le 2016)</ref>. Also, FiLM has potential ties with conditional computation and mixture of experts methods, where specialized network subparts are active on a per-example basis <ref type="bibr" target="#b23">(Jordan and Jacobs 1994;</ref><ref type="bibr" target="#b8">Eigen, Ranzato, and Sutskever 2014;</ref><ref type="bibr" target="#b36">Shazeer et al. 2017)</ref>; we later provide evidence that FiLM learns to selectively highlight or suppress feature maps based on conditioning information. Those methods select at a sub-network level while FiLM selects at a feature map level.</p><p>In the domain of visual reasoning, one leading method is the Program Generator + Execution Engine model <ref type="bibr" target="#b22">(Johnson et al. 2017b</ref>). This approach consists of a sequenceto-sequence Program Generator, which takes in a question and outputs a sequence corresponding to a tree of compos- able neural modules, each of which is a two or three layer residual block. This tree of neural modules is assembled to form the Execution Engine that then predicts an answer from the image. This modular approach is part of a line of neural module network methods <ref type="bibr" target="#b1">(Andreas et al. 2016a;</ref>, of which End-to-End Module Networks ) have also been tested on visual reasoning. These models use strong priors by explicitly modeling the compositional nature of reasoning and by training with additional program labels, i.e. ground-truth step-by-step instructions on how to correctly answer a question. End-to-End Module Networks further build in model biases via per-module, hand-crafted neural architectures for specific functions. Our approach learns directly from visual and textual input without additional cues or a specialized architecture. Relation Networks (RNs) are another leading approach for visual reasoning <ref type="bibr" target="#b35">(Santoro et al. 2017)</ref>. RNs succeed by explicitly building in a comparison-based prior. RNs use an MLP to carry out pairwise comparisons over each location of extracted convolutional features over an image, including LSTM-extracted question features as input to this MLP. RNs then element-wise sum over the resulting comparison vectors to form another vector from which a final classifier predicts the answer. We note that RNs have a computational cost that scales quadratically in spatial resolution, while FiLM's cost is independent of spatial resolution. Notably, since RNs concatenate question features with MLP input, a form of feature-wise conditional biasing as explained earlier, their conditioning approach is related to FiLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>First, we test our model on visual reasoning with the CLEVR task and use trained FiLM models to analyze what FiLM learns. Second, we explore how well our model generalizes to more challenging questions with the CLEVR-Humans task. Finally, we examine how FiLM performs in fewshot and zero-shot generalization settings using the CLEVR Compositional Generalization Test. In the appendix, we provide an error analysis of our model. Our code is available at https://github.com/ethanjperez/film.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CLEVR Task</head><p>CLEVR is a synthetic dataset of 700K (image, question, answer, program) tuples <ref type="bibr" target="#b21">(Johnson et al. 2017a</ref>). Images contain 3D-rendered objects of various shapes, materials, colors, and sizes. Questions are multi-step and compositional in nature, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. They range from counting questions ("How many green objects have the same size as the green metallic block?") to comparison questions ("Are there fewer tiny yellow cylinders than yellow metal cubes?") and can be 40+ words long. Answers are each one word from a set of 28 possible answers. Programs are an additional supervisory signal consisting of step-by-step instructions, such as filter shape[cube], relate <ref type="bibr">[right]</ref>, and count, on how to answer the question.</p><p>Baselines We compare against the following methods, discussed in detail in the Related Work section:</p><p>• Q-type baseline: Predicts based on a question's category.</p><p>• LSTM: Predicts using only the question. Results FiLM achieves a new overall state-of-the-art on CLEVR, as shown in <ref type="table">Table 1</ref>, outperforming humans and previous methods, including those using explicit models of reasoning, program supervision, and/or data augmentation.  For methods not using extra supervision, FiLM roughly halves state-of-the-art error (from 4.5% to 2.3%). Note that using pre-trained image features as input can be viewed as a form of data augmentation in itself but that FiLM performs equally well using raw pixel inputs. Interestingly, the raw pixel model seems to perform better on lower-level questions (i.e. querying and comparing attributes) while the image features model seems to perform better on higher-level questions (i.e. compare numbers of objects).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">What Do FiLM Layers Learn?</head><p>To understand how FiLM visually reasons, we visualize activations to observe the net result of FiLM layers. We also use histograms and t-SNE (van der Maaten and Hinton 2008) to find patterns in the learned FiLM γ and β parameters themselves. In <ref type="figure" target="#fig_0">Figures 14 and 15</ref> in the appendix, we visualize the effect of FiLM at the single feature map level.</p><p>Activation Visualizations <ref type="figure" target="#fig_4">Figure 4</ref> visualizes the distribution of locations responsible for the globally-pooled features which the MLP in the model's final classifier uses to predict answers. These images reveal that the FiLM model predicts using features of areas near answer-related or question-related objects, as the high CLEVR accuracy also suggests. This finding highlights that appropriate feature modulation indirectly results in spatial modulation, as regions with question-relevant features will have large activations while other regions will not. This observation might explain why FiLM outperforms Stacked Attention, the next best method not explicitly built for reasoning, so significantly (21%); FiLM appears to carry many of spatial attention's benefits, while also influencing feature representation. <ref type="figure" target="#fig_4">Figure 4</ref> also suggests that the FiLM-ed network carries out reasoning throughout its pipeline. In the top example, the FiLM-ed network has localized the answer-referenced object alone before the MLP classifier. In the bottom example, the FiLM-ed network retains, for the MLP classifier, fea- tures on objects that are not referred to by the answer but are referred to by the question. The latter example provides evidence that the final MLP itself carries out some reasoning, using FiLM to extract relevant features for its reasoning.</p><p>FiLM Parameter Histograms To analyze at a lower level how FiLM uses the question to condition the visual pipeline, we plot γ and β values predicted over the validation set, as shown in <ref type="figure" target="#fig_5">Figure 5</ref> and in more detail in the appendix <ref type="figure" target="#fig_0">(Figures 16 to 18</ref>). γ and β values take advantage of a sizable range, varying from -15 to 19 and from -9 to 16, respectively. γ values show a sharp peak at 0, showing that FiLM learns to use the question to shut off or significantly suppress whole feature maps. Simultaneously, FiLM learns to upregulate a much more selective set of other feature maps with high magnitude γ values. Furthermore, a large fraction (36%) of γ values are negative; since our model uses a ReLU after FiLM, γ &lt; 0 can cause a significantly different set of activations to pass the ReLU to downstream layers than γ &gt; 0. Also, 76% of β values are negative, suggesting that FiLM also uses β to be selective about which activations pass the ReLU. We show later that FiLM's success is largely architecture-agnostic, but examining a particular model gives insight into the influence FiLM learns to exert in a specific case. Together, these findings suggest that FiLM learns to selectively upregulate, downregulate, and shut off feature maps based on conditioning information.</p><p>FiLM Parameters t-SNE Plot In <ref type="figure" target="#fig_6">Figure 6</ref>, we visualize FiLM parameter vectors (γ, β) for 3,000 random validation points with t-SNE. We analyze the deeper, 6-ResBlock version of our model, which has a similar validation accuracy as our 4-ResBlock model, to better examine how FiLM layers in different layers of a hierarchy behave. First and last layer FiLM (γ, β) are grouped by the low-level and high-level reasoning functions necessary to answer CLEVR questions, respectively. For example, FiLM parameters for equal color and query color are close for the first layer but apart for the last layer. The same is true for shape, size and material questions. Conversely, equal shape, equal size, and equal material FiLM parameters are grouped in the last layer but split in the first layer -likewise for other high level groupings such as integer comparison and querying. These findings suggest that FiLM layers learn a sort of function-based modularity without an architectural prior. Simply with end-to-end training, FiLM learns to handle not only different types of questions differently, but also different types of question sub-parts differently; the FiLM model works from low-level to high-level processes as is the proper approach. For models with fewer FiLM layers, such patterns also appear, but less clearly; these models must begin higher level reasoning sooner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablations</head><p>Using the validation set, we conduct an ablation study on our best model to understand how FiLM learns visual reasoning. We show results for test time ablations in <ref type="figure" target="#fig_7">Figure 7</ref>, for architectural ablations in <ref type="table">Table 2</ref>, and for varied model depths in <ref type="table" target="#tab_4">Table 3</ref>. Without hyperparameter tuning, most architectural ablations and model depths outperform prior state-of-the-art on training from only image-question-answer triplets, supporting FiLM's overall robustness. <ref type="table" target="#tab_4">Table 3</ref> also shows using the validation set that our results are statistically significant. Effect of γ and β To test the effect of γ and β separately, we trained one model with a constant γ = 1 and another with β = 0. With these models, we find a 1.5% and .5% accuracy drop, respectively; FiLM can learn to condition the CNN for visual reasoning through either biasing or scaling alone, albeit not as well as conditioning both together. This result also suggests that γ is more important than β.</p><p>To further compare the importance of γ and β, we run a series of test time ablations <ref type="figure" target="#fig_7">(Figure 7</ref>) on our best, fullytrained model. First, we replace β with the mean β across the training set. This ablation in effect removes all conditioning information from β parameters during test time, from a model trained to use both γ and β. Here, we find that accuracy only drops by 1.0%, while the same procedure on γ results in a 65.4% drop. This large difference suggests that, in practice, FiLM largely conditions through γ rather than β. Next, we analyze performance as we add increasingly more Gaussian noise to the best model's FiLM parameters at test time. Noise in gamma hurts performance significantly more, showing FiLM's higher sensitivity to changes in γ than in β and corroborating the relatively greater importance of γ.</p><p>Restricting γ To understand what aspect of γ is most effective, we train a model that limits γ to (0, 1) using sig-  <ref type="table">Table 2</ref>: CLEVR val accuracy for ablations, trained with the best architecture with only specified changes. We report the standard deviation of the best model accuracy over 5 runs.</p><p>moid, as many models which use feature-wise, multiplicative gating do. Likewise, we also limit γ to (−1, 1) using tanh. Both restrictions hurt performance, roughly as much as removing conditioning from γ entirely by training with γ = 1. Thus, FiLM's ability to scale features by large magnitudes appears to contribute to its success. Limiting γ to (0, ∞) with exp also hurts performance, validating the value of FiLM's capacity to negate and zero out feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional Normalization</head><p>We perform an ablation study on the placement of FiLM to evaluate the relationship between normalization and FiLM that Conditional Normalization approaches assume.  Residual Connection Removing the residual connection causes one of the larger accuracy drops. Since there is a global max-pooling operation near the end of the network, this finding suggests that the best model learns to primarily use features of locations that are repeatedly important throughout lower and higher levels of reasoning to make its final decision. The higher accuracies for models with FiLM modulating features inside residual connections rather than outside residual connections supports this hypothesis. <ref type="table" target="#tab_4">Table 3</ref> shows model performance by the number of ResBlocks. FiLM is robust to varying depth but less so with only 1 ResBlock, backing the earlier theory that the FiLM-ed network reasons throughout its pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CLEVR-Humans: Human-Posed Questions</head><p>To assess how well visual reasoning models generalize to more realistic, complex, and free-form questions, the CLEVR-Humans dataset was introduced <ref type="bibr" target="#b22">(Johnson et al. 2017b</ref>). This dataset contains human-posed questions on CLEVR images along with their corresponding answers.  <ref type="figure">Figure 8</ref>: Examples from CLEVR-Humans, which introduces new words (underlined) and concepts. After fine-tuning on CLEVR-Humans, a CLEVR-trained model can now reason about obstruction, superlatives, and reflections but still struggles with hypothetical scenarios (rightmost). It also has learned human preference to primarily identify objects by shape (leftmost).</p><p>for validation, and 7K for testing. The questions were collected from Amazon Mechanical Turk workers prompted to ask questions that were likely hard for a smart robot to answer. As a result, CLEVR-Humans questions use more diverse vocabulary and complex concepts.</p><p>Method To test FiLM on CLEVR-Humans, we take our best CLEVR-trained FiLM model and fine-tune its FiLMgenerating linguistic pipeline alone on CLEVR-Humans. Similar to prior work <ref type="bibr" target="#b22">(Johnson et al. 2017b</ref>), we do not update the visual pipeline on CLEVR-Humans to mitigate overfitting to the small training set.</p><p>Results Our model achieves state-of-the-art generalization to CLEVR-Humans, both before and after fine-tuning, as shown in <ref type="table" target="#tab_7">Table 4</ref>, indicating that FiLM is well-suited to handle more complex and diverse questions. <ref type="figure">Figure 8</ref> shows examples from CLEVR-Humans with FiLM model answers.</p><p>Before fine-tuning, FiLM outperforms prior methods by a smaller margin. After fine-tuning, FiLM reaches a considerably improved final accuracy. In particular, the gain in accuracy made by FiLM upon fine-tuning is more than 50% greater than those made by other models; FiLM adapts dataefficiently using the small CLEVR-Humans dataset. Notably, FiLM surpasses the prior state-of-the-art method, Program Generator + Execution Engine (PG+EE), after fine-tuning by 9.3%. Prior work on PG+EEs explains that this neural module network method struggles on questions which cannot be well approximated with the model's module inventory <ref type="bibr" target="#b22">(Johnson et al. 2017b)</ref>. In contrast, FiLM has the freedom to modulate existing feature maps, a fairly flexible and fine-grained operation, in novel ways to reason about new concepts. These results thus provide some evidence for the benefits of FiLM's general nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">CLEVR Compositional Generalization Test</head><p>To test how well models learn compositional concepts that generalize, CLEVR-CoGenT was introduced <ref type="bibr" target="#b21">(Johnson et al. 2017a</ref>). This dataset is synthesized in the same way as CLEVR but contains two conditions: in Condition A, all cubes are gray, blue, brown, or yellow and all cylinders are  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We train our best model architecture on Condition A and report accuracies on Conditions A and B, before and after fine-tuning on B, in <ref type="figure" target="#fig_8">Figure 9</ref>. Our results indicate FiLM surpasses other visual reasoning models at learning general concepts. FiLM learns better compositional generalization even than PG+EE, which explicitly models compositionality and is trained with program-level supervision that specifically includes filtering colors and filtering shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample Efficiency and Catastrophic Forgetting</head><p>We show sample efficiency and forgetting curves in <ref type="figure" target="#fig_8">Figure 9</ref>. FiLM achieves prior state-of-the-art accuracy with 1/3 as much fine-tuning data. However, our FiLM model still suffers from catastrophic forgetting after fine-tuning.</p><p>Zero-Shot Generalization FiLM's accuracy on Condition A is much higher than on B, suggesting FiLM has memorized attribute combinations to an extent. For example, the model learns a bias that cubes are not cyan, as learning this training set bias helps minimize training loss.</p><p>To overcome this bias, we develop a novel FiLM-based zero-shot generalization method. Inspired by word embedding manipulations, e.g. "King" -"Man" + "Woman" = "Queen" <ref type="bibr" target="#b30">(Mikolov et al. 2013)</ref>, we test if linear manipula-   <ref type="figure">(Figure)</ref>. Accuracy before and after fine-tuning on 30K of ValB <ref type="table">(Table)</ref>.</p><p>tion extends to reasoning with FiLM. We compute (γ, β) for "How many cyan cubes are there?" via the linear combination of questions in the FiLM parameter space: "How many cyan spheres are there?" + "How many brown cubes are there?" − "How many brown spheres are there?". With this (γ, β), our model can correctly count cyan cubes. We show another example of this method in <ref type="figure" target="#fig_0">Figure 10</ref>. We evaluate this method on validation B, using a parser to automatically generate the right combination of questions. We test previously reported CLEVR-CoGenT FiLM models with this method and show results in <ref type="figure" target="#fig_8">Figure 9</ref>. With this method, there is a 3.2% overall accuracy gain when training on A and testing for zero-shot generalization on B. Yet this method could only be applied to 1/3 of questions in B. For these questions, model accuracy starts at 71.5% and jumps to 80.7%. Before fine-tuning on B, the accuracy between zero-shot and original approaches on A is identical, likewise for B after fine-tuning. We note that difference in the predicted FiLM parameters between these two methods is negligible, likely causing the similar performance.</p><p>We achieve these improvements without specifically training our model for zero-shot generalization. Our method simply allows FiLM to take advantage of any concept disentanglement in the CNN after training. We also observe that convex combinations of the FiLM parameters -i.e. between "How many cyan things are there?" and "How many brown things are there?" -often monotonically interpolates the predicted answer between the answers to endpoint questions. These results highlight, to a limited extent, the flexibility of FiLM parameters for meaningful manipulations.</p><p>As implemented, this method has many limitations. However, approaches from word embeddings, representation learning, and zero-shot learning can be applied to directly optimize (γ, β) for analogy-making <ref type="bibr" target="#b4">(Bordes et al. 2013;</ref><ref type="bibr" target="#b13">Guu, Miller, and Liang 2015;</ref><ref type="bibr" target="#b31">Oh et al. 2017)</ref>. The FiLM-ed network could directly train with this procedure via backpropagation. A learned model could also replace the parser. We find such avenues promising for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>What is the blue big cylinder made of? (1) Swap shape What is the blue big sphere made of? (2) Swap color What is the green big cylinder made of? (3) Swap shape/color What is the green big sphere made of? <ref type="figure" target="#fig_0">Figure 10</ref>: A CLEVR-CoGenT example. The combination of concepts "blue" and "cylinder" is not in the training set. Our zero-shot method computes the original question's FiLM parameters via linear combination of three other questions' FiLM parameters: (1) + (2) -(3). This method corrects our model's answer from "rubber" to "metal".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We show that a model can achieve strong visual reasoning using general-purpose Feature-wise Linear Modulation layers. By efficiently manipulating a neural network's intermediate features in a selective and meaningful manner using FiLM layers, a RNN can effectively use language to modulate a CNN to carry out diverse and multi-step reasoning tasks over an image. Our ablation study suggests that FiLM is resilient to architectural modifications, test time ablations, and even restrictions on FiLM layers themselves. Notably, we provide evidence that FiLM's success is not closely connected with normalization as previously assumed. Thus, we open the door for applications of this approach to settings where normalization is less common, such as RNNs and reinforcement learning. Our findings also suggest that FiLM models can generalize better, more sample efficiently, and even zero-shot to foreign or more challenging data. Overall, the results of our investigation of FiLM in the case of visual reasoning complement broader literature that demonstrates the success of FiLM-like techniques across many domains, supporting the case for FiLM's strength not simply within a single domain but as a general, versatile approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>We thank the developers of PyTorch (pytorch.org) and <ref type="bibr" target="#b22">(Johnson et al. 2017b</ref> Occlusion Many model errors are due to partial occlusion.</p><p>These errors may likely be fixed using a CNN that operates at a higher resolution, which is feasible since FiLM has a computational cost that is independent of resolution.</p><p>Counting 96.1% of counting mistakes are off-by-one errors,</p><p>showing FiLM has learned underlying concepts behind counting such as close relationships between close numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logical Consistency</head><p>The model sometimes makes curious reasoning mistakes a human would not. For example, we find a case where our model correctly counts one gray object and two cyan objects but simultaneously answers that there are the same number of gray and cyan objects. In fact, it answers that the number of gray objects is both less than and equal to the number of yellow blocks. These errors could be prevented by directly minimizing logical inconsistency, an interesting avenue for future work orthogonal to FiLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Model Details</head><p>Rather than output γi,c directly, we output ∆γi,c, where:</p><formula xml:id="formula_2">γi,c = 1 + ∆γi,c,<label>(3)</label></formula><p>since initially zero-centered γi,c can zero out CNN feature map activations and thus gradients. In our implementation, we opt to output ∆γi,c rather than γi,c, but for simplicity, throughout our paper, we explain FiLM using γi,c. However, this modification does not seem to affect our model's performance on CLEVR statistically significantly.</p><p>We present training and validation curves for best model trained from image features in <ref type="figure" target="#fig_0">Figure 11</ref>. We observe fast accuracy gains initially, followed by slow, steady increases to a best validation accuracy of 97.84%, at which point training accuracy is 99.53%. We train on CLEVR for 80 epochs, which takes 4 days using 1 NVIDIA TITAN Xp GPU when learning from image features. For practical reasons, we stop training on CLEVR after 80 epochs, but we observe that accuracy continues to increase slowly even afterwards. <ref type="figure" target="#fig_0">Figure 11</ref>: Best model training and validation curves. Q: Is there a big brown object of the same shape as the green thing? A: Yes (P: No) Q: What number of other things are the same material as the big gray cylinder? A: 6 (P: 5) Q: What shape is the big metal thing that is the same color as the small cylinder? A: Cylinder (P: Sphere) Q: How many other things are the same material as the tiny sphere? A: 3 (P: 2) <ref type="figure" target="#fig_0">Figure 12</ref>: Some image-question pairs where our model predicts incorrectly. Most errors we observe are due to partially occluded objects, as highlighted in the three first examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Answer How many gray things are there? 1 How many cyan things are there? 2 Are there as many gray things as cyan things?</p><p>Yes Are there more gray things than cyan things?</p><p>No Are there fewer gray things than cyan things? Yes <ref type="figure" target="#fig_0">Figure 13</ref>: An interesting failure example where our model counts correctly but compares counts erroneously. Its third answer is incorrect and inconsistent with its other answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">What Do FiLM Layers Learn?</head><p>We visualize FiLM's effect on a single arbitrary feature map in <ref type="figure" target="#fig_0">Figures 14</ref>   <ref type="figure" target="#fig_0">Figure 14</ref>: Visualizations of feature map activations (scaled from 0 to 1) before and after FiLM for a single arbitrary feature map from the first ResBlock. This particular feature map seems to detect gray and brown colors. Interestingly, FiLM modifies activations for specifically colored objects for color-specific questions but leaves activations alone for color-agnostic questions. Note that since this is the first FiLM layer, pre-FiLM activations (Rows 1 and 3) for all questions are identical, and differences in post-FiLM activations (Rows 2 and 4) are solely due FiLM's use of question information.</p><p>Feature 79 -Block 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Before FiLM</head><p>After FiLM Q: How many cyan objects are behind the gray sphere? A: 2 Q: How many cyan objects are in front of the gray sphere? A: 1 Q: How many cyan objects are left of the gray sphere? A: 2 Q: How many cyan objects are right of the gray sphere? A: 1 <ref type="figure" target="#fig_0">Figure 15</ref>: Visualization of the impact of FiLM for a single arbitrary feature map from the last ResBlock. This particular feature map seems to focus on spatial features (i.e. front/back or left/right) Note that since this is the last FiLM layer, the top row activations have already been influenced by question information via several FiLM layers.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>CLEVR examples and FiLM model answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A single FiLM layer for a CNN. The dot signifies a Hadamard product. Various combinations of γ and β can modulate individual feature maps in a variety of ways.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The FiLM generator (left), FiLM-ed network (middle), and residual block architecture (right) of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc>CNN+LSTM: MLP prediction over CNN-extracted image features and LSTM-extracted question features. • Stacked Attention Networks (CNN+LSTM+SA): Linear prediction over CNN-extracted image feature and LSTM-extracted question features combined via two rounds of soft spatial attention (Yang et al. 2016). • End-to-End Module Networks (N2NMN) and Program Generator + Execution Engine (PG+EE): Methods in which separate neural networks learn separate subfunctions and are assembled into a question-dependent structure (Hu et al. 2017; Johnson et al. 2017b). • Relation Networks (CNN+LSTM+RN): An approach which builds in pairwise comparisons over spatial locations to explicitly model reasoning's relational nature (Santoro et al. 2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualizations of the distribution of locations which the model uses for its globally max-pooled features which its final MLP predicts from. FiLM correctly localizes the answer-referenced object (top) or all question-referenced objects (bottom), but not as accurately when it answers incorrectly (rightmost bottom). Questions and images used match (Johnson et al. 2017b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Histograms of γ i,c (left) and β i,c (right) values over all FiLM layers, calculated over the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6</head><label>6</label><figDesc>: t-SNE plots of (γ, β) of the first (left) and last (right) FiLM layers of a 6-FiLM layer Network. FiLM parameters cluster by low-level reasoning functions in the first layer and by high-level reasoning functions in the last layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>An analysis of how robust FiLM parameters are to noise at test time. The horizontal lines correspond to setting γ or β to their respective training set mean values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>CoGenT results. FiLM ValB accuracy reported on ValB without the 30K fine-tuning samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 16 :</head><label>16</label><figDesc>Histograms of γ i,c values for each FiLM layer (layers 1-4 from left to right), computed on CLEVR's validation set. Plots are scaled identically. FiLM layers appear gradually more selective and higher variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 17 :</head><label>17</label><figDesc>Histograms of β i,c values for each FiLM layer (layers 1-4 from left to right) computed on CLEVR's validation set. Plots are scaled identically. β i,c values take a different, higher variance distribution in the first layer than in later layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 18 :</head><label>18</label><figDesc>Histograms of per-channel γ c and β c statistics (mean and standard deviation) computed on CLEVR's validation set. From left to right: γ c means, γ c standard deviations, β c means, β c standard deviations. Different feature maps are modulated by FiLM in different patterns; some are often zero-ed out while other rarely are, some are consistently scaled or shifted by similar values while others by high variance values, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1: CLEVR accuracy (overall and per-question-type) by baselines, competing methods, and FiLM. (*) denotes use of extra supervision via program labels. ( †) denotes use of data augmentation. ( ‡) denotes training from raw pixels.</figDesc><table><row><cell>Model</cell><cell cols="3">Overall Count Exist</cell><cell>Compare Numbers</cell><cell>Query Attribute</cell><cell>Compare Attribute</cell></row><row><cell>Human (Johnson et al. 2017b)</cell><cell>92.6</cell><cell>86.7</cell><cell>96.6</cell><cell>86.5</cell><cell>95.0</cell><cell>96.0</cell></row><row><cell>Q-type baseline (Johnson et al. 2017b)</cell><cell>41.8</cell><cell>34.6</cell><cell>50.2</cell><cell>51.0</cell><cell>36.0</cell><cell>51.3</cell></row><row><cell>LSTM (Johnson et al. 2017b)</cell><cell>46.8</cell><cell>41.7</cell><cell>61.1</cell><cell>69.8</cell><cell>36.8</cell><cell>51.8</cell></row><row><cell>CNN+LSTM (Johnson et al. 2017b)</cell><cell>52.3</cell><cell>43.7</cell><cell>65.2</cell><cell>67.1</cell><cell>49.3</cell><cell>53.0</cell></row><row><cell>CNN+LSTM+SA (Santoro et al. 2017)</cell><cell>76.6</cell><cell>64.4</cell><cell>82.7</cell><cell>77.4</cell><cell>82.6</cell><cell>75.4</cell></row><row><cell>N2NMN* (Hu et al. 2017)</cell><cell>83.7</cell><cell>68.5</cell><cell>85.7</cell><cell>84.9</cell><cell>90.0</cell><cell>88.7</cell></row><row><cell>PG+EE (9K prog.)* (Johnson et al. 2017b)</cell><cell>88.6</cell><cell>79.7</cell><cell>89.7</cell><cell>79.1</cell><cell>92.6</cell><cell>96.0</cell></row><row><cell>PG+EE (700K prog.)* (Johnson et al. 2017b)</cell><cell>96.9</cell><cell>92.7</cell><cell>97.1</cell><cell>98.7</cell><cell>98.1</cell><cell>98.9</cell></row><row><cell>CNN+LSTM+RN † ‡ (Santoro et al. 2017)</cell><cell>95.5</cell><cell>90.1</cell><cell>97.8</cell><cell>93.6</cell><cell>97.9</cell><cell>97.1</cell></row><row><cell>CNN+GRU+FiLM</cell><cell>97.7</cell><cell>94.3</cell><cell>99.1</cell><cell>96.8</cell><cell>99.1</cell><cell>99.1</cell></row><row><cell>CNN+GRU+FiLM ‡</cell><cell>97.6</cell><cell>94.3</cell><cell>99.3</cell><cell>93.4</cell><cell>99.3</cell><cell>99.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>What shape is the... ...purple thing? A: cube ...blue thing? A: sphere ...red thing right of the blue thing? A: sphere ...red thing left of the blue thing? A: cube</figDesc><table><row><cell>Q: How many cyan things are...</cell><cell>...right of the gray cube? A: 3</cell><cell>...left of the small cube? A: 2</cell><cell>...right of the gray cube and left of the small cube? A: 1</cell><cell>...right of the gray cube or left of the small cube? A: 4 (P: 3)</cell></row></table><note>Q:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>CLEVR val accuracy by FiLM model depth.promising directions for future work with FiLM.</figDesc><table><row><cell>Repetitive Conditioning To understand the contribution</cell></row><row><cell>of repetitive conditioning towards FiLM model success, we</cell></row><row><cell>train FiLM models with successively fewer FiLM layers.</cell></row><row><cell>Models with fewer FiLM layers, even a single FiLM layer,</cell></row><row><cell>do not deviate far from the best model's performance, reveal-</cell></row><row><cell>ing that the model can reason and answer diverse questions</cell></row><row><cell>successfully by modulating features even just once. This ob-</cell></row><row><cell>servation highlights the capacity of even one FiLM layer.</cell></row><row><cell>Perhaps one FiLM layer can pass enough question informa-</cell></row><row><cell>tion to the CNN to enable it to carry out reasoning later in</cell></row><row><cell>the network, in place of the more hierarchical conditioning</cell></row><row><cell>deeper FiLM models appear to use. We leave more in-depth</cell></row><row><cell>investigation of this matter for future work.</cell></row><row><cell>Spatial Reasoning To examine how FiLM models ap-</cell></row><row><cell>proach spatial reasoning, we train a version of our best</cell></row><row><cell>model architecture, from image features, with only 1 × 1</cell></row><row><cell>convolutions and without feeding coordinate feature maps</cell></row><row><cell>indicating relative spatial position to the model. Due to the</cell></row><row><cell>global max-pooling near the end of the model, this model</cell></row><row><cell>cannot transfer information across spatial positions. No-</cell></row><row><cell>tably, this model still achieves a high 95.3% accuracy, in-</cell></row><row><cell>dicating that FiLM models are able to reason about space</cell></row><row><cell>simply from the spatial information contained in a single lo-</cell></row><row><cell>cation of fixed image features.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: CLEVR-Humans test accuracy, before (left) and</cell></row><row><cell>after (right) fine-tuning on CLEVR-Humans data</cell></row><row><cell>red, green, purple, or cyan; in Condition B, cubes and cylin-</cell></row><row><cell>ders swap color palettes. Both conditions contain spheres of</cell></row><row><cell>all colors. CLEVR-CoGenT thus indicates how a model an-</cell></row><row><cell>swers CLEVR questions: by memorizing combinations of</cell></row><row><cell>traits or by learning disentangled or general representations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>We examine the errors our model makes to understand where our model fails and how it acts when it does. Examples of these errors are shown inFigures 12 and 13.</figDesc><table><row><cell>7 Appendix</cell></row><row><cell>7.1 Error Analysis</cell></row><row><cell>) for open-source code which</cell></row><row><cell>our implementation was based off. We thank Mohammad</cell></row><row><cell>Pezeshki, Dzmitry Bahdanau, Yoshua Bengio, Nando de</cell></row><row><cell>Freitas, Hugo Larochelle, Laurens van der Maaten, Joseph</cell></row><row><cell>Cohen, Joelle Pineau, Olivier Pietquin, Jérémie Mary, César</cell></row><row><cell>Laurent, Chin-Wei Huang, Layla Asri, Max Smith, and</cell></row><row><cell>James Ough for helpful discussions and Justin Johnson</cell></row><row><cell>for CLEVR test evaluations. We thank NVIDIA for do-</cell></row><row><cell>nating a DGX-1 computer used in this work. We also ac-</cell></row><row><cell>knowledge FRQNT through the CHIST-ERA IGLU project,</cell></row><row><cell>Collège Doctoral Lille Nord de France, and CPER Nord-</cell></row><row><cell>Pas de Calais/FEDER DATA Advanced data science and</cell></row><row><cell>technologies 2015-2020 for funding our work. Lastly, we</cell></row><row><cell>thank acronymcreator.net for the acronym FiLM.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>and 15. We also show histograms of per-layer γi,c values, per-layer βi,c values, and per-channel FiLM parameter statistics in Figures 16, 17, and 18, respectively.</figDesc><table><row><cell></cell><cell>Before FiLM</cell><cell></cell><cell></cell></row><row><cell>Feature 14 -Block 1</cell><cell>After FiLM</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Q: What is the color of the large rubber cylin-der? A: Cyan</cell><cell>Q: What is the color of the large rubber sphere? A: Gray</cell><cell>Q: What is the color of the cube? A: Yellow</cell><cell>Q: How many cylin-ders are there? A: 4</cell></row><row><cell></cell><cell>Before FiLM</cell><cell></cell><cell></cell></row><row><cell>Feature 14 -Block 1</cell><cell>After FiLM</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Q: What is the color of the large rubber cylin-der? A: Yellow</cell><cell>Q: What is the color of the large rubber sphere? A: Gray</cell><cell>Q: What is the color of the cube? A: Yellow</cell><cell>Q: How many cylin-ders are there? A: 4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VQA Workshop at CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. Curran Associates</title>
		<editor>Burges, C. J. C.</editor>
		<editor>Bottou, L.</editor>
		<editor>Welling, M.</editor>
		<editor>Ghahramani, Z.</editor>
		<editor>and Weinberger, K. Q.</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop at NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning factored representations in a deep mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual turing test for computer vision systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hallonquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Acad Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="3618" to="3623" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploring the structure of a real-time, arbitrary neural artistic stylization network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno>abs/1705.06830</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hypernetworks. In ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ILSVRC 2017 Workshop at CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in realtime with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dynamic layer normalization for adaptive neural acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>InterSpeech</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clopath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-shot task generalization with multi-task deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kholi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning visual reasoning without strong priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLSLP Workshop at ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<ptr target="CoRRabs/1706.01427" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">; L</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">; L</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. van der Maaten</title>
		<imprint>
			<date type="published" when="2008-11" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
	<note>Visualizing data using t-sne</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<idno>abs/1706.01433</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
