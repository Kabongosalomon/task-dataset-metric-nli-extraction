<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TextScanner: Reading Characters in Order for Robust Scene Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Megvii</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Megvii</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TextScanner: Reading Characters in Order for Robust Scene Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Driven by deep learning and a large volume of data, scene text recognition has evolved rapidly in recent years. Formerly, RNN-attention-based methods have dominated this field, but suffer from the problem of attention drift in certain situations. Lately, semantic segmentation based algorithms have proven effective at recognizing text of different forms (horizontal, oriented and curved). However, these methods may produce spurious characters or miss genuine characters, as they rely heavily on a thresholding procedure operated on segmentation maps. To tackle these challenges, we propose in this paper an alternative approach, called TextScanner, for scene text recognition. TextScanner bears three characteristics: (1) Basically, it belongs to the semantic segmentation family, as it generates pixel-wise, multi-channel segmentation maps for character class, position and order; (2) Meanwhile, akin to RNN-attention-based methods, it also adopts RNN for context modeling; (3) Moreover, it performs paralleled prediction for character position and class, and ensures that characters are transcripted in the correct order. The experiments on standard benchmark datasets demonstrate that TextScanner outperforms the state-of-the-art methods. Moreover, TextScanner shows its superiority in recognizing more difficult text such as Chinese transcripts and aligning with target characters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past decades, scene text detection and recognition have drawn considerable attention from the computer vision community, due to its wide applications, e.g, automatic driving <ref type="bibr" target="#b6">(Graves et al. 2006)</ref>, visual auxiliaries <ref type="bibr" target="#b5">(Ghosh, Valveny, and Bagdanov 2017)</ref>, and human-computer interaction <ref type="bibr" target="#b32">(Wang et al. 2012)</ref>. As scene text provides pivotal and specific information, accurate recognition of text plays crucial roles in various real-world scenarios <ref type="bibr" target="#b23">(Phan et al. 2013</ref>).</p><p>Among the state-of-the-art methods for scene text recognition, there are two prevalent paradigms: RNN-attentionbased methods and semantic segmentation based algorithms. <ref type="figure">Figure 1</ref>: Our Motivation. RNN-attention-based methods may encounter the problem of attention drift <ref type="bibr" target="#b2">(Cheng et al. 2017</ref>) (see the red rectangle), thus leading to incorrect prediction of character class. In semantic segmentation based algorithms, the search of connected components depends on a thresholding operation, which is prone to oversegmentation or under-segmentation, thus generating spurious characters or missing genuine characters (see the red rectangles). In contrast, TextScanner scans characters one by one and ensures that characters are read in right order and separated properly.</p><p>The former <ref type="bibr" target="#b26">(Shi et al. 2016;</ref><ref type="bibr" target="#b3">Cheng et al. 2018)</ref>, drawing inspiration from neural machine translation <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2014)</ref>, encodes images into features and employs an attention mechanism to align and decode characters. The latter <ref type="bibr" target="#b19">Liao et al. 2019)</ref>, approaching text recognition from a 2D perspective, first adopts a fully convolutional network (FCN) to perform semantic segmentation, then seeks connected components in the segmentation maps, and finally infers the class of each connected component (each is taken as a character).</p><p>Motivation Essentially, to correctly recognize the content in a cropped text image, the number of characters as well as the order and class of each character should be accurately predicted. RNN-attention based methods usually work well in most cases. However, when there is noise in the background or irregular text shape , the attention mechanism may fail that the center of the estimated attention map targets to a wrong position, causing mistakes in character order and class (see <ref type="figure">Fig. 1</ref>). More seriously, due to the recurrent memory mechanism in the RNN module, such errors will accumulate and propagate, making the situation even worse.</p><p>Semantic segmentation based algorithms explore a different way and exhibit stronger adaptability to text of different shapes (horizontal, oriented and curved). However, it is difficult to successfully separate each character from the segmentation maps, since improper binarization will result in such embarrassments: one character might be split into multiple parts or multiple characters may stick together (see <ref type="figure">Fig. 1</ref>). In these cases, the predictions of number and class of characters would be wrong. In summary, existing approaches, either RNN-attention based or semantic segmentation based, are not able to commendably resolve the difficulties in scene text recognition.</p><p>The root cause for the attention drift problem in the RNNattention based methods might be that the alignment operations (realized with attention maps) rely on both visual features and previous decoding results. Mutual interference might occur between these two types of information. Therefore, it is necessary to perform character alignment and classification in independent branches. Regarding semantic segmentation based algorithms, the assumption that characters can be sought via simple binarization does not hold in challenging scenarios. To address this issue, a natural and feasible solution is to represent the position and order of characters with different channels.</p><p>Our Work In this paper, we propose a novel text recognition framework, called TextScanner. Like a scanner in the real world, TextScanner can read characters in correct order.</p><p>Generally, TextScanner is built upon semantic segmentation <ref type="bibr" target="#b19">(Liao et al. 2019)</ref>. It consists of two branches: one for character classification (class branch) and the other for character position and order prediction (geometry branch) (see <ref type="figure" target="#fig_0">Fig. 2</ref>). The class branch produces multi-channel segmentation maps, in which the values at each location represent the probabilities of character classes (including the background class). The geometry branch also produces multi-channel segmentation maps, but the meanings of the values at each location are different from those in the class branch. The characters are sought by multiplying two groups of segmentation maps in an element-wise manner and acquiring the class with maximal probability for each channel. This procedure is termed as word formation in this work.</p><p>Since characters are well aligned and the order is ensured, TextScanner can avoid the attention drift phenomenon observed in RNN-attention based methods. Meanwhile, in the geometry branch different characters, even contiguous with each other or with the same class label, are strictly assigned into different channels, so they can be easily extracted.</p><p>As FAN <ref type="bibr" target="#b2">(Cheng et al. 2017</ref>) and CA-FCN <ref type="bibr" target="#b19">(Liao et al. 2019</ref>), TextScanner also requires character level annotations for training, since the geometry branch takes character centers as supervision signals. However, there are actually plenty of real image examples without character level annotations, which could be very beneficial for training text recognizers. To make use of such real data, we devise a mutual supervision mechanism. For image examples without character level annotations, the predictions of the class branch and the geometry branch can supervise each other with only sequence level annotations. In consequence, TextScanner is able to fully utilize all kinds of available training data, including both synthetic and real text images.</p><p>We conduct experiments on public benchmarks for scene text recognition to validate the proposed TextScanner. It achieves higher or highly competitive accuracy on regular text datasets and obtains significantly enhanced performance on irregular text datasets. The recognition accuracy increases 3.3% on ICDAR 2015 and 4% on CUTE80, compared with the previous art. We also evaluate TextScanner on a Chinese recognition task. The quantitative results further prove the superiority of the proposed algorithm. The contributions in this work are summarized as follows:</p><p>• We propose a novel text recognition framework, which predicts the class and geometry information (position and order) of characters with two separate branches. • We devise a mutual-supervision mechanism, which endows the framework with the ability to make use of both synthetic and real data for training. • The experiments demonstrate that the proposed TextScanner achieves state-of-the-art or highly competitive perfor-mance on public benchmarks.</p><p>• Furthermore, TextScanner exhibits stronger adaptability to longer and more complex text (such as Chinese scripts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Text recognition has been a long-standing research topic in computer vision. Research efforts on text recognition can date back to the early age of AI <ref type="bibr" target="#b9">(Herbert 1982;</ref><ref type="bibr" target="#b16">LeCun et al. 1998)</ref>. With the rise of deep learning, scene text recognition has entered a new era. For recent progress in this field, please refer to the survey paper <ref type="bibr" target="#b20">(Long, He, and Ya 2018;</ref><ref type="bibr" target="#b34">Zhu, Yao, and Bai 2016)</ref>. In this section, we will concentrate on the most relevant works. Inspired by speech recognition and natural language processing, CTC-based <ref type="bibr" target="#b6">Graves et al. 2006</ref>) soft-alignment methods and attention-based <ref type="bibr" target="#b5">(Ghosh, Valveny, and Bagdanov 2017;</ref><ref type="bibr" target="#b28">Shi et al. 2018</ref>) methods are proposed to handle text recognition as a sequence recognition task. Among them, attention-based methods are prevalent recently, and achieve state-of-the-art performance on public benchmarks. On the other hand, there are still challenging problems in the field of scene text recognition. Text images in natural scenes suffer from the complex background, arbitrary text shape and severe image distortion. Most of current text recognition algorithms are not robust enough to solve hard cases such as text instances which are oriented, curved or extremely blurred.</p><p>As stated in Sec. 1, the problem of attention drift is exacerbated by recursive modeling. This problem is also observed in speech recognition <ref type="bibr" target="#b15">(Kim, Hori, and Watanabe 2017)</ref>, which is where the idea of attention decoder originated from. There are existing works aimed at alleviating this problem. <ref type="bibr" target="#b2">(Cheng et al. 2017)</ref> proposed to correct attention positions using characters' class and localization label. New loss function motivated by the formulation of edit distance is presented  to improve the hard alignment of attention decoder. However, these methods do not change the nature of error accumulating which lies in the coupled modeling of attention generation and character classification. Different from these methods, the proposed algorithm uses two separated branches to classify characters and predict the positions and order of characters. The potential mutual interference between alignment and decoding is eliminated and the problem of attention drift can be avoided.</p><p>Recently, segmentation-based methods are also introduced to the field of text recognition <ref type="bibr" target="#b19">Liao et al. 2019</ref>). Segmentation-based methods are usually more flexible than attention decoders in the recognition of irregular text such as oriented or curved text instances. However, the post-processing of these methods may fail to separate closely arranged characters as shown in <ref type="figure">Fig. 1</ref>. As the characters are recognized by finding and voting inside the connected components in the segmentation map, this restriction limits their recognition accuracy. Besides, the application of segmentation-based methods remains limited because these methods can not use real image examples with only sequence-level annotation. With our proposed method, the characters are naturally separated and ordered by dispatching character localization to different channels. The mutual-supervision mechanism further enables the two branches to utilize sequence level annotations to supervise and enhance each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology Overview</head><p>The overall structure of the proposed method is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The decoder of the network is composed of two branches: class branch and geometry branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Branch</head><p>The class branch of TextScanner produces character segmentation G ∈ R h×w×c , which is of resolution h × w and c denotes the number of classes (all character classes plus background). G is directly generated from visual features extracted by a CNN backbone. The prediction module is composed of two stacked convolutional layers with kernel size 3×3 and 1×1. A Softmax normalization is applied over the class dimension to generate the character segmentation maps.</p><p>Geometry Branch Firstly, a character localization map Q ∈ R h×w is produced from the same visual features as the class branch, with a Sigmoid activation function. Concurrently, a top-down pyramid structure, in which features in the upsampling path is added by the features with the same resolution from the downsampling path is taken to generate order segmentation maps. Especially, the feature maps in the top layer of the downsampling path are encoded by an RNN (GRU <ref type="bibr" target="#b4">(Chung et al. 2015)</ref>, to be exact) module for context modeling. Following the upsampling path, two convolutional layers are employed to generate the order segmentation maps S ∈ R h×w×N , where N is the pre-defined max sequence length. The order segmentation maps are also normalized by a Softmax operation. Then an order map H k ∈ R h×w , which indicates the position of the k th character in the sequence, can be computed from the k th (k ∈ [1, N ]) channel of order segmentation and the character localization map Q by an element-wise multiplication:H k = Q * S k . The detail of the geometry branch is depicted in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>Word Formation With the produced character clarification maps G and order maps H, we now present the de- coding procedure which formats characters in order. As the order of character locations is encoded into the order maps H, the classification scores can be computed from the order maps and character clarification maps as:</p><formula xml:id="formula_0">p k = (x,y)∈Ω G(x, y) * H k (x, y)<label>(1)</label></formula><p>where p k ∈ R c is the vector of scores representing the class probabilities of the k th character. Ω is all valid spatial locations in the h × w space. Similar to attention decoders, once the maximal probability of a character is below a predefined threshold T score or k reached the maximal value N , the decoding process is terminated. This decoding procedure is totally differentiable. Therefore, it can be trained within the network using sequence level as well as character level annotations. The optimization process utilizing sequence level annotations is introduced in detail in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-Training with Character-Level Annotations</head><p>When pre-training on synthetic data, TextScanner can be optimized with character-level annotations.</p><p>Label Generation Due to the rectangles are inaccurate in curved or dense text, we keep the definition of character regions polygons P = {(x i , y i )} Np i=1 , where N p is the number of points in polygon.</p><p>To refrain the overlap caused by edges of adjacent characters, the polygon character bounding box P is shrunk to P with the Vatti clipping algorithm <ref type="bibr" target="#b30">(Vati 1992)</ref>. Inside P area, the class of the corresponding character is rendered as ground truth of the character segmentation. Pixels outside P hardly contribute to the decoding of sequences and are ignored in the optimization of text segmentation.</p><p>To generate the ground truth of order maps with characterlevel annotations, the center of Gaussian maps is firstly detected by computing the central points of characters bounding boxes. As <ref type="figure" target="#fig_2">Fig. 4</ref> shown, 2D Gaussian mapsŶ k ∈ R h×w with σ and expectation at central points are generated for each character. Then the order of characters is rendered for pixels insideŶ k area:</p><formula xml:id="formula_1">Z k (i, j) = k, ifŶ k (i,j) maxŶ k (h,w) &gt; ζ order , (i, j) ∈ (h, w) 0, otherwise<label>(2)</label></formula><p>whereẐ k ∈ R h×w is the generated order map ground truth for each character, ζ order is the order threshold, which set to 0.5 in our experiment. FinallyẐ k is normalized to [0, 1], to produce the ground truth Z k of H k . Taking the same operation asẐ k to Z k , 0-1 normalized Gaussian heatmap Y k can be acquired fromŶ k . According to all Y k , the ground truth of localization map Q can be generated by straightforwardly combining heat maps:</p><formula xml:id="formula_2">Y * = N max k=1 Y k<label>(3)</label></formula><p>Loss Function The overall loss function is a weighted sum of losses for the three mentioned tasks as</p><formula xml:id="formula_3">L = λ l * L l + λ o * L o + λ m * L m + L s<label>(4)</label></formula><p>where L c , L o , L s , L m are the losses for localization map, order segmentation, text segmentation and mutual supervision loss respectively. The detail of mutual supervision loss is illustrated in the next section. In our experiments λ l and λ o are set to 10 for scaling the numerical values. λ m is set to 0 during pretraining otherwise to 1. The localization map loss is computed as an average smooth l1 loss. The losses for order segmentation(L o ) and character segmentation(L s ) are computed as cross entropy between the predicted scores and corresponding ground truth. The background class in both segmentation task is ignored in cross entropy computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual-Supervision Mechanism</head><p>For previous semantic segmentation based methods, it's critical to obtain accurate locations of all the characters during model training, since the character classification is at pixel-level. This is problematic when character level annotations are not available. To reduce the reliance on characterlevel annotations, we devise a mutual-supervision mechanism based on the dual-branch structure of TextScanner. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, text sequences can be generated by combining character segmentation maps G and order maps H. Given a sequence label and one of the two outputs, supervision signals can be generated for the other one.</p><p>G and H are further transformed by taking the index of max value across c and o channels intoĜ ∈ R h×w and H ∈ R h×w . Given the text sequence label T , the mutualsupervision process is carried out from the first character in T to the last. For the k th character in T , its order is k and the class is T (k). Ψ k h and Ψ k g are coordinates of the pixels that corresponding to the k th character in H and G:</p><formula xml:id="formula_4">Ψ k h = {(i, j)|Ĥ(i, j) = k, Q(i, j) &gt; } Ψ k g = {(i, j)|Ĝ(i, j) = T (k), Q(i, j) &gt; }<label>(5)</label></formula><p>Apart from the constraint of class or order, we add the constraint of Q(i, j) &gt; to keep the attended regions in the center of characters. is set to 0.2 in our experiment.</p><p>For the mutual supervision purpose, we use Ψ k g , which is derived from G,in the supervision of H and Ψ k h , which is <ref type="figure">Figure 5</ref>: (a) Visualization of step 1 of mutual-supervision mechanism. The selected regions inĜ andĤ are refined using Q to get Ψ 1 g and Ψ 1 h , which are then mapped intoĤ andĜ separately. (b) Two regions inĜ are selected for 'N' in "LONDON".</p><p>derived from H, in the supervision of G:</p><formula xml:id="formula_5">L k g = 1 |Ψ k h | (i,j)∈Ψ k h L CE (G(i, j), onehot(T (k))) L k h = 1 |Ψ k g | (i,j)∈Ψ k g L CE (H(i, j), onehot(k))<label>(6)</label></formula><p>L CE is the cross entropy loss and onehot(·) is the one hot encoding function. The step 1 of cross supervision process is shown in <ref type="figure">Fig 5(a)</ref>. The process is carried on till the last character of T . Note that multiple regions ofĜ are chosen in one selection for characters occur more than once in T and can't be used in supervision of H, as shown in <ref type="figure">Fig 5(b)</ref>. So we remove these samples from the cross supervision process. The confidences for G and H are denoted as Φ g and Φ h :</p><formula xml:id="formula_6">n k g = 1, if Ψ k g = Ø 0, otherwise , Φ g = |T | k=1 n k g |T | n k h = 1, if Ψ k h = Ø 0, otherwise , Φ h = |T | k=1 n k h |T |<label>(7)</label></formula><p>For the k th character in T , if the number of pixels in its corresponding region in G or H is larger than 0, the character is considered exists in G or H. |T | is the length of T . The more the characters in T exists, the higher Φ is. T = 1 when all the characters exists in the prediction map. Φ g and Φ h are included in the loss computation of the text sequence:</p><formula xml:id="formula_7">L m = L h + λ * L g L g = (Φ h ) γ |T | |T | k=1 L k g , L h = (Φ g ) γ |T | |T | k=1 L k h<label>(8)</label></formula><p>In our experiment, λ is set to 0.2, γ is set to 2 to further reduce the impact of inaccurate predictions. After being pre-trained on synthetic datasets with character level annotations, our model can be further fine-tuned on real-world datasets or synthetic datasets with only sequence level annotation by adopting this cross supervision mechanism, which is impossible in previous segmentation-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other Details</head><p>Our model is built on top of the backbone from CA-FCN, in which the character attentions are removed and VGG blocks are replaced with a ResNet-50 <ref type="bibr" target="#b8">(He et al. 2016</ref>) base model. The score threshold ζ score is set as 0.3 empirically, and the max size N is set as 32 in our implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on standard benchmarks to evaluate TextScanner and compare it with other competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>ICDAR 2013(IC13) <ref type="bibr" target="#b13">(Karatzas et al. 2013</ref>) recognition task provides 288 scene images with annotations, from which 1015 word images are cropped. Besides, the dataset provides character-level bounding boxes. ICDAR 2015(IC15) <ref type="bibr" target="#b14">(Karatzas et al. 2015)</ref> consists of 1000 images with word-level quadrangles annotation for training and 500 for testing. IIIT 5K-Words(IIIT) <ref type="bibr" target="#b21">(Mishra, Alahari, and Jawahar 2012)</ref> dataset contains 5K word images for scene text recognition. Street View Text(SVT) (Wang, Babenko, and Belongie 2011) dataset has 350 images and only word-level annotations are provided. SVT-Perspective(SVTP) <ref type="bibr" target="#b23">(Phan et al. 2013</ref>) dataset contains 639 cropped images for testing. Many images in the dataset are heavily distorted. CUTE80(CT) <ref type="bibr" target="#b24">(Risnumawan et al. 2014)</ref> dataset is taken in natural scene. It consists of 80 high-resolution images with no lexicon. ICDAR 2017 MLT(MLT-2017) <ref type="bibr" target="#b22">(Nayef et al. 2017</ref>) is comprised of 9000 training images and 9000 test images. We acquire cropped word instances for recognition by using the quadrilateral word-level annotation. SynthText (Gupta, Vedaldi, and Zisserman 2016) consists of 80k images for training. We cropped about 7 million instances with character and word-level bounding-boxes annotations from the training set. Synth90k <ref type="bibr" target="#b11">(Jaderberg et al. 2014b</ref>) contains 8 millions word images from 90k English words with word-level annotation. <ref type="table">IIIT  SVT  IC13 IC15 SVTP CT  50  1k  0  50  0  0  0  0  0  Almazán et</ref>   <ref type="table">Table 1</ref>: Performance comparison of our methods and others. "ST", "90k", and "real" are the training data of SynthText, 90k, and real data, respectively. The methods marked with star mix SynthText and 90k dataset for training and methods marked with " †" use the training set of real data. "0", "50" and "1k" indicate the size of the lexicons, "0" means no lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Data</head><p>In addition, approximately 16k real images are collected from the training sets released by the mentioned datasets for fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Strategy</head><p>The training procedure of TextScanner includes two stages: we firstly use the synthetic dataset SynthText with characterlevel annotations to pre-train the model for 5 epochs, then the real image examples with sequence-level annotations are mixed into the training set for fine-tuning the network for 1 more epoch. Our methods in Tab.1 use different data for training, "TextScanner-pre" and "TextScanner-mutual" are the models trained on SynthText with and without mutual-supervision, respectively; "TextScanner+90k" and "TextScanner+real" are fine-tuned with the 90k dataset and the training set of real data.</p><p>We use Adam optimizer for training of all experiments. The learning rate is initialized as 10 −3 and the decays to 10 −4 and 10 −5 . During training and inference, the input images are resized to 64 × 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recognition Performance Evaluation</head><p>The recognition accuracies of different methods on standard benchmarks, including regular (IIIT, SVT, IC13) and irregular (IC15, SVTP, CT) text, are shown in Tab. 1.</p><p>The natural modeling of TextScanner makes it more robust to hard cases where the text is curved or oriented. The three variants of TextScanner in Tab. 1 consistently outperform previous methods in comparison with the same training <ref type="figure">Figure 6</ref>: Intermediate results of TextScanner. Obviously TextScanner can track the arrangement of characters well, for long or oriented text. The geometry branch can separate adjacent characters even their segmentation masks connect to each other (note the two adjacent 'O' the middle row).</p><p>data. Especially on curved text, "TextScanner+90k", which is trained with synthetic data, achieves an improvement of 3.3% on IC15, 4.1% on SVTP, and 4.0% on CT. The advantages of TextScanner stem from aspects: (1) It is segmentation based, which makes the prediction more relevant Methods Acc(%) NED CRNN  59.2 0.68 ASTER <ref type="bibr" target="#b28">(Shi et al. 2018)</ref> 57.4 0.69 TextScanner 64.1 0.75 <ref type="table">Table 2</ref>: Results comparison on MLT-17. "NED" is short for normalized edit distance.</p><p>to visual features and free from error accumulation brought by the recursive modeling.</p><p>(2) It scans characters one by one and ensures they are read in the right order and separated properly. Some recognition examples are visualized in <ref type="figure">Fig. 6</ref>. "TextScanner+real" brings an even more significant boost in performance and demonstrates the capability of TextScanner to utilize real data for training, which also verifies the effectiveness of the proposed mutual-supervision mechanism. Moreover, although the mutual-supervision mechanism is designed for making use of real-world data with sequencelevel annotations, fine-tuning with synthetic SynthText and 90k datasets consistently bring performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chinese Recognition Evaluation</head><p>Preceding experiments are based on datasets with English text, whose alphabet is relatively small. To further validate the capability of TextScanner, we conduct experiments on a Chinese recognition task, which is more challenging due to its larger alphabet and more complex visual variations of characters. We compare the performance of TextScanner in Chinese recognition with two representative methods, CRNN  and ASTER <ref type="bibr" target="#b28">(Shi et al. 2018)</ref>. We use their open-source implementations for comparison. The models are trained with the same training data, which is generated by the synthetic engine released with SynthText and evaluated on cropped text images from the validation set of MLT-17.</p><p>The quantitative results are shown in Tab.2. In addition to accuracy, we also evaluate the Normalized Edit Distance of the methods following the ICDAR-2017 competition RCTW-17 :</p><formula xml:id="formula_8">N orm = 1 − 1 N t Nt i ED(s i ,ŝ i ) max(|s i |, |ŝ i |)<label>(9)</label></formula><p>where N t is the number of text instances. As shown in Tab. 2, TextScanner outperforms attention decoder with a large margin, 6.7% in recognition accuracy and 0.06 in normalized edit distance. The results demonstrate that TextScanner can handle such challenging recognition tasks better.</p><p>The main reason for this improvement is that TextScanner separate geometry branch from the class branch. For the Chinese recognition task, models are more prone to classification errors due to the much more complex structures and appearances of Chinese characters.</p><p>In contrast to attention decoders, the dual-branch architecture of TextScanner is more robust to the problem of error accumulating towards difficulty in classification. The class <ref type="figure">Figure 7</ref>: Probability density of deviation of character localization. "Distance" denotes the distance from predicted character center or attention position to the ground truth character center, which is normalized by the image width.</p><p>branch and the geometry branch are optimized individually in pre-training, therefore the extraction of character orders is not affected by the probable errors in classification. In the fine-tuning stage, the accurate order extraction can improve the class branch in return through the mutual-supervision mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Character Localization Accuracy of TextScanner</head><p>For both attention decoder and TextScanner, accurate prediction of attention position or character localization is fundamental for recognition. As they both produce the center of characters, we compare their performance in character localization on the IC13 dataset. As IC13 provides annotations of character positions in the image, the two methods are evaluated by measuring the normalized distance D between the produced character center and ground truth center position in the width axis.</p><p>The probability density of center distance in IC13 is illustrated in <ref type="figure">Fig.7</ref>. The probability of TextScanner to have accurate localization(D &lt; 0.1) is obviously greater than attention decoder. This proves that TextScanner gives more accurate character localization results.  <ref type="table">Table 3</ref>: Recognition performance with different settings. "Geo" denotes the geometry branch, "Ord" denotes word formation using the order maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Study</head><p>The superiority of TextScanner has been validated via the experiments in Chinese recognition task, here we specifically explore the effect of the geometry branch and the word formation process based on order maps. In Tab.3, the geometry branch and word formation are ablated respectively. Note that the word formation with order maps relies on the output of the geometry branch, therefore we use the postprocessing procedure of CA-FCN as a replacement.</p><p>The experimental results clearly show the improvements brought by the geometry branch and its decoding process (the second and the third row in Tab.3). As the order maps ensure the characters are scanned in correct order, the recognition performance is significantly elevated, especially on irregular datasets: 7.4% on IC15 and 10.2% on SVTP. Besides, even with the regular post-processing, the geometry branch still achieves better performance, proving it can facilitate the optimization of the class branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have presented TextScanner, an effective segmentation-based dual-branch framework for scene text recognition. TextScanner can overcome the problems and defects of previous methods, and work well under various challenging scenarios. A novel mutual-supervision mechanism, which makes it possible to take full advantage of both real and synthetic data, is also proposed. Besides, TextScanner shows stronger adaptability in handling difficult text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Schematic illustration of the proposed text recognition framework. Different colors in character segmentation map represent the values in different channels. The values in the localization map and order maps are visualized as heat maps. The predictions of the two branches are fused to extract characters (position, order, and class) and form the final output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the geometry branch. The feature maps are up-sampled and down-sampled by a pyramid architecture with skip connections. Features at the top layer is processed by an RNN module for context modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Ground truth generation for pre-training. Pixels outside shrunk boxes P are represented as gray in character segmentation label, which are ignored in loss computation.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partly supported by National Natural Science Foundation of China (61733007).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Edit probability for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="1508" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5086" to="5094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AON: towards arbitrarily-oriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="5571" to="5579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2067" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual attention models for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR ICDAR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page" from="943" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine learning</title>
		<meeting>the 23rd International Conference on Machine learning<address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IMLS</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The history of ocr, optical character recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VT: Recognition Technologies Users Association</title>
		<imprint>
			<publisher>Manchester Center</publisher>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep structured output learning for unconstrained text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5903</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Heras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Icdar 2015 competition on robust reading</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint ctc-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8610" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scene text recognition from two-dimensional perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04256</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="83" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Scene text detection and recognition: The deep learning era</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC-British Machine Vision Conference. BMVA</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification -rrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chazalon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Khlif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Luqman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ogier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR ICDAR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page" from="1454" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Icdar2017 competition on reading chinese text in the wild (rctw-17)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1429" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aster: An and attentional scene and text recognizer and with flexible and rectification</title>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A generic solution to polygon clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Vati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="56" to="64" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, ICCV &apos;11</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="3304" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to read irregular text with attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3280" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scene text detection and recognition: Recent advances and future trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="36" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
