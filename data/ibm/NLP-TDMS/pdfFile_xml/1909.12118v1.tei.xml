<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JOURNAL OF IEEE TRANSACTIONS ON MULTIMEDIA 1 WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliang</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jun</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansheng</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior, IEEE</roleName><forename type="first">Guodong</forename><surname>Guo</surname></persName>
						</author>
						<title level="a" type="main">JOURNAL OF IEEE TRANSACTIONS ON MULTIMEDIA 1 WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Pedestrian detection</term>
					<term>dataset</term>
					<term>rich diversity</term>
					<term>high density</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pedestrian detection has achieved significant progress with the availability of existing benchmark datasets. However, there is a gap in the diversity and density between real world requirements and current pedestrian detection benchmarks: 1) most of existing datasets are taken from a vehicle driving through the regular traffic scenario, usually leading to insufficient diversity; 2) crowd scenarios with highly occluded pedestrians are still under represented, resulting in low density. To narrow this gap and facilitate future pedestrian detection research, we introduce a large and diverse dataset named WiderPerson for dense pedestrian detection in the wild. This dataset involves five types of annotations in a wide range of scenarios, no longer limited to the traffic scenario. There are a total of 13, 382 images with 399, 786 annotations, i.e., 29.87 annotations per image, which means this dataset contains dense pedestrians with various kinds of occlusions. Hence, pedestrians in the proposed dataset are extremely challenging due to large variations in the scenario and occlusion, which is suitable to evaluate pedestrian detectors in the wild. We introduce an improved Faster R-CNN and the vanilla RetinaNet to serve as baselines for the new pedestrian detection benchmark. Several experiments are conducted on previous datasets including Caltech-USA and CityPersons to analyze the generalization capabilities of the proposed dataset and we achieve state-of-the-art performances on these previous datasets without bells and whistles. Finally, we analyze common failure cases and find the classification ability of pedestrian detector needs to be improved to reduce false alarm and miss detection rates. The proposed dataset is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P EDESTRIAN detection is a long-standing problem in computer vision and pattern recognition with extensive applications including security and surveillance, mobile robotics, autonomous driving, and crowd sourcing, to name a few. The accuracy of pedestrian detection systems has a direct impact on these tasks, hence the success of pedestrian <ref type="figure">Fig. 1</ref>. The diversity and density of the newly introduced WiderPerson dataset. It can bridge the gap between real world requirements and pedestrian detection benchmarks. For visualization, we use bounding boxes of different colors for pedestrians (Cyan), riders (Red), partially-visible persons (Green), crowd (Yellow) and ignore regions (Blue). detection is of crucial importance. Given an arbitrary image, the goal of pedestrian detection is to determine whether or not there are any pedestrians in the image, and if present, return the image location and extent of each pedestrian. While this appears as an effortless task for human, it is a very difficult task for computers. The challenges associated with pedestrian detection can be attributed to variations in pose, scale and occlusion, which need to be addressed while building pedestrian detection algorithms.</p><p>With the remarkable progress over the past few decades, pedestrian detection has been successfully applied in some practical application systems under restricted scenarios. The success of these systems can be attributed to two key steps: <ref type="bibr" target="#b0">(1)</ref> advancements in the field of deep Convolutional Neural Network (CNN) which has had a direct impact on many computer vision tasks including pedestrian detection; <ref type="bibr">(</ref>  collection efforts led by different researchers in the community. Furthermore, improvements in detection algorithms have almost always been followed by the publication of more challenging datasets and vice versa. Such synchronous advancement in both steps has led to an even more rapid progress in the field. In terms of pedestrian detection, publicly available benchmark datasets such as Caltech-USA <ref type="bibr" target="#b0">[1]</ref>, KITTI <ref type="bibr" target="#b1">[2]</ref> and CityPersons <ref type="bibr" target="#b2">[3]</ref> have contributed to spurring interest and progress in pedestrian detection research. Coupled with the development and blooming of deep learning, modern pedestrian detectors <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b7">[8]</ref> have achieved remarkable performance. Although performance has been significantly improved, it's still difficult to assess for real world, since compared with crowd-counting datasets <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> designed in the crowded condition, there is a gap in the diversity and density between current existing pedestrian detection benchmarks and real world requirements. On the one hand, most of existing datasets are collected via a vehicle-mounted camera through the regular traffic scenario. This fixed scenario significantly reduces the richness of the foreground and background, leading to low diversity. Specifically, only pedestrians and backgrounds on the road are taken into consideration while the other scenarios are severely under represented. Thus, diversity in pedestrian and background appearances is limited. On the other hand, crowd scenarios with highly occluded pedestrians are still under-represented. As shown in <ref type="table" target="#tab_1">Table I</ref>, the Caltech-USA and KITTI datasets have less than one person per image, while the CityPersons dataset has âˆ¼ 7 persons per image. Even worse, protocols of these datasets allow annotators to ignore the regions with a large number of persons, since exhaustively annotating crowd regions is incredibly difficult and time consuming, resulting in low density and insufficient occlusions cases. To sum up, current pedestrian detection datasets typically contain a few thousand pedestrians with limited variations in diversity and density. These limitations have partially contributed to the failure of some algorithms in coping with heavy occlusion and atypical scenario. Therefore, more challenging datasets similar to the real world are needed to trigger progress and inspire novel ideas.</p><p>To move forward the field of pedestrian detection, we introduce a diverse and dense pedestrian detection dataset called WiderPerson. It consists of 13, 382 images with 399, 786 annotations, i.e., 29.87 annotations per image, varying largely in scenario and occlusion, as shown in <ref type="figure">Fig. 1</ref>. Besides, the annotations have five fine-grained labels, i.e., pedestrians, riders, partially-visible persons, crowd, and ignore regions. These high quality annotations provide a rich diverse dataset and enable new experiments both for training better models, and as new test benchmark. We split the proposed WiderPerson dataset into three subsets (training, validation, and testing sets). Annotations of training and validation will be public, and an online benchmark will be set-up. We show an example of using the proposed WiderPerson dataset through proposing an improved Faster R-CNN <ref type="bibr" target="#b10">[11]</ref>, which consists of finer feature map, ignore region and tiny pedestrian handling, Region of Interest (RoI) feature enhancing and dynamic sample strategy to deal with large density and diversity variations. The crossdataset generalization results of the proposed WiderPerson dataset show that it is an effective training source for pedestrian detection and we achieve state-of-the-art performance on existing Caltech-USA and CityPersons datasets.</p><p>For clarity, the main contributions of this work can be summarized as three-fold:</p><p>â€¢ We propose the WiderPerson dataset, which provides a large number of highly diverse and dense bounding box annotations for pedestrian detection. â€¢ We build an improved Faster R-CNN to show an example of using WiderPerson, which consists of some improvements to deal with large density and diversity variations. â€¢ We prove the generalization capabilities of detectors trained with the new dataset and achieve state-of-the-art performance on Caltech-USA and CityPersons datasets. The rest of the paper is organized as follows. Section II reviews the related work. Description of the WiderPerson dataset is presented in Section III. Section IV introduces our proposed baseline detector and Section V shows the experimental results. Section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Dataset</head><p>In the last decade, several datasets have been created for pedestrian detection training and evaluation. The GM-ATCI dataset <ref type="bibr" target="#b11">[12]</ref> is collected using a vehicle-mounted standard automotive rear-view display camera for evaluating rear-view pedestrian detection. The INRIA dataset <ref type="bibr" target="#b12">[13]</ref> is one of the most popular static pedestrian detection datasets. The USC dataset <ref type="bibr" target="#b13">[14]</ref> consists of a number of fairly small pedestrian datasets taken largely from surveillance video. The ETH dataset <ref type="bibr" target="#b14">[15]</ref> is captured from a stereo rig mounted on a stroller in the urban. The CVC-ADAS dataset <ref type="bibr" target="#b15">[16]</ref> contains pedestrian videos acquired on-board, virtual-world pedestrians (with part annotations) and occluded pedestrians. The NICTA dataset <ref type="bibr" target="#b16">[17]</ref> is a large scale urban dataset collected in multiple cities and countries, it has no motion and tracking information but significant number of unique pedestrians. The Daimler dataset <ref type="bibr" target="#b17">[18]</ref> is captured in an urban setting and has tracking information and a large number of labelled bounding boxes. The TUD-Brussels dataset <ref type="bibr" target="#b18">[19]</ref> contains image pairs recorded in a crowded urban setting with an onboard camera. These datasets represent early efforts to collect pedestrian datasets.</p><p>Althought these early datasets have contributed to spurring interest and progress of pedestrian detection, however, as algorithm performance improves, they are replaced by the larger and richer datasets. The Tsinghua-Daimler Cyclist (TDC) dataset <ref type="bibr" target="#b19">[20]</ref> focuses on cyclists recorded from a vehiclemounted stereo vision camera, containing a large number of cyclists varying widely in appearance, pose, scale, occlusion and viewpoint. In <ref type="bibr" target="#b20">[21]</ref>, a multi-spectral dataset for pedestrian detection is introduced, combining RGB and infrared modalities. The Caltech-USA <ref type="bibr" target="#b0">[1]</ref> dataset consists of approximately 10 hours of 640 Ã— 480 30Hz video taken from a vehicle driving through regular traffic in an urban environment, which has been extended by <ref type="bibr" target="#b21">[22]</ref> with corrected annotations. The KITTI <ref type="bibr" target="#b1">[2]</ref> dataset focuses autonomous driving and is collected via a standard station wagon with two high-resolution color and grayscale video cameras, around the mid-size city of Karlsruhe, in rural areas and on highways, up to 15 cars and 30 pedestrians are visible per image. The CityPersons <ref type="bibr" target="#b2">[3]</ref> dataset is recorded by a car traversing 27 different cities and provides high quality bounding boxes with larger portions of occluded persons. The EuroCity Persons dataset <ref type="bibr" target="#b22">[23]</ref> provides a large number of highly diverse, accurate and detailed annotations of pedestrians, cyclists and other riders in 31 cities of 12 European countries.</p><p>Despite the prevalence of these datasets, they all suffer a problem of from low diversity. Most of existing datasets are collected via a vehicle-mounted camera through the regular traffic scenario. The diversity in pedestrian and background appearances is limited. Another weakness of both datasets is that the crowd scenarios are significantly under represented, resulting in insufficient occlusions cases. The paper aims at solving these two problems via proposing a diverse and dense pedestrian detection dataset, which can narrow the gap in the diversity and density between real world requirements and current pedestrian detection benchmarks to better evaluate detectors in the wild. Besides, the proposed dataset is also very useful for training a re-detector for dealing with tracking loss for pedestrian tracking <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Method</head><p>Generic Object Detection. Early generic object detection methods rely on the sliding window paradigm based on the hand-crafted features and classifiers to find the objects of interest. In recent years, with the advent of deep Convolutional Neural Network (CNN), a new generation of more effective object detection methods based on CNN significantly improve the state-of-the-art performances, which can be roughly divided into two categories, i.e., the one-stage approach and the two-stage approach. The one-stage approach <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> directly predicts object class label and regresses object bounding box based on the pre-tiled anchor boxes using deep CNN. The main advantage of the one-stage approach is its high computational efficiency. In contrast to the one-stage approach, the two-stage approach <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b27">[28]</ref> always achieves top accuracy on several benchmarks, which first generates a pool of object proposals by a separated proposal generator, and then predicts the class label and accurate location and size of each proposal.</p><p>Pedestrian Detection. Even as one of the long-standing problems in computer vision field with an extensive literature, pedestrian detection still receives considerable interests with a wide range of applications. A common paradigm <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref> to deal with this problem is to train a pedestrian detector that exhaustively operates on the sub-images across all locations and scales. Dalal and Triggs <ref type="bibr" target="#b12">[13]</ref> design the Histograms of Oriented Gradient (HOG) descriptors and Support Vector Machine (SVM) classifier for human detection. DollÃ¡r et al. <ref type="bibr" target="#b31">[32]</ref> demonstrate that using features from multiple channels can greatly improve the performance. Zhang et al. <ref type="bibr" target="#b32">[33]</ref> provide a systematic analysis for the filtered channel features, and find that with the proper filter bank, filtered channel features can reach top detection quality. Paisitkriangkrai et al. <ref type="bibr" target="#b33">[34]</ref> design a new feature built on low-level features and spatial pooling, and directly optimize the partial area under the Receiver Operating Characteristic (ROC) curve for better performance.</p><p>Recently, CNN-based detectors <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b37">[38]</ref> have become a predominating trend in the field of pedestrian detection. Sermanet et al. <ref type="bibr" target="#b34">[35]</ref> present an unsupervised method using the convolutional sparse coding to pre-train CNN for pedestrian detection. In <ref type="bibr" target="#b38">[39]</ref>, a complexity-aware cascaded detector is proposed for an optimal trade-off between accuracy and speed. Angelova et al. <ref type="bibr" target="#b39">[40]</ref> combine the ideas of fast cascade and a deep network to detect pedestrian. Yang et al. <ref type="bibr" target="#b40">[41]</ref> use scaledependent pooling and layer-wise cascaded rejection classifiers to detect objects efficiently. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> present an effective pipeline for pedestrian detection via extracting self-learned features from the Region Proposal Network (RPN) <ref type="bibr" target="#b10">[11]</ref> followed by a boosted decision forest. Cai et al. <ref type="bibr" target="#b42">[43]</ref> propose an architecture which uses different levels of features to detect persons at various scales. Mao et al. <ref type="bibr" target="#b43">[44]</ref> present a multitask network architecture to jointly learn pedestrian detection with the given extra features. Li et al. <ref type="bibr" target="#b6">[7]</ref> use multiple builtin sub-networks to adaptively detect pedestrians across scales. Brazil et al. <ref type="bibr" target="#b37">[38]</ref> exploit weakly annotated bounding boxes via a segmentation infusion network to achieve considerable performance gains.</p><p>Occlusion is one of the most significant challenges in compute vision, especially for pedestrian detection, which increases the difficulty in pedestrian localization. Several methods <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b48">[49]</ref> use part-based model to describe the pedestrian in occlusion handling, which learn a series of part detectors and design some mechanisms to fuse the part detection results to localize partially occluded pedestrians. Besides the partbased model, Leibe et al. <ref type="bibr" target="#b49">[50]</ref> propose an implicit shape model to generate a set of pedestrian hypotheses that are further refined to obtain the visible regions. Wang et al. <ref type="bibr" target="#b50">[51]</ref> divide the template of pedestrian into a set of blocks and conduct occlusion reasoning by estimating the visibility status of each block. Ouyang et al. <ref type="bibr" target="#b51">[52]</ref> exploit multi-pedestrian detectors to aid single-pedestrian detectors to handle partial occlusions, especially when the pedestrians gather together and occlude each other in real-world scenarios. In <ref type="bibr" target="#b52">[53]</ref>, a set of occlusion patterns of pedestrians are discovered to learn a mixture of occlusion-specific detectors. Zhou et al. <ref type="bibr" target="#b53">[54]</ref> propose to jointly learn part detectors to exploit part correlations and reduce the computational cost. Wang et al. <ref type="bibr" target="#b4">[5]</ref> introduce a new bounding box regression loss to detect pedestrians in crowd scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED WIDERPERSON DATASET</head><p>In this section, we present our WiderPerson dataset from aspects of collection process, annotation tool, annotation method, various statistical information and benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Collection</head><p>For the diversity of our dataset, we crawl images from multiple image search engines ranging from Google, Bing, and Baidu. Combined with specially-designed keywords, one of the prominent advantages of using different image search engines together is the collected images possess diverse features in cities, events, and scenarios. We design more than 50 keywords (e.g., pedestrian, cyclist, walking, running, marathon, square dance and group photo) during the crawling process and obtain âˆ¼50, 000 images as our candidate images. To prevent the duplication of images, we leverage a simple but powerful mechanism, the pHash <ref type="bibr" target="#b54">[55]</ref>, along with the union find, for the removal of the repetitions. Moreover, images with sparse distribution of people are filtered out to keep the difficulties of our dataset. Finally, we have 13, 382 images remained, and they are randomly split into training, validation and testing subsets with 8, 000, 1, 000 and 4, 382 images, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Annotation Tool</head><p>We design a new annotation tool whose Graphical User Interface (GUI) is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. It is written using JavaScript and built with a very responsive design. The list of images that need to be marked is displayed on the upper right side. For the selected image to be annotated, the tool displays five kinds of annotation examples on the left side to help annotators to mark. These five different types of annotations are labelled with different colors to better distinguish. All the labelled annotations are shown on the lower right side and annotators can select any labelled annotation to display and correct. To complete the annotation, the user shall next adjust the position of the bounding boxes. For this purpose, the keyboard arrows shall be used. More precisely, the left, right, up and down keys should be used in order to shift the annotations on the image. To help annotators, there are two buttons (display and hide labels) at the top used to make labelled annotations optionally visible while annotating. In addition, a zooming feature at the top can be used to zooming-in the corresponding image and annotations. This is implemented in order to make it easier for the annotators to obtain more precise locations of the annotations. After getting used to the annotation process, annotators become more and more precise on these steps, which significantly reduces the time required to annotate as fewer adjustments are required. Besides, to ensure that profit is not prioritized over the accuracy and the precision of the annotations, we are highly involved in the process and all annotators must pass the strict annotation testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Annotation</head><p>Our Annotations are finely classified into five categories: pedestrians, riders, partially-visible persons, crowd and ignore regions. The annotation process contains the two steps:  1. Annotators are asked to thoroughly search across the whole image for individuals, and annotate them for using the similar protocol from <ref type="bibr" target="#b2">[3]</ref>. For pedestrians and riders (shown in <ref type="figure" target="#fig_1">Fig. 3(a)</ref>), we generate a bounding box by drawing a line across one's head and the middle point between feet, as shown in <ref type="figure" target="#fig_1">Fig. 3(b)</ref>. A bounding box aligned to the center of the line is then generated with an aspect ratio of 0.41 (defined as w/h), as shown in <ref type="figure" target="#fig_1">Fig. 3(c)</ref>. For partially-visible persons, including individuals that are heavily-occluded or with unusual poses and viewpoints, we mark them using bounding boxes with unconstrained aspect ratios. The crowd in our dataset plays another critical role contributing to the variances and difficulties. Similar to the partially-visible persons, we also annotate a group of people using a tightly bounded rectangle. Finally, we annotate regions containing fake human, for instance, human on the posters, reflections, mannequin and statues, etc. 2. After the above-mentioned annotating process, to ensure the quality of the labels, we perform three-fold crossvalidation to check the annotations strictly. Each image is intuitively marked as either correct or erroneous by three <ref type="figure">Fig. 4</ref>. Recall rate with different number of proposals. Proposals are generated by using Edgebox <ref type="bibr" target="#b56">[57]</ref>. Lower recall rate implies higher difficulty. We show histograms of detection rate over the number of proposal for different subsets. different annotators, and if it marked as erroneous by more than half of the annotators, it would be re-annotated until it passes the check. <ref type="figure">Fig. 1</ref> shows some exemplary final annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dataset Statistic</head><p>Capacity. The number of bounding box annotations provided by our WiderPerson dataset is shown in table II, which illustrates the capacity of WiderPerson dataset. In a total of 13, 382 images, there are âˆ¼ 386k person and âˆ¼ 13k ignore region annotations in the WiderPerson dataset. The number of annotations is more than 10Ã— boosted compared with previous challenging pedestrian detection dataset like CityPersons. The total number of persons is also noticeably larger than the others. We randomly select 8000/1000/4382 images as training/validation/testing subsets. Following the principle in WIDER FACE <ref type="bibr" target="#b55">[56]</ref>, we define three levels of difficulty: 'Easy' (â‰¥ 100 pixels), 'Medium' (â‰¥ 50 pixels), 'Hard' (â‰¥ 20 pixels) according to the physical height of ground-truth bounding boxes. As shown in the <ref type="figure">Fig. 4</ref>, we utilize EdgeBox <ref type="bibr" target="#b56">[57]</ref> to evaluate their detection rates with different number of proposals. The average recall rates for these three levels are 81.5%, 73.6% and 63.4% with 10, 000 proposal per image.</p><p>Scale. To analyse the scale characteristic across different datasets, we use the probability density function (PDF) to specify the probability of scale falling within a particular range of values, which can specify the distribution of scales. To this end, we group the persons by their image size (height in pixels) into some scale bins. As can be observed from <ref type="figure" target="#fig_2">Fig. 5</ref>, Caltech-USA and CityPersons have a limited scale distribution, most   and seasons, while our WiderPerson dataset has no limitations on these conditions. WiderPerson contains person in a wide range of scenarios, while Caltech-USA and CityPersons are all recorded by a car traversing on streets. In order to visualize the diversity of annotations on the different datasets, we count the location distribution of persons, i.e., iterating over all person annotations, for each location, if it is inside one annotation, then its count plus 1. The images of Caltech and CityPersons have a fixed resolution (640 Ã— 480 and 2048 Ã— 1024, respectively), while our dataset varies in size, so we resize all images into the same resolution (1400 Ã— 800) to count the location distribution of persons. <ref type="figure" target="#fig_3">Fig. 6</ref> shows the location distribution of persons for different dataset in the way of heat map. We can see that persons on the Caltech-USA and CityPersons dataset are distributed in a narrow band across the center of the image, i.e., persons are concentrated on two sides of the road and mostly appear at the right side, since their images are collected by a biased data collection method that the car drives under the right-handed traffic condition. In contrast, our WiderPerson dataset has a uniform location distribution and persons appear in any position except the upper part (i.e., the sky). Also, the number of identical persons is another important evidence of diversity. As reported in the fifth line in <ref type="table" target="#tab_1">Table I</ref>, the number of identical persons amounts up to âˆ¼236k in our WiderPerson dataset. In contrast, the Caltech-USA dataset only contains âˆ¼1, 300 unique pedestrians, since images in Caltech-USA are not sparsely sampled, resulting in less amount of identical persons. While CityPersons frames are sampled very sparsely and each person is considered as unique. Like CityPersons, each person on our dataset can be considered as unique, but one more order of magnitude. Besides, WiderPerson also provides fine-grained labels for persons. As shown in <ref type="figure" target="#fig_4">Fig. 7</ref>, pedestrians are the majority (64.8%). Partially-visible persons account for 29.9% since our dataset is dense. Although riders only occupy 0.6%, the absolute numbers are still considerable, as we have a large pool of âˆ¼236k persons.</p><p>Occlusion. Occlusion is another important factor for evaluating the pedestrian detection performance. There are two types of occlusion: inter-class occlusion, which occurs when a person is occluded by stuff or objects from other categories; and intra-class occlusion (also referred to as crowd occlusion), which occurs when a person is occluded by other persons. As  0.00 0.00 0.01 described in <ref type="bibr" target="#b4">[5]</ref>, the intra-class occlusion is a more challenging issue than the inter-class occlusion. Lots of works have focused on the former problem and made great progress. However, the crowd occlusion has not been well researched and solved. On the one hand, it is difficult because of the problem itself. On the other hand, there is no suitable dataset. Therefore, we introduce this diversity and dense pedestrian detection dataset.</p><p>To demonstrate its degree of crowd occlusion, we provide statistical information on pair-wise occlusion. For each image, we count the number of person pairs with different intersection over union (IoU) threshold. The results are shown in <ref type="table" target="#tab_1">Table IV</ref>. In average, few person pairs with an IoU threshold of 0.3 are included in Caltech-USA. For CityPersons dataset, the number is less than one pair per image. However, the number is 9.21 for WiderPerson. Moreover, there are averagely 2.15 pairs whose IoU is greater than 0.5 in the WiderPerson dataset. These data can demonstrate that various occlusion levels are <ref type="figure">Fig. 8</ref>. Diagram of our improved Faster R-CNN. Reduced VGG-16 means removing the fourth max pooling and using the "hole algorithm". RoI feature enhancing subnetwork is a reimplementation of SENet with an identical block structure, which consists of one global average pooling layer (Fsq) and two consecutive fully connected layers (F rd and Fex). RoI feature X is re-weighted to generate the output X of the SE block which then be fed directly into subsequent Fast R-CNN subnetwork.</p><p>well-represented in WiderPerson, especially heavily occluded cases, while they can be hardly found in previous datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Benchmarking</head><p>With the publication of this paper, we will create a website for WiderPerson dataset, where its annotations for the training and validation subsets are made freely available to academic and non-profit organizations for non-commercial, scientific use. There is also an evaluation instruction on the website for researchers to evaluate the performance of their detectors over the held-out testing annotations. A leaderboard will be maintained and results are tallied online, either by name or anonymous.</p><p>We follow the same evaluation metric as used for Caltech-USA <ref type="bibr" target="#b0">[1]</ref> and CityPersons <ref type="bibr" target="#b2">[3]</ref>, denoted as M R, which stands for the average log miss rate over false positives per-image ranging in 10 âˆ’2 , 10 0 . M R is a suitable indicator for the algorithms applied in the real world applications. When evaluating pedestrian detection performance, riders/partially-visible persons/crowd/ignore regions are ignored, which means that those annotations are not considered as false negatives and detections matching with those annotations are not counted as false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROVIDED BASELINE METHOD</head><p>Before delving into our new dataset, we first build two strong baseline detectors as a tool for our experiment analyses based on Faster R-CNN <ref type="bibr" target="#b10">[11]</ref> and RetinaNet <ref type="bibr" target="#b57">[58]</ref>, which are two representative detectors from the two-stage and onestage approach, respectively. We aim to find a straightforward architecture to provide good performance on WiderPerson.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Improved Faster R-CNN</head><p>Faster R-CNN is masterpieces of the detection framework for general object detection and has dominated the field of object detection in recent years. It essentially consists of two components: a fully convolutional Region Proposal Network (RPN) for proposing candidate regions which likely contain objects, followed by a downstream Fast R-CNN network to classify a region of image into objects (and background) and refine the boundaries of those regions. Although competitive performance has been achieved on general object detection task, it under-performs on the pedestrian detection task (as reported in <ref type="bibr" target="#b41">[42]</ref>). The reason behind its poor performance on pedestrian detection is that it fails to handle heavily occluded and dense pedestrians, which are dominant on our new dataset. In this work, we propose some improvements to extend the Faster RCNN architecture for occluded and dense pedestrian detection. Our improved Faster R-CNN is based on VGG-16 <ref type="bibr" target="#b58">[59]</ref> and ResNet-50 <ref type="bibr" target="#b59">[60]</ref> as they are very common in pedestrian detection <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>. We use the same anchor setting from <ref type="bibr" target="#b2">[3]</ref>, i.e., 11 different anchor-box scales and 1 aspect ratio (w/h = 0.41) are used to capture objects across all sizes. Moreover, some improvements are proposed to boost the performance on pedestrian detection as follows.</p><p>Finer Feature Map. The vanilla Faster R-CNN uses a coarse feature map as the detection layer, i.e., the last conv layer in the fifth block with stride of 16 pixels. Having such a coarse stride is harmful to small pedestrian detection, since it reduces the chances of having a high score over pedestrian and forces the network to handle large displacement relative to the object appearance. To increase the feature map resolution, we remove the fourth down-sampling operation and reduce the stride from 16 to 8 pixels, helping the detector to handle small pedestrians. Specifically, all layers before the fourth down-sampling operation are unchanged and all convolutional filters after it are modified by the "hole algorithm" <ref type="bibr" target="#b60">[61]</ref> (i.e., "AlgorithmÃ  trous") to compensate for the reduced stride.</p><p>Ignore Region and Tiny Pedestrian Handling. We implement an ignore region handling for Faster R-CNN. Ignore regions might contain objects of a given class without precise localization. Simply treating these regions as background introduces confusing samples and has a negative impact on the detector quality. The ignore region handling prevents the sampling of background boxes in those areas that could potentially overlap with real objects. Besides, training with very tiny samples could lead to models detecting a lot more false positives. Hence, we online filter pedestrians whose height is less than 20 pixels after scaling during training. Filtered pedestrians are handled as ignore regions in order to ensure that they are not sampled as background during training.</p><p>RoI Feature Enhancing. The RoIPooling layer uses max pooling to convert the features inside any valid region of interest into a fixed-size feature map, which is used by subsequent Fast R-CNN network to further classify and regress the proposals for final detections. Therefore, the representational ability of the pooled feature is the key to achieve high performance, especially on our highly diverse dataset. Inspired by <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b61">[62]</ref>, we use a "Squeeze-and-Excitation" (SE) block to enhance the representational ability of the RoIPooling feature by explicitly modelling the interdependencies between the convolutional channels. More specific, the SE block performs sample-dependent feature re-weighting so as to select the more informative channel features while suppress less useful ones. As shown in <ref type="figure">Fig. 8</ref>, the newly added SE block is composed of one global average pooling layer and two consecutive fully connected layers, which is easy to implement and can obtain remarkable improvements while add little additional computational costs.</p><p>Dynamic Sample Strategy. The vanilla Faster R-CNN has a fixed sample strategy, i.e., 256 and 128 samples for RPN and Fast R-CNN with 1 : 1 and 1 : 3 positive-negative ratio, respectively. Since there are âˆ¼ 28.87 persons per image in our dataset, the fixed sample strategy will lead to inadequate use of training positive samples. To solve this issue, we introduce a dynamic sample strategy: if there are too many positive samples, we determine the number of negative samples based on the above positive-negative ratio to ensure that all positive samples are used, otherwise we follow the original strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vanilla RetinaNet</head><p>In addition to the two-stage baseline detector, we also provide another baseline detector based on RetinaNet, the onestage approach, which detects objects by regular and dense sampling over locations, scales and aspect ratios with high efficiency. RetinaNet proposes a focal loss to address the extreme foreground-background class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. We use the same setting of anchor scales as <ref type="bibr" target="#b57">[58]</ref> and only modify the height vs. width ratio of anchors as 1:0.41 in consideration of the pedestrian shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we will introduce our implementation details about data processing and training setting. Notably, all the experiments are conducted based on the improved Faster R-CNN with VGG-16 unless otherwise specified. Firstly, we verify the effectiveness of our improvements via model analysis. Then, we conduct some experiments to analyse our WiderPerson dataset in different aspects, including the detection result, quantity, quality and error. Finally, the generalization ability of our WiderPerson dataset will be evaluated on standard pedestrian benchmarks like Caltech-USA and CityPersons.</p><p>A. Implementation Detail.</p><p>Data Processing. To improve performance for small sized pedestrians, the input images are upscaled to a larger size using the bilinear interpolation algorithm. Specifically, the input image sizes of Caltech and CityPersons are set to 2Ã— and 1.3Ã— of the original images. As the images of WiderPerson are both collected from the Internet with various sizes, we resize the input so that their short edge is at 800 pixels while the long edge should be no more than 1400 pixels at the same time. We use horizontal image flipping as the only form of data augmentation. Multi-scale training and testing are not applied to ensure fair comparisons.</p><p>Training Setting. For the improved Faster R-CNN, all models are trained for 180k iterations with an initial learning rate of 0.01, and decreased by a factor of 10 after 120k on our WiderPerson dataset. On the CityPersons dataset, we set the learning rate to 10 âˆ’3 for the first 40k iterations and decay it to 10 âˆ’4 for another 20k iterations. On the Caltech-USA dataset, we train the network for 120k iterations with the initial learning rate 10 âˆ’3 and decrease it by a factor of 10 after the first 80k iterations. To fine-tune the improved Faster R-CNN from WiderPerson to Caltech-USA and CityPersons, the number of iterations is the same but the learning rate is halved overall. All these models are optimized by the Stochastic Gradient Descent (SGD) algorithm on 1 TITAN X (Maxwell) GPU with a mini-batch 2. Weight decay and momentum are set to 0.0005 and 0.9. Besides, the RetinaNet baseline on the WiderPerson dataset is trained with 16 batch size for 25k iterations with 0.02 initial learning rate, which is then divided by 10 at 16k and again at 21k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Analysis</head><p>We carry out some ablation experiments on the WiderPerson validation subset to analyze our improved Faster R-CNN. For all the experiments, we use the same settings, except for specified changes to the components. We ablate each improvement one after another to examine how each proposed improvement  affects the final performance. Firstly, we replace the dynamic sample strategy with the original strategy. Secondly, the RoI feature enhancing module is ablated. Thirdly, we do not handle ignore regions and tiny ground truths during training stage. Fourthly, we do not reduce the VGG-16 backbone. Finally, we use original anchor scales rather than new anchor setting <ref type="bibr" target="#b2">[3]</ref>. Some promising conclusions can be summed up according to the ablative results in Tab. V. Firstly, the new anchor setting is more suitable for the proposed dataset, which reduces the MR by 3.39%, 2.69% and 2.36% for Easy, Medium and Hard subset, respectively. Secondly, the finer feature map is used to provide more anchors and detailed information, which reduces the M R 39.62%, 46.28% and 53.26% to 35.12%, 42.67% and 50.17% for Easy, Medium and Hard subset, respectively, demonstrating its effectiveness. Thirdly, the ignore region and tiny pedestrian handling is proposed to ignore small ground truths and prevent the sampling of background boxes in those ignored areas. The comparison between the second and third columns in Tab. V demonstrates that it can bring 3.67% (Easy), 2.96% (Medium) and 2.66% (Hard) drops in M R, attributing to not involving confusing samples in training. Fourthly, according to the third and fourth columns, we can observe a drop in M R of 1.47% (Easy), 1.03% (Medium) and 0.86% (Hard), these sharp declines demonstrate the effectiveness of the RoI feature enhancing. Finally, the comparison between the fourth and fifth columns in Tab. V indicates that the dynamic sample strategy decreases the M R by 0.37% (Easy), 0.28% (Medium) and 0.19% (Hard), owning to making full use of training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dataset Analysis</head><p>All experiments in this subsection are trained based on WiderPerson training subset and the results are evaluated on the validation subset. Firstly, we evaluate our improved Faster R-CNN in detail on validation subset, then study on the quantity and quality, finally analyze common failure cases.</p><p>Detection Results. proposed WiderPerson dataset is a challenging benchmark even for the state-of-the-art pedestrian detection algorithms.</p><p>In <ref type="table" target="#tab_1">Table VII and Table VIII</ref>, we also report detection results of the improved Faster R-CNN with VGG-16 on Caltech, i.e., 5.49% M R, and CityPersons, i.e., 12.49% M R. It further demonstrates that our WiderPerson dataset is much challenging than the standard pedestrian detection benchmarks based on the detection performance. Since our WiderPerson dataset varies largely in scenario and occlusion, which bring many difficulties to pedestrian detection. The illustrative examples of pedestrian detection based on our improved Faster R-CNN with VGG-16 are shown in <ref type="figure" target="#fig_5">Fig. 9</ref>.</p><p>Quantity Analysis. As indicated in <ref type="bibr" target="#b62">[63]</ref>, there is a logarithmic relation between the amount of training data and the performance of deep learning methods. To understand the impact of having a larger amount of training data, we show how the performance grows as training data increases on our benchmark. For this purpose, we train our baseline methods on different sized subsets which are randomly sampled from the training set. From <ref type="figure">Fig.10</ref> we can observe that logarithmic relation between training set size and detection performance also holds on our benchmark for the improved Faster R-CNN across three subsets, i.e., performance keeps improving with more data. Therefore, it is of great importance to provide CNNs with a large amount of data.</p><p>Quality Analysis. The importance of fine-grained annotations for riders and additional annotations for ignore regions is now examined. In ablation experiments, we have verified the effectiveness of ignored regions via training the mdoel without ignore region handling, it is in accordance with earlier findings <ref type="bibr" target="#b2">[3]</ref> that detection performance deteriorates when not using ignore regions during training. Besides, the evaluation protocol described in Section III-E ignores detected neighboring classes. For pedestrians this means that riders are not considered as false positives, hence training pedestrian detectors generally treat riders as ignore region.</p><p>To verify whether rider annotations are useful for pedestrian detection, we can directly train the baseline detection method by including riders into pedestrians, since pedestrian and rider are annotated in the same way with a fixed aspect ratio on our dataset. error modes of false positives in pedestrian detectors, i.e., location (LOC) and background (BG). LOC indicates the localization errors that occurs when a pedestrian is detected with a misaligned bounding box, and BG indicates that a background region is mistakenly detected as a pedestrian. <ref type="figure">Fig. 11</ref> shows the distribution of two types of false positives and BG seems the dominating error mode among top-scoring detection. <ref type="figure" target="#fig_0">Figure 12</ref>(a) illustrates some qualitative false positives of this method. As can be seen, animals, clothes and fakes are principal sources for confusion with real pedestrians. Certain pedestrian poses and aspect ratios can lead to multiple detections for the same pedestrian as shown in the Multi Detections category. Non-maximum suppression (NMS) is used by detection methods to suppress multiple detections. We use an IoU threshold of 0.5 which is not sufficient to suppress detections that have very diverse aspects. <ref type="figure" target="#fig_0">Figure 12</ref>(b) illustrates some qualitative false negatives of this method. A lower IoU threshold would lead to more false negatives. These already occur for an IoU threshold of 0.5 as shown in the NMS Repressing category. Because of the high IoU between pedestrians, not all of them can be detected because of the greedy NMS. Thus, NMS is an important part of many deep learning methods that is usually not trained but has a great influence on detection performance. Small and occluded pedestrians are a further common source for false negatives. These two groups have also been analyzed in <ref type="bibr" target="#b63">[64]</ref>. In some scenarios, usually only the lower part of a pedestrian is occluded due to various obstacles. In our qualitative analysis we have false negatives where the head is occluded. These are particularly challenging for pedestrian detection methods, as these cases are quite rare in the training dataset. Further challenges are unusual and extreme poses as shown in the Others group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Generalization Capability</head><p>In this subsection, we evaluate the generalization capability of our WiderPerson dataset. As illustrated in Section III-D, the size of WiderPerson dataset is obviously more diverse and larger than the existing benchmarks, like Caltech-USA <ref type="bibr" target="#b0">[1]</ref> and CityPersons <ref type="bibr" target="#b2">[3]</ref>. Naturally, our dataset, with a reduced bias, should better capture the true world and result in superior generalization capabilities of the detectors which are trained on this dataset. To demonstrate the increased diversity of our dataset, we first train the model on our WiderPerson dataset and then fine-tune it on the other pedestrian detection benchmarks.</p><p>Caltech. The Caltech-USA dataset is one of the most popular and challenging datasets for pedestrian detection, which comes from approximately 10 hours 30Hz VGA video recorded by a car traversing the streets in the greater Los Angeles metropolitan area. We use the new high quality annotations provided by <ref type="bibr" target="#b64">[65]</ref> to train and evaluate. The training and testing sets contains 42, 782 and 4, 024 frames, respectively. The results are shown for the Caltech-USA dataset in <ref type="table" target="#tab_1">Table VII</ref>. The overall detection performance is superior for the cases in which WiderPerson is used for pre-training. Our improved Faster R-CNN achieves 5.49% M R for pedestrians on the (a) False positive: a background region is mistakenly detected as a pedestrian, or a pedestrian is detected with a misaligned bounding box.</p><p>(b) False negative: a pedestrian fails to be detected. Caltech-USA testing set for the reasonable setting. When we directly evaluate the Caltech-USA trained model on the proposed WiderPerson Easy subset, we get a very high MR of 82.79% since Caltech-USA has limited density and diversity. In contrast, our model trained on WiderPerson without finetuning achieves 9.72% M R and can be boost to 4.27% M R with fine-tuning. Based on the pre-training of our WiderPerson dataset, our algorithm has superior performance on the Caltech-USA benchmark against the one without WiderPerson pre-training, and performs on-pair with the state-of-the-arts.</p><p>CityPersons. The CityPersons dataset is built upon the semantic segmentation dataset Cityscapes to provide a new dataset of interest for pedestrian detection. It is recorded across 18 different cities in Germany with 3 different seasons and various weather conditions. The dataset includes 5, 000 images (2, 975 for training, 500 for validation, and 1, 525 for testing) with 35, 000 manually annotated persons plus  that is on-pair with the state-of-the-arts, demonstrating our WiderPerson dataset can serve as an effective pre-training dataset for pedestrian detection task.</p><p>Summary. The superior performance on both Caltech-USA and CityPersons datasets when using WiderPerson for pretraining indicates a high dataset diversity. Models trained on this dataset will have increased generalization capabilities. However, due to the dataset biases, solely training on a dataset from the other domain without fine-tuning results in worse detection performance. Despite the dataset biases, the models are able to learn general features for the task of pedestrians detection when pre-trained on WiderPerson which proves useful for other datasets as well after fine-tuning. Using transfer learning to pre-train a network on generic data and fine-tune on the target domain is widely applied and used to increase overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Current pedestrian detection benchmark datasets have contributed to spurring interest and progress in pedestrian detection research. With the help of CNN, modern methods have achieved remarkable performance on these benchmarks. However, it is still difficult to assess for real world performance, since there is a gap in the diversity and density between existing pedestrian detection benchmarks and real world requirements: 1) most of current datasets are collected in the fixed traffic scenario, which significantly reduces the diversity of the foreground and background. 2) crowd scenarios with occluded pedestrian are still under represented, limiting the variations in density. These limitations have partially contributed to the failure of some algorithms in coping with heavy occlusion and atypical scenario. To move forward the field of pedestrian detection, we introduce a diverse and dense pedestrian detection dataset called WiderPerson, which consists of 13, 382 images with 399, 786 annotations and varies largely in scenario and occlusion. Providing high quality annotations, it enables new experiments both for training better models and as new test benchmark. We propose some strong baseline detectors based on Faster R-CNN and RetinaNet to benchmark the state-of-the-art detector. The cross-dataset generalization results of WiderPerson dataset demonstrate that it is an effective training source for pedestrian detection and can help to achieve state-of-the-art performance on the Caltech-USA and CityPersons datasets. In the future, we will provide continuous improvements and additions to the WiderPerson dataset. Besides, we plan to annotate the head bounding box for each pedestrian and explore their relationship to facilitate further studies on the dense pedestrian detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Graphical User Interface (GUI) of our annotation tool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of bounding box annotations for pedestrians and riders. For each target, the top of the head and middle of the feet is drawn by the annotator. An aligned bounding box is automatically generated using the fixed aspect ratio (0.41).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Scale distribution of different dataset. We use the probability density function (PDF) to specify the probability of scale falling within a particular range of values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>The location distribution of pedestrians on the image. Pedestrians on the Caltech-USA and CityPersons dataset are distributed in a narrow band across the center of the image, while WiderPerson has an uniform location distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fine-grained person categories on WiderPerson.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Qualitative results for pedestrian detection of our improved Faster R-CNN with VGG-16 based on the WiderPerson dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Detection performance (M R) of our improved Faster R-CNN with VGG-16 as a function of training set size Distribution of two error modes of false positives on the WiderPerson validation Hard subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Qualitative detection errors for our improved Faster R-CNN (green: true positives, red: false positives or false negatives).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2) dataset arXiv:1909.12118v1 [cs.CV] 25 Sep 2019</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF PEDESTRIAN DETECTION DATASETS (Training SUBSET ONLY). '-' MEANS THIS TERM IS UNLIMITED AND CAN NOT BE COUNTED.</figDesc><table><row><cell></cell><cell>Caltech-USA</cell><cell>KITTI</cell><cell>CityPersons</cell><cell>WiderPerson</cell></row><row><cell># country</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>âˆ’</cell></row><row><cell># city</cell><cell>1</cell><cell>1</cell><cell>18</cell><cell>âˆ’</cell></row><row><cell># season</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>4</cell></row><row><cell># images</cell><cell>42, 782</cell><cell>3, 712</cell><cell>2, 975</cell><cell>8, 000</cell></row><row><cell># persons</cell><cell>13, 674</cell><cell>2, 322</cell><cell>19, 654</cell><cell>236, 073</cell></row><row><cell># ignore regions</cell><cell>50, 363</cell><cell>45</cell><cell>6, 768</cell><cell>8, 979</cell></row><row><cell># person/image</cell><cell>0.32</cell><cell>0.63</cell><cell>6.61</cell><cell>29.51</cell></row><row><cell># unique persons</cell><cell>1, 273</cell><cell>&lt; 2, 322</cell><cell>19, 654</cell><cell>236, 073</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II STATISTICS</head><label>II</label><figDesc>OF ANNOTATIONS ON WIDERPERSON DATASET.</figDesc><table><row><cell></cell><cell>Training</cell><cell>Validation</cell><cell>Testing</cell><cell>Sum</cell></row><row><cell># images</cell><cell>8, 000</cell><cell>1, 000</cell><cell>4, 382</cell><cell>13, 382</cell></row><row><cell># persons</cell><cell>236, 073</cell><cell>27, 762</cell><cell>122, 518</cell><cell>386, 353</cell></row><row><cell># ignore regions</cell><cell>8, 979</cell><cell>661</cell><cell>3, 793</cell><cell>13, 433</cell></row><row><cell># person/images</cell><cell>29.51</cell><cell>27.76</cell><cell>27.96</cell><cell>28.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III DENSITY</head><label>III</label><figDesc>COMPARISON BETWEEN WIDELY USED PEDESTRIAN DETECTION DATASETS. IT DEMONSTRATES THE NUMBER AND PROPORTION OF IMAGES THAT CONTAIN â‰¥ # PERSONS IN DIFFERENT DATASETS.Both of them are insufficient to serve as an ideal benchmark for the challenging crowd scenes. Thanks to the pre-filtering and annotation protocol of our dataset, WiderPerson can reach a much better density. As shown inTable.II, we notice the density of persons are consistent across training/validation/testing subsets.</figDesc><table><row><cell># persons</cell><cell cols="2">Caltech-USA</cell><cell cols="2">CityPersons</cell><cell cols="2">WiderPerson</cell></row><row><cell>â‰¥1</cell><cell>7,839</cell><cell>18.3%</cell><cell>2,482</cell><cell>83.4%</cell><cell>8,000</cell><cell>100.0%</cell></row><row><cell>â‰¥2</cell><cell>3,257</cell><cell>7.6%</cell><cell>2,082</cell><cell>70.0%</cell><cell>7,999</cell><cell>100.0%</cell></row><row><cell>â‰¥3</cell><cell>1,265</cell><cell>3.0%</cell><cell>1,741</cell><cell>58.5%</cell><cell>7,998</cell><cell>100.0%</cell></row><row><cell>â‰¥5</cell><cell>282</cell><cell>0.7%</cell><cell>1,225</cell><cell>41.2%</cell><cell>7,994</cell><cell>99.9%</cell></row><row><cell>â‰¥10</cell><cell>36</cell><cell>0.1%</cell><cell>610</cell><cell>20.5%</cell><cell>7,924</cell><cell>99.1%</cell></row><row><cell>â‰¥20</cell><cell>0</cell><cell>0.0%</cell><cell>227</cell><cell>7.6%</cell><cell>5,145</cell><cell>64.3%</cell></row><row><cell>â‰¥30</cell><cell>0</cell><cell>0.0%</cell><cell>94</cell><cell>3.2%</cell><cell>2,564</cell><cell>32.1%</cell></row></table><note>of their annotations are between 30âˆ¼100 pixels in height. In contrast, our WiderPerson dataset covers a much wider range of scale and the distribution of persons at all scales is relatively uniform.Density. In terms of density, on average there are âˆ¼ 28.87 persons per image in WiderPerson dataset, as shown in the fourth line of Table II. We also report the density from the existing datasets in Table III. Obviously, WiderPerson dataset is of much higher crowdness compared with all previous datasets. Caltech-USA suffers from extremely low-density, for that on average there is only âˆ¼ 1 person per image. The number in CityPersons reaches âˆ¼7, a significant boost while still not dense enough.Diversity. Diversity is an important factor of a dataset. We compare the diversity of Caltech-USA, CityPersons and WiderPerson in Table. I. Since CityPersons testing set an- notations are not publicly available, we only consider the training subset for a fair comparison. The Caltech-USA and KITTI datasets are recorded in one city at one season, and the CityPersons dataset is recorded across 18 cities, 3 countries</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">OF PAIR-WISE OVERLAP BETWEEN TWO PERSON</cell></row><row><cell></cell><cell>INSTANCES.</cell><cell></cell><cell></cell></row><row><cell>pair/image</cell><cell cols="3">Caltech-USA CityPersons WiderPerson</cell></row><row><cell>IoU&gt;0.3</cell><cell>0.06</cell><cell>0.96</cell><cell>9.21</cell></row><row><cell>IoU&gt;0.4</cell><cell>0.03</cell><cell>0.58</cell><cell>4.78</cell></row><row><cell>IoU&gt;0.5</cell><cell>0.02</cell><cell>0.32</cell><cell>2.15</cell></row><row><cell>IoU&gt;0.6</cell><cell>0.01</cell><cell>0.17</cell><cell>0.81</cell></row><row><cell>IoU&gt;0.7</cell><cell>0.00</cell><cell>0.08</cell><cell>0.24</cell></row><row><cell>IoU&gt;0.8</cell><cell>0.00</cell><cell>0.02</cell><cell>0.06</cell></row><row><cell>IoU&gt;0.9</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V ANALYSIS</head><label>V</label><figDesc>OF PROPOSED IMPROVEMENTS. ALL MODELS ARE BASED ON FASTER R-CNN WITH VGG-16, TRAINED ON WIDERPERSON training SET AND TESTED ON validation SET. NUMBERS INDICATE M R.</figDesc><table><row><cell>Component new anchor setting finer feature map RoI feature enhancing ignore region handling</cell><cell>Step by step improvements ! ! ! ! ! ! ! ! ! ! ! ! ! !</cell></row><row><cell>dynamic sample strategy</cell><cell></cell></row></table><note>! Easy subset 43.01 39.62 35.12 31.45 29.98 29.61 Medium subset 48.97 46.28 42.67 39.71 38.68 38.40 Hard subset 55.62 53.26 50.17 47.51 46.65 46.46</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table VI illustrates our baselines' results on the WiderPerson validation subset. On the one hand, we achieve promising M R performances, i.e., 31.47%, 40.45%, 48.32% for the vanilla RetinaNet, 29.61%, 38.40%, 46.46% for the improved Faster R-CNN with VGG-16, and 28.75%, 37.82%, 46.06% for the improved Faster R-CNN with ResNet-50 on Easy, Medium and Hard subsets, respectively. On the other hand, from these results, we can find that the TABLE VI M R AND SPEED PERFORMANCE OF OUR BASELINES ON THE WIDERPERSON validation SUBSET.</figDesc><table><row><cell>Baseline</cell><cell>Backbone</cell><cell>FPS</cell><cell>Easy</cell><cell>Medium</cell><cell>Hard</cell></row><row><cell>Vanilla RetinaNet</cell><cell>ResNet-50</cell><cell>8.93</cell><cell>31.47</cell><cell>40.45</cell><cell>48.32</cell></row><row><cell>Improved FRCNN</cell><cell>VGG-16</cell><cell>0.83</cell><cell>29.61</cell><cell>38.40</cell><cell>46.46</cell></row><row><cell>Improved FRCNN</cell><cell>ResNet-50</cell><cell>0.77</cell><cell>28.75</cell><cell>37.82</cell><cell>46.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>As expected, comparing with training only with pedestrians, after these neighboring annotations are involved during training, detection performance increases on M R from 29.61%, 38.40% and 46.46% to 29.35%, 38.27% and 46.39% for Easy, Medium and Hard subset, respectively. Therefore, adding fine-grained annotations for riders is helpful for the pedestrian detection performance, since we can treat them as an additional training samples.</figDesc><table /><note>Error Analysis. We now utilize the detection analysis tool 1 to analyze the detection errors of our improved Faster R- CNN qualitatively on our WiderPerson validation dataset. The detection errors consist of false positives and false negatives. Firstly, we analyse the false positive errors. There are two1 http://web.engr.illinois.edu/ âˆ¼ dhoiem/projects/detectionAnalysis</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII EXPERIMENTAL</head><label>VII</label><figDesc>RESULTS ON CALTECH-USA.</figDesc><table><row><cell>Training</cell><cell>Testing</cell><cell>M R</cell></row><row><cell>Caltech-USA</cell><cell>Caltech-USA</cell><cell>5.49</cell></row><row><cell>Caltech-USA</cell><cell>WiderPerson (Easy)</cell><cell>82.79</cell></row><row><cell>WiderPerson</cell><cell>Caltech-USA</cell><cell>9.72</cell></row><row><cell>WiderPersonâ‡’Caltech-USA</cell><cell>Caltech-USA</cell><cell>4.27</cell></row><row><cell>RPN+BF [42]</cell><cell></cell><cell>7.3</cell></row><row><cell cols="2">HyperLearner [44]</cell><cell>5.5</cell></row><row><cell>OR-RCNN [8]</cell><cell></cell><cell>4.1</cell></row><row><cell cols="2">Repulsion Loss [5]</cell><cell>4.0</cell></row></table><note>âˆ¼ 13, 000 ignore region annotations. Both the bounding boxes and visible parts of pedestrians are provided and there are approximately 7 pedestrians in average per image. The results are shown for the CityPersons dataset in Table VIII. The</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII EXPERIMENTAL</head><label>VIII</label><figDesc>RESULTS ON CITYPERSONS. same findings hold for the CityPersons benchmark. Training on CityPersons dataset and testing on WiderPerson Easy subset has 73.45% M R, while training on WiderPerson dataset and testing on CityPersons validation subset achieves 16.17% M R, indicating the difficulty and expandability of our dataset. Again, our improved Faster R-CNN model pre-trained on WiderPerson can reduce the M R from 12.49% to 11.13%</figDesc><table><row><cell>Training</cell><cell>Testing</cell><cell>M R</cell></row><row><cell>CityPersons</cell><cell>CityPersons</cell><cell>12.49</cell></row><row><cell>CityPersons</cell><cell>WiderPerson (Easy)</cell><cell>73.45</cell></row><row><cell>WiderPerson</cell><cell>CityPersons</cell><cell>16.17</cell></row><row><cell>WiderPersonâ‡’CityPersons</cell><cell>CityPersons</cell><cell>11.13</cell></row><row><cell cols="2">Adapted Faster RCNN [3]</cell><cell>12.8</cell></row><row><cell cols="2">Repulsion Loss [5]</cell><cell>11.6</cell></row><row><cell>OR-CNN [8]</cell><cell></cell><cell>11.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by the National Key Research and Development Plan (Grant No.2016YFC0801002), the Chinese National Natural Science Foundation Projects #61876179, #61872367, #61806203, Science and Technology Development Fund of Macau (No. 152/2017/A, 0025/2018/A1, 008/2019/A1). We also acknowledge the support of NVIDIA with the GPU donation for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4457" to="4465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Occluded pedestrian detection through guided attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Repulsion loss: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pedestrian detection via body-part semantic and contextual information with dnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">TMM</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scale-aware fast R-CNN for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="985" to="996" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Occlusion-aware r-cnn: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-source multiscale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeply learned attributes for crowded scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4657" to="4666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vision-based pedestrian detection for rear-view cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gazit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="853" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cluster boosted tree classifier for multi-view, multi-pose object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth and appearance for mobile scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive image sampling and windows classification for on-board pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>GerÃ³nimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>LÃ³pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICVS</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new pedestrian dataset for supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Overett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pettersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="373" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monocular pedestrian detection: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2179" to="2195" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-cue onboard pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Society Workshop on CVPR</title>
		<imprint>
			<biblScope unit="page" from="794" to="801" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new benchmark for vision-based cyclist detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Flohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1028" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection: Benchmark dataset and baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1037" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards reaching human performance in pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="973" to="986" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The eurocity persons dataset: A novel benchmark for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Flohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multicamera tracking of articulated human motion using shape and motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2114" to="2126" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning common and feature-specific patterns: A novel multiple-sparse-representation-based tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2022" to="2037" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attentive contexts for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="944" to="954" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust multi-resolution pedestrian detection in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3033" to="3040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Informed haar-like features improve pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1751" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Strengthening the effectiveness of pedestrian detection with spatially pooled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="546" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3626" to="3633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4073" to="4082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pedestrian detection aided by deep learning semantic tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5079" to="5087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Illuminating pedestrians via simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4960" to="4969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning complexity-aware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3361" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real-time pedestrian detection with deep network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ogale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="32" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate CNN object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Is faster R-CNN doing well for pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="443" to="457" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">What can help pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6034" to="6043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A discriminative deep model for pedestrian detection with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3258" to="3265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1904" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to integrate occlusion-specific detectors for heavily occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="305" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A structural filter approach to human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="238" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pedestrian detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="878" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An HOG-LBP human detector with partial occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Single-pedestrian detection aided by multipedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3198" to="3205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Occlusion patterns for object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3286" to="3293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Multi-label learning of part detectors for heavily occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3506" to="3515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">New iterative geometric methods for robust perceptual image hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>MihÃ§ak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy in Digital Rights Management, ACM Workshop</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">WIDER FACE: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An exploration of why and when pedestrian detection fails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2335" to="2340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">How far are we from solving pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>in CVPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
