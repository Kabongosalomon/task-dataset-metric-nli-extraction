<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Gou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Instance segmentation requires a large number of training samples to achieve satisfactory performance and benefits from proper data augmentation. To enlarge the training set and increase the diversity, previous methods have investigated using data annotation from other domain (e.g. bbox, point) in a weakly supervised mechanism. In this paper, we present a simple, efficient and effective method to augment the training set using the existing instance mask annotations. Exploiting the pixel redundancy of the background, we are able to improve the performance of Mask R-CNN for 1.7 mAP on COCO dataset and 3.3 mAP on Pascal VOC dataset by simply introducing random jittering to objects. Furthermore, we propose a location probability map based approach to explore the feasible locations that objects can be placed based on local appearance similarity. With the guidance of such map, we boost the performance of R101-Mask R-CNN on instance segmentation from 35.7 mAP to 37.9 mAP without modifying the backbone or network structure. Our method is simple to implement and does not increase the computational complexity. It can be integrated into the training pipeline of any instance segmentation model without affecting the training and inference efficiency. Our code and models have been released at https://github.com/GothicAi/InstaBoost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Instance segmentation aims to simultaneously perform instance localization and classification and outputs pixellevel masks denoting the detected instance. It plays an vital role in computer vision and has many practical applications in autonomous driving <ref type="bibr" target="#b8">[10]</ref>, robotic manipulation <ref type="bibr" target="#b36">[38]</ref>, HOI detection <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b34">36]</ref> etc. Recent researches have proposed effective CNN (Convolution Neural Networks) architectures * contributed equally to this paper § Cewu Lu is the corresponding author: lucewu@sjtu.edu.cn. Cewu Lu is a member of MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, and SJTU-SenseTime AI lab. [ <ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b21">23]</ref> for the problem. To fully exploit the power of CNN, a large number of training data is indispensable. However, obtaining the annotations of pixel-wise masks is labor intensive, and thus limits the number of available training samples.</p><p>To tackle this problem, previous works utilize the data from other domains and conducted weakly supervised learning to obtain extra information. These researches mainly follow two lines: i) transform annotations from other domain to object masks <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b28">30]</ref> or ii) utilize data from other domain as extra regularization term <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b2">4]</ref>. However, few of these works investigate leveraging the existing mask annotations to augment the training set.</p><p>Recently, crop-and-paste data augmentation has been exploited in the area of instance detection <ref type="bibr" target="#b14">[16]</ref> and object de-tection <ref type="bibr" target="#b13">[15]</ref>. They crop the object using their masks and paste them on a random chose background randomly or according to the visual context. However, these data augmentation method does not work in the area of instance segmentation, as dataset priors are not efficiently exploited, resulting in poor performance in our experiments. Meanwhile, adopting a deep context model <ref type="bibr" target="#b13">[15]</ref> introduces significant computational overhead, making it less practical in real-world applications.</p><p>In this paper, we first propose a simple but surprisingly effective random augmentation technique. Inspired by the stochastic grammar of images <ref type="bibr" target="#b43">[45]</ref>, we paste objects in the neighboring of its original position, with additional small jittering on scale and rotation. Namely random InstaBoost, such method brings 1.7 mAP improvement with Mask R-CNN on COCO instance segmentation benchmark.</p><p>Further, we look back to the area of visual perception, from which we get inspiration for a better-refined position transformation scheme. Previous research in Bayesian approaches to brain function shows the brain's ability to extract perceptual information from sensory data was modeled in terms of probabilistic estimation <ref type="bibr" target="#b38">[40]</ref> and visual inference requires prior experience of the world <ref type="bibr" target="#b1">[3]</ref>. These researches shed light on the area of crop-paste data augmentation for instance segmentation.</p><p>Intuitively, there exists a probability map representing reasonable placement that aligns with real-world experience. Inspired by <ref type="bibr" target="#b18">[20]</ref>, we link such probability map to appearance consistency heatmap, which is based on local contour similarity since the background usually has redundancy in continuous, but non-aligned features. We sample feasible locations from the heatmap and conduct crop-paste data augmentation, reaching in total 2.2 mAP improvement on the COCO dataset. Such a scheme is denoted as appearance consistency heatmap guided InstaBoost. An example of our appearance consistency heatmap is shown in <ref type="figure" target="#fig_0">Fig.1</ref>.</p><p>We conduct exhaustive experiments on the Pascal VOC dataset and COCO dataset. By augmenting through appearance consistency heatmap guided InstaBoost, we are able to achieve 2.2 mAP improvement of COCO instance segmentation and 3.9 mAP on Pascal dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Instance mask segmentation. Combining instance detection and semantic segmentation, instance segmentation <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b32">34]</ref> is a much harder problem. Earlier methods either propose segmentation candidates followed by classification <ref type="bibr" target="#b33">[35]</ref>, or associate pixels on the semantic segmentation map into different instances <ref type="bibr" target="#b4">[6]</ref>. Recently, FCIS <ref type="bibr" target="#b26">[28]</ref> proposed the first fully convolutional endto-end solution to instance segmentation, which predicted position-sensitive channels <ref type="bibr" target="#b9">[11]</ref> for instance segmentation. This idea is further developed by <ref type="bibr" target="#b7">[9]</ref> which outperforms competing methods on the COCO dataset <ref type="bibr" target="#b31">[33]</ref>. With the help of FPN <ref type="bibr" target="#b30">[32]</ref> and a precise pooling scheme named RoI Align, He et al. <ref type="bibr" target="#b21">[23]</ref> proposed a two-step model Mask R-CNN that extends Faster R-CNN framework with a mask head and achieves state-of-the-art on instance segmentation [2] and pose estimation <ref type="bibr" target="#b25">[27]</ref> tasks. Although these methods have reached impressive performance on public datasets, those heavy deep models are hungry for an extremely large number of training data, which is usually not available in real-world applications. Furthermore, the potential of large datasets are not fully exploited by existing training methods. Instance-level augmentation. One branch of recent work has emerged with more precise instance-level image augmentation, laying potential to fully exploit the supervised information in the existing dataset <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b41">43]</ref>. Dwibedi et al. <ref type="bibr" target="#b14">[16]</ref> improved instance detection by simple cut-and-paste strategy with extra instances that have annotated masks. Khoreva et al. <ref type="bibr" target="#b24">[26]</ref> generate pairs of synthetic images for video object segmentation using cut-andpaste method. However, the object position is uniformly sampled and they just need to guarantee that changes between image pairs are kept small. Such setting does not work for image-level instance segmentation, as we demonstrated in our experiments that randomly pasted object will decrease the segmentation accuracy. Another recent work <ref type="bibr" target="#b13">[15]</ref> proposed a context model to place segmented objects at backgrounds with proper context and demonstrated that it can improve objection detection on the Pascal VOC dataset. Such method requires training an extra model and preprocessing data offline. In this paper, we propose a simple but effective online augmentation method, which is the first attempt that successfully improve overall accuracy on COCO instance segmentation, as to the best of our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given a cropped object patch from a specific image, the placement of that patch on the image can be defined by the affine transformation matrix</p><formula xml:id="formula_0">H =   s cos r s sin r t x −s sin r s cos r t y 0 0 1   (1)</formula><p>where t x , t y denote the coordinate shift in x, y-axis respectively, s denotes the scale variance and r denotes the rotation in degrees. Thus, the placement can be uniquely determined by a 4D tuple</p><formula xml:id="formula_1">B = {(t x , t y , s, r)} t x , t y , r ∈ R, s ∈ R +<label>(2)</label></formula><p>From the view of stochastic grammar of images <ref type="bibr" target="#b43">[45]</ref>, a probabilistic model can be defined on this 4D space to learn the natural occurrence frequency of objects and then sampled to synthesize a large number of configurations to cover novel instances in the test set. By this end, we define probability density function f (·) measuring how reasonable it is to paste the object O on the given image I, following a specific transformation tuple. Assuming (x 0 , y 0 ) as the object's original coordinate and x = x 0 + t x , y = y 0 + t y are new coordinates, a probability map P is defined on set B, which is given as</p><formula xml:id="formula_2">P (x, y, s, r | I, O) = f (t x , t y , s, r | I, O).<label>(3)</label></formula><p>the given image and object conditions (I, O) will be omitted for simplicity in the following context. Specifically, the identity transform (x 0 , y 0 , 1, 0) which corresponds to the original paste configuration should have the highest probability, i.e.</p><formula xml:id="formula_3">arg max P (x, y, s, r) = (x 0 , y 0 , 1, 0)<label>(4)</label></formula><p>Intuitively, in a small neighbor area of (x 0 , y 0 , 1, 0), our probability map P (x, y, s, r) shall also be high-valued since images are usually continuous and redundant in pixel level. Based on such observation, we propose a simple but effective augmentation approach: object jittering that randomly samples transformation tuples from the neighboring space of identity transform (x 0 , y 0 , 1, 0) and paste the cropped object following affine transform H. Experimental result in Sec. 4.4 shows the surprising effectiveness of this simple data augmentation strategy.</p><p>In addition, inspired by <ref type="bibr" target="#b1">[3]</ref>, the feasible location of (x, y) can be further extended without being restricted to the neighboring area of (x 0 , y 0 ) if the background shares a similar pattern for a wide range. Therefore, we proposed a simple appearance consistency heatmap to utilize the redundancy in continuous, but non-aligned features of background. With the guidance of such heatmap, we can maximize the utility of our object jittering.</p><p>In Sec. 3.2, we introduce the pipeline of our vanilla object jittering, while the generation and adoption of our appearance consistency map will be detailed in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Random InstaBoost</head><p>A simple but effective augmentation approach named random InstaBoost is proposed, which draws a sample from an instance segmentation dataset, separate its foreground and background with ground truth annotations aided with matting and inpainting, and apply a restricted random transform to generate an augmented image. With visually appealing images generated via InstaBoost, experiments show the effectiveness of random InstaBoost, achieving 1.7 mAP improvement on COCO instance segmentation. Random InstaBoost mainly contains two steps: i) instance and background preparation via matting and inpainting and ii) ran- dom transform sampled from neighboring space of identity transform. Instance and background preparation. Given an image with ground truth labels for instance segmentation, we need to separate the target instance and the background, where the annotation of an instance segmentation dataset has already given sufficient information. However, in popular datasets e.g. COCO <ref type="bibr" target="#b31">[33]</ref>, annotations are stored in the format of boundary points and edges, leading to a disappointing situation where the outline is zigzag. To overcome such issue, matting <ref type="bibr" target="#b22">[24]</ref> is adopted to get a smoother outline with the alpha channel, which is much more similar to the actual situation. In such a manner, instances can be cut off from the original image properly.</p><p>After the cutting step, we get a reasonable instance patch and an incomplete background with an instance-shaped hole on it. Inpainting method <ref type="bibr" target="#b3">[5]</ref> are adopted to fill in such holes. <ref type="figure" target="#fig_1">Fig. 2</ref> shows an example for inpainting and matting visualization. Random transformation With 4D tuple transformation parameters defined in Eq. (2), our simple but effective In-staBoost technique is proposed, where (t x , t y , s, r) are all random variables sampled from uniform distribution in the neighboring space of identity transform (0, 0, 1, 0). Slight blurring is introduced to the original image, which will not strongly violate the visual content in the original image, but parallelly provides additional supervision to train instance segmentation models. functions f xy (·), f s (·) and f r (·) denoting probability density function w.r.t. (t x , t y ) and (s, r), respectively, whereby the formulation is simplified assuming the independence between (t x , t y ), s and r:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Appearance consistency heatmap guided Insta-Boost</head><formula xml:id="formula_4">P (x, y, s, r) =f xy (t x , t y | s, r)f s (s | r)f r (r) =f xy (t x , t y )f s (s)f r (r)<label>(5)</label></formula><p>where f s (s), f r (r) are uniform distributions adopted by random InstaBoost in Sec. 3.2. Appearance consistency heatmap M is defined as the expectation of probability map P , given x, y, input image I and object patch O, which is proportional to f xy (t x , t y )</p><formula xml:id="formula_5">M (x, y) = E [P (x, y)] ∝ f xy (t x , t y )<label>(6)</label></formula><p>Details of the appearance consistency map will be given as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Appearance consistency heatmap</head><p>Appearance descriptor. To measure the appearance similarity of an object patch pasted on two locations, we first need to define a descriptor which encodes the texture of the background in the neighbor area of the object. Intuitively, the influence of the ambient environment of the target instance on appearance consistency decreases with the increase of distance. Based on this assumption, we define the appearance descriptor D(·) as the weighted combination of three fixed width contour areas with different scales, which can be formulated as</p><formula xml:id="formula_6">D(c x , c y ) = {(C i (c x , c y ), w i ) | i ∈ {1, 2, 3}}<label>(7)</label></formula><p>where C i denotes the contour area i with weight w i , given c x , c y as the center of the instance. With i = 1 being the most inside contour, we define w 1 &gt; w 2 &gt; w 3 emphasizing stronger consistency around neighboring areas of the original object. <ref type="figure">Fig. 4</ref> shows an example of contour areas of appearance consistency heatmap. Appearance distance. In this part, appearance distance is defined as local appearance consistency metric between pairs of appearance descriptor, i.e. instance centers. Since we have already defined affinity descriptor with three contour areas and corresponding weights, appearance distance between</p><formula xml:id="formula_7">D 1 = D(c 1x , c 1y ), D 2 = D(c 2x , c 2y ) is defined as d(D 1 , D 2 ) = 3 i=1 (x1,y1)∈C1i (x2,y2)∈C2i w i ∆(I 1 (x 1 , y 1 ), I 2 (x 2 , y 2 )) (8) where (w i , C 1i ) ∈ D 1 , (w i , C 2i ) ∈ D 2 . I k (x, y)</formula><p>denotes the RGB value of image k on (x, y) pixel coordinate. ∆ can take any distance metric, where Euclidean distance is adopted in our implementation.</p><p>There occurs an exception that when part of the semantic consistency effective area locates outside of the background. For this situation, we consider the semantic consistency distance of this pixel equals to infinity (and therefore ignored). </p><formula xml:id="formula_8">h(x) = − log x − m M − m<label>(9)</label></formula><p>where M = max (d(D, D 0 )) represents the maximum distance in all candidate centers, m = min (d(D, D 0 )) represents the minimum distance. Heatmap H is generated with h(·) applied to every pixel in the background image, with respect to original instance's position (x 0 , y 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Heatmap to transformation tuple</head><p>Coordinate shift. Transformation is performed according to a 4D tuple as introduced in Eq. (1, 2). As suggested in Eq. <ref type="formula" target="#formula_5">(6)</ref>, heatmap values are proportional to the probability density function on x, y-axis, namely f xy (·). Therefore, values in the appearance consistency heatmap are normalized and treated as probabilities, from which candidate points are sampled via Monte Carlo method. Compared to randomly sampling (x, y) from the uniform distribution, the feasible area to placing the new object grows significantly, while avoiding pasting the instance onto semantically inconsistent backgrounds. Such operation on the heatmap introduces extra information for model training, which is an appealing feature for data augmentation. Scaling and rotation. Scale and rotation parameters (s, r) are sampled independently from uniform distribution in the neighboring of (1, 0), as we assume independence among (x, y), s, r. Such practice is identical to our implementation of random InstaBoost in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Acceleration</head><p>Following the steps described in Section 3.3, we can successfully generate a heatmap for any target instance. However, computing the feature map is computationally inefficient as it needs to compute W × H semantic consistency distances for each point in the original effective area, where W represents the width of the image and H represents the height. The time complexity comes to O(W 2 H 2 ) for computing Eq. 8, which is unacceptable in real-world applications. Therefore, we calculate the similarity map after resizing the original images to a fixed size and then upsample the heatmap to the original image size through interpolation.</p><p>With such an acceleration strategy, appearance consistency heatmap is calculated in high quality and high speed, which is decisive in the implementation of our online InstaBoost algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>Our InstaBoost data augmentation strategy can be integrated into the training pipeline of any existing CNN based framework. During the training phase, the dataloader takes an image and applies InstaBoost strategy with a given probability, together with other data augmentation strategies. Our implementation of InstaBoost only introduces little CPU overhead to the original framework, together with parallel processing of dataloader that guarantees the efficiency during the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussion</head><p>Previous method <ref type="bibr" target="#b13">[15]</ref> investigated applying context model to explicitly model the consistency of the object and background in semantic space. Different from their approach, our appearance consistency map does not consider the semantic consistency explicitly but enforces the object to be pasted at places with similar background pattern on the original image. With such tight constraint, although some configurations that are semantic consistent but present a different background pattern may be pruned, we can guarantee that the generated images are visually coherent in most cases. Compared to <ref type="bibr" target="#b13">[15]</ref>, our method can generate images that is more photorealistic and displays less blending artifacts, therefore introducing less noise when training the neural networks. Experimental results (Sec. <ref type="bibr">4.4,</ref><ref type="bibr">Sec. 4.5)</ref> show the superior performance of our method in both qualitative and quantitative manner, while having a much more efficient implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Performance of models on both bounding box detection and instance segmentation has been evaluated on popular benchmarks, including Pascal VOC <ref type="bibr" target="#b15">[17]</ref> with additional mask annotation from VOCSDS <ref type="bibr" target="#b20">[22]</ref> and COCO <ref type="bibr" target="#b31">[33]</ref> dataset. Pascal VOC and VOCSDS. The original Pascal VOC dataset contains 17,125 images in 20 semantic categories with bounding box annotation. 2,913 images are annotated with instance masks for instance segmentation and semantic segmentation tasks. In this paper we adopted additional mask annotation from VOCSDS <ref type="bibr" target="#b20">[22]</ref> with 11,355 images annotated with instance masks, following the train/test split in <ref type="bibr" target="#b26">[28]</ref> where 5,623 images for training and 5,732 for testing. COCO dataset. COCO dataset is the state-of-the-art evaluation benchmark for computer vision tasks including bounding box detection <ref type="bibr" target="#b35">[37]</ref>, instance segmentation <ref type="bibr" target="#b26">[28]</ref>, human pose estimation <ref type="bibr" target="#b17">[19]</ref> and captioning <ref type="bibr" target="#b37">[39]</ref>. COCO is a much larger-scale image set compared to Pascal VOC, with 80 categories and more than 200,000 labeled images. Objects in COCO are annotated with both bounding box and instance mask labels. It contains large amounts of small objects, complicated object-object occlusion and noisy background, and is challenging for augmentation methods to generate "fake" but visually coherent images, to fully exploit the information in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Models</head><p>Nowadays, Mask R-CNN <ref type="bibr" target="#b21">[23]</ref> based methods are widely adopted for instance segmentation [2] due to its promising performance and efficiency. In our experiment, we adopt the original Mask R-CNN <ref type="bibr" target="#b21">[23]</ref> and its variant Cascaded Mask R-CNN <ref type="bibr" target="#b5">[7]</ref> as our baseline networks. For Mask R-CNN, we experiment with both Res-50-FPN and Res-101-FPN backbones using open implementation <ref type="bibr" target="#b0">[1]</ref> while only Res-101-FPN is tested for Cascaded Mask R-CNN based on <ref type="bibr" target="#b6">[8]</ref>. Baselines are retrained using corresponding open implementations. Experimental result reveals the generalizability of our augmentation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation details</head><p>Hyperparameters on COCO For network training on COCO dataset, we adopt the default configuration provided by the authors, with only modifying the training epochs. We evaluated the network performance on 12, 24, 36 and 48 training epochs which are equivalent to 1x, 2x, 3x and 4x the default value in their configuration. The reported results in Tab. 1 and Tab. 2 are obtained using 48 training epochs. Analysis in Sec. 4.5 shows that the network improves substantially after adopting our InstaBoost while suffering from over-fitting problem without such data augmentation. Hyperparameters on VOC For Pascal VOC dataset, we only test the performance of Res-50-FPN based Mask R-CNN to evaluate the effectiveness of our algorithm. We use learning rate 5 × 10 −3 to train 20, 000 iterations, then continue training for 6, 000 iterations with 5 × 10 −4 and 4, 000 iterations with 5×10 −5 . Other hyperparameters keep unchanged according to Res-50-FPN training configuration on COCO dataset. Hyperparameters of InstaBoost For our random Insta-Boost, we need to set the range of the uniform distribution. For the translation, the range in x− and y−axis are set proportional to the width and height of the object. The ratio is set as 1/15 . For scaling, we set the range from 0. <ref type="bibr" target="#b6">8</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis</head><p>Comparison with context model We compare our method with previous state-of-the-art <ref type="bibr" target="#b13">[15]</ref> on COCO detection and instance segmentation. We adopt Res-101-FPN Mask R-CNN as the base network. Results are given in Tab. 5. It shows that our data augmentation strategy can achieve better performance on both tasks. Moreover, <ref type="bibr" target="#b13">[15]</ref> requires extra training step and offline data prepossessing before data augmentation, while our method can be integrated into the training pipeline without tedious preparation or affecting the training efficiency. Comparison with random paste To figure out the decisive role appearance consistency plays in InstaBoost, we compare our method with randomly pasting instances on the image, without overlapping with existing instances. Experiments are done on Mask R-CNN(Res-50-FPN) framework and on both VOC and COCO dataset. Tab. 6 shows a performance degradation for 1.3 and 1.1 mAP compared to the original baseline on instance segmentation task. Such results are aligned with the findings of <ref type="bibr" target="#b13">[15]</ref>. Substantial Improvement We conduct experiments to validate the performance of the network using different training epochs with and without our InstaBoost. Results are shown in <ref type="figure" target="#fig_5">Fig. 7</ref>, where InstaBoost performs a promising resistance of overfitting. Both detection and segmentation accuracy of original Mask R-CNN stop increasing when epochs reaches 24. After applying InstaBoost augmentation method, both accuracy continue going up even in large training epoch. Sensitivity analysis InstaBoost has parameters translation ratio and scaling ratio to decide the extent of the augmentation. We vary these parameters and measure AP, AP50 and AP75 of segmentation task on COCO dataset, see Tab. 4. For translation ratio, AP is stable in range 1 50 to <ref type="bibr">1 5</ref> , and drops a little when it approaches to 1. Scaling ratio is more sensitive than translation ratio, and a variation of 0.1 can cause about 0.1-0.3 drop in AP. In our experiments, we set translation ratio to <ref type="bibr">1 15</ref> and scaling ratio to 0.8-1.2. Interior-boundary study. We compared Mask R-CNN trained with/without InstaBoost, on interior and boundary masks respectively. Following the protocol introduced in <ref type="bibr" target="#b10">[12]</ref>, the interior and boundary masks are obtained from a trimap built from the edges of ground truth mask. Results in <ref type="figure" target="#fig_7">Fig. 8</ref> shows that InstaBoost improves instance segmentation accuracy on better interior detection and finer boundary prediction. The improvement on instance boundary is more significant than interior part. Readers are referred to Sec. 5.1 and <ref type="figure">Fig. 4</ref> in <ref type="bibr" target="#b10">[12]</ref> for details of this evaluation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper studies data augmentation techniques aiding the lack of training data in instance segmentation. By uniform sampling on the neighboring of identity transform in 4D transformation tuple, our simple but effective random InstaBoost achieves 1.7 mAP improvement with Mask R-CNN on COCO instance segmentation benchmark. We further devised InstaBoost with appearance consistency heatmap, reaching in total 2.2 mAP improvement on COCO instance segmentation. Our online implementation of Insta-Boost can be easily embedded into existing instance segmentation frameworks, where free-lunch improvement is offered with little CPU overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An example of random InstaBoost and appearance consistency heatmap guided InstaBoost. (a) An original image with ground truth mask label from COCO dataset. (b) The result of random InstaBoost. Multiple pastes are visualized showing the randomness. (c) Appearance consistency heatmap of this image. (d) The result of appearance consistency heatmap guided Insta-Boost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Example for inpainting and matting visualization. From left to right is original image, inpainting result and instance obtained by matting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Examples of appearance consistency heatmap. The left part of each example is the original image with an instance and the right part is the appearance consistency heatmap for that image. The red region is of high appearance consistency while the blue region is of low appearance consistency. One example of contour areas of appearance consistency heatmap.(a) The effective contour area of this image. (b) The original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Examples of appearance consistency heatmap guided InstaBoost. Each example consisted of the original image with an instance, appearance consistency heatmap and processed image from left to right. Heatmap generation. By fixing D 0 to the object's original position and scanning appearance distance d(D, D 0 ) on all feasible D in the image, a heatmap is produced w.r.t. the center positions are taken by D. Appearance distances are normalized and scaled via negative log for the heatmap H. The mapping is formulated as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Instance segmentation result of vanilla Mask R-CNN [23] (top) vs. Mask R-CNN trained with InstaBoost (bottom). InstaBoost guarantees finer instance segmentation result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Substantial Improvement of our data augmentation technique against overfitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Evaluation on interior/boundary segmentation accuracy of Mask R-CNN trained with and without InstaBoost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>to 1.2 in Method AP det AP det Object detection results on COCO test-dev, where 'vanilla' denotes baseline Mask R-CNN without InstaBoost augmentation, 'jitter' denotes random InstaBoost, and 'map guided' denotes appearance consistency heatmap guided InstaBoost. The improvement in bounding box detection is a by-product of our InstaBoost. Instance Segmentation results on COCO test-dev, where 'vanilla' denotes baseline Mask R-CNN without InstaBoost augmentation, 'jitter' denotes random InstaBoost, and 'map guided' denotes appearance consistency heatmap guided InstaBoost. With the help of InstaBoost, state-of-the-art instance segmentation models surpass their baseline models.</figDesc><table><row><cell>50</cell><cell>AP det 75</cell><cell>AP det S</cell><cell>AP det M</cell><cell>AP det L</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Object detection and instance segmentation results on VOCSDS. Translation Ratio Scaling Ratio AP seg AP seg Sensitive analysis on different hyper-parameter configurations, on COCO val set using Res-50-FPN Mask R-CNN.vanilla one generates incomplete masks or ignores the objects.</figDesc><table><row><cell>15 15 15</cell><cell>0.7-1.3 0.8-1.2 0.9-1.1</cell><cell>34.85 35.10 34.98</cell><cell>50 56.63 56.87 56.49</cell><cell>AP seg 75 36.86 37.21 37.03</cell></row><row><cell>1 5 15 50</cell><cell>0.8-1.2 0.8-1.2 0.8-1.2 0.8-1.2</cell><cell>34.51 34.99 35.10 35.02</cell><cell>55.85 56.51 56.87 56.59</cell><cell>36.74 37.02 37.21 37.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>MethodAP bb AP seg Train speed(s/iter) Comparison against context based model<ref type="bibr" target="#b13">[15]</ref> on Mask R-CNN. Experimental result shows the superiority of our model in both accuracy improvement and computational overhead introduced to the running speed.</figDesc><table><row><cell>vanilla context[15] ours</cell><cell>38.2 38.8 43.0</cell><cell>35.7 36.2 37.9</cell><cell></cell><cell>1.68 -1.71</cell></row><row><cell>Dataset</cell><cell cols="2">Method</cell><cell cols="2">AP bb AP seg</cell></row><row><cell>VOC VOC VOC</cell><cell cols="3">vanilla random paste 36.89 38.06 ours 42.23</cell><cell>38.88 37.58 42.73</cell></row><row><cell cols="5">COCO COCO random paste vanilla COCO ours Table 6. Comparison against random paste on Mask R-CNN(Res-37.6 33.8 36.1 32.7 40.5 36.0</cell></row><row><cell cols="5">50-FPN). Experimental result shows appearance consistency guid-</cell></row><row><cell>ance is essential.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">The feasible transformation of (x, y) coordinates is restricted in the neighborhood of (x 0 , y 0 ) in random Insta-Boost, whose performance could be further elevated with a more complicated metric on the image, i.e. appearance consistency heatmap, to better refine the position where the new instance is pasted. Regarded as one implementation of the probability metric in Eq. (3), appearance consistency heatmap evaluates similarity on the RGB space, between any transformation (x, y) with respect to (x 0 , y 0 ). Examples of appearance consistency heatmap on COCO<ref type="bibr" target="#b31">[33]</ref> dataset are shown inFig. 3. Each example inFig. 3consists of two images, the left image is the original image from COCO dataset and the right one is the corresponding appearance consistency heatmap.We derive f (·) in Eq. (3) as three conditional probability</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported in part by the National Key R&amp;D Program of China, No. 2017YFA0700800, National Natural Science Foundation of China under Grants 61772332.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron</surname></persName>
		</author>
		<ptr target="https://github.com/roytseng-tw/Detectron.pytorch" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rudolf Arnheim. Visual thinking</title>
		<imprint>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Whats the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Navier-stokes, fluid dynamics, and image and video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">L</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Watershed-based segmentation and region merging. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Bleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L Joshua</forename><surname>Leon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmdetection" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning segmentation masks with the independence prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songmin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04682</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling visual context is key to augmenting object detection datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cut, paste and learn: Surprisingly easy synthesis for instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Weakly and semi supervised human body part parsing via pose-guided knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Contour integration by the human visual system: evidence for a local association field. Vision research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Field</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert F</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hess</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Look into person: Self-supervised structuresensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cooperative holistic scene understanding: Unifying 3d object, layout, and camera pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxue</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Complex background subtraction by pursuing dynamic spatio-temporal models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">See more, know more: Unsupervised video object segmentation with co-attention siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segmentation of unknown objects in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Richtsfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mörwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Prankl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zillich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Find and focus: Retrieve and localize video events with natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Visual perception: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Swanston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09146</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Saliency-aware video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="33" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Srda: Generating instance segmentation annotation via scanning, reasoning and domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Explicit shape encoding for real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fubo</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04067</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A stochastic grammar of images. Foundations and Trends R in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mumford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
