<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
							<email>dangwei.li@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CRIPAC &amp; NLPR</orgName>
								<orgName type="institution" key="instit2">CASIA</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
							<email>xtchen@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CRIPAC &amp; NLPR</orgName>
								<orgName type="institution" key="instit2">CASIA</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
							<email>zzhang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CRIPAC &amp; NLPR</orgName>
								<orgName type="institution" key="instit2">CASIA</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
							<email>kaiqi.huang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CRIPAC &amp; NLPR</orgName>
								<orgName type="institution" key="instit2">CASIA</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person Re-identification (ReID) is to identify the same person across different cameras. It is a challenging task due to the large variations in person pose, occlusion, background clutter, etc. How to extract powerful features is a fundamental problem in ReID and is still an open problem today. In this paper, we design a Multi-Scale Context-Aware Network (MSCAN) to learn powerful features over full body and body parts, which can well capture the local context knowledge by stacking multi-scale convolutions in each layer. Moreover, instead of using predefined rigid parts, we propose to learn and localize deformable pedestrian parts using Spatial Transformer Networks (STN) with novel spatial constraints. The learned body parts can release some difficulties, e.g. pose variations and background clutters, in part-based representation. Finally, we integrate the representation learning processes of full body and body parts into a unified framework for person ReID through multi-class person identification tasks. Extensive evaluations on current challenging large-scale person ReID datasets, including the image-based Market1501, CUHK03 and sequence-based MARS datasets, show that the proposed method achieves the state-of-the-art results. Conv Conv Conv FC Conv Conv Conv FC Conv Conv Conv Conv Conv Conv Conv Conv Conv Conv Conv Conv Conv FC Latent Part Localization MSCAN FC FC Concat Concat Concat Concat FC FC MSCAN MSCAN FC Concat MSCAN Full body Rigid body parts Ours FC Conv FC Concat FC submit</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification aims to search for the same person across different cameras with a given probe image. It has attracted much attention in recent years due to its importance in many practical applications, such as video surveillance and content-based image retrieval. Despite of years of efforts, it still has many challenges, such as large variations in person pose, illumination, and background clutter. In addition, similar appearance of clothes among different people and imperfect pedestrian detection results further increase its difficulty in real applications.</p><p>Most existing methods for ReID focus on developing a powerful representation to handle the variations of view- <ref type="figure">Figure 1</ref>. The schematic of typical feature learning framework with deep learning. As shown in black dashed boxes, current approaches focus on the full body or rigid body parts for feature learning. Different from them, we use the spatial transformer networks to learn and localize pedestrian parts and use multiscale context-aware convolutional networks to extract full-body and body-parts representations for ReID. Best viewed in color. point, body pose, background clutter, etc. <ref type="bibr">[7, 10, 18, 19, 22, 27, 41-43, 50, 51]</ref>, or learning an effective distance metric <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b56">57]</ref>. Some of existing methods learn both of them jointly <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref>. Recently, deep feature learning based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, which learn a global pedestrian feature and use Euclidean metric to measure two samples, have obtained the state-of-the-art results. With the increasing sample size of ReID dataset, the learning of features from multi-class person identification tasks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref>, denoted as ID-discriminative Embedding (IDE) <ref type="bibr" target="#b54">[55]</ref>, has shown great potentials on current large-scale person ReID datasets, such as MARS <ref type="bibr" target="#b51">[52]</ref> and PRW <ref type="bibr" target="#b54">[55]</ref>, where the IDE features are taken from the last hidden layer of Deep Convolutional Neural Networks (DCNN). In this paper, we aim to learn the IDE feature for person ReID using DCNN.</p><p>Existing DCNN models for person ReID typically learn a global full-body representation for input person image arXiv:1710.06555v1 [cs.CV] <ref type="bibr" target="#b17">18</ref> Oct 2017 (Full body in <ref type="figure">Figure 1</ref>), or learn a part-based representation for predefined rigid parts (Rigid body parts in <ref type="figure">Figure 1</ref>) or learn a feature embedding for both of them. Although these DCNN models have obtained impressive results on existing ReID datasets, there are still two problems. First, for feature learning, current popular DCNN models typically stack single-scale convolution and max pooling layers to generate deep networks. With the increase of the number of layers, these DCNN models could easily miss some small scale visual cues, such as sunglasses and shoes. However, these fine-grained attributes are very useful to distinguish the pedestrian pairs with small inter-class variations. Thus these DCNN models are not the best choice for pedestrian feature learning. Second, due to the pose variations and imperfect pedestrian detectors, the pedestrian image samples may be misaligned. Sometimes they may have some backgrounds or lack some parts, e.g. legs. In these cases, for part-based representation, the predefined rigid grids may fail to capture correct correspondence between two pedestrian images. Thus the rigid predefined grids are far from robust for effective part-based feature learning.</p><p>In this paper, we propose to learn the features of full body and body parts jointly. To solve the first problem, we propose a Multi-Scale Context-Aware Network (MSCAN). As shown in <ref type="figure">Figure 1</ref>, for each convolutional layer of the MSCAN, we adopt multiple convolution kernels with different receptive fields to obtain multiple feature maps. Feature maps from different convolution kernels are concatenated as current layer's output. To decrease the correlations among different convolution kernels, the dilated convolution <ref type="bibr" target="#b44">[45]</ref> is used rather than general convolution kernels. Through this way, multi-scale context knowledge is obtained at the same layer. Thus the local visual cues for finegrained discrimination is enhanced. In addition, through embedding contextual features layer-by-layer (convolution operation across layers), MSCAN can obtain more contextaware representation for input image. To solve the second problem, instead of using rigid body parts, we propose to localize latent pedestrian parts through Spatial Transform Networks (STN) <ref type="bibr" target="#b12">[13]</ref>, which is originally proposed to learn image transformation. To adapt it to the pedestrian part localization task, we propose three new constraints on the learned transformation parameters. With these constraints, more flexible parts can be localized at the informative regions, so as to reduce the distraction of background contents.</p><p>Generally, the features of full body and body parts are complementary to each other. The full-body features pay more attention to the global information while the body-part features care more about the local regions. To better utilize these two types of representations, in this paper, features of full body and body parts are concatenated to form the final pedestrian representation. In test stage, the Euclidean metric is adopted to compute the distance between two L2 normalized person representations for person ReID.</p><p>The contributions of this paper are summarized as follows: (a) We propose a multi-scale context-aware network to enhance the visual context information for better feature representation of fine-grained visual cues. (b) Instead of using rigid parts, we propose to learn and localize pedestrian parts using spatial transformer networks with novel prior spatial constraints. Experimental results show that fusing the global full-body and local body-part representations greatly improves the performance of person ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Typical person ReID methods focus on two key points: developing a powerful feature for image representation and learning an effective metric to make the same person be close and different persons far away. Recently, deep learning approaches have achieved the state-of-the-art results for person ReID <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>. Here we mainly review the related deep learning methods.</p><p>Deep learning approaches for person ReID tend to learn person representation and similarity (distance) metric jointly. Given a pair of person images, previous deep learning approaches learn each person's features followed by a deep matching function from the convolutional features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20]</ref> or the Fully Connected (FC) features <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref>. In addition to the deep metric learning, some work directly learns image representation through pair-wise contrastive loss or triplet ranking loss, and use Euclidean metric for comparison <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>With the increasing sample size of ReID dataset, the IDE feature which is learned through multi-class person identification tasks, has shown great potentials on current largescale person ReID datasets. Xiao et al. <ref type="bibr" target="#b38">[39]</ref> propose the domain guided dropout to learn features over multiple datasets simultaneously with identity classification loss. Zheng et al. <ref type="bibr" target="#b51">[52]</ref> learn the IDE feature for the video-based person reidentification. Xiao et al. <ref type="bibr" target="#b39">[40]</ref> and Zheng et al. <ref type="bibr" target="#b54">[55]</ref> learn the IDE feature to jointly solve the pedestrian detection and person ReID tasks. Schumann et al. <ref type="bibr" target="#b29">[30]</ref> learn the IDE feature for domain adaptive person ReID. The similar phenomenon has also been validated on face recognition <ref type="bibr" target="#b32">[33]</ref>.</p><p>As we know, previous DCNN models usually adopt the layer-by-layer single-scale convolution kernels to learn the context information. Some DCNN models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref> adopt rigid body parts to learn local pedestrian features. Different from them, we improve the classical models in two ways. Firstly, we propose to enhance the context knowledge through multi-scale convolutions at the same layer. The relationship among different context knowledge are learned by embedding feature maps layer-by-layer (convolution or FC operation). Secondly, instead of using rigid parts, we utilize the spatial transformer networks with proposed prior constraints to learn and localize latent human parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>The focus of this approach is to learn powerful feature representations to describe pedestrians. The overall framework of the proposed method is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In this section, we introduce our model from four aspects: a multiscale context-aware network for efficient feature learning (Section 3.1), the latent parts learning and localization for better local part-based feature representation (Section 3.2), the fusion of global full-body and local body-part features for person ReID (Section 3.3), and our final objective function in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-scale Context-aware Network</head><p>Visual context is an important component to assist visual-related tasks, such as object recognition <ref type="bibr" target="#b23">[24]</ref> and object detection <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b55">56]</ref>. Typical convolutional neural networks model context information through hierarchical convolution and pooling <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>. For person ReID task, the most important visual cues are visual attribute knowledge, such as clothes color and types. However, they have large variations in scale, shape and position, such as the hat/glasses at small local scale and the cloth color at the larger scale. Directly using bottom-to-up single-scale convolution and pooling may not be effective to handle these complex variations. Especially, with the increase number of layers, the small visual regions, such as hat, will be easily missed in top layers. To better learn these diverse visual cues, we propose the Multi-scale Context-Aware Network. The architecture of the proposed MSCAN is shown in Tabel 1. It has an initial convolution layer with kernel size 5 × 5 to capture the low-level visual features. Then we use four multi-scale convolution layers to obtain the complex image context information. In each multi-scale convolution layer, we use a convolution kernel with size 3 × 3. To obtain multi-scale receptive fields, we adopt dilated convolution <ref type="bibr" target="#b44">[45]</ref> for the convolution filters. We use three different dilation ratios, i.e. 1,2 and 3, to capture different scale context information. The feature maps from different dilation ratios are concatenated along the channel axis to form the final output of the current convolution layer. Thus, the visual context information are enhanced explicitly. To integrate different context information together, the feature maps of current convolution layer are embedded through layer-bylayer convolution or FC operation. As a result, the visual cues at different scales are fused in a latent way. Besides, we adopt Batch Normalization <ref type="bibr" target="#b11">[12]</ref> and ReLU neural activation units after each convolution layer.</p><formula xml:id="formula_0">layer dilation kernel pad #filters output input - - - - 3×160×64 conv0 1 5×5 2 32 32×160×64 pool0 - 2×2 - - 32×80×32 conv1 1/2/3 3×3 1/2/3 32/32/32 96×80×32 pool1 - 2×2 - - 96×40×16 conv2 1/2/3 3×3 1/2/3 32/32/32 96×40×16 pool2 - 2×2 - - 96×20×8 conv3 1/2/3 3×3 1/2/3 32/32/32 96×20×8 pool3 - 2×2 - - 96×10×4 conv4 1/2/3 3×3 1/2/3 32/32/32 96×10×4 pool4 - 2×2 - - 96×5×2</formula><p>In this paper, we use the dilated convolutions with dilation ratios 1, 2 and 3 instead of the classic convolution filters with kernel size 3 × 3, 5 × 5 and 7 × 7. The main reason is that the classic convolution filters with kernel size 3×3, 5×5 and 7×7 overlap with each other at the same output position and produce redundant information. To make it clearer, we show the dilated convolution kernel (size 3 × 3) with dilation ratio ranging from 1 to 3 in <ref type="figure" target="#fig_1">Figure 3</ref>. For the same output position which is shown in red circle, the convolution kernel with larger dilation ratio has larger receptive field, while only the center position is overlapped with other convolution kernels. This can reduce the redundant information among filters with different receptive fields.</p><p>In summary, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, we use MSCAN to learn the multi-scale context representation for full body and body parts. In addition, it is also used for feature learning in spatial transformer networks mentioned below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Latent Part Localization</head><p>Pedestrian parts are important in person ReID. Some existing work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref> has explored rigid body parts to develop robust features. However, due to the unsatisfying pedestrian detection algorithms and large pose variations, the method of using rigid body parts for local feature learning is not the optimal solution. As shown in <ref type="figure">Figure 1</ref>, when using rigid body parts, the top part consists of large amount of background. This motivates us to learn and localize the pedestrian parts automatically.</p><p>We integrate STN <ref type="bibr" target="#b12">[13]</ref> as the part localization net in our proposed model. The original STN is proposed to explicitly learn the image transformation parameters, such as translation and scale. It has two main advantages: (1) it is fully differentiable and can be easily integrated into existing deep learning frameworks, (2) it can learn to translate, scale, crop or warp an interesting region without explicit region annotations. These facts make it very suitable for pedestrian parts localization.</p><p>STN includes two components, the spatial localization network to learn the transformation parameters, and the grid generator to sample the input image using an image interpolation kernel. More details about STN can be seen in <ref type="bibr" target="#b12">[13]</ref>. In our implementation of STN, the bilinear interpolation kernel is adopted to sample the input image. And four transformation parameters θ = [s x , t x , s y , t y ] are used, where s x and s y are the horizontal and vertical scale transformation parameters, and t x and t y are the horizontal and vertical  . Only scale and translation parameters are learned because these two types of transformations serve enough to crop the pedestrian parts effectively. The transformation is applied as an inverse warping to generate the output body part regions:</p><formula xml:id="formula_1">x in i y in i = s x 0 t x 0 s y t y   x out i y out i 1  <label>(1)</label></formula><p>where x in and y in are the input image coordinates, x out and y out are the output part image coordinates, and i indexes the pixels in the output body part image.</p><p>In this paper, we expect STN to learn three parts corresponding to the head-shoulder, upper body and lower body. Each part is learned by an independent STN from the original pedestrian image. For the spatial localization network, firstly we use MSCAN to extract the global image feature maps. Then we learn the high-level abstract representation by a 128-dimension FC layer (FC loc in <ref type="figure" target="#fig_0">Figure 2</ref>). At last, we learn the transformation parameters θ with a 4-dimension FC layer based on the FC loc. The MSCAN and FC loc are shared among three spatial localization networks. The grid generator can crop the learned pedestrian parts based on the learned transformation parameters. In this paper, the resolution of the cropped part image is 96 × 64.</p><p>For the part localization network, it is hard to learn three groups of parameters for part localization. There are three problems. First, the predicted parts from STN can easily fall into the same region, e.g., the center region of a person, and result in redundance. Second, the scale parameters can easily become negative and the pedestrian part will be mirrored vertically or horizontally or both. This is not consistent with general human cognition. Because few person will stand upside down in surveillance scenes. At last, the cropped parts may fall out of the person image, thus the network would be hard to converge. To solve the above problems, we propose three prior constraints on the transformation parameters in the part localization network.</p><p>The first constraint is for the positions of predicted parts. We expect the predicted parts to be near the prior center points, so that the learned parts would be complementary to each other. This is termed as the center constraint, which is formalized as follows:</p><formula xml:id="formula_2">L cen = 1 2 max{0, (t x − C x ) 2 + (t y − C y ) 2 − α} (2)</formula><p>where C x and C y are prior center points for each part. α is the threshold to control the translation between estimated and prior center points. In our experiments, we set the prior center point (C x , C y ) to (0, 0.6), (0, 0), and (0, −0.6) for each part. The threshold α is set to 0.5.</p><p>The second one is the value range constraint on the predicted scale parameter. We hope the scale to be positive, so that the predicted parts have a reasonable extent. The value range constraint on the scale parameter is formalized as follows:</p><formula xml:id="formula_3">L pos = max{0, β − s x } + max{0, β − s y }<label>(3)</label></formula><p>where β is threshold parameter and is set to 0.1 in this paper. The last one is to make the localization network focus on the inner region of an image. It is formalized as follows:</p><formula xml:id="formula_4">L in = 1 2 max{0, ||s x ± t x || 2 − γ} + 1 2 max{0, ||s y ± t y || 2 − γ}<label>(4)</label></formula><p>where γ is the boundary parameter. γ is set to 1.0 in our paper, which means the cropped parts should be inside the pedestrian image. Finally the loss for the transformation parameters in the part localization network is described as follows:</p><formula xml:id="formula_5">L loc = L cen + ξ 1 L pos + ξ 2 L in<label>(5)</label></formula><p>where ξ 1 and ξ 2 are hyperparameters. The hyperparameters ξ 1 and ξ 2 are both set to 1.0 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Extraction and Fusion</head><p>The features of full body and body parts are learned by separate networks and then are fused in a unified framework for multi-class person identification tasks. For the body-based representation, we use MSCAN to extract the global feature maps and then learn a 128-dimension feature embedding (denoted as FC body in <ref type="figure" target="#fig_0">Figure 2</ref>). For the part-based representation, first, for each body part, we use the MSCAN to extract its feature maps and learn a 64-dimension feature embedding (denoted as FC part1, FC part2, FC part3). Then, we learn a 128-dimension feature embedding (denoted as FC part) based on features of each body part. The Dropout <ref type="bibr" target="#b31">[32]</ref> is adopted after each FC layer to prevent overfitting. At last, the features of global full body and local body parts are concatenated to be a 256dimension feature as the final person representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective Function</head><p>In this paper, we adopt the softmax loss as the objective function for multi-class person identification tasks.</p><formula xml:id="formula_6">L cls = − N i=1 log exp(W T yi x i + b yi ) C j=1 exp(W T j x i + b j )<label>(6)</label></formula><p>where i is the index of person images, x i is the feature of ith sample, y i is the identity of i-th sample, N is the number of person images, C is the number of person identities, W j is the classifier for j-th identity.</p><p>For the overall network training, we use the classification and localization loss jointly. The final objective function is as follows. L = L cls + λL loc <ref type="bibr" target="#b6">(7)</ref> where the λ is the hyperparameter, which is set to 0.1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this paragraph, the datasets and evaluation protocols are introduced in Section 4.1. Implementation details are described in Section 4.2. Comparisons with state-of-the-art methods are discussed in Section 4.3. The effectiveness of proposed model is analyzed in Section 4.4 and Section 4.5. Cross-dataset evaluation is described in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Protocols</head><p>Datasets. In this paper, we evaluate our proposed method on current largest person ReID datasets, including Market1501 <ref type="bibr" target="#b52">[53]</ref>, CUHK03 <ref type="bibr" target="#b19">[20]</ref> and MARS <ref type="bibr" target="#b51">[52]</ref>. We do not directly train our model on small datasets, such as VIPeR <ref type="bibr" target="#b8">[9]</ref>. It would be easily overfitting and insufficient to learn such a large capacity network on small datasets from scratch. However, we give some results through finetuneing the model from Market1501 to VIPeR and make cross-dataset ReID on VIPeR for generalization validation. Related experimental results are discussed in Section 4.6.</p><p>Market1501: It contains 1,501 identities which are captured by six manually set cameras. There are 32,368 pedestrian images in total. Each person has 3.6 images on average at each viewpoint. It provides two types of images, including cropped and automatically detected pedestrians by the Deformable Part based Model (DPM) <ref type="bibr" target="#b7">[8]</ref>. Following <ref type="bibr" target="#b52">[53]</ref>, 751 identities are used for training and the rest 750 identities are used for testing. CUHK03: It contains 1,360 identities which are captured by six surveillance cameras in campus. Each identity is captured by two disjoint cameras. Totally it consists of 13,164 person images and each identity has about 4.8 images at each viewpoint. This dataset provides two types of annotations, including manually annotated bounding boxes, and bounding boxes detected using DPM. We validate our proposed model on both types of data. Following <ref type="bibr" target="#b19">[20]</ref>, we use 1,260 person identities for training and the rest 100 identities for testing. Experiments are conducted 20 times and the mean result is reported.</p><p>MARS: It is the largest sequence-based person ReID dataset. It contains 1,261 identities with each identity captured by at least two cameras. It consists of 20,478 tracklets and 1,191,003 bounding boxes. Following <ref type="bibr" target="#b51">[52]</ref>, we use 625 identities for training and the rest 631 identities for testing.</p><p>Protocols. Following original evaluation protocols in each dataset, we adopt three evaluation protocols for fair comparison with existing methods. The first one is Cumulated Matching Characteristics (CMC) which is adopted on the CUHK03 and MARS datasets. The second one is Rank-1 identification rate on the Market1501 dataset. The third one is mean Average Precision (mAP) on the Market1501 and MARS datasets. mAP considers both precision and recall rate, which could be complementary to CMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Model: We try to learn the pedestrian representation through multi-class person identification tasks using full body and body parts. To evaluate the effectiveness of full body and body parts independently, we extract two submodels from the whole network of <ref type="figure" target="#fig_0">Figure 2</ref>. The first one only uses the full body to learn the person representation with identity classification loss. The second one only uses the parts to learn the person representation with identity classification and body parts localization loss. For person re-identification, we use the L2 normalized person representation and Euclidean metric to measure the distance between two pedestrian samples.</p><p>Optimization: Our model is implemented based on Caffe <ref type="bibr" target="#b13">[14]</ref>. We use all the available training identities for training and randomly select one sample for each identity for validation. As the dataset can be quite large, in practice we use a stochastic approximation of the objective function. Training data is randomly divided into mini-batches with a batch size of 64. The model performs forward propagation on each mini-batch and computes the loss. Backpropagation is then used to compute the gradients on each minibatch and the weights are updated with stochastic gradient descent. We start with a base learning rate of η = 0.01 and gradually decrease it after each 1 × 10 4 iterations. It should be noted that the learning rate of part localization network is 1% of that in feature learning network. We use a momentum of µ = 0.9 and weight decay λ = 5×10 −3 . For overall network training, we initialize the network using pretrained body-based and part-based model and then follow the same training strategy as described above. We use the model at 5 × 10 4 iterations for testing.</p><p>Data Preprocessing: For each image, we resize it to 160×64, subtract the mean value on each channel (B, G and R), and then normalize it with scale 1.0/256 for network training. To prevent overfitting, we randomly reflect each image horizontally in the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-art Methods</head><p>Market1501: For the Market1501 dataset, several state-of-the-art methods are compared, including Bag of Words (BOW) <ref type="bibr" target="#b52">[53]</ref>, Weighted Approximate Rank Component Analysis (WARCA) <ref type="bibr" target="#b14">[15]</ref>, Discriminative Null Space (DNS) <ref type="bibr" target="#b46">[47]</ref>, Spatially Constrained Similarity function on Polynomial feature map (SCSP) <ref type="bibr" target="#b1">[2]</ref>, and deep learning based approaches, such as PersonNet <ref type="bibr" target="#b37">[38]</ref>, Comparative Attention Network (CAN) <ref type="bibr" target="#b24">[25]</ref>, Siamese Long Short-Term Memory (S-LSTM) <ref type="bibr" target="#b34">[35]</ref>, Gated Siamese Convolutional Neural Network (Gate-SCNN) <ref type="bibr" target="#b33">[34]</ref>. The experimental results are shown in <ref type="table">Table 2</ref>. Compared with existing full body-based convolutional neural networks, such as CAN and Gate-SCNN, the proposed network structure can better capture pedestrian features with multi-class person identification tasks. Our fullbody representation improves Rank-1 identification rate by 9.57% on the state-of-the-art results produced by the Gate-CNN in single query. Compared with the full body, our body-part representation increase 0.80%. The main reason is that the pedestrians detected by DPM consists much more background information and the part-based representation can better reduce the influences of background clutter.</p><p>The full-body and body-part representations are complementary to each other. The full-body representation cares more about the global information, such as the background and body shape. The body-part representation pays more attention to parts, such as head, upper body and lower body. As shown in <ref type="table">Table 2</ref>, the fusion model of full body and body parts improves Rank-1 identification rate by more than 4.00% compared with the body and parts-based models separately in single query. The mAP improves about 17.98% compared with the best result produced by Gate-CNN.</p><p>CUHK03: For the CUHK03 dataset, we compare our method with many existing approaches, including Filter Pair Neural Networks (FPNN) <ref type="bibr" target="#b19">[20]</ref>, Improved Deep Learning Architecture (IDLA) <ref type="bibr" target="#b0">[1]</ref>, Cross-view Quadratic Discriminant Analysis (XQDA) <ref type="bibr" target="#b21">[22]</ref>, PSD constrained asymmetric metric learning (denoted as MLAPG) <ref type="bibr" target="#b22">[23]</ref>, Sample-Specific SVM (SS) <ref type="bibr" target="#b48">[49]</ref>, Single image and Cross image representation (SI-CI) <ref type="bibr" target="#b35">[36]</ref>, Embedding Deep Metric (EDM) <ref type="bibr" target="#b30">[31]</ref>, Domain Guided Dropout (DGD) <ref type="bibr" target="#b38">[39]</ref>, DNS, S-LSTM and Gate-SCNN. On this dataset, we conduct experiments on both the detected and the labeled datasets. As presented in previous work <ref type="bibr" target="#b19">[20]</ref>, we use the CMC curve in the single shot case to evaluate the performance. The overall results are shown in the <ref type="table" target="#tab_3">Table 3 and Table 4</ref>   Compared with metric learning methods, such as the state-of-the-art approach DNS, the proposed fusion model improves the Rank-1 identification rate by 11.66% and 13.29% on the labeled and detected datasets respectively. Compared with the similar multi-class person identification network DGD, the Rank-1 identification rate improves by 1.63% using our fusion model on the labeled dataset. It should be noted that we only use the labeled sets for training, while the DGD is trained on both the labeled and detected datasets. This demonstrates the effectiveness of the proposed model.</p><p>MARS: This dataset is the largest sequence-based person ReID dataset. On this dataset, we compare the proposed method with several classical methods, including Keep It as Simple and straightforward Metric (KISSME) <ref type="bibr" target="#b15">[16]</ref>, XQDA <ref type="bibr" target="#b21">[22]</ref>, and CaffeNet <ref type="bibr" target="#b16">[17]</ref>. Similar to previous work <ref type="bibr" target="#b51">[52]</ref>, both single query and multiple query are evaluated on MARS. The overall experimental results on the MARS are shown in <ref type="table">Table 5</ref> and  trates the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Effectiveness of MSCAN</head><p>To determine the effectiveness of MSCAN, we explore four variants of MSCANs to learn IDE feature based on the whole body image, which is denoted as MSCAN-k, k = {1, 2, 3, 4}. k is the number of dilation ratios. For example, MSCAN-3 means for each convolution layer in Conv1-Conv4, there are three convolution kernels with dilation ratio 1, 2, and 3 respectively. With the increase of k, the MSCAN captures larger context information at the same convolution layer.</p><p>The experimental results based on these four types of MSCANs on the Market1501 dataset are shown in <ref type="table" target="#tab_8">Table 7</ref>. As we can see, with the increase of the number of dilation ratios, the Rank-1 identification rate and mAP improve stably in single query case. For multiple query case, which means fusing all images belonging to the same query person at the same camera through average pooling in feature space, the Rank-1 identification rate and mAP also improves step by step. However, the Rank-1 identification rate and mAP increase not much when K increase from 3 to 4. We think there is a suitable number of dilation ratios for feature learning. Considering the model complexity and accuracy improvements in Rank-1 identification rate, we adopt the MSCAN-3 as our final MSCAN model in this paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Effectiveness of Latent Part Localization</head><p>Learned parts vs. rigid parts To compare with popular rigid pedestrian parts, we divide the pedestrian into three  overlapped regions as predefined rigid parts. We use the rigid body parts instead of the learned latent body parts for part-based feature learning. Experimental results with rigid and learned body parts are shown in <ref type="table" target="#tab_10">Table 8</ref>. Compared with rigid body parts, the learned body parts improve Rank-1 identification rate and mAP by 3.27% and 3.73% in single query, and by 1.70% and by 2.67% in multiple query. This validate the effectiveness of learned person parts.</p><p>For better understanding the learned pedestrian parts, we visualize the localized latent parts in <ref type="figure" target="#fig_3">Figure 4</ref> using our fusion model. For these detected person with large background (the first row in <ref type="figure" target="#fig_3">Figure 4</ref>), the proposed model can learn foreground information with complementary latent pedestrian parts. As we can see, the learned parts consist of three main components, including upper body, middle body (combination of upper body and lower body), and lower body. Similar results can be achieved when original detection pedestrians contain less background or occlusion (the second row in <ref type="figure" target="#fig_3">Figure 4</ref>). It is easy to see that, the automatically learned pedestrian parts are not strictly head-shoulder, upper body and lower-body. But it indeed consists of these three parts with large overlap. Compared with rigid parts, the proposed model can automatically localize the appropriate latent parts for feature learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of localization loss</head><p>To evaluate the effectiveness of the proposed constraints on the latent part localization network, we conduct additional experiments by adding or deleting proposed L loc in the training stage of body parts network for ReID. Experimental results are shown in <ref type="table" target="#tab_12">Table 9</ref>. As we can see, with the additional L loc , the Rank-1 accuracy increases by 9.03%. We owe the improvements to the effectiveness of the proposed constraints on the part localization network.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Cross-dataset Evaluation</head><p>Similar with typical image classification task with CNN, our approach requires large scale of data, not only more identities, but also more instances for each identity. So we do not train the proposed model on each single small person ReID dataset, such as VIPeR. Instead, we conduct cross-dataset evaluation from the pretrained model on the Market1501, CUHK03 and MARS datasets to the VIPeR dataset. The experimental results are shown in <ref type="table" target="#tab_0">Table 10</ref>. Compared with other methods, such as Domain Transfer Rank Support Vector Machines <ref type="bibr" target="#b25">[26]</ref> and DML <ref type="bibr" target="#b43">[44]</ref>, the models trained on large-scale datasets have better generalization ability and have better Rank-1 identification rate.</p><p>To take further analysis of the proposed method, we also fine-tune the model from large dataset Market1501 to small dataset VIPeR. Experimental results are shown in <ref type="table" target="#tab_0">Table 11</ref>. Our fusion-based model obtains better Rank-1 identification rate than existing deep models, e.g. IDLA <ref type="bibr" target="#b0">[1]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have studied the problem of person ReID in three levels: 1) a multi-scale context-aware network to capture the context knowledge for pedestrian feature learning, 2) three novel constraints on STN for effective latent parts localization and body-part feature representation, 3) the fusion of full-body and body-part identity discriminative features for powerful pedestrian representation. We have validated the effectiveness of the proposed method on current large-scale person ReID datasets. Experimental results have demonstrated that the proposed method achieves the state-of-the-art results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overall framework of the proposed model. The proposed model consists three components: the global body-based feature learning with MSCAN, the latent pedestrian parts localization with spatial transformer networks and local part-based feature embedding, the fusion of full body and body parts for multi-class person identification tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Example of dilated convolution for the same input feature map. The convolutional kernel is 3 × 3 and the dilation ratio from left to right is 1, 2, and 3. The blue boxes are effective positions for convolution at the red circle. Best viewed in color. translation parameters. The image height and width are normalized to be in [−1, 1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Original</head><label></label><figDesc>Rigid Latent Original Rigid Latent Original Rigid Latent</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Six samples of original image, rigid predefined parts and learned latent pedestrian parts. Samples in each column are the same person with different backgrounds. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table /><note>Model architecture of MSCAN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. The full CMC curves are shown in supplementary materials.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">CUHK03 detected</cell><cell></cell></row><row><cell>Rank</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>FPNN [20]</cell><cell>19.89</cell><cell>50.00</cell><cell>64.00</cell><cell>78.50</cell></row><row><cell>IDLA [1]</cell><cell>44.96</cell><cell>76.01</cell><cell>83.47</cell><cell>93.15</cell></row><row><cell>XQDA [22]</cell><cell>46.25</cell><cell>78.90</cell><cell>88.55</cell><cell>94.25</cell></row><row><cell>MLAPG [23]</cell><cell>51.15</cell><cell>83.55</cell><cell>92.05</cell><cell>96.90</cell></row><row><cell>SS-SVM [49]</cell><cell>51.20</cell><cell>80.80</cell><cell>89.60</cell><cell>95.50</cell></row><row><cell>SI-CI [36]</cell><cell>52.17</cell><cell>84.30</cell><cell>92.30</cell><cell>95.00</cell></row><row><cell>DNS [47]</cell><cell>54.70</cell><cell>84.75</cell><cell>94.80</cell><cell>95.20</cell></row><row><cell>S-LSTM [35]</cell><cell>57.30</cell><cell>80.10</cell><cell>88.30</cell><cell>-</cell></row><row><cell>Gate-SCNN [34]</cell><cell>61.80</cell><cell>80.90</cell><cell>88.30</cell><cell>-</cell></row><row><cell>EDM [31]</cell><cell>52.09</cell><cell>82.87</cell><cell>91.78</cell><cell>97.17</cell></row><row><cell>Our-Part</cell><cell>62.74</cell><cell>88.53</cell><cell>93.97</cell><cell>97.21</cell></row><row><cell>Our-Body</cell><cell>64.95</cell><cell>89.82</cell><cell>94.58</cell><cell>97.56</cell></row><row><cell>Our-Fusion</cell><cell>67.99</cell><cell>91.04</cell><cell>95.36</cell><cell>97.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Experimental results on the CUHK03 detected dataset.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">CUHK03 labeled</cell><cell></cell></row><row><cell>Rank</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>FPNN [20]</cell><cell>20.65</cell><cell>51.50</cell><cell>66.50</cell><cell>80.00</cell></row><row><cell>IDLA [1]</cell><cell>54.74</cell><cell>86.50</cell><cell>93.88</cell><cell>98.10</cell></row><row><cell>XQDA [22]</cell><cell>52.20</cell><cell>82.23</cell><cell>92.14</cell><cell>96.25</cell></row><row><cell>MLAPG [23]</cell><cell>57.96</cell><cell>87.09</cell><cell>94.74</cell><cell>98.00</cell></row><row><cell>Ensemble [28]</cell><cell>62.10</cell><cell>89.10</cell><cell>94.80</cell><cell>98.10</cell></row><row><cell>SS-SVM [49]</cell><cell>57.00</cell><cell>85.70</cell><cell>94.30</cell><cell>97.80</cell></row><row><cell>DNS [47]</cell><cell>62.55</cell><cell>90.05</cell><cell>94.80</cell><cell>98.10</cell></row><row><cell>EDM [31]</cell><cell>61.32</cell><cell>88.90</cell><cell>96.44</cell><cell>99.94</cell></row><row><cell>DGD [39]</cell><cell>72.58</cell><cell>91.59</cell><cell>95.21</cell><cell>97.72</cell></row><row><cell>Our-Part</cell><cell>69.41</cell><cell>92.68</cell><cell>96.68</cell><cell>99.02</cell></row><row><cell>Our-Body</cell><cell>71.88</cell><cell>93.66</cell><cell>97.46</cell><cell>99.18</cell></row><row><cell>Our-Fusion</cell><cell>74.21</cell><cell>94.33</cell><cell>97.54</cell><cell>99.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Experimental results on the CUHK03 labeled dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .Table 5 .</head><label>65</label><figDesc>The full CMC curves are shown in supplementary materials. Experimental results on the MARS with single query.</figDesc><table><row><cell>Compared with CaffeNet, a similar multi-class person</cell></row><row><cell>identification network, our body-based model improves the</cell></row><row><cell>Rank-1 identification rate by 2.93% and mAP by 4.22%</cell></row><row><cell>using XQDA in single query. It should be noticed that</cell></row><row><cell>our network does not use any pre-training with additional</cell></row><row><cell>data. Usually deep learning network can obtain better re-</cell></row><row><cell>sults when pretrained with on ImageNet classification task.</cell></row><row><cell>Our fusion model improves Rank-1 identification rate and</cell></row><row><cell>mAP by 6.47% and by 8.45% in single query. This illus-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Experimental results on the MARS with multiple query.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Experimental results of four types of MSCAN using bodybased representation for ReID on the Market1501 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Experimental results of rigid parts and learned parts for ReID on the Market1501 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>The influences of L loc on part-based network on the Mar-ket1501 dataset.</figDesc><table><row><cell>Methods</cell><cell>Training Set</cell><cell>1</cell><cell>10</cell><cell>20</cell><cell>30</cell></row><row><cell>DTRSVM [26]</cell><cell>i-LIDS</cell><cell>8.26</cell><cell>31.39</cell><cell>44.83</cell><cell>53.88</cell></row><row><cell>DTRSVM [26]</cell><cell>PRID</cell><cell>10.90</cell><cell>28.20</cell><cell>37.69</cell><cell>44.87</cell></row><row><cell>DML [44]</cell><cell>CUHK Campus</cell><cell>16.17</cell><cell>45.82</cell><cell>57.56</cell><cell>64.24</cell></row><row><cell>Ours-Fusion</cell><cell>CUHK03 detected</cell><cell>17.30</cell><cell>44.58</cell><cell>55.51</cell><cell>61.77</cell></row><row><cell>Ours-Fusion</cell><cell>CHUK03 labeled</cell><cell>19.44</cell><cell>49.99</cell><cell>60.78</cell><cell>66.74</cell></row><row><cell>Ours-Fusion</cell><cell>MRAS</cell><cell>18.46</cell><cell>43.65</cell><cell>52.96</cell><cell>59.34</cell></row><row><cell>Ours-Fusion</cell><cell>Market1501</cell><cell>22.21</cell><cell>47.24</cell><cell>57.13</cell><cell>62.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 .</head><label>10</label><figDesc>Cross-dataset person ReID on the VIPeR dataset</figDesc><table><row><cell>Method</cell><cell>Rank-1</cell><cell>Rank-5</cell><cell>Rank-10</cell><cell>Rank-20</cell></row><row><cell>Our-Part</cell><cell>32.70</cell><cell>57.49</cell><cell>67.62</cell><cell>78.90</cell></row><row><cell>Our-Body</cell><cell>33.12</cell><cell>60.23</cell><cell>72.05</cell><cell>82.59</cell></row><row><cell>Our-Fusion</cell><cell>38.08</cell><cell>64.14</cell><cell>73.52</cell><cell>82.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 .</head><label>11</label><figDesc>Experimental results on VIPeR through fine-tuneing the model from Market1501 to VIPeR.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>(34.8%), Gate-SCNN [34] (37.8%), SI-CI [36] (35.8%), and achieves comparable results with DGD [39] (38.6%).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multi-task deep network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable metric learning via weighted approximate rank component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Color invariants for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kviatkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1622" to="1634" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A richly annotated dataset for pedestrian attribute recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07054</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient psd constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04404</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domain transfer support vector ranking for person re-identification without target camera label information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian descriptor for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Person re-identification by support vector ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Prosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep learning prototype domains for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schuchert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05047</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Embedding deep metric for person re-identification: A study against large variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint learning of single-image and cross-image representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dari: Distance metric and representation integration for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Personnet: Person re-identification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07255</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">End-to-end deep learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01850</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Person search in a scene by jointly modeling people commonness and person uniqueness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-level descriptors for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Crafting gbd-net for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02579</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4766" to="4779" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sample-specific svm learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Person reidentification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02531</idno>
		<title level="m">Person re-identification in the wild</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Quantifying and transferring contextual information in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="762" to="777" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Reidentification by relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="653" to="668" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
