<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
							<email>yelongshen@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
							<email>jianshuchen@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Guo</surname></persName>
							<email>yuqguo@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider the problem of learning to walk over a graph in order to find a target node for a given source node and a query. Such problems appear in, for example, knowledge base completion (KBC) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7]</ref>. A knowledge graph is a structured representation of world knowledge in the form of entities and their relations (e.g., <ref type="figure" target="#fig_0">Figure 1(a)</ref>), and has a wide range of downstream applications such as question answering. Although a typical knowledge graph may contain millions of entities and billions of relations, it is usually far from complete. KBC aims to predict the missing relations between entities using information from the existing knowledge graph. More formally, let G = (N , E) denote a graph, which consists of a set of nodes, N = {n i }, and a set of edges, E = {e ij }, that connect the nodes, and let q denote an input query. The problem is stated as using the graph G, the source node n S ∈ N and the query q as inputs to predict the target node n T ∈ N . In KBC tasks, G is a given knowledge graph, N is a collection of entities (nodes), and E is a set of relations (edges) that connect the entities. In the example in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, the objective of KBC is to identify the target node n T = USA for the given head entity n S = Obama and the given query q = CITIZENSHIP.</p><p>The problem can also be understood as constructing a function f (G, n S , q) to predict n T , where the functional form of f (·) is generally unknown and has to be learned from a training dataset consisting of samples like (n S , q, n T ). In this work, we model f (G, n S , q) by means of a graph-walking agent that intelligently navigates through a subset of nodes in the graph from n S towards n T . Since n T is unknown, the problem cannot be solved by conventional search algorithms such as A * -search <ref type="bibr" target="#b10">[11]</ref>, which seeks to find paths between the given source and target nodes. Instead, the agent needs to learn its search policy from the training dataset so that, after training is complete, the agent knows how to walk over the graph to reach the correct target node n T for an unseen pair of (n S , q). Moreover, each training sample is in the form of "(source node, query, target node)", and there is no intermediate supervision for the correct search path. Instead, the agent receives only delayed evaluative feedback: when the agent correctly (or incorrectly) predicts the target node in the training set, the agent will receive a positive (or zero) reward. For this reason, we formulate the problem as a Markov decision process (MDP) and train the agent by reinforcement learning (RL) <ref type="bibr" target="#b26">[27]</ref>.</p><p>The problem poses two major challenges. Firstly, since the state of the MDP is the entire trajectory, reaching a correct decision usually requires not just the query, but also the entire history of traversed nodes. For the KBC example in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, having access to the current node n t = Hawaii alone is not sufficient to know that the best action is moving to n t+1 = USA. Instead, the agent must track the entire history, including the input query q = Citizenship, to reach this decision. Secondly, the reward is sparse, being received only at the end of a search path, for instance, after correctly predicting n T =USA.</p><p>In this paper, we develop a neural graph-walking agent, named M-Walk, that effectively addresses these two challenges. First, M-Walk uses a novel recurrent neural network (RNN) architecture to encode the entire history of the trajectory into a vector representation, which is further used to model the policy and the Q-function. Second, to address the challenge of sparse rewards, M-Walk exploits the fact that the MDP transition model is known and deterministic. <ref type="bibr" target="#b1">2</ref> Specifically, it combines Monte Carlo Tree Search (MCTS) with the RNN to generate trajectories that obtain significantly more positive rewards than using the RNN policy alone. These trajectories can be viewed as being generated from an improved version of the RNN policy. But while these trajectories can improve the RNN policy, their off-policy nature prevents them from being leveraged by policy gradient RL methods. To solve this problem, we design a structure for sharing parameters between the Q-value network and the RNN's policy network. This allows the policy network to be indirectly improved through Q-learning over the off-policy trajectories. Our method is in sharp contrast to existing RL-based methods for KBC, which use a policy gradients (REINFORCE) method <ref type="bibr" target="#b35">[36]</ref> and usually require a large number of rollouts to obtain a trajectory with a positive reward, especially in the early stages of learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b13">14]</ref>. Experimental results on several benchmarks, including a synthetic task and several real-world KBC tasks, show that our approach learns better policies than previous RL-based methods and traditional KBC methods.</p><p>The rest of the paper is organized as follows: Section 3 develops the M-Walk agent, including the model architecture, the training and testing algorithms. <ref type="bibr" target="#b2">3</ref> Experimental results are presented in Section 4. Finally, we discuss related work in Section 5 and conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Graph Walking as a Markov Decision Process</head><p>In this section, we formulate the graph-walking problem as a Markov Decision Process (MDP), which is defined by the tuple (S, A, R, P), where S is the set of states, A is the set of actions, R is the reward function, and P is the state transition probability. We further define S, A, R and P below.  <ref type="figure" target="#fig_0">Figure 1</ref>(a). Let s t ∈ S denote the state at time t. Recalling that the agent needs the entire history of traversed nodes and the query to make a correct decision, we define s t by the following recursion:</p><formula xml:id="formula_0">s t = s t−1 ∪ {a t−1 , n t , E nt , N nt }, s 0 {q, n S , E n S , N n S }<label>(1)</label></formula><p>where a t ∈ A denotes the action selected by the agent at time t, n t ∈ G denotes the currently visited node at time t, E nt ⊂ E is the set of all edges connected to n t , and N nt ⊂ N is the set of all nodes We want to identify the target node nT = USA for a given pair of query q = Citizenship and source node nS = Obama. (b) The activated circles and edges (in black lines) denote all the observed information up to time t (i.e., the state st). The double circle denotes the current node nt, while En t and Nn t denote the edges and nodes connected to the current node. connected to n t (i.e., the neighborhood). Note that state s t is a collection of (i) all the traversed nodes (along with their edges and neighborhoods) up to time t, (ii) all the previously selected (up to time t − 1) actions, and (iii) the initial query q. The set S consists of all the possible values of {s t , t ≥ 0}. Based on s t , the agent takes one of the following actions at each time t: (i) choosing an edge in E nt and moving to the next node n t+1 ∈ N nt , or (ii) terminating the walk (denoted as the "STOP" action). Once the STOP action is selected, the MDP reaches the terminal state and outputsn T = n t as a prediction of the target node n T . Therefore, we define the set of feasible actions at time t as A t E nt ∪ {STOP}, which is usually time-varying. The entire action space A is the union of all A t , i.e., A = ∪ t A t . Recall that the training set consists of samples in the form of (n S , q, n T ). The reward is defined to be +1 when the predicted target noden T is the same as n T (i.e.,n T = n T ), and zero otherwise. In the example of <ref type="figure" target="#fig_0">Figure 1</ref>(a), for a training sample (Obama, Citizenship, USA), if the agent successfully navigates from Obama to USA and correctly predictsn T = USA, the reward is +1. Otherwise, it will be 0. The rewards are sparse because positive reward can be received only at the end of a correct path. Furthermore, since the graph G is known and static, the MDP transition probability p(s t |s t−1 , a t−1 ) is known and deterministic, and is defined by <ref type="bibr" target="#b0">(1)</ref>. To see this, we observe from <ref type="figure" target="#fig_0">Figure 1</ref>(b) that once an action a t (i.e., an edge in E nt or "STOP") is selected, the next node n t+1 and its associated E nt+1 and N nt+1 are known. By (1) (with t replaced by t + 1), this means that the next state s t+1 is determined. This important (model-based) knowledge will be exploited to overcome the sparse-reward problem using MCTS and significantly improve the performance of our method (see Sections 3-4 below).</p><p>We further define π θ (a t |s t ) and Q θ (s t , a t ) to be the policy and the Q-function, respectively, where θ is a set of model parameters. The policy π θ (a t |s t ) denotes the probability of taking action a t given the current state s t . In M-Walk, it is used as a prior to bias the MCTS search. And Q θ (s t , a t ) defines the long-term reward of taking action a t at state s t and then following the optimal policy thereafter. The objective is to learn a policy that maximizes the terminal rewards, i.e., correctly identifies the target node with high probability. We now proceed to explain how to model and jointly learn π θ and Q θ to achieve this objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The M-Walk Agent</head><p>In this section, we develop a neural graph-walking agent named M-Walk (i.e., MCTS for graph Walking), which consists of (i) a novel neural architecture for jointly modeling π θ and Q θ , and (ii) Monte Carlo Tree Search (MCTS). We first introduce the overall neural architecture and then explain how MCTS is used during the training and testing stages. Finally, we describe some further details of the neural architecture. Our discussion focuses on addressing the two challenges described earlier: history-dependent state and sparse rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1</head><p>The neural architecture for jointly modeling π θ and Q θ</p><p>Recall from Section 2 (e.g., <ref type="bibr" target="#b0">(1)</ref>) that one challenge in applying RL to the graph-walking problem is that the state s t nominally includes the entire history of observations. To address this problem, we propose a special RNN encoding the state s t at each time t into a vector representation, h t = ENC θe (s t ), where θ e is the associated model parameter. We defer the discussion of this RNN state encoder to Section 3.4, and focus in this section on how to use h t to jointly model π θ and Q θ . Specifically, the vector h t consists of several sub-vectors of the same dimension M : h S,t , {h n ,t : n ∈ N nt } and h A,t . Each sub-vector encodes part of the state s t in <ref type="bibr" target="#b0">(1)</ref>. For instance, the vector h S,t encodes (s t−1 , a t−1 , n t ), which characterizes the history in the state. The vector h n ,t encodes the (neighboring) node n and the edge e nt,n connected to n t , which can be viewed as a vector representation of the n -th candidate action (excluding the STOP action). And the vector h A,t is a vector summarization of E nt and N nt , which is used to model the STOP action probability. In summary, we use the sub-vectors to model π θ and Q θ according to:</p><formula xml:id="formula_1">u 0 = f θπ (h S,t , h A,t ), u n = h S,t , h n ,t , n ∈ N nt (2) Q θ (s t , ·) = σ(u 0 , u n 1 , . . . , u n k ), π θ (·|s t ) = φ τ (u 0 , u n 1 , . . . , u n k )<label>(3)</label></formula><p>where ·, · denotes inner product, f θπ (·) is a fully-connected neural network with model parameter θ π , σ(·) denotes the element-wise sigmoid function, and φ τ (·) is the softmax function with temperature parameter τ . Note that we use the inner product between the vectors h S,t and h n ,t to compute the (pre-softmax) score u n for choosing the n -th candidate action, where n ∈ N nt . The inner product operation has been shown to be useful in modeling Q-functions when the candidate actions are described by vector representations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3]</ref> and in solving other problems <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b0">1]</ref>. Moreover, the value of u 0 is computed by f θπ (·) using h S,t and h A,t , where u 0 gives the (pre-softmax) score for choosing the STOP action. We model the Q-function by applying element-wise sigmoid to u 0 , u n 1 , . . . , u n k , and we model the policy by applying the softmax operation to the same set of u 0 , u n 1 , . . . , u n k . <ref type="bibr" target="#b3">4</ref> Note that the policy network and the Q-network share the same set of model parameters. We will explain in Section 3.2 how such parameter sharing enables indirect updates to the policy π θ via Q-learning from off-policy data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The training algorithm</head><p>We now discuss how to train the model parameters θ (including θ π and θ e ) from a training dataset {(n S , q, n T )} using reinforcement learning. One approach is the policy gradient method (RE-INFORCE) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28]</ref>, which uses the current policy π θ (a t |s t ) to roll out multiple trajectories (s 0 , a 0 , r 0 , s 1 , . . .) to estimate a stochastic gradient, and then updates the policy π θ via stochastic gradient ascent. Previous RL-based KBC methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b4">5]</ref> typically use REINFORCE to learn the policy. However, policy gradient methods generally suffer from low sample efficiency, especially when the reward signal is sparse, because large numbers of Monte Carlo rollouts are usually needed to obtain many trajectories with positive terminal reward, particularly in the early stages of learning. To address this challenge, we develop a novel RL algorithm that uses MCTS to exploit the deterministic MDP transition defined in <ref type="bibr" target="#b0">(1)</ref>. Specifically, on each MCTS simulation, a trajectory is rolled out by selecting actions according to a variant of the PUCT algorithm <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref> from the root state s 0 (defined in (1)):</p><formula xml:id="formula_2">a t = argmax a c · π θ (a|s t ) β a N (s t , a ) (1+N (s t , a))+W (s t , a)/N (s t , a)<label>(4)</label></formula><p>where π θ (a|s) is the policy defined in Section 3.1, c and β are two constants that control the level of exploration, and N (s, a) and W (s, a) are the visit count and the total action reward accumulated on the (s, a)-th edge on the MCTS tree. Overall, PUCT treats π θ as a prior probability to bias the MCTS The key idea of our method is that running multiple MCTS simulations generates a set of trajectories with more positive rewards (see Section 4 for more analysis), which can also be viewed as being generated by an improved policy π θ . Therefore, learning from these trajectories can further improve π θ . Our RL algorithm repeatedly applies this policy-improvement step to refine the policy. However, since these trajectories are generated by a policy that is different from π θ , they are off-policy data, breaking the assumptions inherent in policy gradient methods. For this reason, we instead update the Q-network from these trajectories in an off-policy manner using Q-learning:</p><formula xml:id="formula_3">θ ← θ + α · ∇ θ Q θ (s t , a t ) × (r(s t , a t ) + γ max a Q θ (s t+1 , a ) − Q θ (s t , a t ))</formula><p>. Recall from Section 3.1 that π θ and Q θ (s, a) share the same set of model parameters; once the Q-network is updated, the policy network π θ will also be automatically improved. Finally, the new π θ is used to control the MCTS in the next iteration. The main idea of the training algorithm is summarized in <ref type="figure" target="#fig_3">Figure 3</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The prediction algorithm</head><p>At test time, we want to infer the target node n T for an unseen pair of (n S , q). One approach is to use the learned policy π θ to walk through the graph G to find n T . However, this would not exploit the known MDP transition model <ref type="bibr" target="#b0">(1)</ref>. Instead, we combine the learned π θ and Q θ with MCTS to generate an MCTS search tree, as in the training stage. Note that there could be multiple paths that reach the same terminal node n ∈ G, meaning that there could be multiple leaf states in MCTS corresponding to that node. Therefore, the prediction results from these MCTS leaf states need to be merged into one score to rank the node n. Specifically, we use</p><formula xml:id="formula_4">Score(n) = s T →n N (s T , a T )/N × Q θ (s T , STOP),</formula><p>where N is the total number of MCTS simulations, and the summation is over all the leaf states s T that correspond to the same node n ∈ G. Score(n) is a weighted average of the terminal state values associated with the same candidate node n. <ref type="bibr" target="#b4">5</ref> Among all the candidates nodes, we select the predicted target node to be the one with the highest score:n T = argmax n Score(n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The RNN state encoder</head><p>We now discuss the details of the RNN state encoder h t = ENC θe (s t ), where θ e {θ A , θ S , θ q }, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>(b). Specifically, we explain how the sub-vectors of h t are computed. We introduce q t s t−1 ∪ {a t−1 , n t } as an auxiliary variable. Then, the state s t in (1) can be written as s t = q t ∪ {E nt , N nt }. Note that the state s t is composed of two parts: (i) E nt and N nt , which represent the candidate actions to be selected (excluding the STOP action), and (ii) q t , which represents the history. We use two different neural networks to encode these separately. For the n -th candidate action (n ∈ N nt ), we concatenate n with its associated e nt,n ∈ E nt and input them into a fully connected network (FCN) f θ A (·) to compute their joint vector representation h n ,t , where θ A is the model parameter. Recall that the action space A t = E nt ∪ {STOP} can be time-varying when the size of E nt changes over time. To address this issue, we apply the same FCN f θ (·) to different (n , e nt,n ) to obtain their respective representations. Then, we use a coordinate-wise maxpooling operation over {h n ,t : n ∈ N nt } to obtain a (fixed-length) overall vector representation of {E nt , N nt }. To encode q t , we call upon the following recursion for q t (see Appendix A for the derivation): q t+1 = q t ∪ {E nt , N nt , a t , n t+1 }. Inspired by this recursion, we propose using the GRU-RNN <ref type="bibr" target="#b3">[4]</ref> to encode q t into a vector representation <ref type="bibr" target="#b5">6</ref> :</p><formula xml:id="formula_5">q t+1 = f θq (q t , [h A,t , h at,t , n t+1 ]) with initialization q 0 = f θq (q, [0, 0, n S ]),</formula><p>where θ q is the model parameter, and h at,t denotes the vector h n ,t at n = a t . We use h A,t and h at,t computed by the FCNs to represent (E nt , N nt ) and a t , respectively. Then, we map q t to h S,t using another FCN f θ S (·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate and analyze the effectiveness of M-Walk on a synthetic Three Glass Puzzle task and two real-world KBC tasks. We briefly describe the tasks here, and give the experiment details and hyperparameters in Appendix B.</p><p>Three Glass Puzzle The Three Glass Puzzle <ref type="bibr" target="#b19">[20]</ref> is a problem studied in math puzzles and graph theory. It involves three milk containers A, B, and C, with capacities A, B and C liters, respectively. The containers display no intermediate markings. There are three feasible actions at each time step: (i) fill a container (to its capacity), (ii) empty all of its liquid, and (iii) pour its liquid into another container (up to its capacity). The objective of the problem is, given a desired volume q, to take a sequence of actions on the three containers after which one of them contains q liters of liquid. We formulate this as a graph-walking problem; in the graph G, each node n = (a, b, c) denotes the amounts of remaining liquid in the three containers, each edge denotes one of the three feasible actions, and the input query is the desired volume q. The reward is +1 when the agent successfully fills one of the containers to q and 0 otherwise (see Appendix B.2.1 for the details). We use vanilla policy gradient (REINFORCE) <ref type="bibr" target="#b35">[36]</ref> as the baseline, with task success rate as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Base Completion</head><p>We use WN18RR and NELL995 knowledge graph datasets for evaluation. WN18RR <ref type="bibr" target="#b5">[6]</ref> is created from the original WN18 <ref type="bibr" target="#b1">[2]</ref> by removing various sources of test leakage, making the dataset more challenging. The NELL995 dataset was released by <ref type="bibr" target="#b37">[38]</ref> and has separate graphs for each query relation. We use the same data split and preprocessing protocol as in <ref type="bibr" target="#b5">[6]</ref> for WN18RR and in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b4">5]</ref> for NELL995. As in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b4">5]</ref>, we study the 10 relation tasks of NELL995 separately. We use HITS@1,3 and mean reciprocal rank (MRR) as the evaluation metrics for WN18RR, and use mean average precision (MAP) for NELL995, <ref type="bibr" target="#b6">7</ref> where HITS@K computes the percentage of the desired entities being ranked among the top-K list, and MRR computes an average of the reciprocal rank of the desired entities. We compare against RL-based methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b4">5]</ref>, embedding-based models (including DistMult <ref type="bibr" target="#b38">[39]</ref>, ComplEx <ref type="bibr" target="#b31">[32]</ref> and ConvE <ref type="bibr" target="#b5">[6]</ref>) and recent work in logical rules (NeuralLP) <ref type="bibr" target="#b39">[40]</ref>. For all the baseline methods, we used the implementation released by the corresponding authors with their best-reported hyperparameter settings. <ref type="bibr" target="#b7">8</ref> The details of the hyperparameters for M-Walk are described in Appendix B.2.2 of the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance of M-Walk</head><p>We first report the overall performance of the M-Walk algorithm on the three tasks and compare it with other baseline methods. We ran the experiments three times and report the means and standard deviations (except for PRA, TransE, and TransR on NELL995, whose results are directly quoted from <ref type="bibr" target="#b37">[38]</ref>). On the Three Glass Puzzle task, M-Walk significantly outperforms the baseline: the best model of M-Walk achieves an accuracy of (99.0 ± 1.0)% while the best REINFORCE method achieves (49.0 ± 2.6)% (see Appendix C for more experiments with different settings on this task). For the two KBC tasks, we report their results in <ref type="table" target="#tab_0">Tables 1-2</ref>, where PG-Walk and Q-Walk are two methods we created just for the ablation study in the next section. The proposed method outperforms previous works in most of the metrics on NELL995 and WN18RR datasets. Additional experiments on the FB15k-237 dataset can be found in Appendix C.1.1 of the supplementary material.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of M-Walk</head><p>We performed extensive experimental analysis to understand the proposed M-Walk algorithm, including (i) the contributions of different components, (ii) its ability to overcome sparse rewards, (iii) hyperparameter analysis, (iv) its strengths and weaknesses compared to traditional KBC methods, and (v) its running time. First, we used ablation studies to analyze the contributions of different components in M-Walk. To understand the contribution of the proposed neural architecture in M-Walk, we created a method, PG-Walk, which uses the same neural architecture as M-Walk but with the same training (PG) and testing (beam search) algorithms as MINERVA <ref type="bibr" target="#b4">[5]</ref>. We observed that the novel neural architecture of M-Walk contributes an overall 1% gain relative to MINERVA on NELL995, and it is still 1% worse than M-Walk, which uses MCTS for training and testing. To further understand the contribution of MCTS, we created another method, Q-Walk, which uses the same model architecture as M-Walk except that it is trained by Q-learning only without MCTS. Note that this lost about 2% in overall performance on NELL995. We observed similar trends on WN18RR.</p><p>In addition, we also analyze the importance of MCTS in the testing stage in Appendix C.1.</p><p>Second, we analyze the ability of M-Walk to overcome the sparse-reward problem. In <ref type="figure" target="#fig_4">Figure 4</ref>, we show the positive reward rate (i.e., the percentage of trajectories with positive reward during training) on the Three Glass Puzzle task and the NELL995 tasks. Compared to the policy gradient method (PG-Walk), and Q-learning method (Q-Walk) methods under the same model architecture, M-Walk with MCTS is able to generate trajectories with more positive rewards, and this continues to improve as training progresses. This confirms our motivation of using MCTS to generate higher-quality trajectories to alleviate the sparse-reward problem in graph walking.</p><p>Third, we analyze the performance of M-Walk under different numbers of MCTS rollout simulations and different search horizons on WN18RR dataset, with results shown in <ref type="figure" target="#fig_5">Figure 5</ref>(a). We observe that the model is less sensitive to search horizon and more sensitive to the number of MCTS rollouts. Finally, we analyze the strengths and weaknesses of M-Walk relative to traditional methods on the WN18RR dataset. The first question is how M-Walk performs on reasoning paths of different lengths compared to baselines. To answer this, we analyze the HITS@1 accuracy against ConvE in <ref type="figure" target="#fig_5">Fig. 5(b)</ref>. We categorize each test example using the BFS (breadth-first search) steps from the query entity to the target entity (-1 means not reachable). We observe that M-Walk outperforms the strong baseline ConvE by 4.6-10.9% in samples that require 2 or 3 steps, while it is nearly on par for paths of length one. Therefore, M-Walk does better at reasoning over longer paths than ConvE. Another question is what are the major types of errors made by M-Walk. Recall that M-Walk only walks through a subset of the graph and ranks a subset of candidate nodes (e.g., MCTS produces about 20-60 unique candidates on WN18RR). When the ground truth is not in the candidate set, M-Walk always makes mistakes and we define this type of error as out-of-candidate-set error. To examine this effect, we show in <ref type="figure" target="#fig_5">Figure 5</ref>   set. <ref type="bibr" target="#b8">9</ref> It shows that M-Walk has very high accuracy in this case, which is significantly higher than ConvE (80% vs 39.6% in HITS@1). We further examine the percentage of out-of-candidate-set errors among all errors in <ref type="figure" target="#fig_5">Figure 5</ref>(c)-bottom. It shows that the major error made by M-Walk is the out-of-candidate-set error. These observations point to an important direction for improving M-Walk in future work: increasing the chance of covering the target by the candidate set. In <ref type="table" target="#tab_3">Table 3</ref>, we show the running time of M-Walk (in-house C++ &amp; Cuda) and MINERVA (TensorFlowgpu) for both training and testing on WN18RR with different values of search horizon and number of rollouts (or MCTS simulation number). Note that the running time of M-Walk is comparable to that of MINERVA. Additional results can be found in <ref type="figure" target="#fig_9">Figure 9</ref>(c) of the supplementary material. Finally, in <ref type="table">Table 4</ref>, we show examples of reasoning paths found by M-Walk. <ref type="bibr" target="#b9">10</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Reinforcement Learning Recently, deep reinforcement learning has achieved great success in many artificial intelligence problems <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. The use of deep neural networks with RL allows policies to be learned from raw data (e.g., images) in an end-to-end manner. Our work also aligns <ref type="table">Table 4</ref>: Examples of reasoning paths found by M-Walk on the NELL-995 dataset for the relation "AthleteHomeStadium". True (False) means the prediction is correct (wrong).</p><p>AthleteHomeStadium: with this direction. Furthermore, the idea of using an RNN to encode the history of observations also appeared in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>. The combination of model-based and model-free information in our work shares the same spirit as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref>. Among them, the most relevant are <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, which combine MCTS with neural policy and value functions to achieve superhuman performance on Go. Different from our work, the policy and the value networks in <ref type="bibr" target="#b23">[24]</ref> are trained separately without the help of MCTS, and are only used to help MCTS after being trained. The work <ref type="bibr" target="#b24">[25]</ref> uses a new policy iteration method that combines the neural policy and value functions with MCTS during training. However, the method in <ref type="bibr" target="#b24">[25]</ref> improves the policy network from the MCTS probabilities of the moves, while our method improves the policy from the trajectories generated by MCTS. Note that the former is constructed from the visit counts of all the edges connected to the MCTS root node; it only uses information near the root node to improve the policy. By contrast, we improve the policy by learning from the trajectories generated by MCTS, using information over the entire MCTS search tree.</p><formula xml:id="formula_6">Example</formula><p>Knowledge Base Completion In KBC tasks, early work <ref type="bibr" target="#b1">[2]</ref> focused on learning vector representations of entities and relations. Recent approaches have demonstrated limitations of these prior approaches: they suffer from cascading errors when dealing with compositional (multi-step) relationships <ref type="bibr" target="#b9">[10]</ref>. Hence, recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref> have proposed approaches for injecting multi-step paths such as random walks through sequences of triples during training, further improving performance on KBC tasks. IRN <ref type="bibr" target="#b22">[23]</ref> and Neural LP <ref type="bibr" target="#b39">[40]</ref> explore multi-step relations by using an RNN controller with attention over an external memory. Compared to RL-based approaches, it is hard to interpret the traversal paths, and these models can be computationally expensive to access the entire graph in memory <ref type="bibr" target="#b22">[23]</ref>. Two recent works, DeepPath <ref type="bibr" target="#b37">[38]</ref> and MINERVA <ref type="bibr" target="#b4">[5]</ref>, use RL-based approaches to explore paths in knowledge graphs. DeepPath requires target entity information to be in the state of the RL agent, and cannot be applied to tasks where the target entity is unknown. MINERVA <ref type="bibr" target="#b4">[5]</ref> uses a policy gradient method to explore paths during training and test. Our proposed model further exploits state transition information by integrating the MCTS algorithm. Empirically, our proposed algorithm outperforms both DeepPath and MINERVA in the KBC benchmarks. <ref type="bibr" target="#b10">11</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Discussion</head><p>We developed an RL-agent (M-Walk) that learns to walk over a graph towards a desired target node for given input query and source nodes. Specifically, we proposed a novel neural architecture that encodes the state into a vector representation, and maps it to Q-values and a policy. To learn from sparse rewards, we propose a new reinforcement learning algorithm, which alternates between an MCTS trajectory-generation step and a policy-improvement step, to iteratively refine the policy. At test time, the learned networks are combined with MCTS to search for the target node. Experimental results on several benchmarks demonstrate that our method learns better policies than other baseline methods, including RL-based and traditional methods on KBC tasks. Furthermore, we also performed extensive experimental analysis to understand M-Walk. We found that our method is more accurate when the ground truth is in the candidate set. We also found that the out-of-candidate-set error is the main type of error made by M-Walk. Therefore, in future work, we intend to improve this method by reducing such out-of-candidate-set errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Derivation of the recursion for q t</head><p>Recalling the definition q t s t−1 ∪ {a t−1 , n t } and using the recursion (1), we have q t+1</p><formula xml:id="formula_7">(a) = s t ∪ {a t , n t+1 } (b) = s t−1 ∪ {a t−1 , n t , E nt , N nt } ∪ {a t , n t+1 } (c) = q t ∪ {E nt , N nt , a t , n t+1 }</formula><p>where step (a) uses the definition of q t+1 , step (b) substitutes the recursion (1), and step (c) uses the definition of q t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Algorithm Implementation Details</head><p>The detailed algorithm of M-Walk is described in Algorithm 1. Set current node n 0 = n S ; q 0 = f θq (q, 0, 0, n 0 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>for t = 0 . . . T max do <ref type="bibr">5:</ref> Lookup from dictionary to obtain W (s t , a) and N (s t , a) <ref type="bibr">6:</ref> Select the action a t with the maximum PUCT value:</p><formula xml:id="formula_8">a t = argmax a c·π θ (a|s t ) β a N (s t , a ) 1+N (s t , a) + W (s t , a) N (s t , a) 7:</formula><p>Update q t+1 = f θq (q t , h A,t , h at,t , n t+1 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>if a t is STOP then 9:</p><p>Compute estimated reward value V θ (s t ) = Q(s t , a t = STOP) 10:</p><p>Add generated path p into a path list <ref type="bibr">11:</ref> Backup along the path p to update the visit count N (s t , a) using (5) and the total action reward W (s t , a) using (6) on the (s t , a)-th edge on the MCTS tree 12:</p><p>Break 13:</p><p>end if <ref type="bibr">14:</ref> end for 15: end for <ref type="bibr">16:</ref> for each path p in the path list do <ref type="bibr">17:</ref> Set reward r = 1 if the end of the path n t = n T otherwise r = 0 <ref type="bibr">18:</ref> Repeatedly update the model parameters with Q-learning:</p><formula xml:id="formula_9">θ ← θ + α · ∇ θ Q θ (s t , a t ) × r(s t , a t ) + γ max a Q θ (s t+1 , a ) − Q θ (s t , a t ) 19: end for B.1 MCTS implementation</formula><p>In the MCTS implementation, we maintain a lookup table to record values W (s t , a) and N (s t , a) for each visited state-action pair. The state s t in the graph walk problem contains all the information along the traversal path, and n t is the node at the current step t. We assign an index i a to each candidate action a from n t , indicating that a is the i a -th action of the node n t . Thus, the state s t can be encoded as a path string P st = (q, n 0 , i a0 , n 1 , i a1 , . . . , n t ). We build a dictionary D using the path string as a key, and we record W (s t , a) and N (s t , a) as values in D. In the backup stage, the W (s t , a) and N (s t , a) values are updated for each state-action pair along with the traversal path in MCTS:</p><formula xml:id="formula_10">N (s t , a) = N (s t , a) + γ T −t (5) W (s t , a) = W (s t , a) + γ T −t V θ (s T ),<label>(6)</label></formula><p>where T is the length of the traversal path, γ is the discount factor of the MDP, and V θ (s T ) is the terminal state-value function modeled by V θ (s T ) Q(s T , a = STOP).</p><p>In our experiments, the softmax temperature parameter τ in the policy network π θ (see <ref type="formula" target="#formula_1">(3)</ref>) is set to be a constant. An alternative choice is to anneal it during training (e.g., τ = 1 → 0). However, we did not observe this to produce any significant difference in performance in our experiments. We believe the main reason is that π θ is only used as a prior to bias the MCTS search, while the exploration of MCTS is controlled by the parameters c and β of (4). Data generation In the Three Glass Puzzle experiments, we randomly draw four integers from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">50)</ref> to represent the capacities A, B, C, and the desired volume q. We further restrict the values so that A ≥ B ≥ C and q &lt; A, to avoid data duplication. We discard puzzles for which there is no solution. Finally, we keep 600 unique puzzles as the experimental dataset, where 500 puzzles are used for training and the other 100 are used to test a model's generalization capability on the unseen test set.</p><p>Experiment settings and hyperparameters Let a, b, c be the current status of each container, and define the puzzle status at step t as n t = [I T A , I T B , I T C , I T a , I T b , I T c ] T , where I x is the one-hot representation to encode the value of x. Given that A, B, C, a, b and c are all smaller than 50 in the experiment, the dimension of n t is 300. The initial query q is obtained by q = E mb [q], where E mb is a query embedding lookup table and E mb [x] indicates the x-th column. The query embedding dimension is set to 64. In the Three Glass Puzzle, there are 13 actions in total: fill one container to its capacity, empty one container, pour one container into another container, and a STOP action to terminate the game. We set the maximum length of an action sequence (i.e., the search horizon) to be 12, where only the STOP action can be taken on the final step. After the STOP action has been taken, the system evaluates the action sequence and assigns a reward r = 1 if the final status is a success, otherwise r = 0. The f θ S and f θ A functions are modeled by two different DNNs with the same architecture: two fully-connected layers with 32 hidden dimensions and ReLU activation function.   <ref type="table" target="#tab_6">Table 6</ref>.</p><p>Experiment settings and hyperparameters For the proposed M-Walk, we set the entity embedding dimension to 4 and relation embedding dimension to 64. The maximum length of the graph walking path (i.e., the search horizon) is 8 in the NELL-995 dataset and 5 in the WN18RR dataset. After the STOP action has been taken, the system evaluates the action sequence and assigns a reward r = 1 if the agent reaches the target node, otherwise r = 0. The initial query q is the concatenation of the entity embedding vector and the relation embedding vector. The f θ S and f θ A functions are modeled by two different DNNs with the same architecture: two fully-connected layers with 64 hidden dimensions and the ReLU activation function. f θv is two fully-connected layers with 16 hidden dimensions, where the first hidden layer uses a Tanh activation function and the output layer uses a linear activation function. f θq is modeled by a GRU with hidden size 64. The hyperparameters in PUCT are set to c = 2 and β = 0.5. We roll out 32 MCTS paths in both training and testing in the NELL-995 dataset and 128 MCTS paths in the WN18RR dataset. We use the ADAM optimization algorithm for model training with learning rate 0.0001, and we set the mini-batch size to 8.  We now present more experiments on the Three Glass Puzzle task under different settings. First, to see how fast M-Walk converges, we show in <ref type="figure">Figure 7</ref> the learning curves of M-Walk and PG. It shows that M-Walk converges much faster than PG and achieves better results on this task. In <ref type="table" target="#tab_7">Table 7</ref> As mentioned earlier, conventional graph traversal algorithms such as Breadth-First Search (BFS) and Depth-First Search (DFS) cannot be applied to the graph walking problem, because the ground truth target node is not known at test time. However, to understand how quickly M-Walk with MCTS can find the correct target node, we compare it with BFS and DFS in the following cheating setup. Specifically, we apply BFS and DFS to the test set of the Three Glass Puzzle task by disclosing the target node to them. In <ref type="table" target="#tab_8">Table 8</ref>, we report the average traversal steps and maximum steps to reach the target node. The M-Walk with MCTS algorithm is able to find the target node more efficiently than BFS or DFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 Knowledge Graph Link Prediction</head><p>In this section, we first provide additional experimental results for the NELL995 and WN18RR tasks to support our analysis. In <ref type="figure" target="#fig_8">Figure 8</ref>, we show the positive reward rate during training on the NELL995 task. And in <ref type="figure" target="#fig_9">Figure 9</ref>, we provide more hyperparameter analysis (search horizon and MCTS simulation number) and training-time analysis. Furthermore, in <ref type="table" target="#tab_11">Table 9</ref>, we show the HITS@K and MRR results on NELL995.</p><p>In addition, we conduct further experiments on the FB15k-237 dataset <ref type="bibr" target="#b28">[29]</ref>, which is a subset of FB15k <ref type="bibr" target="#b1">[2]</ref> with inverse relations being removed. We use the same data split and preprocessing protocol as in <ref type="bibr" target="#b5">[6]</ref> for FB15k-237. The results are reported in <ref type="table" target="#tab_0">Table 10</ref>. We observe that M-Walk outperforms the other RL-based method (MINERVA). However, it is still worse than the embedding-based methods.</p><p>In future work, we intend to combine the strength of embedding-based methods and our method to further improve the performance of M-Walk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 The Reasoning (Traversal) Paths</head><p>In <ref type="table" target="#tab_0">Table 11</ref>, we show the reasoning paths of M-Walk on the NELL995 dataset. Each reasoning path is generated by following the edges on the MCTS tree with the highest visiting count N (s, a).     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 (</head><label>1</label><figDesc>b) illustrates the MDP corresponding to the KBC example of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( a )Figure 1 :</head><label>a1</label><figDesc>An example of Knowledge Base Completion (b) The corresponding Markov Decision Process An example of Knowledge Base Completion and its formulation as a Markov Decision Process. (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The neural architecture for M-Walk. (a) The vector representation of the state is mapped into π θ and Q θ . (b) The GRU-RNN state encoder maps the state into its vector representation ht. Note that the inputs hA,t−1 and ha t−1 ,t−1 are from the output of the previous time step t − 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>An example of MCTS path (in red) in M-Walk (b) Iterative policy improvement in M-Walk MCTS is used to generate trajectories for iterative policy improvement in M-Walk. search; PUCT initially prefers actions with high values of π θ and low visit count N (s, a) (because the first term in (4) is large), but then asympotically prefers actions with high value (because the first term in (4) vanishes and the second term W (s, a)/N (s, a) dominates). When PUCT selects the STOP action or the maximum search horizon has been reached, MCTS completes one simulation and updates W (s, a) and N (s, a) using V θ (s T ) = Q θ (s T , a = STOP). (See Figure 3(a) for an example and Appendix B.1 for more details.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>(c)-top the HITS@K accuracies when the ground truth is in the candidate The positive reward rate. Figures (a)-(d) are the results on the Three Glass Puzzle task and Figures (e)-(h) are the results on the NELL-995 task. (See Appendix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>M-Walk hyperparameter and error analysis on WN18RR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 : 1 −Example 2 : 1 −Example 3 : 1 −</head><label>112131</label><figDesc>athlete ernie banks AthleteHomeStadium − −−−−−−−−− →? athlete ernie banks AthletePlaysInLeague − −−−−−−−−− → SportsLeague mlb TeamPlaysInLeague −−−−−−−−−−− → SportsTeam chicago cubs TeamHomeStadium − −−−−−−−− → StadiumOrEventVenue wrigley field, (True) coach jim zorn AthleteHomeStadium − −−−−−−−−− →? coach jim zorn CoachWonTrophy −−−−−−−−→ AwardTrophyTournament super bowl TeamWonTrophy −−−−−−−−−− → SportsTeam redskins TeamHomeStadium − −−−−−−−− → StadiumOrEventVenue fedex field, (True) athlete oliver perez AthleteHomeStadium − −−−−−−−−− →? athlete oliver perez AthletePlaysInLeague − −−−−−−−−− → SportsLeague mlb TeamPlaysInLeague −−−−−−−−−−− → SportsTeam chicago cubs TeamHomeStadium − −−−−−−−− → StadiumOrEventVenue wrigley field, (False)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Graph traversal in the Three Glass Puzzle problem. An example Figure 6 illustrates one step in solving a Three Glass Puzzle. The following action sequences provide one solution to achieve the target q = 4, given initially empty containers with capacities (A = 8, B = 5, C = 3), where a, b, c denote the current contents of the containers: • Initial state → (a = 0, b = 0, c = 0) • Fill B → (a = 0, b = 5, c = 0) • Pour from B to C → (a = 0, b = 2, c = 3) • Empty C → (a = 0, b = 2, c = 0) • Pour from B to C → (a = 0, b = 0, c = 2) • Fill B → (a = 0, b = 5, c = 2) • Pour from B to C → (a = 0, b = 4, c = 3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>The positive reward rate during training (i.e., percentage of trajectories with positive reward during training) on the NELL-995 task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>M-Walk hyperparameter and error analysis on WN18RR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The MAP scores (%) on NELL995 task, where we report RL-based methods in terms of "mean (standard deviation)". PG-Walk and Q-Walk are methods we created just for the ablation study.</figDesc><table><row><cell>Tasks</cell><cell>M-Walk</cell><cell>PG-Walk</cell><cell>Q-Walk</cell><cell>MINERVA</cell><cell>DeepPath</cell><cell>PRA</cell><cell>TransE</cell><cell>TransR</cell></row><row><cell>AthletePlaysForTeam</cell><cell>84.7 (1.3)</cell><cell>80.8 (0.9)</cell><cell>82.6 (1.2)</cell><cell>82.7 (0.8)</cell><cell>72.1 (1.2)</cell><cell>54.7</cell><cell>62.7</cell><cell>67.3</cell></row><row><cell>AthletePlaysInLeague</cell><cell>97.8 (0.2)</cell><cell>96.0 (0.6)</cell><cell>96.2 (0.8)</cell><cell>95.2 (0.8)</cell><cell>92.7 (5.3)</cell><cell>84.1</cell><cell>77.3</cell><cell>91.2</cell></row><row><cell>AthleteHomeStadium</cell><cell>91.9 (0.1)</cell><cell>91.9 (0.3)</cell><cell>91.1 (1.3)</cell><cell>92.8 (0.1)</cell><cell>84.6 (0.8)</cell><cell>85.9</cell><cell>71.8</cell><cell>72.2</cell></row><row><cell>AthletePlaysSport</cell><cell>98.3 (0.1)</cell><cell>98.0 (0.8)</cell><cell>97.0 (0.2)</cell><cell>98.6 (0.1)</cell><cell>91.7 (4.1)</cell><cell>47.4</cell><cell>87.6</cell><cell>96.3</cell></row><row><cell>TeamPlaySports</cell><cell>88.4 (1.8)</cell><cell>87.4 (0.9)</cell><cell>78.5 (0.6)</cell><cell>87.5 (0.5)</cell><cell>69.6 (6.7)</cell><cell>79.1</cell><cell>76.1</cell><cell>81.4</cell></row><row><cell>OrgHeadquaterCity</cell><cell>95.0 (0.7)</cell><cell>94.0 (0.4)</cell><cell>94.0 (0.6)</cell><cell>94.5 (0.3)</cell><cell>79.0 (0.0)</cell><cell>81.1</cell><cell>62.0</cell><cell>65.7</cell></row><row><cell>WorksFor</cell><cell>84.2 (0.6)</cell><cell>84.0 (1.6)</cell><cell>82.7 (0.2)</cell><cell>82.7 (0.5)</cell><cell>69.9 (0.3)</cell><cell>68.1</cell><cell>67.7</cell><cell>69.2</cell></row><row><cell>BornLocation</cell><cell>81.2 (0.0)</cell><cell>82.3 (0.6)</cell><cell>81.4 (0.5)</cell><cell>78.2 (0.0)</cell><cell>75.5 (0.5)</cell><cell>66.8</cell><cell>71.2</cell><cell>81.2</cell></row><row><cell>PersonLeadsOrg</cell><cell>88.8 (0.5)</cell><cell>87.2 (0.5)</cell><cell>86.9 (0.5)</cell><cell>83.0 (2.6)</cell><cell>79.0 (1.0)</cell><cell>70.0</cell><cell>75.1</cell><cell>77.2</cell></row><row><cell>OrgHiredPerson</cell><cell>88.8 (0.6)</cell><cell>87.2 (0.4)</cell><cell>87.8 (0.9)</cell><cell>87.0 (0.3)</cell><cell>73.8 (1.9)</cell><cell>59.9</cell><cell>71.9</cell><cell>73.7</cell></row><row><cell>Overall</cell><cell>89.9</cell><cell>88.9</cell><cell>87.8</cell><cell>87.6</cell><cell>78.8</cell><cell>69.7</cell><cell>72.3</cell><cell>77.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The results on the WN18RR dataset, in the form of "mean (standard deviation)".</figDesc><table><row><cell>Metric (%)</cell><cell>M-Walk</cell><cell>PG-Walk</cell><cell>Q-Walk</cell><cell>MINERVA</cell><cell>ComplEx</cell><cell>ConvE</cell><cell>DistMult</cell><cell>NeuralLP</cell></row><row><cell>HITS@1</cell><cell>41.4 (0.1)</cell><cell>39.3 (0.2)</cell><cell>38.2 (0.3)</cell><cell>35.1 (0.1)</cell><cell>38.5 (0.3)</cell><cell>39.6 (0.3)</cell><cell>38.4 (0.4)</cell><cell>37.2 (0.1)</cell></row><row><cell>HITS@3</cell><cell>44.5 (0.2)</cell><cell>41.9 (0.1)</cell><cell>40.8 (0.4)</cell><cell>44.5 (0.4)</cell><cell>43.9 (0.3)</cell><cell>44.7 (0.2)</cell><cell>42.4 (0.3)</cell><cell>43.4 (0.1)</cell></row><row><cell>MRR</cell><cell>43.7 (0.1)</cell><cell>41.3 (0.1)</cell><cell>40.1 (0.3)</cell><cell>40.9 (0.1)</cell><cell>42.2 (0.2)</cell><cell>43.3 (0.2)</cell><cell>41.3 (0.3)</cell><cell>43.5 (0.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Running time of M-Walk and MINERVA for different combinations of (horizon, rollouts).</figDesc><table><row><cell>Model</cell><cell>M-Walk (5,64)</cell><cell>M-Walk (5,128)</cell><cell>M-Walk (3,64)</cell><cell>M-Walk (3,128)</cell><cell>MINERVA (3,100), best</cell></row><row><cell>Training (hrs.)</cell><cell>8</cell><cell>14</cell><cell>5</cell><cell>8</cell><cell>3</cell></row><row><cell>Testing (sec/sample)</cell><cell>3 × 10 −3</cell><cell>6 × 10 −3</cell><cell>1.6 × 10 −3</cell><cell>2.7 × 10 −3</cell><cell>2 × 10 −2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>A List of actions for each container in the Three Glass Puzzle. The agent can also determine to take the STOP action to terminate the game.EmptyA Fill A Pour A to B Pour A to C Empty B Fill B Pour B to A Pour B to C Empty C Fill C Pour C to A Pour C to B</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Knowledge base completion datasets statistics. Three Glass Puzzle test accuracy, where "PG" stands for policy gradient.f θv is two fully-connected layers with 16 hidden dimensions, where the first hidden layer uses a ReLU activation function and the output layer uses a linear activation function. f θq is modeled by a GRU with hidden size 64. The hyperparameters in PUCT are set to c = 0.5 and β = 0.2. We use the ADAM optimization algorithm with learning rate 0.0005 during training, and we set the mini-batch size to 8.B.2.2 Knowledge Base CompletionStatistics of the three datasets The NELL-995 knowledge dataset contains 75, 492 unique entities and 200 relations. WN18RR contains 93, 003 triples with 40, 943 entities and 11 relations. And FB15k-237, a subset of FB15k where inverse relations are removed, contains 14, 541 entities and 237 relations. The detailed statstics are shown in</figDesc><table><row><cell>Dataset</cell><cell cols="3"># Train</cell><cell></cell><cell cols="12"># Test # Relation # Entity avg. degree median degree</cell></row><row><cell>WN18RR</cell><cell cols="3">86,835</cell><cell></cell><cell cols="2">3,134</cell><cell></cell><cell>11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40,943</cell><cell></cell><cell></cell><cell>2.19</cell><cell>2</cell></row><row><cell cols="7">NELL-995 154,213 3,992</cell><cell></cell><cell cols="2">200</cell><cell></cell><cell></cell><cell></cell><cell>75,492</cell><cell></cell><cell></cell><cell>4.07</cell><cell>1</cell></row><row><cell cols="7">FB15K-237 272,115 20,466</cell><cell></cell><cell cols="2">237</cell><cell></cell><cell></cell><cell></cell><cell>14,541</cell><cell></cell><cell></cell><cell>19.74</cell><cell>14</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Test Accuracy</cell><cell>0.3 0.4 0.5 0.6 0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Accuracy</cell><cell>0.3 0.4 0.5 0.6 0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M-Walk</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M-Walk</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PG</cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PG</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>Epoch</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell></cell><cell>0</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>Epoch</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell></cell><cell cols="9">(a) Test Beam / Rollout = 128</cell><cell cols="7">(b) Test Beam / Rollout = 300</cell></row><row><cell>Figure 7:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Three Glass Puzzle test accuracy (%), where "Beam" denotes beam search. (2.1) 30.7 (4.5) 39.3 (3.2) 45.3 (4.5) 47.7 (3.2) 48.7 (3.2) 49.0 (2.6) M-Walk (Beam) 18.0 (1.7) 46.0 (7.0) 60.3 (7.8) 67.0 (7.0) 69.0 (6.2) 69.3 (6.4) 71.7 (4.5) M-Walk (MCTS) 18.0 (1.7) 63.3 (5.0) 84.3 (3.1) 90.7 (2.5) 95.0 (2.6) 96.3 (1.5) 99.0 (1.0)</figDesc><table><row><cell>Size</cell><cell>1</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell></row><row><cell>PG (Beam)</cell><cell>9.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>BFS, DFS and M-Walk on Three Glass Puzzle.</figDesc><table><row><cell cols="3">Method Average # Steps Max # Steps</cell></row><row><cell>BFS</cell><cell>264.7</cell><cell>1030</cell></row><row><cell>DFS</cell><cell>192.2</cell><cell>1453</cell></row><row><cell>M-Walk</cell><cell>94.9</cell><cell>897</cell></row><row><cell>C Additional Experiments</cell><cell></cell><cell></cell></row><row><cell cols="2">C.1 The Three Glass Puzzle task in different settings</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>, we report the test accuracy of M-Walk and vanilla policy gradient (REINFORCE/PG) with different beam search sizes and different MCTS rollouts during testing. The number of MCTS simulations for training M-Walk is fixed to be 32. We observe that M-Walk with MCTS achieves the best test accuracy overall. In addition, with larger beam search sizes and MCTS rollouts, the test accuracy improves substantially. Furthermore, replacing the MCTS in M-Walk by beam search at test time degrades the performance greatly, which shows that MCTS is also very important for M-Walk at test time.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>The HITS@K and MRR results on the NELL995 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Examples of paths found by M-Walk on the NELL-995 dataset.</figDesc><table><row><cell>(i) WorksFor:</cell><cell></cell></row><row><cell>journalist jerome holtzman</cell><cell>WorksFor − −−−− →?</cell></row><row><cell>journalist jerome holtzman</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Whenever the agent takes an action, by selecting an edge connected to a next node, the identity of the next node (which the environment will transition to) is already known. Details can be found in Section 2.<ref type="bibr" target="#b2">3</ref> The code of this paper is available at: https://github.com/yelongshen/GraphWalk</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">An alternative choice is applying softmax to the Q-function to get the policy, which is known as softmax selection<ref type="bibr" target="#b26">[27]</ref>. We found in our experiments that these two designs do not differ much in performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">There could be alternative ways to compute the score, such as Score(n) = maxs T →n Q θ (sT , STOP). However, we found in our (unreported) experiments that they do not make much difference.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">For simplicity, we use the same notation qt to denote its vector representation.<ref type="bibr" target="#b6">7</ref> We use these metrics in order to be consistent with<ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b4">5]</ref>. We also report the HITS and MRR scores for NELL995 inTable 9of the supplementary material.8 ConvE: https://github.com/TimDettmers/ConvE, Neural-LP: https://github.com/fanyangxyz/Neural-LP/, DeepPath: https://github.com/xwhan/DeepPath, MINERVA: https://github.com/shehzaadzd/MINERVA/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">The ground truth is always in the candidate list in ConvE, as it examines all the nodes.<ref type="bibr" target="#b9">10</ref> More examples can be found in Appendix C.2 of the supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">A preliminary version of M-Walk with limited experiments was reported in the workshop paper<ref type="bibr" target="#b21">[22]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Ricky Loynd, Adith Swaminathan, and anonymous reviewers for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09940</idno>
		<title level="m">Neural combinatorial optimization with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Q-LDA: Uncovering latent patterns in text-based sequential decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4984" to="4993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional 2D knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minervini</forename><surname>Pasquale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stenetorp</forename><surname>Pontus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08267</idno>
		<title level="m">Neural approaches to conversational AI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating vector space similarity in random walk inference over knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Q-prop: Sample-efficient policy gradient with an off-policy critic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A formal basis for the heuristic determination of minimum cost paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">J</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raphael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems Science and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="100" to="107" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep recurrent Q-learning for partially observable MDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stone</surname></persName>
		</author>
		<idno>abs/1507.06527</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with a natural language action space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1621" to="1630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A natural policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1531" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Graphs and Their Uses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oystein</forename><surname>Ore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-armed bandits with episode context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="230" />
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ReinforceWalk: Learning to walk in graph with Monte Carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling large-scale structured relationships with shared memory for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mastering the game of Go without human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The predictron: End-to-end learning and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Degris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Compositional learning of embeddings for relation paths in knowledge bases and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Knowledge graph completion via complex tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Christopher R Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagination-augmented agents for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Racanière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrià</forename><forename type="middle">Puigdomènech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Förster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Logic Journal of the IGPL</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="620" to="634" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5285" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DeepPath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">Metric (%) M-Walk PG-Walk Q-Walk MINERVA ComplEx ConvE</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">the form of &quot;mean (standard deviation)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>The results on the FB15k-237 dataset</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<title level="m">Metric (%) M-Walk PG-Walk Q-Walk MINERVA ComplEx ConvE</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
