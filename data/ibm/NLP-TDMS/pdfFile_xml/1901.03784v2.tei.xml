<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UPSNet: A Unified Panoptic Segmentation Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uber ATG</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
							<email>rjliao@uber.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Uber ATG</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
							<email>hszhao@cse.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
							<email>rui.hu@uber.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Uber ATG</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uber ATG</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
							<email>yumer@uber.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Uber ATG</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
							<email>urtasun@uber.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Uber ATG</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UPSNet: A Unified Panoptic Segmentation Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a unified panoptic segmentation network (UPSNet) for tackling the newly proposed panoptic segmentation task. On top of a single backbone residual network, we first design a deformable convolution based semantic segmentation head and a Mask R-CNN style instance segmentation head which solve these two subtasks simultaneously. More importantly, we introduce a parameter-free panoptic head which solves the panoptic segmentation via pixel-wise classification. It first leverages the logits from the previous two heads and then innovatively expands the representation for enabling prediction of an extra unknown class which helps better resolve the conflicts between semantic and instance segmentation. Additionally, it handles the challenge caused by the varying number of instances and permits back propagation to the bottom modules in an end-to-end manner. Extensive experimental results on Cityscapes, COCO and our internal dataset demonstrate that our UPSNet achieves stateof-the-art performance with much faster inference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Relying on the advances in deep learning, computer vision systems have been substantially improved, especially in tasks such as semantic segmentation <ref type="bibr" target="#b38">[39]</ref> and instance segmentation <ref type="bibr" target="#b14">[15]</ref>. The former focuses on segmenting amorphous image regions which share similar texture or material such as grass, sky and road, whereas the latter focuses on segmenting countable objects such as people, bicycle and car. Since both tasks aim at understanding the visual scene at the pixel level, a shared model or representation could arguably be beneficial. However, the dichotomy of these two tasks lead to very different modeling strategies despite the inherent connections between them. For example, fully convolutional neural networks <ref type="bibr" target="#b25">[26]</ref> are often adopted for semantic segmentation while proposal based detectors <ref type="bibr" target="#b29">[30]</ref> are frequently exploited for instance segmentation.</p><p>As an effort to leverage the possible complementariness of these two tasks and push the segmentation systems further towards real-world application, Kirillov et al. <ref type="bibr" target="#b16">[17]</ref> unified them and proposed the so-called panoptic segmentation task. It is interesting to note that tasks with the same spirit have been studied under various names before deep learning became popular. Notable ones include image parsing <ref type="bibr" target="#b31">[32]</ref>, scene parsing <ref type="bibr" target="#b31">[32]</ref> and holistic scene understanding <ref type="bibr" target="#b34">[35]</ref>. In panoptic segmentation, countable objects (those that map to instance segmentation tasks well) are called things whereas amorphous and uncountable regions (those that map to semantic segmentation tasks better) are called stuff. For any pixel, if it belongs to stuff, the goal of a panoptic segmentation system is simply to predict its class label within the stuff classes. Otherwise the system needs to decide which instance it belongs to as well as which thing class it belongs to. The challenge of this task lies in the fact that the system has to give a unique answer for each pixel.</p><p>In this paper, we propose a unified panoptic segmentation network (UPSNet) to approach the panoptic segmentation problem as stated above. Unlike previous methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> which have two separate branches designed for semantic and instance segmentation individually, our model exploits a single network as backbone to provide shared representations. We then design two heads on top of the backbone for solving these tasks simultaneously. Our semantic head builds upon deformable convolution <ref type="bibr" target="#b8">[9]</ref> and leverages multi-scale information from feature pyramid networks (FPN) <ref type="bibr" target="#b21">[22]</ref>. Our instance head follows the Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> design and outputs mask segmentation, bounding box and its associated class. As shown in experiments, these two lightweight heads along with the single backbone provide good semantic and instance segmentations which are comparable to separate models. More importantly, we design a panoptic head which predicts the final panoptic segmentation via pixel-wise classification of which the number of classes per image could vary. It exploits the logits from the above two heads and adds a new channel of logits corresponding to an extra unknown class. By doing so, it provides a better way of resolving the conflicts between semantic and instance segmentation. Moreover, our parameterfree panoptic head is very lightweight and could be used with various backbone networks. It facilitates end-to-end training which is not the case for previous methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>. To verify the effectiveness of our UPSNet, we perform extensive experiments on two public datasets: Cityscapes <ref type="bibr" target="#b5">[6]</ref> and COCO <ref type="bibr" target="#b22">[23]</ref>. Furthermore, we test it on our internal dataset which is similar in spirit to Cityscapes (i.e., images are captured from ego-centric driving scenarios) but with significantly larger (≈ 3×) size. Results on these three datasets manifest that our UPSNet achieves state-of-the-art performances and enjoys much faster inference compared to recent competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Segmentation: Semantic segmentation is one of the fundamental computer vision tasks that has a long history. Earlier work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref> focused on introducing datasets for this task and showed the importance of global context by demonstrating the gains in bayesian frameworks, whether structured or free-form. Recent semantic segmentation methods that exploit deep convolutional feature extraction mainly approach this problem from either multiscale feature aggregation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref>, or end-to-end structured prediction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b3">4]</ref> perspectives. As context is crucial for semantic segmentation, one notable improvement to most convolutional models emerged from dilated convolutions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> which allows for a larger receptive field without the need of more free parameters. Pyramid scene parsing network (PSPNet) <ref type="bibr" target="#b38">[39]</ref> that uses dilated convolutions in its backbone, and its faster variant <ref type="bibr" target="#b37">[38]</ref> for real-time applications are widely utilized in practical applications. Based on FPN and PSPNet, a multi-task framework is proposed in <ref type="bibr" target="#b33">[34]</ref> and demonstrated to be versatile in segmenting a wide range of visual concepts.</p><p>Instance Segmentation: Instance segmentation deals not only with identifying the semantic class a pixel is associated with, but also the specific object instance that it belongs to. Beginning with the introduction of region-based CNN (R-CNN) <ref type="bibr" target="#b11">[12]</ref>, many early deep learning approaches to instance segmentation attacked the problem by casting the solution to instance segmentation as a two stage approach where a number of segment proposals are made, which is then followed by a voting between those proposals to choose the best <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b40">41]</ref>. The common denominator for these methods is that the segmenta-tion comes before classification, and are therefore slower. Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed a fully convolutional instance-aware segmentation method, where instance mask proposals <ref type="bibr" target="#b6">[7]</ref> are married with fully convolutional networks <ref type="bibr" target="#b25">[26]</ref>. Most recently, Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> introduced a joint approach to both mask prediction and recognition where one of the two parallel heads are dedicated to each task.</p><p>Panoptic Segmentation: Instance segmentation methods that focus on detection bounding box proposals, as mentioned above, ignore the classes that are not well suited for detection, e.g., sky, street. On the other hand, semantic segmentation does not provide instance boundaries for classes like pedestrian and bicycle in a given image. Panoptic segmentation task, first coined by Kirillov et al. <ref type="bibr" target="#b16">[17]</ref> unifies these tasks and defines an ideal output for thing classes as instance segmentations, as well as for stuff classes as semantic segmentation. The baseline panoptic segmentation method introduced in <ref type="bibr" target="#b16">[17]</ref> processes the input independently for semantic segmentation via a PSPNet, and for instance segmentation utilizing a Mask R-CNN <ref type="bibr" target="#b14">[15]</ref>, followed by simple heuristic decisions to produce a single void, stuff, or thing instance label per pixel. Recently, Li et al. <ref type="bibr" target="#b18">[19]</ref> introduced a weakly-and semi-supervised panoptic segmentation method where they relieve some of the ground truth constraints by supervising thing classes using bounding boxes, and stuff classes by utilizing image level tags. De Gaus et al. <ref type="bibr" target="#b9">[10]</ref> uses a single feature extraction backbone for the pyramid semantic segmentation head <ref type="bibr" target="#b38">[39]</ref>, and the instance segmentation head <ref type="bibr" target="#b14">[15]</ref>, followed by heuristics for merging pixel level annotations, effectively introducing an end-to-end version of <ref type="bibr" target="#b16">[17]</ref> due to the shared backbone for the two task networks. Li et al. <ref type="bibr" target="#b19">[20]</ref> propose the attentionguided unified network (AUNet) which leverages proposal and mask level attention to better segment the background. Similar post-processing heuristics as in <ref type="bibr" target="#b16">[17]</ref> are used to generate the final panoptic segmentation. Li et al. <ref type="bibr" target="#b17">[18]</ref> propose things and stuff consistency network (TASCNet) which constructs a binary mask predicting things vs. stuff for each pixel. An extra loss is added to enforce the consistency between things and stuff prediction.</p><p>In contrast to most of these methods, we use a single backbone network to provide both semantic and instance segmentation results. More importantly, we develop a simple yet effective panoptic head which helps accurately predict the instance and class label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Unified Panoptic Sementation Network</head><p>In this section, we first introduce our model and then explain the implementation details. Following the convention of <ref type="bibr" target="#b16">[17]</ref>, we divide the semantic class labels into stuff and thing. Specifically, thing refers to the set of labels of instances (e.g. pedestrian, bicycle), whereas stuff refers to the rest of the labels that represent semantics without clear in- stance boundaries (e.g. street, sky). We denote the number of stuff and thing classes as N stuff and N thing respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">UPSNet Architecture</head><p>UPSNet consists of a shared convolutional feature extraction backbone and multiple heads on top of it. Each head is a sub-network which leverages the features from the backbone and serves a specific design purpose that is explained in further detail below. The overall model architecture is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Backbone: We adopt the original Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> backbone as our convolutional feature extraction network. This backbone exploits a deep residual network (ResNet) <ref type="bibr" target="#b15">[16]</ref> with a feature pyramid network (FPN) <ref type="bibr" target="#b21">[22]</ref>.</p><p>Instance Segmentation Head: The instance segmentation head follows the Mask R-CNN design with a bounding box regression output, a classification output, and a segmentation mask output. The goal of the instance head is to produce instance aware representations that could identify thing classes better. Ultimately these representations are passed to the panoptic head to contribute to the logits for each instance.</p><p>Semantic Segmentation Head: The goal of the semantic segmentation head is to segment all semantic classes without discriminating instances. It could help improving instance segmentation where it achieves good results of thing classes. Our semantic head consists of a deformable convolution <ref type="bibr" target="#b8">[9]</ref> based sub-network which takes the multi-scale feature from FPN as input. In particular, we use P 2 , P 3 , P 4 and P 5 feature maps of FPN which contain 256 channels and are 1/4, 1/8, 1/16 and 1/32 of the original scale respectively. These feature maps first go through the same deformable convolution network independently and are subsequently upsampled to the 1/4 scale. We then concatenate them and apply 1 × 1 convolutions with softmax to predict the semantic class. The architecture is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. As will be experimentally verified later, the deformable convolution along with the multi-scale feature concatenation pro-vide semantic segmentation results as good as a separate model, e.g., a PSPNet adopted in <ref type="bibr" target="#b16">[17]</ref>. Semantic segmentation head is associated with the regular pixel-wise cross entropy loss. To put more emphasis on the foreground objects such as pedestrians, we also incorporate a RoI loss. During training, we use the ground truth bounding box of the instance to crop the logits map after the 1 × 1 convolution and then resize it to 28 × 28 following Mask R-CNN. The RoI loss is then the cross entropy computed over 28 × 28 patch which amounts to penalizing more on the pixels within instances for incorrect classification. As demonstrated in the ablation study later, we empirically found that this RoI loss helps improve the performance of panoptic segmentation without harming the semantic segmentation.</p><p>Panoptic Segmentation Head: Given the semantic and instance segmentation results from the above described two heads, we combine their outputs (specifically per pixel logits) in the panoptic segmentation head.</p><p>The logits from semantic head is denoted as X of which the channel size, height and width are N stuff + N thing , H and W respectively. X can then be divided along channel dimension into two tensors X stuff and X thing which are logits corresponding to stuff and thing classes. For any image, we determine the number of instances N inst according to the number of ground truth instances during training. During inference, we rely on a mask pruning process to determine N inst which is explained in Section 3.2. N stuff is fixed since number of stuff classes is constant across different images, whereas N inst is not constant since the number of instances per image can be different. The goal of our panoptic segmentation head is to first produce a logit tensor Z which is of size (N stuff +N inst )×H ×W and then uniquely determine both the class and instance ID for each pixel.</p><p>We first assign X stuff to the first N stuff channels of Z to provide the logits for classifying stuffs. For any instance i, we have its mask logits Y i from the instance segmentation head which is of size 28 × 28. We also have its box B i and class ID C i . During training B i and C i are ground truth box and class ID whereas during inference they are predicted by  Mask R-CNN. Therefore, we can obtain another representation of i-th instance from semantic head X maski by only taking the values inside box B i from the channel of X thing corresponding to C i . X maski is of size H × W and its values outside box B i are zero. We then interpolate Y i back to the same scale as X maski via bilinear interpolation and pad zero outside the box to achieve a compatible shape with</p><formula xml:id="formula_0">X maski , denoted as Y maski . The final representation of i-th instance is Z Nstuff+i = X maski + Y maski .</formula><p>Once we fill in Z with representations of all instances, we perform a softmax along the channel dimension to predict the pixel-wise class.</p><p>In particular, if the maximum value falls into the first N stuff channel, then it belongs to one of stuff classes. Otherwise the index of the maximum value tells us the instance ID. The architecture is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. During training, we generate the ground truth instance ID following the order of the ground truth boxes we used to construct the panoptic logits. The panoptic segmentation head is then associated with the standard pixel-wise cross entropy loss. During inference, once we predict the instance ID following the above procedure, we still need to determine the class ID of each instance. One can either use the class ID C inst predicted by Mask R-CNN or the one predicted by the semantic head C sem . As shown later in the ablation study, we resort to a better heuristic rule. Specifically, for any instance, we know which pixels correspond to it, i.e., those of which the argmax of Z along channel dimension equals to its instance ID. Among these pixels, we first check whether C inst and C sem are consistent. If so, then we assign the class ID as C inst . Otherwise, we compute the mode of their C sem , denoting asĈ sem . If the frequency of the mode is larger than 0.5 andĈ sem belongs to stuff, then the predicted class ID isĈ sem . Otherwise, we assign the class ID as C inst . In short, while facing inconsistency, we trust the majority decision made by the semantic head only if it prefers a stuff class. The justification of such a conflict resolution heuristic is that semantic head typically achieves very good segmentation results over stuff classes.</p><p>Unknown Prediction: In this section, we explain a novel mechanism to allow UPSNet to classify a pixel as the unknown class instead of making a wrong prediction. To motivate our design, we consider a case where a pedestrian is instead predicted as a bicycle. Since the prediction missed the pedestrian, the false negative (FN) value of pedestrian class will be increased by 1. On the other hand, predicting it as a bicycle will be increasing the false positive (FP) of bicycle class also by 1. Recall that, the panoptic quality (PQ) <ref type="bibr" target="#b16">[17]</ref> metric for panoptic segmentation is defined as,</p><formula xml:id="formula_1">P Q = (p,g)∈TP IoU(p, g) |TP| SQ |TP| |TP| + 1 2 |FP| + 1 2 |FN| RQ ,</formula><p>which consist of two parts: recognition quality (RQ) and semantic quality (SQ). It is clear that increasing either FN or FP degrades this measurement. This phenomena extends to wrong predictions of the stuff classes as well. Therefore, if a wrong prediction is inevitable, predicting such pixel as unknown is preferred since it will increase FN of one class by 1 without affecting FP of the other class.</p><p>To alleviate the issue, we compute the logits of the extra unknown class as Z unknown = max (X thing ) − max (X mask ) where X mask is the concatenation of X maski of all masks along channel dimension and of shape N inst × H × W . The maximum is taken along the channel dimension. The rationale behind this is that for any pixel if the maximum of X thing is larger than the maximum of X mask , then it is highly likely that we are missing some instances (FN). The construction of the logits is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. To generate the ground truth for the unknown class, we randomly sample 30% ground truth masks and set them as unknown during training. In evaluating the metric, any pixel belonging to unknown is ignored, i.e., setting to void which will not contribute to the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>In this section, we explain the implementation details of UPSNet. We follow most of settings and hyper-parameters of Mask R-CNN which will be introduced in the supplementary material. Hereafter, we only explain those which are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training:</head><p>We implement our model in PyTorch <ref type="bibr" target="#b27">[28]</ref> and train it with 16 GPUs using the distributed training framework Horovod <ref type="bibr" target="#b30">[31]</ref>. Images are preprocessed following <ref type="bibr" target="#b14">[15]</ref>. Each mini-batch has 1 image per GPU. As mentioned, we use ground truth box, mask and class label to construct the logits of panoptic head during training. Our region proposal network (RPN) is trained end-to-end with the backbone whereas it was trained separately in <ref type="bibr" target="#b14">[15]</ref>. Due to the high resolution of images, e.g., 1024 × 2048 in Cityscapes, logits from semantic head and panoptic head are downsampled to 1/4 of the original resolution. Although we do not fine-tune batch normalization (BN) layers within the backbone for simplicity, we still achieve comparable results with the state-of-the-art semantic segmentation networks like PSPNet. Based on common practice in semantic <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">39]</ref> and instance segmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b23">24]</ref>, we expect the performance to be further improved with BN layers fine-tuned. Our UPSNet contains 8 loss functions in total: semantic segmentation head (whole image and RoI based pixel-wise classification losses), panoptic segmentation head (whole image based pixel-wise classification loss), RPN (box classification, box regression) and instance segmentation head (box classification, box regression and mask segmentation). Different weighting schemes on these multi-task loss functions could lead to very different training results. As shown in the ablation study, we found the loss balance strategy, i.e., assuring the scales of all losses are roughly on the same order of magnitude, works well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference:</head><p>During inference, once we obtained output boxes, masks and predicted class labels from the instance segmentation head, we apply a mask pruning process to determine which mask will be used for constructing the panoptic logits. In particular, we first perform the class-agnostic non-maximum suppression with the box IoU threshold as 0.5 to filter out some overlapping boxes. Then we sort the predicted class probabilities of the remaining boxes and keep those whose probability are larger than 0.6. For each class, we create a canvas which is of the same size as the image. Then we interpolate masks of that class to the image scale and paste them onto the corresponding canvas one by one following the decreasing order of the probability. Each time we copy a mask, if the intersection between the current mask and those already existed over the size of the current mask is larger than a threshold, we discard this mask. Otherwise we copy the non-intersecting part onto the canvas. The threshold of this intersection over itself is set to 0.3 in our experiments. Logits from the semantic segmentation head and panoptic segmentation head are of the original scale of the input image during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present the experimental results on COCO <ref type="bibr" target="#b22">[23]</ref>, Cityscapes <ref type="bibr" target="#b5">[6]</ref> and our internal dataset. COCO We follow the setup of COCO 2018 panoptic segmentation task which consists of 80 and 53 classes for thing and stuff respectively. We use train2017 and val2017 subsets which contain approximately 118k training images and 5k validation images.</p><p>Cityscapes Cityscapes has 5000 images of ego-centric driving scenarios in urban settings which are split into 2975, 500 and 1525 for training, validation and testing respectively. It consists of 8 and 11 classes for thing and stuff.</p><p>Our Dataset We also use an internal dataset which is similar to Cityscapes and consists of 10235 training, 1139 validation and 1186 test images of ego-centric driving scenarios. Our dataset consists of 10 and 17 classes for thing (e.g., car, bus) and stuff (e.g., building, road) respectively.</p><p>Experimental Setup For all datasets, we report results on the validation set. To evaluate the performance, we adopt panoptic quality (PQ), recognition quality (RQ) and semantic quality (SQ) <ref type="bibr" target="#b16">[17]</ref> as the metrics. We also report average precision (AP) of mask prediction, mean IoU of semantic segmentation on both stuff and thing and the inference runtime for comparison. At last, we show results of ablation study on various design components of our model. Full results with all model variants are shown in the supplementary material.</p><p>We set the learning rate and weight decay as 0.02 and 0.0001 for all datasets. For COCO, we train for 90K iterations and decay the learning rate by a factor of 10 at 60K and 80K iterations. For Cityscapes, we train for 12K iterations and apply the same learning rate decay at 9K iterations. For our dataset, we train for 36K iterations and apply the same learning rate decay at 24K and 32K iterations. Loss weights of semantic head are 0.2, 1.0 and 1.0 on COCO, Cityscapes and ours respectively. RoI loss weights are one fifth of those of semantic head. Loss weights of panoptic head are 0.1, 0.5 and 0.3 on COCO, Cityscapes and ours respectively. All other loss weights are set to 1.0.</p><p>We mainly compare with the combined method used in <ref type="bibr" target="#b16">[17]</ref>. For a fair comparison, we adopt the model which uses a Mask R-CNN with a ResNet-50-FPN and a PSPNet with a ResNet-50 as the backbone and apply the combine   heuristics to compute the panoptic segmentation. We denote our implementation of the combined method as 'MR-CNN-PSP' and its multi-scale testing version as 'MR-CNN-PSP-M'. Unless specified otherwise, the combined method hereafter refers to 'MR-CNN-PSP'. For PSPNet, we use 'poly' learning rate schedule as in <ref type="bibr" target="#b2">[3]</ref> and train 220K, 18K and 76K on COCO, Cityscapes and our dataset with mini-batch size 16. We test all available models with the multi-scale testing. Specifically, we average the multi-scale logits of PSPNet for the combined method and the ones of semantic segmentation head for our UPSNet. For simplicity, we just use single scale testing on Mask R-CNN of the combined method and our instance segmentation head. During evaluation, due to the sensitivity of PQ with respect to RQ, we predict all stuff segments of which the areas are smaller than a threshold as unknown. The thresholds on COCO, Cityscapes and our dataset are 4096, 2048 and 2048 respectively. To be fair, we apply this area thresholding to all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">COCO</head><p>We compare with several recent published methods including JSIS-Net <ref type="bibr" target="#b9">[10]</ref>, RN50-MR-CNN 1 and the combined method <ref type="bibr" target="#b16">[17]</ref> on COCO. Since authors in <ref type="bibr" target="#b16">[17]</ref> do not report results on COCO, we use our MR-CNN-PSP model as the alternative to do the experiments. JSIS-Net uses a ResNet-50 wheres RN50-MR-CNN uses two separate ResNet-50-FPNs as the backbone. Our UPSNet adopts a ResNet-50-FPN as the backbone. In order to better leverage context information, we use a global average pooling over the feature map of the last layer in the 5-th stage of ResNet ('res5'), reduce its dimension to 256 and add back to FPN before producing P 5 feature map. <ref type="table">Table 7</ref> shows the results of all metrics. The mIoU metric is computed over the 133 classes of stuff and thing in the COCO 2018 panoptic segmentation task which is different from previous 172 classes of COCO-Stuff. We are among the first to evaluate mIoU on this 133-class subset. From the table, we can see that our UPSNet achieves better performance in all metrics except the SQ. It is typically the case that the an increase in RQ leads to the slight decrease of SQ since we include more TP segments which could have possibly lower IoU. Note that even with multi-scale testing, MR-CNN-PSP is still worse than ours on PQ. Moreover, from the mIoU column, we can see that the performance of our semantic head is even better than a separate PSPNet which verifies its effectiveness. With multi-scale testing, both MR-CNN-PSP and UPSNet are improved and ours is still better. We also add the comparisons on the test-dev of MS-COCO 2018 in <ref type="table" target="#tab_3">Table 2</ref>. Although we just use ResNet-101 as the backbone, we achieve slightly better results compared to the recent AUNet <ref type="bibr" target="#b19">[20]</ref> which uses ResNeXt-152. We also list the top three results on the leadboard which uses ensemble and other tricks. It is clear from the table that we are on par with the second best model without using any such tricks. In terms of the model size, RN50-MR-CNN, MR-CNN-PSP and UP-SNet consists of 71.2M, 91.6M and 46.1M parameters respectively. Therefore, our model is significantly lighter.  dataset in the first two rows of <ref type="figure">Fig. 4</ref>. From the 1-st row of the figure, we can see that the combined method completely ignores the cake and other objects on the table whereas ours successfully segments them out. This is due to the inherent limitations of the combine heuristic which first pastes the high confidence segment, i.e., table, and then ignores all highly overlapped objects thereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We show visual examples of panoptic segmentation on this</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cityscapes</head><p>We compare our model on Cityscapes with Li et al. <ref type="bibr" target="#b18">[19]</ref>, the combined method <ref type="bibr" target="#b16">[17]</ref> and TASCNet <ref type="bibr" target="#b17">[18]</ref>. Note that the method in <ref type="bibr" target="#b18">[19]</ref> uses a ResNet-101 as the backbone whereas all other reported methods use ResNet-50 within their backbones. We use 2 deformable convolution layers for the semantic head. The results are reported in <ref type="table">Table 8</ref>. It is clear from the table that both our UPSNet and MR-CNN-PSP significantly outperform the method in <ref type="bibr" target="#b18">[19]</ref>, especially on PQ Th . This may possibly be caused by the fact that their CRF based instance subnetwork performs worse compared to Mask R-CNN on instance segmentation. Under the same single scale testing, our model achieves better performance than the combined method. Although multi-scale testing significantly improves both the combined method and UP-SNet, ours is still slightly better. Results reported in <ref type="bibr" target="#b16">[17]</ref> are different from the ones of our MR-CNN-PSP-M since: 1) they use ResNet-101 as the backbone for PSPNet; 2) they pre-train Mask R-CNN on COCO and PSPNet on extra coarsely labeled data. We also have a model variant, denoting as 'Ours-101-M', which adopts ResNet-101 as the backbone and pre-trains on COCO. As you can see, it outperforms the reported metrics of the combined method. We show the visual examples of panoptic segmentation on this dataset in the 3-rd and 4-th rows of <ref type="figure">Fig. 4</ref>. From the 3rd row of the figure, we can see that the combined method tends to produce large black area, i.e., unknown, whenever the instance and semantic segmentation conflicts with each other. In contrast, our UPSNet resolves the conflicts better. Moreover, it is interesting to note that some unknown prediction of our model has the vertical or horizontal boundaries. This is caused by the fact that instance head predicts nothing whereas semantic head predicts something for these out-of-box areas. The logits of unknown class will then stand out by design to avoid contributing to both FP and FN as described in section 3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Our Dataset</head><p>Last but not least, we compare our model on our own dataset with the combined method <ref type="bibr" target="#b16">[17]</ref>. All reported methods use ResNet-50 within their backbones. We use 2 deformable convolution layers for the semantic head. The results are reported in <ref type="table">Table 9</ref>. We can observe that similar to COCO, our model performs significantly better than the combined method on all metrics except SQ. We show the visual examples of panoptic segmentation on this dataset in the last two rows of <ref type="figure">Fig. 4</ref>. From the examples, similar observations as COCO and Cityscapes are found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Run Time Comparison</head><p>We compare the run time of inference on all three datasets in <ref type="table" target="#tab_7">Table 5</ref>. We use a single NVIDIA GeForce GTX 1080 Ti GPU and an Intel Xeon E5-2687W CPU (3.00GHz). All entries are averaged over 100 runs on the same image with single scale test. For COCO, the PSPNet within the combined method uses the original scale. We also list the image size on each dataset. It is clearly seen in the table that as the image size increases, our UPSNet is significantly faster in run time. For example, the combined method takes about 3× time than ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>We perform extensive ablation studies on COCO dataset to verify our design choices as listed in <ref type="table" target="#tab_8">Table 6</ref>. Empty cells in the table indicate the corresponding component is not used. All evaluation metrics are computed over the output of the panoptic head on the validation set.</p><p>Panoptic Head: Since the inference of our panoptic head is applicable as long as we have outputs from both semantic and instance segmentation heads, we can first train these two heads simultaneously and then directly evaluate the panoptic head. We compare the results with the ones obtained by training all three heads. By doing so, we can verify the gain of training the panoptic head over treating it as a post processing procedure. From the first two rows of <ref type="table" target="#tab_8">Table 6</ref>, we can see that training the panoptic head does improve the PQ metric.</p><p>Instance Class Assignment: Here, we focus on differ-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground truth Combined method <ref type="bibr" target="#b16">[17]</ref> Ours <ref type="figure">Figure 4</ref>: Visual examples of panoptic segmentation. 1-st and 2-nd rows are from COCO. 3-rd and 4-th rows are from Cityscapes. 5-th and 6-th rows are from our internal dataset. ent alternatives of assigning instance class. We compare our heuristic as described in section 3.1 with the one which only trusts the predicted class given by the instance segmentation head. As you can see from the 2-nd and 3-rd rows of the Table 6, our instance class assignment is better.</p><p>Loss Balance: We investigate the weighting scheme of loss functions. Recall that without the proposed RoI loss, our UPSNet contains 7 loss functions. In order to weight them, we follow the principle of loss balance, i.e., making sure their values are roughly on the same order of magni-tude. In particular, with loss balance, we set the weights of semantic and panoptic losses as 0.2 and 0.1 and the rest ones as 1.0. Without loss balance, we set the weights of both semantic and panoptic losses as 0.1 and the rest ones as 1.0. The 3-rd and 4-th rows of <ref type="table" target="#tab_8">Table 6</ref> show that introducing the loss balance improves the performance.</p><p>RoI Loss &amp; Unknown Prediction: Here, we investigate the effectiveness of our RoI loss function over the semantic head and the unknown prediction. From 4-th and 5-th rows of   <ref type="table" target="#tab_8">Table 6</ref>, along with the RoI loss, predicting unknown class improves the metrics significantly.</p><p>ORACLE Results: We also explore the room for improvement of the current system by replacing some inference results with the ground truth (GT) ones. Specifically, we study the box, instance class assignment and semantic segmentation results which correspond to GT Box, GT ICA and GT Seg. columns in <ref type="table" target="#tab_8">Table 6</ref>. It is clear from the table that using GT boxes along with our predicted class probabilities improves PQ which indicates that better region proposals are required to achieve higher recall. On top of the GT boxes, using the GT class assignment greatly improves the PQ, e.g., ≈ +7.0. The imperfect PQ Th indicates that our mask segmentation is not good enough. Moreover, using the GT semantic segmentation gives the largest gain of PQ, i.e., +29.5, which highlights the importance of improving semantic segmentation. PQ St is imperfect since we resize images during inference which causes the misalignment with labels. It is worth noticing that increasing semantic segmentation also boosts PQ Th for 10 points. This is because our model leverages semantic segments while producing instance segments. However, it is not the case for the combined method as its predicted instance segments only relies on the instance segmentation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed the UPSNet which provides a unified framework for panoptic segmentation. It exploits a single backbone network and two lightweight heads to predict semantic and instance segmentation in one shot. More importantly, our parameter-free panoptic head leverages the logits from the above two heads and has the flexibility to predict an extra unknown class. It handles the varying number of classes per image and enables back propagation for the bottom representation learning. Empirical results on three large datasets show that our UPSNet achieves stateof-the-art performance with significantly faster inference compared to other methods. In the future, we would like to explore more powerful backbone networks and smarter parameterization of panoptic head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary Material</head><p>We first explain the hyper-parameters and then provide full experimental results on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Hyperparameters</head><p>For all our experiments, we exploit a 1500-iteration warm-up phase where the learning rate gradually increases from 0.002 to 0.02. We also initialize all models with Ima-geNet pre-trained weights released by MSRA 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO:</head><p>We resize the image such that the length of the shorter edge is 800 and the length of the longer edge does not exceed 1333. We do not utilize multiscale training. For testing, we feed multi-scale images for all models. Specifically, we resize the images to multiple scales of which the shorter edge equals to {480, 544, 608, 672, 736, 800, 864, 928, 992, 1056, 1120} respectively. We also add left-right flipping. Finally, we average the semantic segmentation logits under different scales. For PSPNet, we do sliding window test with 513 × 513 cropped image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cityscapes:</head><p>We do multi-scale training where we resize the input image in a way that the length of the shorter edge is randomly sampled from [800, 1024].</p><p>For multi-scale testing, we use the same protocol as COCO except the set of scales is {704, 768, 832, 896, 960, 1024, 1088, 1152, 1216, 1280, 1344}. For PSPNet, we do sliding window test with 713 × 713 cropped image.</p><p>Our Dataset: We utilize multi-scale training where the length of the shorter edge is randomly sampled from [800, 1200]. We do not perform multi-scale testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Full Experimental Results</head><p>We show the full experimental results including run time in <ref type="table">Table 7</ref>, <ref type="table">Table 8 and Table 9</ref>. We also add two more variants of our model as baselines, i.e., UPSNet-C and UPSNet-CP. UPSNet-C is UPSNet without the panoptic head. We train it with just semantic and instance segmentation losses. During test, we use the same combine heuristics as in <ref type="bibr" target="#b16">[17]</ref> to generate the final prediction. UPSNet-CP has the same model as UPSNet. We train it in the same way as UPSNet as well. During test, we use the same combine heuristics as in <ref type="bibr" target="#b16">[17]</ref> to generate the final prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overall architecture of our UPSNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of our semantic segmentation head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of our panoptic segmentation head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>ModelsPQ</head><label></label><figDesc>SQ RQ PQ Th PQ St mIoU AP MR-CNN-PSP 45.5 77.1 57.6 40.1 48.7 70.5 29.6 Ours 47.1 77.1 59.4 43.8 49.0 70.8 30.4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Models PQ SQ RQ PQ Th PQ St mIoU AP JSIS-Net [10] 26.9 72.4 35.7 29.3 23.3 --RN50-MR-CNN 38.6 76.4 47.5 46.2 27.1 --MR-CNN-PSP 41.8 78.4 51.3 47.8 32.8 53.9 34.2 Ours 42.5 78.0 52.4 48.5 33.4 54.3 34.3</figDesc><table><row><cell>Multi-scale</cell><cell>PQ SQ RQ PQ Th PQ St mIoU AP</cell></row><row><cell cols="2">MR-CNN-PSP-M 42.2 78.5 51.7 47.8 33.8 55.3 34.2</cell></row><row><cell>Ours-M</cell><cell>43.2 79.2 52.9 49.1 34.1 55.8 34.3</cell></row><row><cell cols="2">Table 1: Panoptic segmentation results on COCO. Super-</cell></row><row><cell cols="2">scripts Th and St stand for thing and stuff. '-' means inap-</cell></row><row><cell>plicable.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Models backbone PQ SQ RQ PQ Th SQ Th RQ Th PQ St SQ St RQ St Megvii (Face++) ensemble model 53.2 83.2 62.9 62.2 85.5 72.5 39.5 79.7 48.5 Caribbean ensemble model 46.8 80.5 57.1 54.3 81.8 65.9 35.5 78.5 43.8 PKU 360 ResNeXt-152-FPN 46.3 79.6 56.1 58.6 83.7 69.6 27.6 73.6 35.6</figDesc><table><row><cell>JSIS-Net [10]</cell><cell>ResNet-50</cell><cell>27.2 71.9 35.9 29.6 71.6 39.4 23.4 72.3 30.6</cell></row><row><cell>AUNet [20]</cell><cell cols="2">ResNeXt-152-FPN 46.5 81.0 56.1 55.9 83.7 66.3 32.5 77.0 40.7</cell></row><row><cell>Ours</cell><cell cols="2">ResNet-101-FPN 46.6 80.5 56.9 53.2 81.5 64.6 36.7 78.9 45.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Panoptic segmentation results on MS-COCO 2018 test-dev. The top 3 rows contain results of top 3 models taken from the official leadboard. COCO 61.8 81.3 74.8 57.6 64.8 79.2 39.0</figDesc><table><row><cell>Models</cell><cell cols="4">PQ SQ RQ PQ Th PQ St mIoU AP</cell></row><row><cell>Li et al. [19]</cell><cell>53.8 -</cell><cell cols="3">-42.5 62.1 71.6 28.6</cell></row><row><cell>MR-CNN-PSP</cell><cell cols="4">58.0 79.2 71.8 52.3 62.2 75.2 32.8</cell></row><row><cell>TASCNet [18]</cell><cell>55.9 -</cell><cell>-50.5 59.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="4">59.3 79.7 73.0 54.6 62.7 75.2 33.3</cell></row><row><cell cols="2">TASCNet-COCO [18] 59.2 -</cell><cell>-56.0 61.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours-COCO</cell><cell cols="4">60.5 80.9 73.5 57.0 63.0 77.8 37.8</cell></row><row><cell>Multi-scale</cell><cell cols="4">PQ SQ RQ PQ Th PQ St mIoU AP</cell></row><row><cell cols="5">Kirillov et al. [17] 61.2 80.9 74.4 54.0 66.4 80.9 36.4</cell></row><row><cell cols="5">MR-CNN-PSP-M 59.2 79.7 73.0 52.3 64.2 76.9 32.8</cell></row><row><cell>Ours-M</cell><cell cols="4">60.1 80.3 73.5 55.0 63.7 76.8 33.3</cell></row><row><cell>Ours-101-M-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Panoptic segmentation results on Cityscapes. '-COCO' means the model is pretrained on COCO. '-101' means the model uses ResNet-101 as the backbone. Unless specified, all models use ResNet-50 as the backbone and are pretrained on ImageNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Panoptic segmentation results on our dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Run time (ms) comparison. Note the bracket below the MR-CNN-PSP results contains their breakdown into the network inference (left) and the combine heurisitc (right).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 ,</head><label>6</label><figDesc>one can conclude that adding such a new Pano.</figDesc><table><row><cell>Our ICA</cell><cell>Loss Bal.</cell><cell>RoI Loss</cell><cell>Unk.</cell><cell>GT Box</cell><cell>GT ICA</cell><cell>GT Seg.</cell><cell>PQ PQ Th PQ St</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>41.2 47.6 31.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>41.6 47.6 32.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>41.7 47.7 32.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>42.3 48.4 33.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>42.3 48.3 33.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>42.5 48.5 33.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.7 55.3 33.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>53.0 64.6 35.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>72.0 58.6 92.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on COCO dataset. 'Pano.', 'Loss Bal.', 'Unk.' and 'ICA' stand for training with panoptic loss, loss balance, unknown prediction and instance class assignment respectively.</figDesc><table /><note>loss function does slightly boost the PQ St . From 5-th and 6-th rows of</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://competitions.codalab.org/competitions/ 19507#results</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/KaimingHe/deep-residual-networks</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher order conditional random fields in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Panoptic segmentation with a joint semantic and instance segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Geus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02110</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00868</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Panoptic segmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01192</idno>
		<title level="m">Learning to fuse things and stuff</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Attention-guided unified network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03904</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07240</idno>
		<title level="m">Megdet: A large mini-batch object detector</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="140" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="702" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-End Instance Segmentation with Recurrent Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Q</forename><surname>Sq Rq Pq Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Q</forename><surname>Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Q</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Q</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>St</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AP box AP mask Run Time</title>
		<imprint/>
	</monogr>
	<note>ms</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Q</forename><surname>Multi-Scale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Q</forename><surname>Sq Rq Pq Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Q</forename><surname>Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Q</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>St</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AP box AP mask Run Time</title>
		<imprint/>
	</monogr>
	<note>ms</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">Panoptic segmentation results on COCO. Superscripts Th and St stand for thing and stuff. &apos;-&apos; means inapplicable. &apos; * &apos; means the run time of UPSNet-CP is the same with the one of UPSNet-C. Models PQ SQ RQ PQ Th SQ Th RQ Th PQ St SQ St RQ St mIoU AP box AP mask Run Time (ms)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno>19] 53.8 - -42.5 - - 62.1 - - 71.6 - 28.6 - MR-CNN-PSP 58</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Q</forename><surname>Multi-Scale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Q</forename><surname>Sq Rq Pq Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Q</forename><surname>Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Q</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>St</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AP box AP mask Run Time</title>
		<imprint/>
	</monogr>
	<note>ms</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Panoptic segmentation results on Cityscapes</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Q</forename><surname>Models</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Q</forename><surname>Sq Rq Pq Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Q</forename><surname>Th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Q</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>St</surname></persName>
		</author>
		<idno>mIoU AP box AP mask Run Time MR-CNN-PSP 45</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
