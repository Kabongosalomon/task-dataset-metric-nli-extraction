<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ECO: Efficient Convolution Operators for Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
							<email>martin.danelljan@liu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fahad</roleName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
							<email>goutam.bhat@liu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
							<email>michael.felsberg@liu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ECO: Efficient Convolution Operators for Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, Discriminative Correlation Filter (DCF) based methods have significantly advanced the state-of-theart in tracking. However, in the pursuit of ever increasing tracking performance, their characteristic speed and realtime capability have gradually faded. Further, the increasingly complex models, with massive number of trainable parameters, have introduced the risk of severe over-fitting. In this work, we tackle the key causes behind the problems of computational complexity and over-fitting, with the aim of simultaneously improving both speed and performance.</p><p>We revisit the core DCF formulation and introduce: (i) a factorized convolution operator, which drastically reduces the number of parameters in the model; (ii) a compact generative model of the training sample distribution, that significantly reduces memory and time complexity, while providing better diversity of samples; (iii) a conservative model update strategy with improved robustness and reduced complexity. We perform comprehensive experiments on four benchmarks: VOT2016, UAV123, OTB-2015, and Temple-Color. When using expensive deep features, our tracker provides a 20-fold speedup and achieves a 13.0% relative gain in Expected Average Overlap compared to the top ranked method <ref type="bibr" target="#b11">[12]</ref> in the VOT2016 challenge. Moreover, our fast variant, using hand-crafted features, operates at 60 Hz on a single CPU, while obtaining 65.0% AUC on OTB-2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ECO</head><p>C-COT arXiv:1611.09224v2 [cs.CV]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generic visual tracking is one of the fundamental problems in computer vision. It is the task of estimating the trajectory of a target in an image sequence, given only its initial state. Online visual tracking plays a crucial role in numerous real-time vision applications, such as smart surveillance systems, autonomous driving, UAV monitoring, intelligent traffic control, and human-computer-interfaces. Due to the online nature of tracking, an ideal tracker should be accurate and robust under the hard computational constraints of real-time vision systems.</p><p>In recent years, Discriminative Correlation Filter (DCF) <ref type="figure">Figure 1</ref>. A comparison of our approach ECO with the baseline C-COT <ref type="bibr" target="#b11">[12]</ref> on three example sequences. In all three cases, C-COT suffers from severe over-fitting to particular regions of the target. This causes poor target estimation in cases of scale variations (top row), deformations (middle row), and out-of-plane rotations (bottom row). Our ECO tracker successfully tackles the causes of over-fitting, leading to better generalization of the target appearance, while achieving a 20-fold speedup.</p><p>based approaches have shown continuous performance improvements in terms of accuracy and robustness on tracking benchmarks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref>. The recent advancement in DCF based tracking performance is driven by the use of multi-dimensional features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>, robust scale estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>, non-linear kernels <ref type="bibr" target="#b19">[20]</ref>, long-term memory components <ref type="bibr" target="#b27">[28]</ref>, sophisticated learning models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> and reducing boundary effects <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>. However, these improvements in accuracy come at the price of significant reductions in tracking speed. For instance, the pioneering MOSSE tracker by Bolme et al. <ref type="bibr" target="#b3">[4]</ref> is about 1000× faster than the recent top-ranked DCF tracker, C-COT <ref type="bibr" target="#b11">[12]</ref>, in the VOT2016 challenge <ref type="bibr" target="#b22">[23]</ref>, but obtains only half the accuracy. As mentioned above, the advancement in DCF tracking performance is predominantly attributed to powerful features and sophisticated learning formulations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref>. This has led to substantially larger models, requiring hundreds of thousands of trainable parameters. On the other hand, such complex and large models have introduced the risk of severe over-fitting (see <ref type="figure">figure 1</ref>). In this paper, we tackle the issues of over-fitting in recent DCF trackers, while restoring their hallmark real-time capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Motivation</head><p>We identify three key factors that contribute to both increased computational complexity and over-fitting in stateof-the-art DCF trackers. Model size: The integration of high-dimensional feature maps, such as deep features, has led to a radical increase in the number of appearance model parameters, often beyond the dimensionality of the input image. As an example, C-COT <ref type="bibr" target="#b11">[12]</ref> continuously updates about 800,000 parameters during the online learning of the model. Due to the inherent scarcity of training data in tracking, such a highdimensional parameter space is prone to over-fitting. Further, the high dimensionality causes an increase in the computational complexity, leading to slow tracking speed. Training set size: State-of-the-art DCF trackers, including C-COT, require a large training sample set to be stored due to their reliance on iterative optimization algorithms. In practice however, the memory size is limited, particularly when using high-dimensional features. A typical strategy for maintaining a feasible memory consumption is to discard the oldest samples. This may however cause overfitting to recent appearance changes, leading to model drift (see <ref type="figure">figure 1</ref>). Moreover, a large training set increases the computational burden. Model update: Most DCF-based trackers apply a continuous learning strategy, where the model is updated rigorously in every frame. On the contrary, recent works have shown impressive performance without any model update, using Siamese networks <ref type="bibr" target="#b1">[2]</ref>. Motivated by these findings, we argue that the continuous model update in state-of-theart DCF is excessive and sensitive to sudden changes caused by, e.g., scale variations, deformations, and out-of-plane rotations (see <ref type="figure">figure 1</ref>). This excessive update strategy causes both lower frame-rates and degradation of robustness due to over-fitting to the recent frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>We propose a novel formulation that addresses the previously listed issues of state-of-the-art DCF trackers. As our first contribution, we introduce a factorized convolution operator that dramatically reduces the number of parameters in the DCF model. Our second contribution is a compact generative model of the training sample space that effectively reduces the number of samples in the learning, while maintaining their diversity. As our final contribution, we introduce an efficient model update strategy, that simultaneously improves tracking speed and robustness.</p><p>Comprehensive experiments clearly demonstrate that our approach concurrently improves both tracking performance and speed, thereby setting a new state-of-the-art on four benchmarks: VOT2016, UAV123, OTB-2015, and Temple-Color. Our approach significantly reduces the number of model parameters by 80%, training samples by 90% and optimization iterations by 80% in the learning, compared to the baseline. On VOT2016, our approach outperforms the top ranked tracker, C-COT <ref type="bibr" target="#b11">[12]</ref>, in the challenge, while achieving a significantly higher frame-rate. Furthermore, we propose a fast variant of our tracker that maintains competitive performance, with a speed of 60 frames per second (FPS) on a single CPU, thereby being especially suitable for computationally restricted robotics platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Baseline Approach: C-COT</head><p>In this work, we collectively address the problems of computational complexity and over-fitting in state-of-theart DCF trackers. We adopt the recently introduced Continuous Convolution Operator Tracker (C-COT) <ref type="bibr" target="#b11">[12]</ref> as our baseline. The C-COT obtained the top rank in the recent VOT2016 challenge <ref type="bibr" target="#b22">[23]</ref>, and has demonstrated outstanding results on other tracking benchmarks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38]</ref>. Unlike the standard DCF formulation, Danelljan et al. <ref type="bibr" target="#b11">[12]</ref> pose the problem of learning the filters in the continuous spatial domain. The generalized formulation in C-COT yields two advantages that are relevant to our work.</p><p>The first advantage of C-COT is the natural integration of multi-resolution feature maps, achieved by performing convolutions in the continuous domain. This provides the flexibility of choosing the cell size (i.e. resolution) of each visual feature independently, without the need for explicit re-sampling. The second advantage is that the predicted detection scores of the target are directly obtained as a continuous function, enabling accurate sub-grid localization.</p><p>Here, we briefly describe the C-COT formulation, adopting the same notation as in <ref type="bibr" target="#b11">[12]</ref> for convenience. The C-COT discriminatively learns a convolution filter based on a collection of M training samples {x j } M 1 ⊂ X . Unlike the standard DCF, each feature layer x d j ∈ R N d has an independent resolution N d . <ref type="bibr" target="#b0">1</ref> The feature map is transfered to the continuous spatial domain t ∈ [0, T ) by introducing an interpolation model, given by the operator J d ,</p><formula xml:id="formula_0">J d x d (t) = N d −1 n=0 x d [n]b d t − T N d n .<label>(1)</label></formula><p>Here, b d is an interpolation kernel with period T &gt; 0. The result J d x d is thus an interpolated feature layer, viewed as a continuous T -periodic function. We use J{x} to denote the entire interpolated feature map, where J{x}(t) ∈ R D .</p><p>In the C-COT formulation, a continuous T -periodic multi-channel convolution filter f = (f 1 . . . f D ) is trained to predict the detection scores S f {x}(t) of the target as,</p><formula xml:id="formula_1">S f {x} = f * J{x} = D d=1 f d * J d x d .<label>(2)</label></formula><p>The scores are defined in the corresponding image region t ∈ [0, T ) of the feature map x ∈ X . In (2), the convolution of single-channel T -periodic functions is defined as</p><formula xml:id="formula_2">f * g(t) = 1 T T 0 f (t − τ )g(τ ) dτ .</formula><p>The multi-channel convolution f * J{x} is obtained by summing the result of all channels, as defined in <ref type="bibr" target="#b1">(2)</ref>. The filters are learned by minimizing the following objective,</p><formula xml:id="formula_3">E(f ) = M j=1 α j S f {x j } − y j 2 L 2 + D d=1 wf d 2 L 2 . (3)</formula><p>The labeled detection scores y j (t) of sample x j is set to a periodically repeated Gaussian function. The data term consists of the weighted classification error, given by the</p><formula xml:id="formula_4">L 2 -norm g 2 L 2 = 1 T T 0 |g(t)| 2 dt, where α j ≥ 0 is the weight of sample x j .</formula><p>The regularization integrates a spatial penalty w(t) to mitigate the drawbacks of the periodic assumption, while enabling an extended spatial support <ref type="bibr" target="#b8">[9]</ref>.</p><p>As in previous DCF methods, a more tractable optimization problem is obtained by changing to the Fourier basis. Parseval's formula implies the equivalent loss,</p><formula xml:id="formula_5">E(f ) = M j=1 α j S f {x j } −ŷ j 2 2 + D d=1 ŵ * f d 2 2 . (4)</formula><p>Here, the hatĝ of a T -periodic function g denotes the Fourier series coefficientsĝ[k] = 1</p><formula xml:id="formula_6">T T 0 g(t)e −i 2π</formula><p>T kt dt and the 2 -norm is defined by</p><formula xml:id="formula_7">ĝ 2 2 = ∞ −∞ |ĝ[k]| 2 .</formula><p>The Fourier coefficients of the detection scores (2) are given by the formula</p><formula xml:id="formula_8">S f {x} = D d=1f d X db d , where X d is the Dis- crete Fourier Transform (DFT) of x d .</formula><p>In practice, the filters f d are assumed to have finitely many non-zero Fourier coefficients</p><formula xml:id="formula_9">{f d [k]} K d −K d , where K d = N d 2 .</formula><p>Eq. (4) then becomes a quadratic problem, optimized by solving the normal equations,</p><formula xml:id="formula_10">A H ΓA + W H W f = A H Γŷ .<label>(5)</label></formula><p>Here,f andŷ are vectorizations of the Fourier coefficients in f d and y j , respectively. The matrix A exhibits a sparse structure, with diagonal blocks containing elements of the form</p><formula xml:id="formula_11">X d j [k]b d [k]</formula><p>. Further, Γ is a diagonal matrix of the weights α j and W is a convolution matrix with the kernelŵ[k]. The C-COT <ref type="bibr" target="#b11">[12]</ref> employs the Conjugate Gradient (CG) method <ref type="bibr" target="#b31">[32]</ref> to iteratively solve (5), since it was shown to effectively utilize the sparsity structure of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>As discussed earlier, over-fitting and computational bottlenecks in the DCF learning stem from common factors. We therefore proceed with a collective treatment of these issues, aiming at both improved performance and speed. Robust learning: As mentioned earlier, the large number of optimized parameters in (3) may cause over-fitting due to limited training data. We alleviate this issue by introducing a factorized convolution formulation in section 3.1. This strategy radically reduces the number of model parameters by 80% in the case of deep features, while increasing tracking performance. Moreover, we propose a compact generative model of the sample distribution in section 3.2, that boosts diversity and avoids the previously discussed problems related to storing a large sample set. Finally, we investigate strategies for updating the model in section 3.3 and conclude that a less frequent update of the filter stabilizes the learning, which results in more robust tracking. Computational complexity: The learning step is the computational bottleneck in optimization-based DCF trackers, such as C-COT. The computational complexity of the appearance model optimization in C-COT is obtained by analyzing the Conjugate Gradient algorithm applied to <ref type="bibr" target="#b4">(5)</ref>. The complexity can be expressed as O(N CG DMK), 2 where N CG is the number of CG iterations andK = 1 D d K d is the average number of Fourier coefficients per filter channel. Motivated by this complexity analysis of the learning, we propose methods for reducing D, M and N CG in sections 3.1, 3.2, and 3.3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Factorized Convolution Operator</head><p>We first introduce a factorized convolution approach, with the aim of reducing the number of parameters in the model. We observed that many of the filters f d learned in C-COT contain negligible energy. This is particularly apparent for high-dimensional deep features, as visualized in <ref type="figure">figure 2</ref>. Such filters hardly contribute to target localization, but still affect the training time. Instead of learning one separate filter for each feature channel d, we use a smaller set of basis filters f 1 , . . . , f C , where C &lt; D. The filter for feature layer d is then constructed as a linear combination C c=1 p d,c f c of the filters f c using a set of learned coefficients p d,c . The coefficients can be compactly represented as a D × C matrix P = (p d,c ). The new multi-channel filter can then be written as the matrix-vector product P f . We obtain the factorized convolution operator, be viewed as a two-step operation where the feature vector J{x}(t) at each location t is first multiplied with the matrix P T . The resulting C-dimensional feature map is then convolved with the filter f . The matrix P T thus resembles a linear dimensionality reduction operator, as used in e.g. <ref type="bibr" target="#b12">[13]</ref>. The key difference is that we learn the filter f and matrix P jointly, in a discriminative fashion, by minimizing the classification error (3) of the factorized operator <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_12">S P f {x} = P f * J{x} = c,d p d,c f c * J d x d = f * P T J{x}.</formula><p>For simplicity, we consider learning the factorized operator (6) from single training sample x. To simplify notation, we useẑ</p><formula xml:id="formula_13">d [k] = X d [k]b d [k]</formula><p>to denote the Fourier coefficients of the interpolated feature map z = J{x}. The corresponding loss in the Fourier domain (4) is derived as,</p><formula xml:id="formula_14">E(f, P ) = ẑ T Pf −ŷ 2 2 + C c=1 ŵ * f c 2 2 +λ P 2 F . (7)</formula><p>Here we have added the Frobenius norm of P as a regularization, controlled by the weight parameter λ. Unlike the original formulation (4), our new loss <ref type="formula">(7)</ref> is a non-linear least squares problem. Due to the bi-linearity of z T Pf , the loss <ref type="formula">(7)</ref> is similar to a matrix factorization problem <ref type="bibr" target="#b20">[21]</ref>. Popular optimization strategies for these applications, including Alternating Least Squares, are however not feasible due to the parameter size and online nature of our problem. Instead, we employ Gauss-Newton <ref type="bibr" target="#b31">[32]</ref> and use the Conjugate Gradient method to optimize the quadratic subproblems. The Gauss-Newton method is derived by linearizing the residuals in (7) using a first order Taylor series expansion. Here, this corresponds to approximating the bilinear termẑ T Pf around the current estimate (f i , P i ) as,</p><formula xml:id="formula_15">z T (P i + ∆P )(f i + ∆f ) ≈ẑ T P ifi,∆ +ẑ T ∆Pf i (8) =ẑ T P ifi,∆ + (f i ⊗ẑ) T vec(∆P ).</formula><p>Here, we setf i,∆ =f i + ∆f . In the last equality, the Kronecker product ⊗ is used to obtain a vectorization of the matrix step ∆P .</p><p>The Gauss-Newton subproblem at iteration i is derived by substituting the first-order approximation (8) into <ref type="formula">(7)</ref>,</p><formula xml:id="formula_16">E(f i,∆ , ∆P ) = ẑ T P ifi,∆ + (f i ⊗ẑ) T vec(∆P ) −ŷ 2 2 + C c=1 ŵ * f c i,∆ 2 2 + µ P i + ∆P 2 F . (9)</formula><p>Since the filter f is constrained to have finitely many nonzero Fourier coefficients, eq. (9) is a linear least squares problem. The corresponding normal equations have a partly similar structure to (5), with additional components corresponding to the matrix increment ∆P variable. <ref type="bibr" target="#b2">3</ref> We employ the Conjugate Gradient method to optimize each Gauss-Newton subproblem to obtain the new filterf * i,∆ and matrix increment ∆P * . The filter and matrix estimates are then updated asf i+1 =f * i,∆ and P i+1 = P i + ∆P * . The main objective of our factorized convolution operation is to reduce the computational and memory complexity of the tracker. Due to the adaptability of the filter, the matrix P can be learned just from the first frame. This has two important implications. Firstly, only the projected feature map P T J{x j } requires storage, leading to significant memory savings. Secondly, the filter can be updated in subsequent frames using the projected feature maps P T J{x j } as input to the method described in section 2. This reduces the linear complexity in the feature dimensionality D to the filter dimensionality C, i.e. O(N CG CMK).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generative Sample Space Model</head><p>Here, we propose a compact generative model of the sample set that averts the earlier discussed issues of storing a large set of recent training samples. Most DCF trackers, such as SRDCF <ref type="bibr" target="#b8">[9]</ref> and C-COT <ref type="bibr" target="#b11">[12]</ref>, add one training sample x j in each frame j. The weights are typically set to decay exponentially α j ∼ (1 − γ) M −j , controlled by <ref type="bibr" target="#b2">3</ref> See supplementary material for the derivation of the normal equations. the learning rate γ. If the number of samples has reached a maximum limit M max , the sample with the smallest weight α j is replaced. This strategy however requires a large sample limit M max to obtain a representative sample set. We observe that collecting a new sample in each frame leads to large redundancies in the sample set, as visualized in <ref type="figure" target="#fig_1">figure 3</ref>. The standard sampling strategy (bottom row) populates the whole training set with similar samples x j , despite containing almost the same information. Instead, we propose to use a probabilistic generative model of the sample set that achieves a compact description of the samples by eliminating redundancy and enhancing variety (top).</p><p>Our approach is based on the joint probability distribution p(x, y) of the sample feature maps x and corresponding desired outputs scores y. Given p(x, y), the intuitive objective is to find the filter that minimizes the expected correlation error. This is obtained by replacing <ref type="formula">(3)</ref> with</p><formula xml:id="formula_17">E(f ) = E S f {x} − y 2 L 2 + D d=1 wf d 2 L 2 . (10)</formula><p>Here, the expectation E is evaluated over the joint sample distribution p(x, y). Note that the original loss (3) is obtained as a special case by estimating the sample distribution as p(x, y) = M j=1 α j δ xj ,yj (x, y), where δ xj ,yj denotes the Dirac impulse at the training sample (x j , y j ). <ref type="bibr" target="#b3">4</ref> Instead, we propose to estimate a compact model of the sam- <ref type="bibr" target="#b3">4</ref> We can without loss of generality assume the weights α j sum to one. ple distribution p(x, y) that leads to a more efficient approximation of the expected loss <ref type="bibr" target="#b9">(10)</ref>.</p><p>We observe that the shape of the desired correlation output y for a sample x is predetermined, here as a Gaussian function. The label functions y j in (3) only differ by a translation that aligns the peak with the target center. This alignment is equivalently performed by shifting the feature map x. We can thus assume that the target is centered in the image region and that all y = y 0 are identical. Hence, the sample distribution can be factorized as p(x, y) = p(x)δ y0 (y) and we only need to estimate p(x). For this purpose we employ a Gaussian Mixture Model (GMM) such that p(x) = L l=1 π l N (x; µ l ; I). Here, L is the number of Gaussian components N (x; µ l ; I), π l is the prior weight of component l, and µ l ∈ X is its mean. The covariance matrix is set to the identity matrix I to avoid costly inference in the high-dimensional sample space.</p><p>To update the GMM, we use a simplified version of the online algorithm by Declercq and Piater <ref type="bibr" target="#b13">[14]</ref>. Given a new sample x j , we first initialize a new component m with π m = γ and µ m = x j (concatenate in <ref type="bibr" target="#b13">[14]</ref>). If the number of components exceeds the limit L, we simplify the GMM. We discard a component if its weight π l is below a threshold. Otherwise, we merge the two closest components k and l into a common component n <ref type="bibr" target="#b13">[14]</ref>, π n = π k + π l , µ n = π k µ k + π l µ l π k + π l .</p><p>The required distance comparisons µ k −µ l are efficiently computed in the Fourier domain using Parseval's formula.</p><p>Finally, the expected loss (10) is approximated as,</p><formula xml:id="formula_19">E(f ) = L l=1 π l S f {µ l } − y 0 2 L 2 + D d=1 wf d 2 L 2 . (12)</formula><p>Note that the Gaussian means µ l and the prior weights π l directly replace x j and α j , respectively, in <ref type="bibr" target="#b2">(3)</ref>. So, the same training strategy as described in section 2 can be applied.</p><p>The key difference in complexity compared to <ref type="formula">(3)</ref> is that the number of samples has decreased from M to L. In our experiments, we show that the number of components L can be set to M/8, while obtaining an improved tracking performance. Our sample distribution model p(x, y) is combined with the factorized convolution from section 3.1 by replacing the sample x with the projected sample P T Jx. The projection does not affect our formulation since the matrix P is constant after the first frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Update Strategy</head><p>The standard approach in DCF based tracking is to update the model in each frame <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20]</ref>. In C-COT, this implies optimizing (3) after each new sample is added, by iteratively solving the normal equations <ref type="bibr" target="#b4">(5)</ref>. Iterative optimization based DCF methods exploit that the loss function changes gradually between frames. The current estimate of the filter therefore provides a good initialization of the iterative search. Still, updating the filter in each frame have a severe impact on the computational load.</p><p>Instead of updating the model in a continuous fashion every frame, we use a sparser updating scheme, which is a common practice in non-DCF trackers <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b38">39]</ref>. Intuitively, an optimization process should only be started once sufficient change in the objective has occurred. However, finding such conditions is non-trivial and may lead to unnecessarily complex heuristics. Moreover, optimality conditions based on the gradient of the loss (3), given by the residual of (5), are expensive to evaluate in practice. We therefore avoid explicitly detecting changes in the objective and simply update the filter by starting the optimization process in every N S th frame. The parameter N S determines how often the filter is updated, where N S = 1 corresponds to optimizing the filter in every frame, as in standard DCF methods. In every N S th frame, we perform a fixed number of N CG Conjugate Gradient iterations to refine the model. As a result, the average number of CG iterations per frame is reduced to N CG /N S , which has a substantial effect on the overall computational complexity of the learning. Note that N S does not affect the updating of the sample space model, introduced in section 3.2, which is updated every frame.</p><p>To our initial surprise, we observed that a moderately infrequent update of the model (N S ≈ 5) generally led to improved tracking results. We mainly attribute this effect to reduced over-fitting to the recent training samples. By postponing the model update a few frames, the loss is updated  <ref type="figure">1)</ref>. While increasing N S leads to reduced computations, it may also reduce the convergence speed of the optimization, resulting in a less discriminative model. A naive compensation by increasing the number of CG iterations N CG would counteract the achieved computational gains. Instead, we aim to achieve a faster convergence by better adapting the CG algorithm to online tracking, where the loss changes dynamically. This is obtained by substituting the standard Fletcher-Reeves formula to the Polak-Ribière formula <ref type="bibr" target="#b33">[34]</ref> for finding the momentum factor, since it has shown improved convergence rates for inexact and flexible preconditioning <ref type="bibr" target="#b17">[18]</ref>, which have similarities to our scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We validate our proposed formulation by performing comprehensive experiments on four benchmarks: VOT2016 <ref type="bibr" target="#b22">[23]</ref>, UAV123 <ref type="bibr" target="#b28">[29]</ref>, OTB-2015 <ref type="bibr" target="#b37">[38]</ref>, and TempleColor <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Our tracker is implemented in Matlab. We apply the same feature representation as C-COT, namely a combination of the first (Conv-1) and last (Conv-5) convolutional layer in the VGG-m network <ref type="bibr" target="#b4">[5]</ref>, along with HOG <ref type="bibr" target="#b5">[6]</ref> and Color Names (CN) <ref type="bibr" target="#b34">[35]</ref>. For the factorized convolution presented in section 3.1, we learn one coefficient matrix P for each feature type. The settings for each feature is summarized in table 1. The regularization parameter λ in <ref type="formula">(7)</ref> is set to 2 · 10 −7 . The loss <ref type="formula">(7)</ref> is optimized in the first frame using 10 Gauss-Newton iterations and 20 CG iterations for the subproblems (9). In the first iteration i = 0, the filterf 0 is initialized to zero. To preserve the deterministic property of the tracker, we initialize the coefficient matrix P 0 by PCA, though we found random initialization to be equally robust.</p><p>For the sample space model, presented in section 3.2, we set the learning rate to γ = 0.012. The number of components are set to L = 50, which represents an 8-fold reduction compared to the number of samples (M = 400) used in C-COT. We update the filter in every N S = 6 frame (section 3.3). We use the same number of N CG = 5 Conjugate Gradient iterations as in C-COT. Note that all parameters settings are kept fixed for all videos in a dataset.  <ref type="table">Table 2</ref>. Analysis of our approach on the VOT2016. The impact of progressively integrating one contribution at the time, from left to right, is displayed. We show the performance in Expected Average Overlap (EAO) and speed in FPS (benchmarked on a single CPU). We also summarize the reduction in learning complexity O(NCGDMK) obtained in each step, both symbolically and in absolute numbers (bottom row) using our settings. Our contributions systematically improve both performance and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baseline Comparison</head><p>Here, we analyze our approach on the VOT2016 benchmark by demonstrating the impact of progressively integrating our contributions. The VOT2016 dataset consists of 60 videos compiled from a set of more than 300 videos. The performance is evaluated both in terms of accuracy (average overlap during successful tracking) and robustness (failure rate). The overall performance is evaluated using Expected Average Overlap (EAO) which accounts for both accuracy and robustness. We refer to <ref type="bibr" target="#b23">[24]</ref> for details. <ref type="table">Table 2</ref> shows an analysis of our contributions. The integration of our factorized convolution into the baseline leads to a performance improvement and a significant reduction in complexity (6×). The sample space model further improves the performance by a relative gain of 2.9% in EAO, while reducing the learning complexity by a factor of 8. Additionally incorporating our proposed model update elevates us to an EAO score of 0.374, leading to a final relative gain of 13.0% compared to the baseline. In table 2 we also show the impact on the tracker speed achieved by our contributions. For a fair comparison, we report the FPS measured on a single CPU for all entries in the table, without accounting for feature extraction time. Each of our contributions systematically improves the speed of the tracker, combining to a 20-fold final gain compared to the baseline. When including all steps (also feature extraction), the GPU version of our tracker operates at 8 FPS.</p><p>We found the settings in table 1 to be insensitive to minor changes. Substantial gain in speed can be obtained by reducing the number of filters C, at the cost of a slight reduction in performance. To further analyze the impact of our jointly learned factorized convolution approach, we compare with applying PCA in the first frame to obtain the matrix P . PCA degrades the EAO from 0.331 to 0.319, while our discriminative learning based method achieves 0.342.</p><p>We observed that our sample model provides consistently better results compared to the training sample set management employed in C-COT when using the same number of components and samples (L = M ). This  is particularly evident for a smaller number of components/samples: When reducing the number of samples from M = 400 to M = 50 in the standard approach, the EAO decreases from 0.342 to 0.338 (−1.2%). Instead, when using our approach with L = 50 components, the EAO increases by +2.9% to 0.351. In case of the model update, we observed an upward trend in performance when increasing N S from 1 to 6. When increasing N S further, a gradual downward trend was observed. We therefore use N S = 6 throughout our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">State-of-the-art Comparison</head><p>Here, we compare our approach with state-of-the-art trackers on four challenging tracking benchmarks. Detailed results are provided in the supplementary material. VOT2016 Dataset: In table 3 we compare our approach, in terms of expected average overlap (EAO), robustness, accuracy and speed (in EFO units), with the top-ranked trackers in the VOT2016 challenge. The first-ranked performer in VOT2016 challenge, C-COT, provides an EAO score of 0.331. Our approach achieves a relative gain of 13.0% in EAO compared to C-COT. Further, our ECO tracker achieves the best failure rate of 0.72 while maintaining a competitive accuracy. We also report the total speed in terms of EFO, which normalizes the speed with respect to hardware performance. Note that EFO also takes feature ex-   UAV123 Dataset: Aerial tracking using unmanned aerial vehicles (UAVs) has received much attention recently, with many vision applications, including wild-life monitoring, search and rescue, navigation, and crowd surveillance. In these applications, persistent UAV navigation is required, for which real-time tracking output is crucial. In such cases, the desired tracker should be accurate and robust, while operating in real-time under limited hardware capabilities, e.g., CPUs or mobile GPU platforms. We therefore introduce a real-time variant of our method (ECO-HC), based on hand-crafted features (HOG and Color Names), operating at 60 FPS on a single i7 CPU (including feature extraction). We evaluate our trackers on the recently introduced aerial video benchmark, UAV123 <ref type="bibr" target="#b28">[29]</ref>, for low altitude UAV target tracking. The dataset consists of 123 aerial videos with more than 110K frames. The trackers are evaluated using success plot <ref type="bibr" target="#b36">[37]</ref>, calculated as percentage of frames with an intersection-over-union (IOU) overlap exceeding a threshold. Trackers are ranked using the areaunder-the-curve (AUC) score. <ref type="figure" target="#fig_3">Figure 5a</ref> shows the success plot over all the 123 videos in the dataset. We compare with all tracking results reported in <ref type="bibr" target="#b28">[29]</ref> and further add Staple <ref type="bibr" target="#b0">[1]</ref>, due to its high frame-rate, and C-COT <ref type="bibr" target="#b11">[12]</ref>. Among the top 5 compared trackers, only Staple runs at real-time, with an AUC score of 45.3%. Our ECO-HC tracker also operates in real-time (60 FPS), with an AUC score of 51.7%, significantly outperforming Staple by 6.4%. C-COT obtains an AUC score of 51.7%. Our ECO outperforms C-COT, achieving an AUC score of 53.7%, using same features. OTB2015 Dataset: We compare our tracker with 20 stateof-the-art methods: TLD <ref type="bibr" target="#b21">[22]</ref>, Struck <ref type="bibr" target="#b18">[19]</ref>, CFLB <ref type="bibr" target="#b15">[16]</ref>, ACT <ref type="bibr" target="#b12">[13]</ref>, TGPR <ref type="bibr" target="#b16">[17]</ref>, KCF <ref type="bibr" target="#b19">[20]</ref>, DSST <ref type="bibr" target="#b6">[7]</ref>, SAMF <ref type="bibr" target="#b24">[25]</ref>, MEEM <ref type="bibr" target="#b38">[39]</ref>, DAT <ref type="bibr" target="#b32">[33]</ref>, LCT <ref type="bibr" target="#b27">[28]</ref>, HCF <ref type="bibr" target="#b26">[27]</ref>, SRDCF <ref type="bibr" target="#b8">[9]</ref>, SRDCFad <ref type="bibr" target="#b9">[10]</ref>, DeepSRDCF <ref type="bibr" target="#b7">[8]</ref>, Staple <ref type="bibr" target="#b0">[1]</ref>, MDNet <ref type="bibr" target="#b30">[31]</ref>, SiameseFC <ref type="bibr" target="#b1">[2]</ref>, TCNN <ref type="bibr" target="#b29">[30]</ref> and C-COT <ref type="bibr" target="#b11">[12]</ref>. <ref type="figure" target="#fig_3">Figure 5b</ref> shows the success plot over all the 100 videos in the OTB-2015 dataset <ref type="bibr" target="#b37">[38]</ref>. Among the compared trackers using hand-crafted features, SRDCFad provides the best results with an AUC score of 63.4%. Our proposed method, ECO-HC, also employing hand-crafted features outperforms SRDCFad with an AUC score of 65.0%, while running on a CPU with a speed of 60 FPS. Among the compared deep feature trackers, C-COT, MDNet and TCNN provide the best results with AUC scores of 69.0%, 68.5% and 66.1% respectively. Our approach ECO, provides the best performance with an AUC score of 70.0%. TempleColor Dataset: In figure 5c we present results on the TempleColor dataset <ref type="bibr" target="#b25">[26]</ref> containing 128 videos. Our method again achieves a substantial improvement over C-COT, with a gain of 0.8% in AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We revisit the core DCF formulation to counter the issues of over-fitting and computational complexity. We introduce a factorized convolution operator to reduce the number of parameters in the model. We also propose a compact generative model of the training sample distribution to drastically reduce memory and time complexity of the learning, while enhancing sample diversity. Lastly, we suggest a simple yet effective model update strategy that reduces over-fitting to recent samples. Experiments on four datasets demonstrate state-of-the-art performance with improved frame rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This supplementary material contains additional details and derivations related to the our approach presented in section 3. It also includes hardware specifications and additional experimental results on the VOT2016 and OTB-2015 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity Analysis of the Learning</head><p>Here, we derive the computational complexity of the learning step in the baseline C-COT <ref type="bibr" target="#b11">[12]</ref>. The learning itself is completely dominated by the problem of solving the normal equations (5). This linear system is iteratively solved using the Conjugate Gradient (CG) method <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>. The dominating computation in CG is the evaluation of the lefthand side of <ref type="bibr" target="#b4">(5)</ref>, which is performed once per CG iteration. This computation is performed as</p><formula xml:id="formula_20">A H (Γ(Af )) + W H (Wf ),<label>(13)</label></formula><p>where the parentheses are used to indicate the order in which the operations are performed. Since the conjugate symmetry in the filterf is preserved by the operations in <ref type="bibr" target="#b12">(13)</ref>, only half of the spectrum needs to be processed. We can therefore regardf as a complex vector of d K d = DK elements, where K d is the bandwidth of channel d in the filter (see section 2),K = 1 D d K d is the average number of Fourier coefficients per channel and D is the number of feature channels d.</p><p>The matrix A contains a diagonal block A j,d of size K × K d for each sample j ∈ {1, . . . , M } and channel d ∈ {1, . . . , D}. Here, we have defined K = max d K d . The diagonal of A j,d consists of the elements</p><formula xml:id="formula_21">{X d j [k]b d [k]} K d k=0 .</formula><p>As previously shown for the discrete DCF case <ref type="bibr" target="#b14">[15]</ref>, the matrix A can be permuted to a block diagonal matrixÃ = ⊕ K k=0Ã k , whereÃ k contains the ele-</p><formula xml:id="formula_22">ments (Ã k ) j,d = X d j [k]b d [k]</formula><p>. The operationsf → Af and v → A Hv can thus be implemented as block-wise dense matrix-vector multiplications, with a total of O(DMK) operations. Moreover, Γ is a diagonal matrix containing the weights α j , giving O(MK) operations.</p><p>In the second term of (13), arising from the spatial regularization in the loss (3), W and W H are convolution matrices with the kernelŵ[k]. These operations have a complexity of O(DKK w ), where K w are the number of non-zero coefficients inŵ (i.e. the size of the kernel). In practice however, the kernelŵ[k] is small (typically 5 × 5), having a lesser impact on the overall complexity. To simplify the complexity expression, we therefore disregard this term. By taking the number of CG iterations N CG into account, we thus obtain the final expression O(N CG DMK) for the complexity of the learning step.</p><p>The preprocessing steps needed for the CG optimization only have a marginal impact on the overall learning time.</p><p>The most significant of these being the Fast Fourier Transform (FFT) of the feature map, having a O( d N d log N d ) complexity, where N d is the resolution of feature channel d. But since the FFT computations correspond to roughly 1% of the total time in C-COT, we exclude this part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factorized Convolution Operator Optimization</head><p>Here, we provide more details regarding the optimization procedure for learning the factorized convolution operator (section 3.1). We consider the case of learning the factorized operator S P f {x} in (6) based on a single sample (x, y),</p><formula xml:id="formula_23">E(f, P ) = S f,P {x} − y 2 L 2 + C c=1 wf c 2 L 2 + λ P 2 F .<label>(14)</label></formula><p>The loss is obtained by employing the factorized operator S f,P {x} in the data term of the original loss (3) and adding a regularization on the Frobenius norm P 2 F of P . By applying the Parseval's formula to the first two terms of <ref type="bibr" target="#b13">(14)</ref> and utilizing the linearity and convolution properties of the Fourier series coefficients, we obtain the equivalent loss <ref type="formula">(7)</ref>, where we have defined the interpolated feature map as z = J{x} to simplify notation. Note that the matrixvector products in <ref type="bibr" target="#b6">(7)</ref> are performed point-wise,</p><formula xml:id="formula_24">(ẑ T Pf )[k] = D d=1 C c=1ẑ d [k]p d,cf c [k] , k ∈ Z . (15)</formula><p>We use the Gauss-Newton method <ref type="bibr" target="#b31">[32]</ref> to optimize the non-linear least squares problem <ref type="bibr" target="#b6">(7)</ref>. In each iteration i, the residual in the data-term is linearized by performing a first order Taylor expansion (8) at the current estimate (f i , P i ). This gives the following quadratic sub-problem (9). To derive a simple formula for the normal equations of (9), we first introduce some notation. Letf be the vectorization of f i,∆ , analogously to <ref type="bibr" target="#b4">(5)</ref>, and define ∆p = vec(∆P ). Further, let p c denote the cth column in P i and set p = vec(P i ). We then define the matrices,</p><formula xml:id="formula_25">A P =          0 K−K1×2K1+1 · · · 0 K−K C ×2K C +1 diag   ẑ [−K 1 ] T p 1 . . . z[K 1 ] T p 1    · · · diag   ẑ [−K C ] T p C . . . z[K C ] T p C    0 K−K1×2K1+1 · · · 0 K−K C ×2K C +1          B f =    (f i ⊗ẑ)[−K] T .</formula><p>. .</p><formula xml:id="formula_26">(f i ⊗ẑ)[K] T    .<label>(16)</label></formula><p>Here, A P has a structure very similar to the matrix A in <ref type="bibr" target="#b4">(5)</ref>, but contains only a single training sample. Note that the diagonal blocks in A P are padded with zero matrices 0 M ×N along the columns to achieve the same number of 2K + 1 rows. The Gauss-Newton sub-problem (9) can then be expressed as,</p><formula xml:id="formula_27">E(f ,∆p) = A Pf +B f ∆p−ŷ 2 2 + Wf 2 2 +λ p+∆p 2 2 .<label>(17)</label></formula><p>Here, the convolution matrix W and the vectorizationŷ are defined as in <ref type="bibr" target="#b4">(5)</ref>. The normal equations of (17) are obtained by setting the gradient to zero,</p><formula xml:id="formula_28">A H P A P + W H W A H P B f B H f A P B H f B f + λI f ∆p = A H Pŷ B H fŷ − λp .</formula><p>(18) We employ the Conjugate Gradient method to iteratively solve the sub-problem <ref type="bibr" target="#b17">(18)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hardware Specifications</head><p>Our tracker is implemented in Matlab and uses Matconvnet <ref type="bibr" target="#b35">[36]</ref> for deep feature extraction. The frame-rate measurements of our CPU implementation were performed on a desktop computer with a 4-core Intel Core i7-6700 CPU at 3.4 GHz. The frame-rate measurements of our GPU implementation were performed on a Tesla K40 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results on VOT2016</head><p>Here, we provide further experimental evaluation on the VOT2016 dataset <ref type="bibr" target="#b22">[23]</ref> with 60 videos. The videos and the evaluation toolkit can be obtained from http://www. votchallenge.net/vot2016/.</p><p>In the VOT2016 dataset, each frame is labeled with five different attributes: camera motion, illumination change, occlusion, size change and motion change. <ref type="figure">Figure 6</ref> visualizes the EAO of each attribute individually. Our approach achieves the best results on three attributes and improves over the baseline C-COT <ref type="bibr" target="#b11">[12]</ref> on all five attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results on OTB-2015</head><p>Here, we report additional results on the OTB-2015 dataset <ref type="bibr" target="#b37">[38]</ref> with 100 videos. The ground truth annotations and videos are available at https://sites.google. com/site/benchmarkpami/.</p><p>In the OTB-2015 dataset, each video is annotated with 11 different attributes: scale variation, background clutter, outof-plane rotation, in-plane rotation, illumination variation, motion blur, fast motion, deformation, occlusion, out of view and low resolution. The success plots of all attributes are shown in figure 7. Our ECO tracker achieves the best performance on 8 out of 11 attributes. Further, our method improves over the baseline C-COT [12] on 9 out of 11 attributes. For a fair comparison, we employ the same combination of deep and hand-crafted features in the baseline C-COT and as in our ECO tracker on the OTB, TempleColor  <ref type="figure">Figure 7</ref>. Success plots on the OTB-2015 dataset <ref type="bibr" target="#b37">[38]</ref>. The total success plot (top-left) is displayed along with the plots for all 11 attributes.</p><p>The title text indicate the name of the attribute and the number of videos associated with it. The area-under-the-curve scores for the top 10 trackers are shown in the legend.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 6 )Figure 2 .</head><label>62</label><figDesc>The last equality follows from the linearity of convolution. The factorized convolution<ref type="bibr" target="#b5">(6)</ref> can thus alternatively (a) C-COT (b) Ours Visualization of the learned filters corresponding to the last convolutional layer in the deep network. We display all the 512 filters f d learned by the baseline C-COT (a) and the reduced set of 64 filters f c obtained by our factorized formulation (b). The vast majority of the baseline filters contain negligible energy, indicating irrelevant information in the corresponding feature layers. Our factorized convolution formulation learns a compact set of discriminative basis filters with significant energy, achieving a radical reduction of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of the training set representation in the baseline C-COT (bottom row) and our method (top row). In C-COT, the training set consists of a sequence of consecutive samples. This introduces large redundancies due to slow change in appearance, while previous aspects of the appearance are forgotten. This can cause over-fitting to recent samples. Instead, we model the training data as a mixture of Gaussian components, where each component represent a different aspect of the appearance. Our approach yields a compact yet diverse representation of the data, thereby reducing the risk of over-fitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Success plots on the UAV-123 (a),OTB-2015 (b)  and TempleColor (c) datasets. Only the top 10 trackers are shown in the legend for clarity. The AUC score of each tracker is shown in the legend. Our approach significantly improves the state-of-the-art on all datasets. traction time into account, a major additive complexity that is independent of our DCF improvements. In the comparison, our tracker ECO-HC using only hand-crafted features (HOG and Color Names) achieves the best speed. Among the top three trackers in the challenge, which are all based on deep features, TCNN<ref type="bibr" target="#b29">[30]</ref> obtains the best speed with an EFO of 1.05.Our deep feature version (ECO) achieves an almost 5-fold speedup in EFO and a relative performance improvement of 15.1% in EAO compared to TCNN. Figure 4 displays the EAO curves of the top-10 trackers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The settings of the proposed factorized convolution approach, as employed in our experiments. For each feature, we show the dimensionality D and the number of filters C. by adding a new mini-batch to the training samples, instead of only a single one. This might contribute to stabilizing the learning, especially in scenarios where a new sample is affected by sudden changes, such as out-of-plane rotations, deformations, clutter, and occlusions (see figure</figDesc><table><row><cell></cell><cell cols="4">Conv-1 Conv-5 HOG CN</cell></row><row><cell>Feature dimension, D</cell><cell>96</cell><cell>512</cell><cell>31</cell><cell>11</cell></row><row><cell>Filter dimension, C</cell><cell>16</cell><cell>64</cell><cell>10</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>State-of-the-art in terms of expected average overlap (EAO), robustness (failure rate), accuracy, and speed (in EFO units) on the VOT2016 dataset. Only the top-10 trackers are shown. Our deep feature based ECO achieve superior EAO, while our hand-crafted feature version (ECO-HC) has the best speed.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Expected Average Overlap (EAO) scores for each attribute on the VOT2016 dataset. Here, empty denotes frames with no labeled attribute. and UAV123 datasets (Conv1, Conv5 and HOG). Note that this set of features provides substantially improved performance in C-COT compared to the original results reported in<ref type="bibr" target="#b11">[12]</ref>, where only deep convolutional features are used.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Camera Motion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ECO</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ECO-HC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>C-COT</cell></row><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell cols="2">Success plot</cell><cell></cell><cell></cell><cell>100</cell><cell cols="4">Illumination Change Success plot of scale variation (64)</cell><cell></cell><cell></cell><cell>100</cell><cell cols="3">TCNN SSAT Success plot of background clutter (31)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MLDF</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Staple</cell></row><row><cell>Overlap Precision [%]</cell><cell>80 40 60</cell><cell></cell><cell cols="2">ECO [70.0] C-COT [69.0] MDNet [68.5] TCNN [66.1] ECO-HC [65.0] DeepSRDCF [64.3] SRDCFad [63.4]</cell><cell></cell><cell></cell><cell>Overlap Precision [%]</cell><cell>80 40 60</cell><cell cols="2">ECO [67.5] C-COT [67.0] MDNet [66.6] TCNN [64.5] ECO-HC [61.7] SRDCFad [61.4] DeepSRDCF [61.2]</cell><cell cols="2">Occlusion Size Change</cell><cell cols="2">Overlap Precision [%]</cell><cell>80 40 60</cell><cell cols="2">ECO [71.0] MDNet [68.4] C-COT [68.1] SRDCFad [65.0] ECO-HC [64.4] TCNN [63.6] DeepSRDCF [63.5]</cell><cell>DDC EBT SRBT</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell cols="2">SRDCF [60.5]</cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell cols="2">SRDCF [56.7]</cell><cell cols="2">Motion Change</cell><cell></cell><cell></cell><cell>20</cell><cell>HCF [59.1]</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Staple [58.4]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SiameseFC [55.8]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SRDCF [59.0]</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SiameseFC [57.5]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Staple [52.5]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Staple [56.8]</cell><cell></cell></row><row><cell></cell><cell>0 100</cell><cell cols="11">0.1 No Attribute 0.8 Figure 6. 0 0 0.2 0.4 0.6 0.8 1 Overlap threshold 0 0.2 0.4 0.6 Overlap threshold Success plot of out-of-plane rotation (63) 100 Success plot of in-plane rotation (51)</cell><cell>1</cell><cell cols="5">0.2 Expected Average Overlap 0.3 0.4 0.5 0.6 0 0 0.2 0.4 Overlap threshold 0.7 0.6 100 Success plot of illumination variation (38) 0.8</cell><cell>1</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell></row><row><cell>Overlap Precision [%]</cell><cell>40 60</cell><cell></cell><cell cols="2">ECO [68.1] C-COT [67.9] MDNet [66.8] TCNN [64.7] ECO-HC [61.5] DeepSRDCF [61.3] SRDCFad [59.8]</cell><cell></cell><cell></cell><cell>Overlap Precision [%]</cell><cell>40 60</cell><cell cols="2">ECO [66.2] MDNet [66.2] C-COT [65.3] TCNN [65.1] DeepSRDCF [59.4] ECO-HC [58.8] SRDCFad [57.9]</cell><cell></cell><cell></cell><cell cols="2">Overlap Precision [%]</cell><cell>40 60</cell><cell cols="2">C-COT [72.3] ECO [72.3] MDNet [69.7] TCNN [68.5] SRDCFad [65.5] ECO-HC [64.9] DeepSRDCF [62.8]</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell cols="2">SRDCF [55.5]</cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell cols="2">SiameseFC [57.2]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell cols="2">SRDCF [62.0]</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SiameseFC [55.2]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HCF [56.3]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Staple [60.0]</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>LCT [54.3]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LCT [56.2]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LCT [57.2]</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell></row><row><cell></cell><cell>100</cell><cell></cell><cell cols="4">Success plot of motion blur (29)</cell><cell></cell><cell>100</cell><cell cols="4">Success plot of fast motion (39)</cell><cell></cell><cell></cell><cell>100</cell><cell cols="3">Success plot of deformation (44)</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell></row><row><cell>Overlap Precision [%]</cell><cell>40 60</cell><cell></cell><cell cols="2">ECO [71.9] C-COT [71.1] MDNet [68.8] TCNN [68.2] DeepSRDCF [65.1] SRDCFad [64.9] ECO-HC [63.5]</cell><cell></cell><cell></cell><cell>Overlap Precision [%]</cell><cell>40 60</cell><cell cols="2">ECO [69.3] C-COT [68.8] MDNet [68.3] TCNN [66.0] ECO-HC [64.2] DeepSRDCF [63.5] SRDCFad [61.3]</cell><cell></cell><cell></cell><cell cols="2">Overlap Precision [%]</cell><cell>40 60</cell><cell cols="2">MDNet [65.5] C-COT [64.4] ECO [64.0] TCNN [62.1] ECO-HC [60.1] DeepSRDCF [57.1] SRDCFad [55.9]</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell cols="2">SRDCF [60.3]</cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell cols="2">SRDCF [60.4]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell>Staple [55.5]</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>MEEM [59.4]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HCF [57.5]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SRDCF [55.0]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>HCF [59.1]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SiameseFC [57.2]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HCF [53.4]</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell></row><row><cell></cell><cell>100</cell><cell></cell><cell cols="4">Success plot of occlusion (49)</cell><cell></cell><cell>100</cell><cell cols="4">Success plot of out of view (14)</cell><cell></cell><cell></cell><cell>100</cell><cell cols="3">Success plot of low resolution (9)</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell></row><row><cell>Overlap Precision [%]</cell><cell>40 60</cell><cell></cell><cell cols="2">ECO [68.8] C-COT [66.9] MDNet [65.3] ECO-HC [63.6] TCNN [62.8] DeepSRDCF [60.8] SRDCFad [59.6]</cell><cell></cell><cell></cell><cell>Overlap Precision [%]</cell><cell>40 60</cell><cell cols="2">ECO [67.0] C-COT [64.5] MDNet [63.5] ECO-HC [60.1] TCNN [59.1] DeepSRDCF [56.0] MEEM [52.6]</cell><cell></cell><cell></cell><cell cols="2">Overlap Precision [%]</cell><cell>40 60</cell><cell cols="2">MDNet [62.9] SiameseFC [61.4] ECO [61.2] TCNN [60.8] ECO-HC [59.9] C-COT [59.1] SRDCF [50.3]</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell cols="2">SRDCF [56.6]</cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell cols="2">SRDCFad [51.8]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell cols="2">SRDCFad [50.0]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SAMF [55.0]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SiameseFC [51.8]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DeepSRDCF [48.2]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Staple [54.8]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SAMF [49.1]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TGPR [44.5]</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For clarity, we present the one-dimensional domain formulation. The generalization to higher dimensions, including images, is detailed in<ref type="bibr" target="#b11">[12]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See the supplementary material for a derivation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work has been supported by SSF (SymbiCloud), VR (EMC 2 , starting grant 2016-05543), SNIC, WASP, Visual Sweden, and Nvidia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Target response adaptation for correlation filter tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive decontamination of the training set: A unified formulation for discriminative visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Discriminative scale space tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<idno>2016. 1</idno>
		<imprint>
			<publisher>TPAMI</publisher>
			<biblScope unit="page">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive color attributes for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online learning of gaussian mixture models -a two-level approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Declercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Piater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISAPP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-channel correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Correlation filters with limited boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transfer learning based visual tracking with gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inexact preconditioned conjugate gradient method with inner-outer iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1305" to="1320" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Secrets of matrix factorization: Approximations, numerics, manifold optimization and random restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">P-n learning: Bootstrapping binary classifiers by structural constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">˙The visual object tracking vot2016 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vojír</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Häger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojír</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A scale adaptive kernel correlation filter tracker with feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Encoding color information for visual tracking: Algorithms and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5630" to="5644" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long-term correlation tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Modeling and propagating cnns in a tree structure for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1608.07242</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Springer</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">In defense of color-based model-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An introduction to the conjugate gradient method without the agonizing pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Shewchuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning color names for real-world applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1512" to="1524" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Matconvnet -convolutional neural networks for matlab. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4564</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">MEEM: robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond local search: Tracking objects everywhere with instance-specific proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
