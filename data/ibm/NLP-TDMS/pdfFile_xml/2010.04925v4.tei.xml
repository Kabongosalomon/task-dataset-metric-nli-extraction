<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regularizing Neural Networks via Adversarial Model Perturbation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sklsde</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
							<email>zhangrc@act.buaa.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
							<email>ymao@uottawa.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">BDBC and SKLSDE</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">University of Ottawa</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Regularizing Neural Networks via Adversarial Model Perturbation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Effective regularization techniques are highly desired in deep learning for alleviating overfitting and improving generalization. This work proposes a new regularization scheme, based on the understanding that the flat local minima of the empirical risk cause the model to generalize better. This scheme is referred to as adversarial model perturbation (AMP), where instead of directly minimizing the empirical risk, an alternative "AMP loss" is minimized via SGD. Specifically, the AMP loss is obtained from the empirical risk by applying the "worst" norm-bounded perturbation on each point in the parameter space. Comparing with most existing regularization schemes, AMP has strong theoretical justifications, in that minimizing the AMP loss can be shown theoretically to favour flat local minima of the empirical risk. Extensive experiments on various modern deep architectures establish AMP as a new state of the art among regularization schemes. Our code is available at https:// github.com/ hiyouga/ AMP-Regularizer. arXiv:2010.04925v4 [cs.LG] 7 May 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>To date, the generalization behaviour of deep neural networks is still a mystery, despite some recent progress (see, e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>). A commonly accepted and empirically verified understanding in this regard is that the model parameter that corresponds to a flat minimum of the empirical risk tends to generalize better. For example, the authors of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref> argue that flat minima correspond to simple models, which are less likely to overfit. This understanding has inspired great effort studying factors in the optimization process (such as learning rate and batch size) that impacting the flatness of the found minima <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref> so as to better understand the generalization behaviour of deep networks.</p><p>Meanwhile developing effective regularization techniques remains as the most important approach in practice to alleviate overfitting and force model towards better generalization * Corresponding author Accepted to CVPR 2021 (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51]</ref>). Some recent research in fact suggests that the effectiveness of certain regularization techniques is due to their ability to find flatter minima <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45]</ref>. Additionally, there have been significant research advances in recent years in developing more effective regularization schemes, which include, for example, MixUp, Flooding <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b50">51]</ref>. Despite their great success, these techniques usually fall short of strong principles or theoretical justifications. Thus one expects more principled and more powerful regularization schemes are yet to be discovered.</p><p>This work sets out to develop a powerful regularization scheme under the principle of finding flat local minima of the empirical risk. To that end, we propose a novel regularization scheme which can be strongly justified in terms of its ability to finding flat minima. This scheme is referred to as Adversarial Model Perturbation or AMP, where instead of minimizing the empirical risk L ERM (θ) over model parameter θ, it minimizes an alternative "AMP loss". Briefly, the AMP loss L AMP (θ) at a parameter setting θ is the worst (or highest) empirical risk of all perturbations of θ with the perturbation norm no greater than a small value , namely,</p><formula xml:id="formula_0">L AMP (θ) := max ∆: ∆ ≤ L ERM (θ + ∆)<label>(1)</label></formula><p>To see why minimizing the AMP loss provides opportunities to find flat local minima of the empirical risk, consider the example in <ref type="figure" target="#fig_0">Figure 1</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> (left) sketches an empirical risk curve L ERM , which contains two local minima, a sharp one on the left and a flat one on the right. The process of obtaining the AMP loss from the empirical risk can be seen as on test set. These curves are computed using the technique presented in <ref type="bibr" target="#b30">[31]</ref>, where δ indicates a random direction and α is a displacement in that direction.</p><p>a "max-pooling" operation, which slides a window of width 2 (in high dimension, more precisely, a sphere with radius ) across the parameter space and, at each location, returns the maximum value inside the window (resp. sphere). The resulting AMP loss is shown as the blue curve in <ref type="figure" target="#fig_0">Figure 1</ref> (right). Since the right minimum in the AMP loss is lower than the left one, minimizing the AMP loss gives the right minimum as its solution.</p><p>In this paper, we formally analyze the AMP loss minimization problem and its preference of flat local minima. Specifically, we show that this minimization problem implicitly uses the "narrowest width" of a local minimum as a notion of flatness, and tries to penalize the minima that are not flat in this sense.</p><p>We derive a mini-batch SGD algorithm for solving this minimization problem, which gives rise to the proposed AMP regularization scheme. Interestingly, we show that this algorithm can also be seen as the regular empirical risk minimization with an additional penalty term on the gradient norm. This provides an alternative justification of the AMP scheme. <ref type="figure" target="#fig_1">Figure 2</ref> contains an experimental result suggesting that AMP indeed selects flatter minima than ERM does.</p><p>We conduct experiments on several benchmark image classification datasets (SVHN, CIFAR-10, CIFAR-100) to validate the effectiveness of the proposed AMP scheme. Compared with other popular regularization schemes, AMP demonstrates remarkable regularization performance, establishing itself as a new state of the art.</p><p>Our contributions can be summarized as follows. 1) Motivated by the understanding that flat minima help generalization, we propose adversarial model perturbation (AMP) as a novel and efficient regularization scheme.</p><p>2) We theoretically justify that AMP is capable of finding flatter local minima, thereby improving generalization.</p><p>3) Extensive experiments on the benchmark datasets demonstrate that AMP achieves the best performance among the compared regularization schemes on various modern neural network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Flat Minima and Generalization</head><p>There has been a rich body of works that investigate the relationship between the flatness of the local minima and the generalization of a deep neural network <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. <ref type="bibr" target="#b18">[19]</ref> suggests that flat minima correspond to low-complexity networks, which tend to generalize well under the principle of minimum description length <ref type="bibr" target="#b37">[38]</ref>. <ref type="bibr" target="#b3">[4]</ref> presents another explanation supporting this argument through the lens of Gibbs free energy. <ref type="bibr" target="#b30">[31]</ref> demonstrates the better generalization of flat minima by visualizing the loss landscape. The empirical results in <ref type="bibr" target="#b25">[26]</ref> show that large-batch SGD finds sharp minima while small-batch SGD leads to flatter minima and provides better generalization. On the other hand, <ref type="bibr" target="#b7">[8]</ref> argues that sharp minima do not necessarily lead to poor generalization. The argument is that due to the parameterization redundancy in deep networks and under a certain notion of flatness, one can transform a flat minimum to an equivalent sharp one. Nonetheless, it is in general accepted and empirically verified that flat minima tend to give better generalization performance, and this understanding underlies the design of several recent regularization techniques (e.g., <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>). A concurrent work of <ref type="bibr" target="#b8">[9]</ref> further provides a PAC-Bayesian justification as to why the flatness of the minima helps generalization. A similar technique designed by <ref type="bibr" target="#b45">[46]</ref> suggests that flat minima also improve robust generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Regularization</head><p>Regularization may broadly refer to any training techniques that help to improve generalization. Despite the wellknown regularization techniques such as weight decay <ref type="bibr" target="#b28">[29]</ref>, Dropout <ref type="bibr" target="#b39">[40]</ref>, normalization tricks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref> and data augmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref>, various techniques have been developed. Label smoothing <ref type="bibr" target="#b41">[42]</ref> mixes the one-hot label of the training example with a uniform distribution. Shake-Shake regularization <ref type="bibr" target="#b10">[11]</ref> combines parallel branches with a stochastic affine function in multi-branch networks. ShakeDrop regularization <ref type="bibr" target="#b46">[47]</ref> extends Shake-Shake to single-branch architectures. Cutout <ref type="bibr" target="#b6">[7]</ref> randomly masks out some regions of input images. MixUp <ref type="bibr" target="#b50">[51]</ref> regularizes deep networks by perturbing training samples along the direction of other samples. Flooding <ref type="bibr" target="#b21">[22]</ref> forces training loss to stay above zero to avoid overfitting. Adversarial training <ref type="bibr" target="#b12">[13]</ref>, originally designed for improving the model's adversarial robustness, is also shown to have great regularization effect when their parameters are carefully chosen <ref type="bibr" target="#b33">[34]</ref>. But it is also observed that training the model excessively towards adversarial robustness may hurt generalization <ref type="bibr" target="#b42">[43]</ref>. Some recent progresses (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>) exploit multiple states of the model parameter while training and ensembling them to improve generalization. The concurrent work <ref type="bibr" target="#b8">[9]</ref> also independently discovered a similar regularization scheme as we present in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">AMP: Adversarial Model Perturbation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">From Empirical Risk to AMP Loss</head><p>Consider a classification setting, where we aim at finding a classifier f : X → Y that maps the input space X to the label space Y. Note that we may take Y as the set of all distributions over the set of possible labels, so that not only f (x) belongs to Y, each ground-truth label also belongs to Y since it can be written as a degenerated distribution or a "one-hot" vector.</p><p>We will take f as a neural network parameterized by θ, taking values from its weight space Θ. For each training example (x, y) ∈ X × Y, we will denote by (x, y; θ) the loss of the prediction f (x) by the model with respect to the true label y. That is, the function absorbs the function f within.</p><p>Under the empirical risk minimization (ERM) principle <ref type="bibr" target="#b43">[44]</ref>, the training of the neural network using a training set D is performed by minimizing the following empirical risk (which we also refer to as "ERM loss") L ERM over parameter θ:</p><formula xml:id="formula_1">L ERM (θ) := 1 |D| (x,y)∈D (x, y; θ)<label>(2)</label></formula><p>It is however well known that ERM training is prone to overfitting <ref type="bibr" target="#b43">[44]</ref> and that the learned model often fails to generalize well on the unseen examples.</p><p>This work is motivated by the need of effective regularization schemes to improve the model's generalization capability. Specifically, our objective is to develop a technique that forces the training process to find flatter minima or low-norm solutions when minimizing the empirical risk. The technique we develop is termed "adversarial model perturbation" or AMP, which we now elaborate.</p><p>For any positive and any µ ∈ Θ, let B(µ; ) denote the norm ball in Θ with radius centred at µ. That is</p><formula xml:id="formula_2">B(µ; ) := {θ ∈ Θ : θ − µ ≤ }<label>(3)</label></formula><p>We note that the norm used in defining the norm ball is chosen as L 2 norm. However, we expect that the norm can be extended beyond this choice.</p><p>We now define an "AMP loss" L AMP as follows.</p><formula xml:id="formula_3">L AMP (θ) := max ∆∈B(0; ) 1 |D| (x,y)∈D (x, y; θ + ∆) (4)</formula><p>where the is a small positive value serving as a hyperparameter. <ref type="figure" target="#fig_0">Figure 1</ref> sketches an example demonstrating that minimizing the AMP loss L AMP over θ presents opportunities of finding flatter minima of the ERM loss L ERM . Theoretical justifications for AMP's capability of finding flatter minima are given in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Algorithm</head><p>Practical considerations on computation complexity and training speed motivate a mini-batched training approach to minimizing the AMP loss L AMP . Specifically, we may approximate the AMP loss in (4) using the corresponding loss obtained from a random mini-batch B, namely,</p><formula xml:id="formula_4">L AMP (θ) ≈ max ∆ B ∈B(0; ) 1 |B| (x,y)∈B (x, y; θ + ∆ B ) := J AMP,B (θ)<label>(5)</label></formula><p>Then the AMP loss minimization problem can be approximated as minimizing the expected batch-level AMP loss J AMP,B , formally as finding</p><formula xml:id="formula_5">θ * AMP := arg min θ E B J AMP,B (θ)<label>(6)</label></formula><p>Under this formulation, each batch B is associated with a perturbation vector ∆ B on the model parameter θ. The training involves an inner maximization nested inside and outer minimization: in the inner maximization, a fixed number, say N , of steps are used to update ∆ B in the direction of increasing the ERM loss so as to obtain J AMP,B ; the outer minimization loops over random batches and minimizes E B J AMP,B using mini-batched SGD. The precise algorithm is described in Algorithm 1. Note that at the end of training, the learned θ * AMP is used as model parameter for predictions; that is, no perturbation is applied in testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Adversarial Model Perturbation Training</head><p>Require: Training set D = {(x, y)}, Batch size m, Loss function , Initial model parameter θ 0 , Outer learning rate η, Inner learning rate ζ, Inner iteration number N , L 2 norm ball radius 1: while θ k not converged do 2:</p><p>Update iteration: k ← k + 1 3:</p><formula xml:id="formula_6">Sample B = {(x i , y i )} m i=1 from training set D 4:</formula><p>Initialize perturbation: ∆ B ← 0 5:</p><p>for n ← 1 to N do 6:</p><p>Compute gradient:</p><formula xml:id="formula_7">∇J AMP,B ← m i=1 ∇ θ (x i , y i ; θ k + ∆ B )/m 7:</formula><p>Update perturbation:</p><formula xml:id="formula_8">∆ B ← ∆ B + ζ∇J AMP,B 8: if ∆ B 2 &gt; then 9:</formula><p>Normalize perturbation:</p><formula xml:id="formula_9">∆ B ← ∆ B / ∆ B 2 10: end if 11:</formula><p>end for 12:</p><p>Compute gradient:</p><formula xml:id="formula_10">∇J AMP,B ← m i=1 ∇ θ (x i , y i ; θ k + ∆ B )/m 13:</formula><p>Update parameter: θ k+1 ← θ k − η∇J AMP,B 14: end while 4. Theoretical Justifications of AMP</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">AMP Finds Flatter Local Minima</head><p>For the ease of obtaining analytic results, we will assume that the loss surface of each local minimum in L ERM can be approximated as an inverted Gaussian surface.</p><p>More precisely, suppose Θ = R K . For any scalar C, any positive scalar A, any µ ∈ Θ and any K × K positive definite matrix κ, let</p><formula xml:id="formula_11">γ(θ; µ, κ, A, C) := C −A exp −(θ−µ) T κ −1 (θ−µ)/2 (7)</formula><p>Note that this function is merely an inverted Gaussian surface over the space Θ. Locally Gaussian Assumption of Empirical Risk Using the above notation, we assume that each minimum of the empirical risk can be locally approximated by such an inverted Gaussian surface, namely, that if µ ∈ Θ gives a local minimum of L ERM , then there exist some &gt; 0, a positive definite matrix κ and scalars A and C with A &gt; 0, such that</p><formula xml:id="formula_12">L ERM (θ) = γ(θ; µ, κ, A, C)<label>(8)</label></formula><p>at any θ ∈ B(µ; 2 ). We note that in general, such an assumption is only an approximation. But for small , the approximation is arguably accurate. We will use γ * (µ, κ, A, C) to denote the minimum value of function γ(θ; µ, κ, A, C) (obtained by minimizing over θ). It is apparent that</p><formula xml:id="formula_13">γ * (µ, κ, A, C) = C − A<label>(9)</label></formula><p>Let γ AMP denote the AMP loss derived from γ, namely,</p><formula xml:id="formula_14">γ AMP (θ; µ, κ, A, C) := max ∆∈B(0; ) γ(θ+∆; µ, κ, A, C) (10)</formula><p>We will use γ * AMP (µ, κ, A, C) to denote the minimum value of γ AMP (θ; µ, κ, A, C) (minimized over θ). Theorem 1. For any given (µ, κ, A, C), the function γ AMP (θ; µ, κ, A, C) is minimized when θ = µ and the minimum value is</p><formula xml:id="formula_15">γ * AMP (µ, κ, A, C) = C − A exp − 2 2σ 2 (11)</formula><p>where σ 2 is the smallest eigenvalue of κ.</p><p>Proof. The properties of a Gaussian surface suggest that the minimum of γ AMP = (θ; µ, κ, A, C) is given by θ = µ and the inner maximum is reached at ∆ = q, where q is the normalized eigenvector corresponding to the smallest eigenvalue of κ. That is because that the direction of q is one of the fastest increase of γ AMP . Such a direction can also be regarded as the direction of the "narrowest width" of the Gaussian surface, which is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. The symmetric positive definite matrix κ can be factorized as QΛQ T , where Q is the K × K orthogonal matrix whose i-th column is the normalized eigenvector q i of the matrix κ, and Λ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues. Let σ 2 denote the smallest eigenvalue of κ, and suppose that the eigenvalues are in ascending order along the diagonal of Λ. Let e 1 denote the vector (1, 0, · · · , 0) T whose first element is 1 and others are 0. We have:</p><formula xml:id="formula_16">γ * AMP (µ, κ, A, C) = C − A exp − 2 q T Q T Λ −1 Q q 2 = C − A exp − 2 e T 1 Λ −1 e 1 2 = C − A exp − 2 2σ 2<label>(12)</label></formula><p>This proves the theorem.</p><p>From this theorem, it is clear that the minimum value achieved with γ * AMP although related to the minimum ERM loss value γ * through A and C, it also takes into account the curvature of the surface around the local minimum. Specifically, the smallest eigenvalue σ 2 measures the "width" of the surface along its narrowest principal direction (noting that the cross-section of the surface is an ellipsoid). Thus the value σ 2 can be treated as a "worst-case" measure of the flatness or width of the surface. The larger is σ 2 , the flatter or wider is the local minimum.</p><p>Following this theorem, we next show that minimizing the AMP loss L AMP favours the solutions corresponding to "flatter" local minima of the empirical risk L ERM . Corollary 1. Let µ 1 , µ 2 ∈ Θ be two local minima of L ERM . Assume the locally Gaussian assumption hold such that the surface of the two local minima follow respectively γ 1 (θ;</p><formula xml:id="formula_17">µ 1 , κ 1 , A 1 , C 1 ) and γ 2 (θ; µ 2 , κ 2 , A 2 , C 2 ). Then γ * 1 &lt; γ * 2 but γ * 1,AMP &gt; γ * 2,AMP if and only if A 1 − A 2 &gt; C 1 − C 2 &gt; A 1 exp − 2 2σ 2 1 − A 2 exp − 2 2σ 2 2 (13)</formula><p>where σ 2 1 and σ 2 2 are the smallest eigenvalues of κ 1 and κ 2 respectively.</p><p>Under the condition of this corollary, although µ 1 gives a lower empirical risk than µ 2 , but when we minimize the AMP loss, µ 2 is a more preferred solution since the local curvatures of the two minima in L ERM are also considered. We next show, using the special case of C 1 = C 2 , that indeed minimizing AMP loss favours the local minima with a flatter surface.</p><formula xml:id="formula_18">Corollary 2. Suppose that C 1 = C 2 . Let A 2 = βA 1 for some β &lt; 1. Note that in this setting, γ * 1 &lt; γ * 2 . Suppose that σ 2 2 = rσ 2 1 for some positive r. Then γ * 2,AMP &lt; γ * 1,AMP<label>(14)</label></formula><p>if and only if</p><formula xml:id="formula_19">β &gt; exp − 2 2σ 2 1 and r &gt; 1 1 + 2σ 2 1 2 log β<label>(15)</label></formula><p>Please refer to Appendix A for the details of the proof. Note that in this setting, the local minimum in the empirical risk corresponding to µ 1 is lower than that corresponding to µ 2 . The value β governs how close the two minimum values are; the closer to 1 is β, the closer two minimum values are. The value r governs the flatness of the second local minimum relative to the first: r &gt; 1 indicates the second is flatter than the first, and the larger is r, the flatter is the second minimum. This corollary presents a sufficient and necessary condition for the second minimum to be preferred to the first when the AMP loss is minimized. Specifically, we may refer to the set of all (β, r) pairs that satisfy the condition as the "operational region of AMP", since the region specifies all points on which minimizing the AMP loss will give a solution that deviates from that given by minimizing the ERM loss. The general shape of such a region is shown in <ref type="figure" target="#fig_3">Figure 4</ref> (left). In <ref type="figure" target="#fig_3">Figure 4</ref> (right), the regions are plotted for different values of 2 /2σ 2 1 . Assuming σ 2 1 to be a fixed value, it can then be seen that as decreases, the operational region of AMP shrinks, namely that the minimization of the AMP loss has decreased opportunity to deviate from minimizing the empirical risk; in the limit when approaches 0, minimizing the AMP loss reduces to minimizing the ERM loss. On the other hand, for a large value of , the operational region of AMP is large, then minimizing the AMP loss may frequently find different solutions from those obtained from minimizing the empirical risk. In this case however, the operational region includes points (β, r) with small β and relatively small r. Such points, for example the one marked with "×" in <ref type="figure" target="#fig_3">Figure 4</ref> (right), corresponds to local minima not flatter than µ 1 by much but having much higher empirical risk values. When such solutions are obtained by minimizing the AMP loss, the learned model risks significant underfitting. Thus, in general there is a sweet spot of setting that gives the optimal tradeoff between the flatness and depth of the selected local minima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">AMP Regularizes Gradient Norm</head><p>The fact that AMP favours flatter minima can also be seen from the AMP training algorithm, independent of the above analysis. Specifically, we now show that the AMP algorithm may be viewed as minimizing the empirical risk with a certain penalty on its gradient norm.</p><p>To see this, consider that the number N of inner updates per batch is 1 (this is in fact used in our experiments). Denote by J ERM,B the approximation of the empirical loss L ERM using batch B, that is,</p><formula xml:id="formula_20">J ERM,B (θ) := 1 |B| (x,y)∈B (x, y; θ)<label>(16)</label></formula><p>Theorem 2. Let N = 1. Then for a sufficiently small inner learning rate ζ, a minimization update step in AMP training for batch B is equivalent to a gradient-descent step on the following loss function with learning rate η:</p><formula xml:id="formula_21">J ERM,B (θ) := J ERM,B (θ) + Ω(θ)<label>(17)</label></formula><p>where</p><formula xml:id="formula_22">Ω(θ) := ζ ∇ θ J ERM,B (θ) 2 2 , ζ∇ θ J ERM,B (θ) 2 ≤ ∇ θ J ERM,B (θ) 2 , ζ∇ θ J ERM,B (θ) 2 &gt;<label>(18)</label></formula><p>Please refer to Appendix B for the details of the proof. We note that the regularization term Ω(θ), in either one of the two cases, penalizes the gradient norm of J ERM,B . Thus the AMP training algorithm effectively tries to find local minima of J ERM,B (and hence of L ERM ) that not only have low values, but also have small gradient norm near the minima. Note that a minimum with smaller gradient norms around it is a flatter minimum. This theorem therefore provides another justification of the AMP training algorithm, in addition to our development from constructing the AMP loss L AMP . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Perspectives from the Input Space</head><p>The construction and analysis so far have focused on the parameter space. Further insights may be obtained by inspecting AMP in the input space.</p><p>It is evident that the effect of parameter θ on the input space is no more than defining the class boundaries and specifying how the loss (x, y) changes with input x. Then minimizing the AMP loss can be seen as finding a class boundary (and loss function (x, y)) which has the lowest average loss over the training examples, even when the worst -bounded perturbation is applied. A consequence of such a minimization is arguably creating smoother class boundaries and keeping the training examples not too close to the boundaries. This is illustrated in <ref type="figure">Figure 5</ref> and experimentally validated in <ref type="figure">Figure 6</ref>.</p><p>When viewed from input space, the adversarial perturbation of the model parameter in AMP shares some similarity with adversarial training <ref type="bibr" target="#b12">[13]</ref>. There is also a significant difference between the two: adversarial training defends the model against adversarial attacks whereas AMP defends it against overfitting. We further elaborate why AMP is different from adversarial training in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>We empirically investigate the performance of AMP in various perspectives. Firstly, we compare the generalization ability of AMP on benchmark image classification datasets with several popular regularization schemes in Section 5.1, including Dropout <ref type="bibr" target="#b39">[40]</ref>, label smoothing <ref type="bibr" target="#b41">[42]</ref>, Flooding <ref type="bibr" target="#b21">[22]</ref>, MixUp <ref type="bibr" target="#b50">[51]</ref> and adversarial training <ref type="bibr" target="#b12">[13]</ref>. We also compare our scheme with ERM <ref type="bibr" target="#b43">[44]</ref>, which does not utilize any regularization and optimizes the neural network with L ERM . We include a baseline named random model perturbation (RMP) for comparison. Specifically, RMP applies a random perturbation (instead of the "worst perturbation") to the parameter within a small range to help the model to find a better minimum. Then, we study the performance of AMP on more complex deep architectures with powerful data augmentation techniques in Section 5.2. In addition, we investigate the calibration effect of AMP in Section 5.3 and demonstrate the influence of perturbation in Section 5.4. Finally, we compare the computational cost of AMP with ERM in Section 5.5. The implementation is on PyTorch framework <ref type="bibr" target="#b35">[36]</ref>, and the experiments are carried out on NVIDIA Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison of the Generalization Ability</head><p>Three publicly available benchmark image datasets are used for performance evaluation. The SVHN dataset <ref type="bibr" target="#b34">[35]</ref> has 10 classes containing 73,257 digits for training and 26,032 digits for testing. Limited by the computing resource, we did not use the additional 531,131 images in SVHN training. The CIFAR datasets contain 32×32-pixel colour images, where CIFAR-10 has 10 classes containing 5,000 images for training and 1,000 images for testing per class, CIFAR-100 has 100 classes containing 500 images for training and 100 images for testing per class <ref type="bibr" target="#b26">[27]</ref>.</p><p>Two representative deep architectures for image classification, PreActResNet18 <ref type="bibr" target="#b17">[18]</ref> and VGG16 <ref type="bibr" target="#b38">[39]</ref>, are taken as the underlying classifier. In the training procedure, random crops and horizontal flips are adopted as data augmentation schemes. We compute the mean and standard derivation on the training set to normalize the input images. SGD with momentum is exploited as the optimizer with a step-wise learning rate decay. Specifically, the outer learning rate is initialized as 0.1 and divided by 10 after 100 and 150 epochs. We train each model for 200 epochs on the training set with 50 examples per mini-batch. Weight decay is set to 10 −4 for all compared models. We tune hyper-parameters on each dataset using 10% of the training set as the validation set. For Dropout, we randomly choose 10% of the neurons in each layer after ReLU activation and deactivate them at each training iteration. The label smoothing coefficient is set to 0.2 and the flooding level is set to 0.02. For MixUp, we follow the original study <ref type="bibr" target="#b50">[51]</ref> and linearly combine random pairs of training examples by using coefficient variables drawn from Beta(1, 1). For adversarial training, we set the perturbation size to 1 for each pixel and take one single step to generate adversarial examples. For RMP, we set the L 2 norm ball radius to 0.1. For AMP, we fix the number of inner iteration as N = 1, and adopt = 0.5, γ = 1 for PreActResNet18   <ref type="table" target="#tab_1">Table 1</ref>.</p><p>From <ref type="table" target="#tab_1">Table 1</ref>, it is evident that AMP outperforms the baseline methods in various settings, in terms of both classification error and test neg-log-likelihood, except on the CIFAR-10 dataset where VGG16 is employed. Despite the remarkable performance of AMP, MixUp also demonstrates competitive improvement in classification accuracy, and Flooding achieves small testing neg-log-likelihood in many settings. We note that, compared with AMP, the results of RMP suggest that randomly perturbing parameters cannot obtain comparable performance to the AMP. This result confirms that the adversarial perturbation provides the most useful information to the regularizer. Above results describe the efficiency of AMP in regularizing deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Improvement over Data Augmentation</head><p>Data augmentation techniques can be viewed as regularization schemes since through introducing additional training examples, they impose additional constraints on the model parameter thereby improving generalization. To validate the effectiveness of AMP over other data augmentation techniques, we choose vanilla augmentation <ref type="bibr" target="#b27">[28]</ref>, Cutout <ref type="bibr" target="#b6">[7]</ref> and AutoAugment <ref type="bibr" target="#b5">[6]</ref> as underlying augmentation methods and compare the classification accuracy of AMP with ERM. The vanilla augmentation exploits manually designed policies including random crops and horizontal flips. We use the same Cutout configuration and AutoAugment policy as their corresponding original studies. For the hyper-parameters of AMP, we fix N = 1 and adopt = 0.5, γ = 1 for vanilla augmentation, = 0.3, γ = 0.5 for Cutout, and = 0.1, γ = 0.1 for AutoAugment. We employ two recent powerful deep architectures, WideResNet <ref type="bibr" target="#b48">[49]</ref> and PyramidNet <ref type="bibr" target="#b16">[17]</ref>, with the  <ref type="table">Table 2</ref>: Mean and standard deviation of top-1 errors (%) on SVHN, CIFAR-10 and CIFAR-100 over 10 trials. compared data augmentation techniques on SVHN, CIFAR-10 and CIFAR-100. The top-1 classification errors are shown in <ref type="table">Table 2</ref>. The results suggest AMP's regularization effect in the presence of advanced data augmentation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Calibration Effect</head><p>A well calibrated neural network is one in which the predicted softmax scores give better indicators of the actual likelihood of a correct prediction. Deep neural networks without any regularizer are prone to overconfidence on incorrect predictions, and a well calibrated network is required especially in some application areas like object detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37]</ref> and autonomous vehicle control <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>. AMP chooses a flatter minimum of the empirical risk, which gives less confidence on possibly misclassified examples, ensuring the neural network to be better calibrated. On the contrary, the sharp minima given by ERM may contain incorrectly classified examples, which makes overconfident predictions on misclassified examples. In this section, we demonstrate that AMP can improve the calibration effect of neural networks. We adopt the measurement of calibration described in <ref type="bibr" target="#b14">[15]</ref>, namely, the Expected Calibration Error (ECE) (see Appendix D for definition). To evaluate the calibration effect of different regularization schemes, we compute ECEs of the pretrained PreActResNet18 models on the SVHN, CIFAR-10 and CIFAR-100 datasets. The results are shown in <ref type="figure" target="#fig_5">Figure 7</ref>. From the results, AMP achieves excellent calibration performance on various datasets. We can also find that Flooding outperforms other methods in calibration error on the CIFAR-10 dataset. Additionally, label smoothing degrades the calibration effect of neural networks, since it excessively biases the labels to the uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Influence of Perturbation</head><p>To investigate the relationship between the perturbation magnitudes and the geometry of the selected local minima, we compare the empirical risks of models trained with ERM and AMP. To clearly illustrate this, we adopt η = 2 and N = 2. <ref type="figure" target="#fig_6">Figure 8</ref> shows the empirical risks on CIFAR-10 training and test set by varying the perturbation radius . It can be seen that L ERM (θ * AMP ) on the training set tends to be high when radius is large. This indicates that the selected minimum has a smaller depth. Moreover, when evaluating on the test set, L ERM (θ * AMP ) arrives at minimum when the perturbation radius is around 0.06. This suggests that an appropriate magnitude of perturbation regularizes networks efficiently, corresponds to the good properties of the selected minimum both in flatness and depth. Results on more datasets are given in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Computational Cost</head><p>AMP computes the adversarial perturbation of the model parameter in the training phase, such an operation arguably increases the computational cost. In addition to the gradient computation for updating the parameter, each stochastic gradient descent iteration requires multiple gradient computations to produce the adversarial perturbations. The compu- tational cost will significantly increase as the inner iteration number N grows. However, we find that N = 1 is sufficient to regularize the neural networks. Under this setting, we evaluate the practical computational cost of AMP compared to the ERM method. From our observation, AMP usually takes around 1.8× that of ERM training. Therefore, the extra effort for adversarial perturbation is affordable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Regularization is the main tool for deep learning practitioners to combat overfitting. In this work, we propose a novel regularization scheme, Adversarial Model Perturbation (AMP), built upon the understanding that flat local minima lead to better generalization. Unlike many other data-dependent regularization schemes, which are large of a heuristic nature, AMP has strong theoretical justifications under a certain approximating assumption. These justifications also allow us to predict its behaviour with respect to varying hyper-parameters. Our theoretical analysis and the regularization performance of AMP are confirmed through extensive experiments on image classification datasets. It is also observed that AMP helps the model to better calibrate itself. The outstanding performance of AMP arguably makes it into the current state of the art among all regularization schemes. The empirical validation of AMP presented in this paper appears to further confirm the connection between flat minima and generalization.</p><formula xml:id="formula_23">≈ θ k − η∇ θ (J ERM,B (θ k ) + ∇ θ J ERM,B (θ k ) 2 )</formula><p>This proves the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Why AMP is not Adversarial Training</head><p>In this section, we will further discuss the difference between AMP and adversarial training (ADV).</p><p>It is sensible that perturbing weights θ may have an effect similar to perturbing the examples x since θ and x usually appear together via inner product θ T x. However we note that except for some peculiar cases (such as linear network with some peculiar choices of the loss function or a set of peculiarly constructed training examples), in general the solution θ * AMP to the AMP optimization problem is different from the solution θ * ADV to the ADV counterpart. The difference between θ * AMP and θ * ADV can be attributed to two sources.</p><p>First, let (x; θ) denote the ERM loss for a single training example x. For N examples, the overall ERM loss L ERM is the sum (or average) of (x i ; θ) over all examples x i , i = 1, . . . , N . In AMP, the perturbation is to maximize the overall empirical loss L ERM and this perturbation is applied globally to weights θ. However, in ADV, the perturbation is applied individually to each training example x i , with the objective of maximizing the individual ERM loss (x i ; θ).</p><p>Second, even in the case when there is only one training example x so that L ERM = , θ * AMP and θ * ADV may still be different. Here is an example. Let</p><formula xml:id="formula_24">g(z) := z if z ≥ 0 −2z if z &lt; 0</formula><p>Consider that there is a single scalar example x = 1 and the weight θ is a scalar. Define (x; θ) = g(θx). It can be verified that θ * ADV = θ * ERM = 0 regardless of the perturbation radius , but θ * AMP = /3 (see <ref type="figure" target="#fig_7">Figure 9</ref>, where the losses are plotted as functions of θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Definition of Expected Calibration Error</head><p>We follow the definition presented in the previous work <ref type="bibr" target="#b14">[15]</ref>. Firstly, the predictions are grouped into M interval bins of equal sizes. Let B m be the set of indices of samples whose prediction scores (the winning softmax score) fall into the interval I m = ( m−1 M , m M ]. The accuracy and confidence of B m are defined as:</p><formula xml:id="formula_25">acc(B m ) = 1 |B m | i∈Bm 1(ŷ i = y i ) conf(B m ) = 1 |B m | i∈Bmp i</formula><p>whereŷ i and y i are the predicted label and true class labels for sample i,p i is the confidence (the winning softmax score) of sample i. The Expected Calibration Error (ECE) is defined as the difference in expectation between confidence and accuracy, i.e.:</p><formula xml:id="formula_26">ECE = M m=1 |B m | n acc(B m ) − conf(B m )</formula><p>where n is the number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Influence of Perturbation</head><p>We plot the empirical risks of the pretrained PreActRes-Net18 models on three image datasets with varying perturbation radius in <ref type="figure" target="#fig_0">Figure 10</ref>. To clearly illustrate this, we adopt  <ref type="table">Table 3</ref>: Test errors (%) against the while-box FGSM and PGD adversarial attacks. Each experiment has been run ten times to report the mean and standard derivation of errors. η = 2 and N = 2. In these experiments, the perturbation radius meets the sweet spots around 0.06 on all the three datasets, where L ERM (θ * AMP ) gets the minimum value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Robustness to Adversarial Attacks</head><p>The previous work <ref type="bibr" target="#b51">[52]</ref> suggests that the flat minima make the adversarial attacks take more efforts for the input to leave the minima, so AMP is expected to improve the model's adversarial robustness. To validate this, we use the models trained with different regularization schemes to evaluate their adversarial robustness against the Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b12">[13]</ref> and Projected Gradient Decent (PGD) <ref type="bibr" target="#b32">[33]</ref> attacks. For FGSM, we set the perturbation radius to 4 per pixel. For PGD, we set the step size to 1 and perform 10 steps to generate adversarial examples, the perturbation radius is the same as FGSM. PreActResNet18 is chosen as the model architecture. We report the top-1 classification error on the adversarial examples constructed from the test set in <ref type="table">Table 3</ref>. From the results, adversarial training outperforms all other schemes, since it directly trains models on the adversarial examples. AMP and label smoothing also show an effect in improving the model's robustness against both single-step FGSM attack and multi-step PGD attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Loss Curve</head><p>To investigate the mechanism of different regularization schemes in the training course, we plot the evolution curves of the training loss and the test loss in <ref type="figure" target="#fig_0">Figure 11</ref> using PreAc-tResNet18. We select ERM and two representative analogues (Flooding and MixUp) which achieved the second-best performance in the previous experiment to compare with AMP. From <ref type="figure" target="#fig_0">Figure 11</ref>, ERM obtains the smallest training loss, and MixUp retains a high training loss since it trains models on augmented examples. AMP injects a small perturbation into the model parameter, and hence the training loss is slightly increased. It appears that the Flooding scheme affects training only when the training loss drops to a very low value, whereas MixUp and AMP take effects much earlier. For the test loss, AMP converges at a similar speed as other schemes, and reduces the test loss to a smaller value at the final stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Flatness of Selected Minima</head><p>We visualize the landscapes around the minima of the empirical risk selected by ERM or AMP, the 2D views are plotted in <ref type="figure" target="#fig_0">Figure 12</ref> and the 3D views are in <ref type="figure" target="#fig_0">Figure 13</ref>. Specifically, we compute the empirical risks of the PreAc-tResNet18 models whose parameter is perturbed along two random directions d x , d j with different step sizes δ x , δ y , where the direction vectors are normalized by the norm of filters suggested by <ref type="bibr" target="#b30">[31]</ref>. Specifically, we visualize the landscapes by computing</p><formula xml:id="formula_27">L ERM (θ * + δ x d x + δ y d y )</formula><p>The results suggest that AMP indeed selects flatter minima via adversarial perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Computing Environment and Resources</head><p>Our PyTorch code is executed in a CUDA environment. When evaluated on a single Tesla V100 GPU, the code takes around 2.4 hours to train a PreActResNet18 model with ERM on the CIFAR-10 dataset, and around 4.2 hours with AMP. The computation time mainly depends on the number of inner iterations, the number of epochs, and the number of GPUs. The code and datasets for reproduction can be found at https://github.com/hiyouga/AMP-Regularizer.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example showing an empirical risk curve (left) and its corresponding AMP loss curve (right, blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Landscapes of the empirical risks obtained from the PreActResNet18 [18] models trained with ERM (red) and AMP (blue) on CIFAR-10. Left: on training set; right:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Locally Gaussian assumption and the minimum values of γ and γ AMP , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The operational region of AMP (left) and how it varies with varying values of 2 /2σ 2 1 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>A bad classification boundary (left) usually has a poor generalization performance and is fragile to adversarial attacks. A better classification boundary (right) keeps itself far from the training examples, and tends to generalize well. WHVWORVVWHVWDFF WHVWORVVWHVWDFF AMP yields a better classification boundary (right) than ERM does (left). The experiment is conducted on a spiral dataset<ref type="bibr" target="#b40">[41]</ref> using a feed-forward network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Expected calibration errors (ECEs) of AMP and other baseline methods. Results are averaged over 10 trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>The empirical risks of the model trained with ERM (red) and AMP (blue) on the CIFAR-10 training set and test set with varying perturbation radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>The losses of ERM and AMP with varying θ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>The comparison of L ERM of the models trained with ERM (red) and AMP (blue) with varying perturbation radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :Figure 13 :</head><label>1113</label><figDesc>Loss curves for PreActResNet18 with different regularization schemes on three benchmark image datasets. 3D visualization of the minima of the empirical risk selected by ERM and AMP on the SVHN dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Top-1 classification errors and test neg-log-likelihoods on (a) SVHN, (b) CIFAR-10 and (c) CIFAR-100. We run experiments 10 times to report the mean and the standard deviation of errors and neg-log-likelihoods.and = 0.1, γ = 0.2 for VGG16. Top-1 classification error and test neg-log-likelihood are reported in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Vanilla 19.17±0.270 17.33±0.110 17.13±0.210 15.09±0.092 CIFAR-100 Cutout 18.12±0.114 16.04±0.071 16.45±0.136 14.34±0.153 AutoAug 17.79±0.185 14.95±0.088 15.43±0.269 13.36±0.245</figDesc><table><row><cell></cell><cell></cell><cell cols="2">WideResNet-28-10</cell><cell cols="2">PyramidNet-164-270</cell></row><row><cell></cell><cell></cell><cell>ERM</cell><cell>AMP</cell><cell>ERM</cell><cell>AMP</cell></row><row><cell></cell><cell>Vanilla</cell><cell>2.57±0.067</cell><cell>2.19±0.036</cell><cell>2.47±0.034</cell><cell>2.11±0.041</cell></row><row><cell>SVHN</cell><cell>Cutout</cell><cell>2.27±0.085</cell><cell>1.83±0.018</cell><cell>2.19±0.021</cell><cell>1.82±0.023</cell></row><row><cell></cell><cell cols="2">AutoAug 1.91±0.059</cell><cell>1.61±0.024</cell><cell>1.80±0.044</cell><cell>1.35±0.056</cell></row><row><cell></cell><cell>Vanilla</cell><cell>3.87±0.167</cell><cell>3.00±0.059</cell><cell>3.60±0.197</cell><cell>2.75±0.040</cell></row><row><cell>CIFAR-10</cell><cell>Cutout</cell><cell>3.38±0.081</cell><cell>2.67±0.043</cell><cell>2.83±0.102</cell><cell>2.27±0.034</cell></row><row><cell></cell><cell cols="2">AutoAug 2.78±0.134</cell><cell>2.32±0.097</cell><cell>2.49±0.128</cell><cell>1.98±0.062</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>41±0.569 36.06±1.908 68.78±0.699 Dropout 22.36±0.591 34.13±0.844 64.70±0.549 Label Smoothing 17.74±1.674 23.24±0.427 57.30±0.410 Flooding 17.40±0.656 36.42±1.303 68.45±0.407 MixUp 19.95±0.637 25.82±0.384 65.90±0.498 Adv. Training 14.33±0.200 18.58±0.304 48.51±0.260 RMP 23.73±0.965 35.40±0.572 68.52±0.515 AMP 16.82±1.561 28.61±0.359 59.04±1.325</figDesc><table><row><cell>FGSM</cell><cell>SVHN</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell cols="2">ERM 23.PGD SVHN</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell>ERM</cell><cell cols="3">45.17±1.085 58.88±2.296 85.46±0.770</cell></row><row><cell>Dropout</cell><cell cols="3">41.76±1.346 55.21±1.088 78.46±1.081</cell></row><row><cell cols="4">Label Smoothing 32.55±2.005 34.93±0.443 65.31±0.700</cell></row><row><cell>Flooding</cell><cell cols="3">33.50±1.707 60.32±1.393 84.66±0.285</cell></row><row><cell>MixUp</cell><cell cols="3">75.75±2.129 62.77±1.018 89.58±0.596</cell></row><row><cell>Adv. Training</cell><cell cols="3">20.20±0.409 21.46±0.373 51.72±0.327</cell></row><row><cell>RMP</cell><cell cols="3">44.74±0.960 58.06±0.650 84.80±0.488</cell></row><row><cell>AMP</cell><cell cols="3">25.15±1.942 49.72±0.785 73.95±2.608</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. Proof of Corollary 2 Corollary 2. Suppose that C 1 = C 2 . Let A 2 = βA 1 for some β &lt; 1. Note that in this setting, γ * 1 &lt; γ *</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<meeting>the 36th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reconciling modern machine-learning practice and the classical bias-variance trade-off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumik</forename><surname>Mandal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="15849" to="15854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Entropy-sgd: Biasing gradient descent into wide valleys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision, ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition, CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sharp minima can generalize for deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML</title>
		<meeting>the 34th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Loss surfaces, mode connectivity, and fast ensembling of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8789" to="8798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shake-shake regularization of 3-branch residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop Track Proceedings in International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fast R-Cnn</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision, ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML</title>
		<meeting>the 34th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3714" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition, CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5927" to="5935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1731" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Do we need zero training loss after achieving zero training error?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Ishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikko</forename><surname>Yamane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML</title>
		<meeting>the 37th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI</title>
		<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8571" to="8580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04623</idno>
		<title level="m">Three factors influencing minima in SGD</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On largebatch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping Tak Peter</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="950" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards fully autonomous driving: Systems and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Askeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Dolson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Kammel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Pink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaughan</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="163" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6665" to="6675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling by shortest data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="465" to="471" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Introduction to statistical machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition, CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robustness may be at odds with accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The implicit and explicit regularization effects of dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML</title>
		<meeting>the 37th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarial weight perturbation helps robust generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2958" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Shakedrop regularization for deep residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Kise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="186126" to="186136" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking bias-variance trade-off for generalization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML</title>
		<meeting>the 37th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference, BMVC</title>
		<meeting>the British Machine Vision Conference, BMVC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bridging mode connectivity in loss landscapes and adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payel</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Karthikeyan Natesan Ramamurthy, and Xue Lin</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
