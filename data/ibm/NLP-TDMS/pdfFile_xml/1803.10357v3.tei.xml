<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Communicating Agents for Abstractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
							<email>antoineb@cs.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<email>xiaodong.he@jd.com</email>
							<affiliation key="aff2">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Communicating Agents for Abstractive Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subsection of the input text. These encoders are connected to a single decoder, trained end-to-end using reinforcement learning to generate a focused and coherent summary. Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single encoder or multiple non-communicating encoders.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We focus on the task of abstractive summarization of a long document. In contrast to extractive summarization, where a summary is composed of a subset of sentences or words lifted from the input text as is, abstractive summarization requires the generative ability to rephrase and restructure sentences to compose a coherent and concise summary. As recurrent neural networks (RNNs) are capable of generating fluent language, variants of encoder-decoder RNNs <ref type="bibr" target="#b27">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> have shown promising results on the abstractive summarization task <ref type="bibr" target="#b23">(Rush et al., 2015;</ref><ref type="bibr" target="#b18">Nallapati et al., 2017)</ref>.</p><p>The fundamental challenge, however, is that the strong performance of neural models at encoding short text does not generalize well to long text. The motivation behind our approach is to be able to dynamically attend to different parts of the input * Work done while author was at Microsoft Research to capture salient facts. While recent work in summarization addresses these issues using improved attention models <ref type="bibr" target="#b3">(Chopra et al., 2016)</ref>, pointer networks with coverage mechanisms <ref type="bibr" target="#b25">(See et al., 2017)</ref>, and coherence-focused training objectives <ref type="bibr" target="#b20">(Paulus et al., 2018;</ref><ref type="bibr" target="#b12">Jaques et al., 2017)</ref>, an effective mechanism for representing a long document remains a challenge. Simultaneous work has investigated the use of deep communicating agents <ref type="bibr" target="#b26">(Sukhbaatar et al., 2016)</ref> for collaborative tasks such as logic puzzles <ref type="bibr" target="#b8">(Foerster et al., 2016)</ref>, visual dialog <ref type="bibr" target="#b4">(Das et al., 2017)</ref>, and reference games <ref type="bibr" target="#b13">(Lazaridou et al., 2016)</ref>. Our work builds on these approaches to propose the first study on using communicating agents to encode long text for summarization.</p><p>The key idea of our model is to divide the hard task of encoding a long text across multiple collaborating encoder agents, each in charge of a different subsection of the text <ref type="figure" target="#fig_0">(Figure 1</ref>). Each of these agents encodes their assigned text independently, and broadcasts their encoding to others, allowing agents to share global context information with one another about different sections of the document. All agents then adapt the encoding of their at each layer k. Communication is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. The word context vectors c t a are condensed into agent context c * t . Agent specific generation probabilities, p t a , enable voting for the suitable out-of-vocabulary words (e.g., 'yen') in the final distribution.</p><p>assigned text in light of the global context and repeat the process across multiple layers, generating new messages at each layer. Once each agent completes encoding, they deliver their information to the decoder with a novel contextual agent attention ( <ref type="figure" target="#fig_1">Figure 2</ref>). Contextual agent attention enables the decoder to integrate information from multiple agents smoothly at each decoding step. The network is trained end-to-end using self-critical reinforcement learning <ref type="bibr" target="#b22">(Rennie et al., 2016)</ref> to generate focused and coherent summaries.</p><p>Empirical results on the CNN/DailyMail and New York Times datasets demonstrate that multiple communicating encoders lead to higher quality summaries compared to strong baselines, including those based on a single encoder or multiple non-communicating encoders. Human evaluations indicate that our model is able to produce more focused summaries. The agents gather salient information from multiple areas of the document, and communicate their information with one another, thus reducing common mistakes such as missing key facts, repeating the same content, or including unnecessary details. Further analysis reveals that our model attains better performance when the decoder interacts with multiple agents in a more balanced way, confirming the benefit of representing a long document with multiple encoding agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We extend the CommNet model of <ref type="bibr" target="#b26">Sukhbaatar et al. (2016)</ref> for sequence generation.</p><p>Notation Each document d is a sequence of paragraphs x a , which are split across multiple encoding agents a=1,..,M (e.g., agent-1 encodes the first paragraph x 1 , agent-2 the second paragraph x 2 , so on). Each paragraph x a ={w a,i } I , is a sequence of I words. We construct a V -sized vocabulary from the training documents from the most frequently appearing words. Each word w a,i is embedded into a n-dimensional vector e a,i . All W variables are linear projection matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-Agent Encoder</head><p>Each agent encodes the word sequences with the following two stacked encoders. Local Encoder The first layer is a local encoder of each agent a, where the tokens of the corresponding paragraph x a are fed into a single layer bi-directional LSTM (bLSTM), producing the local encoder hidden states, h</p><formula xml:id="formula_0">(1) i ∈ R H : − → h (1) i , ← − h (1) i = bLSTM(e i , − → h (1) i−1 , ← − h (1) i+1 ) (1) h (1) i = W 1 [ − → h (1) i , ← − h (1) i ] (2)</formula><p>where H is the hidden state dimensionality. The output of the local encoder layer is fed into the contextual encoder. Contextual Encoder Our framework enables agent communication cycles across multiple encoding layers. The output of each contextual encoder is an adapted representation of the agent's encoded information conditioned on the information received from the other agents. At each layer k=1,..,K, each agent a jointly encodes the information received from the previous layer (see <ref type="bibr">Figure 3)</ref>. Each cell of the (k+1)th contextual layer is a bLSTM that takes three inputs: the hidden states from the adjacent LSTM cells,</p><formula xml:id="formula_1">− → h (k+1) i−1 ∈R H or ← − h (k+1)</formula><p>i+1 ∈R H , the hidden state from the previous layer h (k) i , and the message vector from other agents z (k) ∈R H and outputs h</p><formula xml:id="formula_2">(k+1) i ∈R H : − → h (k+1) i , ← − h (k+1) i =bLSTM(f (h (k) i , z (k) ), (3) − → h (k+1) i−1 , ← − h (k+1) i+1 ) (4) h (k+1) i = W 2 [ − → h (k+1) i , ← − h (k+1) i ]<label>(5)</label></formula><p>where i=1..I indicates the index of each token in the sequence. The message z (k) received by any agent a in layer k is the average of the outputs of the other agents from layer k:</p><formula xml:id="formula_3">z (k) = 1 M −1 m =a h (k) m,I<label>(6)</label></formula><p>where h (k) m,I is the last hidden state output from the kth contextual layer of each agent where m = a. Here, we take the average of the messages received from other encoder agents, but a parametric function such as a feed forward model or an attention over messages could also be used.</p><p>The message z (k) is projected with the agent's previous encoding of its document:</p><formula xml:id="formula_4">f (h (k) i , z (k) ) = v T 1 tanh(W 3 h (k) i + W 4 z (k) ) (7)</formula><p>where v 1 , W 3 , and W 4 are learned parameters shared by every agent. Equation (7) combines the information sent by other agents with the context of the current token from this paragraph. This yields different features about the current context in relation to other topics in the source document. At each layer, the agent modifies its representation of its own context relative to the information received from other agents, and updates the information it sends to other agents accordingly. a as additional input to its next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decoder with Agent Attention</head><p>The output from the last contextual encoder layer of each agent {h (K) a,i } I , which is a sequence of hidden state vectors of each token i, is sent to the decoder to calculate word-attention distributions. We use a single-layer LSTM for the decoder and feed the last hidden state from the first agent s 0 = h (K) 1,I as the initial state. At each time step t, the decoder predicts a new word in the summary w t and computes a new state s t by attending to relevant input context provided by the agents.</p><p>The decoder uses a new hierarchical attention mechanism over the agents. First, a word attention distribution l t a <ref type="bibr" target="#b0">(Bahdanau et al. (2015)</ref>) is computed over every token {h (K) a,i } I for each agent a:</p><formula xml:id="formula_5">l t a = softmax(v T 2 tanh(W 5 h (K) a + W 6 s t + b 1 ))<label>(8)</label></formula><p>where l t a ∈[0, 1] I is the attention over all tokens in a paragraph x a and v 2 , W 5 , W 6 , b 1 are learned parameters. For each decoding step t, a new decoder context is calculated for each agent:</p><formula xml:id="formula_6">c t a = i l t a,i h (K) a,i<label>(9)</label></formula><p>which is the weighted sum of the encoder hidden states of agent a. Each word context vector represents the information extracted by the agent from the paragraph it has read. Here the decoder has to decide which information is more relevant to the current decoding step t. This is done by weighting each context vector by an agent attention yielding the document global agent attention distribution g t (see <ref type="figure" target="#fig_1">Figure 2</ref>):</p><formula xml:id="formula_7">g t = softmax(v T 3 tanh(W 7 c t + W 8 s t + b 2 )) (10)</formula><p>where v 3 , W 7 , W 8 , and b 2 are learned, and g t ∈[0,1] M is a soft selection over M agents. Then, we compute the agent context vector c * t :</p><formula xml:id="formula_8">c * t = a g t a c t a<label>(11)</label></formula><p>The agent context c * t ∈R H is a fixed length vector encoding salient information from the entire document provided by the agents. It is then concatenated with the decoder state s t and fed through a multi-layer perception to produce a vocabulary distribution (over all vocabulary words) at time t:</p><formula xml:id="formula_9">P voc (w t |s t , w t−1 ) = softmax(MLP([s t , c * t ]))<label>(12)</label></formula><p>To keep the topics of generated sentences intact, it is reasonable that the decoder utilize the same agents over the course of short sequences (e.g., within a sentence). Because the decoder is designed to select which agent to attend to at each time step, we introduce contextual agent attention (caa) to prevent it from frequently switching between agents. The previous step's agent attention c * t−1 is used as additional information to the decoding step to generate a distribution over words:</p><formula xml:id="formula_10">P voc (w t |·) = softmax(MLP([s t , c * t , c * t−1 ])) (13)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Agent Pointer Network</head><p>Similar to <ref type="bibr" target="#b25">See et al. (2017)</ref>, we allow for copying candidate words from different paragraphs of the document by computing a generation probability value for each agent p t a ∈[0,1] at each timestep t using the context vector c t a , decoder state s t and decoder input y t :</p><formula xml:id="formula_11">p t a = σ(v T 5 c t a + v T 6 s t + v T 7 y t + b)<label>(14)</label></formula><p>where b is a learned scalar, y t is the groundtruth/predicted output (depending on the training/testing time). The generation probability determines whether to generate a word from the vocabulary by sampling from P voc (w|·), or copying a word from the corresponding agent's input paragraph x a by sampling from its attention distribution l t a . This produces an extended vocabulary that includes words in the document that are considered out-of-vocabulary (OOV). A probability distribution over the extended vocabulary is computed for each agent:</p><formula xml:id="formula_12">P a (w t |·) = p t a P voc (w t |·) + (1 − p t a )u t a,w (15)</formula><p>where u t a,w is the sum of the attention for all instances where w appears in the source document. The final distribution over the extended vocabulary, from which we sample, is obtained by weighting each agent by their corresponding agent attention values g t a :</p><formula xml:id="formula_13">P (w t |s t , w t−1 ) = a g t a P a (w t |·)<label>(16)</label></formula><p>In contrast to a single-agent baseline <ref type="bibr" target="#b25">(See et al., 2017)</ref>, our model allows each agent to vote for different OOV words at time t (Equation <ref type="formula" target="#formula_3">(16)</ref>). In such a case, only the word that is relevant to the generated summary up to time t is collaboratively voted as a result of agent attention probability g t a .</p><p>3 Mixed Objective Learning</p><p>To train the deep communicating agents, we use a mixed training objective that jointly optimizes multiple losses, which we describe below.</p><p>MLE Our baseline multi-agent model uses maximum likelihood training for sequence generation. Given y * = {y * 1 ,y * 2 ,...,y * T } as the groundtruth output sequence (human summary word sequences) for a given input document d, we minimize the negative log-likelihood of the target word sequence:</p><formula xml:id="formula_14">L MLE = − N t=1 logp(y * t |y * 1 . . . y * t−1 , d) (17)</formula><p>Semantic Cohesion To encourage sentences in the summary to be informative without repetition, we include a semantic cohesion loss to integrate sentence-level semantics into the learning objective. As the decoder generates the output word sequence {y 1 , y 2 . . . y T }, it keeps track of the end of sentence delimiter token ('.') indices. The hidden state vectors at the end of each sentence s q , q=1. . . Q, where s q ∈{s t :y t ='·', 1≤t≤T }, are used to compute the cosine similarity between two consecutively generated sentences. To minimize the similarity between end-of-sentence hidden states we define a semantic cohesion loss:</p><formula xml:id="formula_15">L SEM = Q q=2 cos(s q , s q−1 )<label>(18)</label></formula><p>The final training objective is then:</p><formula xml:id="formula_16">L MLE-SEM = L MLE + λL SEM (19)</formula><p>where λ is a tunable hyperparameter.</p><p>Reinforcement Learning (RL) Loss Policy gradient methods can directly optimize discrete target evaluation metrics such as ROUGE that are non-differentiable <ref type="bibr" target="#b20">(Paulus et al., 2018;</ref><ref type="bibr" target="#b12">Jaques et al., 2017;</ref><ref type="bibr" target="#b19">Pasunuru and Bansal, 2017;</ref>. At each time step, the word generated by the model can be viewed as an action taken by an RL agent. Once the full sequenceŷ is generated, it is compared against the ground truth sequence y * to compute the reward r(ŷ). Our model learns using a self-critical training approach (Rennie et al., 2016), which learns by exploring new sequences and comparing them to the best greedily decoded sequence. For each training example d, two output sequences are generated: y, which is sampled from the probability distribution at each time step, p(ŷ t |ŷ 1 . . .ŷ t−1 , d), andỹ, the baseline output, which is greedily generated by argmax decoding from p(ỹ t |ỹ 1 . . .ỹ t−1 , d). The training objective is then to minimize:</p><formula xml:id="formula_17">L RL = (r(ỹ) − r(ŷ)) N t=1 logp(ŷ t |ŷ 1 . . .ŷ t−1 , d)<label>(20)</label></formula><p>This loss ensures that, with better exploration, the model learns to generate sequencesŷ that receive higher rewards compared to the baselineỹ, increasing overall reward expectation of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed Loss</head><p>While training with only MLE loss will learn a better language model, this may not guarantee better results on global performance measures. Similarly, optimizing with only RL loss may increase the reward gathered at the expense of diminished readability and fluency of the generated summary <ref type="bibr" target="#b20">(Paulus et al., 2018)</ref>. A combination of the two objectives can yield improved task-specific scores while maintaining fluency:</p><formula xml:id="formula_18">L MIXED = γL RL + (1 − γ)L MLE<label>(21)</label></formula><p>where γ is a tunable hyperparameter used to balance the two objective functions. We pre-train our models with MLE loss, and then switch to the mixed loss. We can also add the semantic cohesion loss term: L MIXED-SEM = γL RL +(1−γ)L MLE-SEM to analyze its impact in RL training.</p><p>Intermediate Rewards We introduce sentencebased rewards as opposed to end of summary rewards, using differential ROUGE metrics, to promote generating diverse sentences. Rather than rewarding sentences based on the scores obtained at the end of the generated summary, we compute incremental rouge scores of a generated sentenceô q :</p><formula xml:id="formula_19">r(ô q ) = r([ô 1 , . . .ô q ]) − r([ô 1 , . . .ô q−1 ]) (22)</formula><p>Sentences are rewarded for the increase in ROUGE they contribute to the full sequence, ensuring that the current sentence contributed novel information to the overall summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Datasets We conducted experiments on two summarization datasets: CNN/DailyMail <ref type="bibr" target="#b18">(Nallapati et al., 2017;</ref><ref type="bibr" target="#b9">Hermann et al., 2015)</ref> and New York Times (NYT) <ref type="bibr" target="#b24">(Sandhaus, 2008)</ref>. We replicate the preprocessing steps of <ref type="bibr" target="#b20">Paulus et al. (2018)</ref> to obtain the same data splits, except that we do not anonymize named entities. For our DCA models, we initialize the number of agents before training, and partition the document among the agents (i.e., three agent → three paragraphs). Additional details can be found in Appendix A.1.</p><p>Training Details During training and testing we truncate the article to 800 tokens and limit the length of the summary to 100 tokens for training and 110 tokens at test time. We distribute the truncated articles among agents for multi-agent models, preserving the paragraph and sentences as possible. For both datasets, we limit the input and output vocabulary size to the 50,000 most frequent tokens in the training set. We train with up to two contextual layers in all the DCA models as more layers did not provide additional performance gains. We fix γ = 0.97 for the RL term in Equation <ref type="formula" target="#formula_18">(21)</ref>  Model ROUGE-1 ROUGE-2 ROUGE-L SummaRuNNer <ref type="bibr" target="#b18">(Nallapati et al., 2017)</ref> 39.60 16.20 35.30 graph-based attention <ref type="bibr" target="#b28">(Tan et al., 2017)</ref> 38.01 13.90 34.00 pointer generator <ref type="bibr" target="#b25">(See et al., 2017)</ref> 36.44 15.66 33.42 pointer generator + coverage <ref type="bibr" target="#b25">(See et al., 2017)</ref> 39.53 17.28 36.38 controlled summarization with fixed values <ref type="bibr" target="#b6">(Fan et al., 2017)</ref> 39.75 17.29 36.54 RL, with intra-attention <ref type="bibr" target="#b20">(Paulus et al., 2018)</ref> 41.16 15.75 39.08 ML+RL, with intra-attention <ref type="bibr" target="#b20">(Paulus et al., 2018)</ref> 39  Baselines We compare our DCA models against previously published models: SummaRuNNer <ref type="bibr" target="#b18">(Nallapati et al., 2017)</ref>, a graph-based attentional neural model <ref type="bibr" target="#b28">(Tan et al., 2017)</ref> an RNN-based extractive summarizer that combines abstractive features during training; Pointer-networks with and without coverage <ref type="bibr" target="#b25">(See et al., 2017)</ref>, RL-based training for summarization with intra-decoder attention <ref type="bibr" target="#b20">(Paulus et al., 2018)</ref>), and Controllable Abstractive Summarization <ref type="bibr" target="#b6">(Fan et al., 2017)</ref> which allows users to define attributes of generated summaries and also uses a copy mechanism for source entities and decoder attention to reduce repetition.</p><p>Ablations We investigate each new component of our model with a different ablation, producing seven different models. Our first three ablations are: a single-agent model with the same local encoder, context encoder, and pointer network architectures as the DCA encoders trained with MLE loss (m1); the same model trained with additional semantic cohesion SEM loss (m2), and the same model as the (m1) but trained with a mixed loss and end-of-summary rewards (m3).</p><p>The rest of our models use 3 agents and incrementally add one component. First, we add the semantic cohesion loss (m4). Then, we add multi-agent pointer networks (mpgen) and agent communication (m5). Finally, we add contextual agent attention (caa) (m6), and train with the mixed MLE+RL+SEM loss (m7). All DCA models use pointer networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative Analysis</head><p>We show our results on the CNN/DailyMail and NYT datasets in <ref type="table" target="#tab_1">Table 1</ref> and 2 respectively. Overall, our (m6) and (m7) models with multiagent encoders, pointer generation, and communication are the strongest models on ROUGE-1 and ROUGE-2. While weaker on ROUGE-L than the RL model from <ref type="bibr" target="#b20">Paulus et al. (2018)</ref>, the human evaluations in that work showed that their model received lower readability and relevance scores than a model trained with MLE, indicating the additional boost in ROUGE-L was not correlated with summary quality. This result can also account for our best models being more abstractive. Our models use mixed loss not just to op-timize for sentence level structure similarity with the reference summary (to get higher ROUGE as reward), but also to learn parameters to improve semantic coherence, promoting higher abstraction (see <ref type="table" target="#tab_5">Table 4</ref> and Appendix B for generated summary examples).  Single vs. Multi-Agents All multi-agent models show improvements over the single agent baselines. On the CNN/DailyMail dataset, compared to MLE published baselines, we improve across all ROUGE scores. We found that the 3-agent models generally outperformed both 2-and 5agent models (see <ref type="table" target="#tab_4">Table 3</ref>). This is in part because we truncate documents before training and the larger number of agents might be more efficient for multi-document summarization.</p><p>Independent vs. Communicating Agents When trained on multiple agents with no communication (m4), the performance of our DCA models is similar to the single agent baselines (m1) and (m3). With communication, the biggest jump in ROUGE is seen on the CNN/DailyMail data, indicating that the encoders can better identify the key facts in the input, thereby avoiding unnecessary details.</p><p>Contextual Agent Attention (caa) Compared to the model with no contextualized agent attention (m5), the (m6) model yields better ROUGE scores. The stability provided by the caa helps the decoder avoid frequent switches between agents that would dilute the topical signal captured by each encoder.</p><p>Repetition Penalty As neurally generated summaries can be redundant, we introduced the semantic cohesion penalty and incremental rewards for RL to generate semantically diverse summaries. Our baseline model optimized together with SEM loss (m2) improves on all ROUGE scores over the baseline (m1). Similarly, our model trained with reinforcement learning uses sentence based intermediate rewards, which also improves ROUGE scores across both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human Evaluations</head><p>We perform human evaluations to establish that our model's ROUGE improvements are correlated with human judgments. We measure the communicative multi-agent network with contextual agent attention in comparison to a single-agent network with no communication. We use the following as evaluation criteria for generated summaries: (1) non-redundancy, fewer of the same ideas are repeated, (2) coherence, ideas are expressed clearly;</p><p>(3) focus, the main ideas of the document are shared while avoiding superfluous details, and (4) overall, the summary effectively communicates the article's content. The focus and non-redundancy dimensions help quantify the impact of multi-agent communication in our model, while coherence helps to evaluate the impact of the reward based learning and repetition penalty of the proposed models.</p><p>Evaluation Procedure We randomly selected 100 samples from the CNN/DailyMail test set and use workers from Amazon Mechanical Turk as judges to evaluate them on the four criteria defined above. Judges are shown the original document, the ground truth summary, and two model summaries and are asked to evaluate each summary on the four criteria using a Likert scale from 1 (worst) to 5 (best). The ground truth and model summaries are presented to the judges in random order. Each summary is rated by 5 judges and the results are averaged across all examples and judges.</p><p>We also performed a head-to-head evaluation (more common in DUC style evaluations) and randomly show two model generated summaries. We ask the human annotators to rate each summary on the same metrics as before without seeing the source document or ground truth summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Human evaluators significantly prefer summaries generated by the communicating encoders. In the rating task, evaluators preferred the multi-agent summaries to the single-agent cases for all metrics. In the head-to-head evaluation, humans consistently preferred the DCA summaries to those generated by a single agent. In both the head-to-head and the rating evaluation, the largest improvement for the DCA model was on the focus question, indicating that the model learns to generate summaries with more pertinent details by capturing salient information from later portions of the document.</p><p>Human Mr Turnbull was interviewed about his childhood and his political stance. He also admitted he planned to run for prime minister if Tony Abbott had been successfully toppled in February's leadership spill. The words 'primed minister' were controversially also printed on the cover.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single</head><p>Malcolm Turnbull is set to feature on the front cover of the GQ Australia in a bold move that will no doubt set senators' tongues wagging. Posing in a suave blue suit with a pinstriped shirt and a contrasting red tie , Mr Turnbull's confident demeanour is complimented by the bold, confronting words printed across the page: 'primed minister'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi</head><p>Malcolm Turnbull was set to run for prime minister if Tony Abbott had been successfully toppled in February's leadership spill. He is set to feature on the front cover of the liberal party's newsletter. Human Daphne Selfe has been modelling since the fifties. She has recently landed a new campaign with vans and &amp; other stories. The 86-year-old commands 1,000 a day for her work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single</head><p>Daphne Selfe, 86, shows off the collaboration between the footwearsuper-brandand theetherealhigh street store with uncompromisinggrace. Daphne said of the collection , in which she appears with 22-year-old flo dron: 'the &amp; other stories collection that is featured in this story is truly relaxed and timeless with a modern twist'. The shoes are then worn with pieces from the brands ss2015 collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi</head><p>Daphne Selfe, 86, has starred in the campaign for vans and &amp; other stories. The model appears with 22-year-old flo dron &amp; other hair collection . She was still commanding 1,000 a day for her work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Communication improves focus</head><p>To investigate how much the multi-agent models discover salient concepts in comparison to single agent models, we analyze ROUGE-L scores based on the average attention received by each agent. We compute the average attention received by each agent per decoding time step for every generated summary in the CNN/Daily Mail test corpus, bin the document-summary pairs by the attention received by each agent, and average the ROUGE-L scores for the summaries in each bin. <ref type="figure" target="#fig_3">Figure 4</ref> outlines two interesting results. First, summaries generated with a more distributed attention over the agents yield higher ROUGE-L scores, indicating that attending to multiple areas of the document allows the discovery of salient concepts in the later sections of the text. Second, if we use the same bins and generate summaries for the documents in each bin using the singleagent model, the average ROUGE-L scores for the single-agent summaries are lower than for the cor- responding multi-agent summaries, indicating that even in cases where one agent dominates the attention, communication between agents allows the model to generate more focused summaries. Qualitatively, we see this effect in <ref type="table" target="#tab_5">Table 4</ref>, where we compare the human generated summaries against our best single agent model (m3) and our best multi-agent model (m7). Model (m3) generates good summaries but does not capture all the facts in the human summary, while (m7) is able to include all the facts with few extra details, generating more relevant and diverse summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Several recent works investigate attention mechanisms for encoder-decoder models to sharpen the context that the decoder should focus on within the input encoding <ref type="bibr" target="#b15">(Luong et al., 2015;</ref><ref type="bibr" target="#b31">Vinyals et al., 2015b;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. For example, <ref type="bibr" target="#b15">Luong et al. (2015)</ref> proposes global and local attention networks for machine translation, while others investigate hierarchical attention networks for document classification <ref type="bibr">(Yang et al., 2016)</ref>, sentiment classification , and dialog response selection <ref type="bibr" target="#b35">(Zhou et al., 2016)</ref>.</p><p>Attention mechanisms have shown to be crucial for summarization as well <ref type="bibr" target="#b23">(Rush et al., 2015;</ref><ref type="bibr" target="#b34">Zeng et al., 2016;</ref><ref type="bibr" target="#b18">Nallapati et al., 2017)</ref>, and pointer networks <ref type="bibr" target="#b30">(Vinyals et al., 2015a)</ref>, in particular, help address redundancy and saliency in generated summaries <ref type="bibr" target="#b2">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b25">See et al., 2017;</ref><ref type="bibr" target="#b20">Paulus et al., 2018;</ref><ref type="bibr" target="#b6">Fan et al., 2017)</ref>. While we share the same motivation as these works, our work uniquely presents an approach based on CommNet, the deep communicating agent framework <ref type="bibr" target="#b26">(Sukhbaatar et al., 2016)</ref>. Compared to prior multi-agent works on logic puzzles <ref type="bibr" target="#b7">(Foerster et al., 2017)</ref>, language learning <ref type="bibr" target="#b13">(Lazaridou et al., 2016;</ref><ref type="bibr" target="#b17">Mordatch and Abbeel, 2017)</ref> and starcraft games <ref type="bibr" target="#b29">(Vinyals et al., 2017)</ref>, we present the first study in using this framework for long text generation. Finally, our model is related to prior works that address repetitions in generating long text. <ref type="bibr" target="#b25">See et al. (2017)</ref> introduce a post-trained coverage network to penalize repeated attentions over the same regions in the input, while <ref type="bibr" target="#b20">Paulus et al. (2018)</ref> use intra-decoder attention to punish generating the same words. In contrast, we propose a new semantic coherence loss and intermediate sentencebased rewards for reinforcement learning to discourage semantically similar generations ( §3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We investigated the problem of encoding long text to generate abstractive summaries and demonstrated that the use of deep communicating agents can improve summarization by both automatic and manual evaluation. Analysis demonstrates that this improvement is due to the improved ability of covering all and only salient concepts and maintaining semantic coherence in summaries.   <ref type="bibr" target="#b18">(Nallapati et al., 2017;</ref><ref type="bibr" target="#b9">Hermann et al., 2015)</ref> is a collection of online news articles along with multisentence summaries. We use the same data splits as in <ref type="bibr" target="#b18">Nallapati et al. (2017)</ref>. While earlier work anonymized entities by replacing each named entity with a unique identifier (e.g., Dominican Republic→entity15), we opted for non-anonymized version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head><p>New York Times (NYT): Although this dataset has mainly been used to train extractive summarization systems <ref type="bibr" target="#b11">(Hong and Nenkova, 2014;</ref><ref type="bibr" target="#b10">Hong et al., 2015;</ref><ref type="bibr" target="#b14">Li et al., 2016;</ref><ref type="bibr" target="#b5">Durrett et al., 2016)</ref>, it has recently been used for the abstractive summarization task <ref type="bibr" target="#b20">(Paulus et al., 2018)</ref>. NYT dataset <ref type="bibr" target="#b24">(Sandhaus, 2008</ref>) is a collection of articles published between 1996 and 2007. We use the scripts provided in <ref type="bibr" target="#b14">Li et al. (2016)</ref> to extract and preprocess the NYT dataset with some modifications in order to replicate the pre-processing steps presented in <ref type="bibr" target="#b20">Paulus et al. (2018)</ref>. Similar to <ref type="bibr" target="#b20">(Paulus et al., 2018)</ref>, we sorted the documents by their publication date in chronological order and used the first 90% for training, the next 5% for validation and last 5% for testing. They also use pointer supervision by replacing all named entities in the abstract if the type is "PERSON", "LOCA-TION", "ORGANIZATION" or "MISC" using the Stanford named entity recognizer . By contrast, we did not anonymize the NYT dataset to reduce pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Details</head><p>We train our models on an NVIDIA P100 GPU machine. We set the hidden state size of the encoders and decoders to 128. For both datasets, we limit the input and output vocabulary size to the 50,000 most frequent tokens in the training set. We initialize word embeddings with 200-d GloVe vectors <ref type="bibr" target="#b21">(Pennington et al., 2014)</ref> and finetune them during training. We train using Adam with a learning rate of 0.001 for the MLE models and 10 −5 for the MLE+RL models. We tune the gamma hyper-parameter in the mixed loss by iterating γ={0.95, 0.97, 0.99}. In almost all DCA models, the 0.97 value yielded the best gains. We train our models for 200,000 iterations. which took 4-5 days for 2-3 agents and 5-6 days for 5 agents since it has more encoder parameters to tune.</p><p>To avoid repetition, we prevent the decoder from generating the same trigram more than once during test, <ref type="bibr">following Paulus et.al. (2018)</ref>. In addition, for every predicted out-of-vocabulary token (UNK), we replace it with its most likely origin by choosing the source word w with the largest cascaded attention w := arg max a,i l t a,i * g t a (Eq. <ref type="formula" target="#formula_5">(8)</ref>, <ref type="formula">(10)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Generated Summary Examples</head><p>This appendix provides example documents from the test set, with side-by-side comparisons of the human generated (golden) summaries and the summaries produced by our models. Baseline is a single-agent model trained with MLE+RL loss, (m3) model in <ref type="table" target="#tab_1">Table 1</ref>, while our best multi-agent model is optimized by mixed MLE+SEM+RL loss, the (m7) model in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>• red highlights : indicate details that should not appear in the summary but the models generated them.</p><p>• red : indicates factual errors in the summary.</p><p>• green highlights : indicate key facts in the human (gold) summary that only one of the models manage to capture.</p><p>Document model abbey clancy is helping to target breast cancer , by striking a sultry pose in a new charity campaign . the winner of 2013 's strictly come dancing joins singer foxes , 25 , victoria 's secret angel lily donaldson , 28 , and model alice dellal , 27 , in the new series of pictures by photographer simon emmett for fashion targets breast cancer . clancy , 29 , looks chic as she shows off her famous legs , wearing just a plain white shirt . abbey clancy leads the glamour as she joins forces with her famous friends to target breast cancer , by striking a sultry pose in a new charity campaign the model , who is mother to four -year -old daughter sophia with footballer husband peter crouch , said: ' as a mum , it makes me proud to be part of a campaign that funds vital work towards ensuring the next generation of young women do not have be afraid of a diagnosis of breast cancer . ' i'm wearing my support , and i want everyone across the uk to do the same and get behind this campaign . ' holding onto heaven singer foxes looks foxy in cropped stripy top and jeans . abbey says she is proud to be part of a campaign that funds vital work towards ensuring the next generation of young women do not have be afraid of a diagnosis of breast cancer victoria 's secret angel lily donaldson , who has been in the industry for years , also adds some glamour to the charity campaign holding onto heaven singer foxes dons a stripy top and jeans for the campaign she says she 's ' honoured ' to be a part of she said: ' i'm so honoured to be taking part in this year 's fashion targets breast cancer , and becoming part of the campaign 's awesome heritage . ' fashion is a huge part of my life , and if by taking part i can inspire women to wear their support , join the fight and take on breast cancer head on , then that will be something to be really proud of . ' now in its 19th year , the campaign has so far raised 13 . 5m for breakthrough breast cancer 's research funding . this year the range of clothes and accessories have been produced in conjunction with high street partners m&amp;s , river island , warehouse , topshop , laura ashley , debenhams , superga , baukjen and the cambridge satchel company . they can be viewed online at www . fashiontargetsbreastcancer . org . uk/lookbook the campaign , which also stars alice dellal , has so far raised 13 . 5m for breakthrough breast cancer 's research funding Human (Gold) models abbey and lily are joined by alice dellal and singer foxes . the women are pictured ' wearing ' their support . abbey , 29 , says she is proud to be part of a campaign that funds vital work . campaign has raised 13 . 5m for breakthrough breast cancer 's research .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Agent Baseline</head><p>strictly come dancing joins singer foxes , 25 , victoria 's secret angel lily donaldson , 28 , and model alice dellal , 27 , in the new series of pictures by photographer simon emmett for fashion targets breast cancer . clancy , 29 , looks chic as she shows off her famous legs , wearing just a plain white shirt .</p><p>Multi Agent abbey says she is proud to be part of a campaign that funds vital work towards ensuring the next generation of young women do not have been afraid of a diagnosis of breast cancer . the campaign has raised 13 . 5m for breakthrough breast cancer 's research . <ref type="table">Table 7</ref>: In this example both single-and multi-agent models demonstrate extractive behaviors. However, each select sentences from different sections of the document. While the single model extracts the second and the third sentences, the multi-agent model successfully selects salient sentences from sentences that are further down in the document, specifically sentence 8 and 10. This can be attributed to the fact that agents can successfully encode salient aspects distributed in distant sections of the document. An interesting result is that even though the multiagent model shows extractive behaviour in this example, it successfully selects the most salient sentences while the single agent model includes superfluous details. Document michelle pfeiffer is the latest hollywood star preparing to hit the small screen . the oscar nominated star known for her roles in iconic films such as scarface , dangerous liaisons andthe age of innocence , has teamed up with katie couric to pitch a new television comedy about a morning news program . also involved in the project , in which pfeiffer is attached to star , is diane english , the creator of murphy brown . scroll down for video michelle pfeiffer ( left ) is set to star in a new television comedy about a morning news program produced by katie couric ( right ) the series was created by diane english ( above with candice bergen ) , who was behind the show murphy brown , about a female news anchor according to variety , pfeiffer 's role will be that of a morning news anchor , making it very similar to the real life role couric played as co -host of today for 15 years . couric will serve as an executive producer and help ' ensure the series strikes realistic notes . ' the creator behind the project , english , was previously the brains behind brown , the show starring candice bergen that centered around a female news anchor and ran for ten seasons , winning 18 emmys . english would also serve as a writer , producer and showrunner on the program. the ladies are currently in talks with hbo , showtime , amc , netflix and amazon to pick up the program . couric will serve as an executive producer , drawing on her experience as an anchor on today for 15 years pfeiffer would be the one of the biggest stars yet to move to television , joining a group that now includes house of cards stars robin wright and kevin spacey , true detective leads matthew mcconaughey and woody harrelson , and even lady gaga , who recently announced she would be appearing on the next season of american horror story . the actress has kept a low profile for the past 20 years since becoming a mother , only doing a handful of films in that time . she most recently appeared alongside robert de niro in the mob comedy ' the family . ' <ref type="bibr">(Gold)</ref> michelle pfeiffer is set to star in a new television comedy about a morning news program . katie couric will serve as an executive producer , drawing on her experience as an anchor on today for 15 years . the series was created by diane english , who was behind the show murphy brown , about a female news anchor . the ladies are currently in talks with hbo , showtime , amc , netflix and amazon to pick up the program .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Agent Baseline</head><p>the oscar nominated star known for her roles in iconic filmssuch as scarface , dangerous liaisons and the age of innocence , has teamed up with katie couric to pitch a new television comedy about a morning news program . also involved in the project , in which pfeiffer is attached to star , is diane english , the creator of murphy brown .</p><p>Multi Agent michelle pfeiffer is set to star in a new tv comedy about a morning news program . couric will serve as an executive producer and showrunner on the project . the series was created by diane english , the creator of murphy brown . pfeiffer is the one of the biggest stars . <ref type="table">Table 8</ref>: The baseline model generates non-coherent summary that references the main character "Michelle Pfeiffer" in an ambiguous way towards the end of the generated summary. In contrast, the multi-agent model successfully captures the main character including the key facts. One interesting feature that the multi-agent model showcases is its simplification property, which accounts for its strength in abstraction. Specifically, it simplified the bold long sentence in the document starting with "couric will... and only generated the salient words.</p><p>Document everton manager roberto martinez was forced to defend another penalty fiasco at the club after ross barkley missed from the spot in their 1 -0 win against burnley at goodison park . the untried barkley inexplicably took the 10th minute kick awarded for a foul by david jones on aaron lennon rather than leighton baines , who has scored 15 penalties from 16 attempts in the premier league . although there was no dispute between the team -mates this time , it brought back memories of everton 's match against west brom in january when kevin mirallas grabbed the ball from baines to take a penalty -and missed . ross barkley steps up to take a 10th minute penalty despite the presence of leighton baines on the pitch barkley 's effort is saved byburnley goalkeeper tom heaton at goodison park martinez insisted barkley was within his rights to request penalty -taking duties on saturday . ' if romelu lukaku had been on the pitch , he would have taken it . otherwise , i am happy to have three or four players who can take penalties and let it depend on how they feel at that moment , ' argued the everton manager . baines ( left )has scored 15 penalties from 16 attempts in the premier league ' ross showed incredible responsibility to take it . i love seeing players take control of the big moments and leighton was happy to given him that responsibility . ' barkley 's penalty was well -struck but wasn't put in the corner and burnley goalkeeper tom heaton dived to his right to save . fortunately for the young england player , it didn't prove costly as mirallas went on to score the only goal of the game after 29 minutes . everton boss roberto martinez issues instructions to his players during a break in play against burnley Human (Gold) everton defeated burnley 1 -0 at goodison park on saturday . kevin mirallas scored the only goal of the game in the 29th minute . ross barkley had earlier missed a 10th -minute penalty . leighton baines has scored 15 penalties from 16 attempts this season .</p><p>Single Agent Baseline everton manager roberto martinez was forced to defend another penalty fiasco at the club after ross barkley missed from the spot in their 1 -0 win against burnley at goodison park . the untried barkley inexplicably took the 10th minute kick awarded for a foul by david jones on aaron lennon rather than leighton baines , who has scored 15 penalties from 16 attempts in the premier league .</p><p>Multi Agent everton beat burnley 1 -0 at goodison park in the premier league . ross barkley steps up to take a 10th minute penalty but missed it . barkley has scored 15 penalties from 16 attempts in the pitch . <ref type="table">Table 9</ref>: The single agent model generates summary with superfluous details and the facts are not clearly expressed.</p><p>Although it was able to capture the statistics of the player correctly (e.g., 15 penalties, 16 attempts), it still missed the player who scored the only goal in the game (i.e., kevin mirallas). On the other hand multi-agent model was able to generate a concise summary with several key facts. However, similar to single agent model, it missed to capture the player who scored the only goal in the game. Interestingly, the document contains the word "defeated' but the multi-agent model chose to use beat instead, which does not exist in the original document.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of deep communicating agents presented in this paper. Each agent a and b encodes one paragraph in multiple layers. By passing new messages through multiple layers the agents are able to coordinate and focus on the important aspects of the input text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Multi-agent-encoder-decoder overview. Each agent a encodes a paragraph using a local encoder followed by multiple contextual layers with agent communication through concentrated messages z (k) a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Multi-agent encoder message passing. Agents b and c transmit the last hidden state output (I) of the current layer k as a message, which are passed through an average pool (Eq. (6)). The receiving agent a uses the new message z (k)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The average ROUGE-L scores for summaries that are binned by each agent's average attention when generating the summary (see Section 5.2). When the agents contribute equally to the summary, the ROUGE-L score increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and λ = 0.1 for the SEM term in MLE and MIXED training. Additional details are provided in Appendix A.2.</figDesc><table><row><cell>Evaluation We evaluate our system using</cell></row><row><cell>ROUGE-1 (unigram recall), ROUGE-2 (bigram</cell></row><row><cell>recall) and ROUGE-L (longest common se-</cell></row><row><cell>quence). 1 We select the MLE models with the</cell></row><row><cell>lowest negative log-likelihood and the MLE+RL</cell></row><row><cell>models with the highest ROUGE-L scores on a</cell></row><row><cell>sample of validation data to evaluate on the test</cell></row></table><note>1 We use pyrouge (pypi.python.org/pypi/pyrouge/0.1.3).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models are bolded.</figDesc><table><row><cell>.87</cell><cell>15.82</cell><cell>36.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models are bolded.set. At test time, we use beam search of width 5 on all our models to generate final predictions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of multi-agent models varying the number of agents using ROUGE results of model (m7) from Table 1 on CNN/Daily Maily Dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="6">: Comparison of a human summary to best single-and multi-agent model summaries, (m3) and (m7)</cell></row><row><cell cols="6">from CNN/DailyMail dataset. Although single-agent model generates a coherent summary, it is less focused and</cell></row><row><cell cols="6">contains more unnecessary details ( highlighed red ) and misses keys facts that the multi-agent model successfully</cell></row><row><cell>captures (bolded).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Head-to-Head</cell><cell cols="2">Score Based</cell></row><row><cell>Criteria</cell><cell cols="2">SA MA</cell><cell>=</cell><cell>SA</cell><cell>MA</cell></row><row><cell>non-redundancy</cell><cell>68</cell><cell cols="3">159 73 4.384</cell><cell>4.428</cell></row><row><cell>coherence</cell><cell>89</cell><cell cols="3">173 38 3.686</cell><cell>3.754</cell></row><row><cell>focus</cell><cell>83</cell><cell cols="4">181 36 3.694 3.884  *</cell></row><row><cell>overall</cell><cell cols="5">102 158 40 3.558 3.682  *</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Head-to-Head and score-based comparison of human evaluations on random subset of CNN/DM dataset. SA=single, MA=multi-agent.* indicates sta- tistical significance at p &lt; 0.001 for focus and p &lt; 0.03 for the overall.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Summary statistics of CNN/DailyMail (DM)</cell></row><row><cell>and New York Times (NYT) Datasets.</cell></row><row><cell>A.1 Datasets</cell></row><row><cell>CNN/DailyMail: CNN/DailyMail dataset</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported in part by NSF (IIS-1524371), and DARPA under the CwC program through the ARO (W911NF-15-1-0543).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural sentiment classification with user and product attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning cooperative visual dialog agents with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv Batra Stefan</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning-based single-document summarization with compression and anaphoricity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05217v1</idno>
		<title level="m">Controllable abstractive summarization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Counterfactual multi-agent policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08926</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to communicate with deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">System combination for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving the estimation of word importance for news multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In Extended technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Miguel</forename><surname>Hernandez-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Towards multi-agent communicationbased language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07133</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The role of discourse units in near-extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessy</forename><surname>Junyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Thandani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialog</title>
		<meeting>the 17th Annual Meeting of the Special Interest Group on Discourse and Dialog</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Emergence of grounded compositional language in multi-agent populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04908</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reinforced video captioning with entailment rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00563</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">New york times annotated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gettothepoint: Summarization with pointergeneratornetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigale</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with back-propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Abstractive document summarization with a graphbased attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In ACL</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Starcraft ii: A new challenge for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1708.04782</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efcient summarization with readagain and copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03382</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiview response selection for human-computer conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Yu R Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
