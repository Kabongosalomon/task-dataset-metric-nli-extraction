<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HyperML: A Boosting Metric Learning Approach in Hyperbolic Space for Recommender Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 3-7, 2020. February 3-7, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><forename type="middle">Vinh</forename><surname>Tran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
							<email>shuai.zhang@student.unsw.edu.au</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
							<email>gaocong@ntu.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
							<email>xlli@i2r.a-star.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><forename type="middle">Vinh</forename><surname>Tran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><forename type="middle">Li</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">A*STAR</orgName>
								<orgName type="institution">Nanyang Technological University Institute for Infocomm Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of New</orgName>
								<address>
									<country>South Wales</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institute for Infocomm Research, A*STAR</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HyperML: A Boosting Metric Learning Approach in Hyperbolic Space for Recommender Systems</title>
					</analytic>
					<monogr>
						<title level="m">HyperML: A Boosting Metric Learning Approach in Hyperbolic Space for Recommender Systems</title>
						<meeting> <address><addrLine>Houston, TX; Houston, TX, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">February 3-7, 2020. February 3-7, 2020</date>
						</imprint>
					</monogr>
					<note>ACM ISBN 978-1-4503-6822-3/20/02. . . $15.00 USA. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommender Systems</term>
					<term>Collaborative Filtering</term>
					<term>Hyperbolic Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates the notion of learning user and item representations in non-Euclidean space. Specifically, we study the connection between metric learning in hyperbolic space and collaborative filtering by exploring Möbius gyrovector spaces where the formalism of the spaces could be utilized to generalize the most common Euclidean vector operations. Overall, this work aims to bridge the gap between Euclidean and hyperbolic geometry in recommender systems through metric learning approach. We propose HyperML (Hyperbolic Metric Learning), a conceptually simple but highly effective model for boosting the performance. Via a series of extensive experiments, we show that our proposed HyperML not only outperforms their Euclidean counterparts, but also achieves state-of-the-art performance on multiple benchmark datasets, demonstrating the effectiveness of personalized recommendation in hyperbolic geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Recommender systems; • Computing methodologies → Neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A diverse plethora of machine learning models solves the personalized ranking problem in recommender systems via building matching functions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Across the literature, a variety of matching functions have been traditionally adopted, such as inner product <ref type="bibr" target="#b30">[31]</ref>, metric learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref> and/or neural networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Among those approaches, metric learning models (e.g., Collaborative Metric Learning (CML) <ref type="bibr" target="#b18">[19]</ref> and Latent Relational Metric Learning (LRML) <ref type="bibr" target="#b32">[33]</ref>) are primarily focused on designing distance functions over objects (i.e., between users and items), demonstrating reasonable empirical success for collaborative ranking with implicit feedback. Nevertheless, those matching functions only covered the scope of Euclidean space.</p><p>For the first time, our work explores the notion of learning useritem representations in terms of metric learning in hyperbolic space, in which hyperbolic representation learning has recently demonstrated great potential across a diverse range of applications such as learning entity hierarchies <ref type="bibr" target="#b26">[27]</ref> and/or natural language processing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34]</ref>. Due to the exponentially expansion property of hyperbolic space, we discovered that metric learning with the pull-push mechanism in hyperbolic space could boost the performance significantly: moving a point to a certain distance will require a much smaller force in hyperbolic space than in Euclidean space. To this end, in order to perform metric learning in hyperbolic space, we employ Möbius gyrovector spaces to generally formalize most common Euclidean operations such as addition, multiplication, exponential map and logarithmic map <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Moreover, the ultimate goal when embedding a space into another is to preserve distances and more complex relationships. Thus, our work also introduces the definition of distortion to maintain good representations in hyperbolic space both locally and globally, while controlling the performance through the multi-task learning framework. This reinforces the key idea of modeling user-item pairs in hyperbolic space, while maintaining the simplicity and effectiveness of the metric learning paradigm.</p><p>We show that a conceptually simple hyperbolic adaptation in terms of metric learning is capable of not only achieving very competitive results, but also outperforming recent advanced Euclidean metric learning models on multiple personalized ranking benchmarks.</p><p>Our Contributions. The key contributions of our work are summarized as follows:</p><p>• We investigate the notion of training recommender systems in hyperbolic space as opposed to Euclidean space by exploring Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model. To the best of our knowledge, this is the first work that explores the use of hyperbolic space for metric learning in the recommender systems domain. • We devise a new method HyperML (Hyperbolic Metric Learning), a strong competitive metric learning model for one-class collaborative filtering (i.e., personalized ranking). Unlike previous metric learning models, we incorporate a penalty term called distortion to control and balance between accuracy and preservation of distances. • We conduct a series of extensive experiments delving into the inner workings of our proposed HyperML on ten public benchmark datasets. Our model demonstrates the effectiveness of hyperbolic geometry, outperforming not only its Euclidean counterparts but also a suite of competitive baselines. Notably, HyperML outperforms the state-of-the-art CML and LRML models, which are also metric learning models in Euclidean space across all benchmarks. We achieve a boosting performance gain over competitors, pulling ahead by up to 32.32% performance in terms of standard ranking metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">HYPERBOLIC METRIC LEARNING</head><p>This section provides the overall background and outlines the formulation of our proposed model. The key motivation behind our proposed model is to embed the two user-item pairs into hyperbolic space, creating the gradients of pulling the distance between the positive user-item pair close and pushing the negative user-item pair away. <ref type="figure" target="#fig_0">Figure 1</ref> depicts our overall proposed HyperML model. The figures illustrate our two approaches: 1) optimizing the embeddings within the unit ball and 2) transferring the points to the tangent space via the exponential and logarithmic maps for optimization.</p><p>In the experiments, we also compare the mentioned variants of HyperML where both approaches achieve competitive results compared to Euclidean metric learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hyperbolic Geometry &amp; Poincaré Embeddings</head><p>The hyperbolic space D is uniquely defined as a complete and simply connected Riemannian manifold with constant negative curvature <ref type="bibr" target="#b23">[24]</ref>. In fact, there are three types of the Riemannian manifolds of constant curvature, which are Euclidean geometry (constant vanishing sectional curvature), spherical geometry (constant positive sectional curvature) and hyperbolic geometry (constant negative sectional curvature). In this paper, we focus on Euclidean space and hyperbolic space due to the key difference in their space expansion. Indeed, hyperbolic spaces expand faster (exponentially) than Euclidean spaces (polynomially). Specifically, for instance, in the two-dimensional hyperbolic space D 2 ϵ of constant curvature K = −ϵ 2 &lt; 0, ϵ &gt; 0 with the hyperbolic radius of r , we have:</p><formula xml:id="formula_0">L(r ) = 2π sinh(ϵr ),<label>(1)</label></formula><formula xml:id="formula_1">A(r ) = 2π (cosh(ϵr ) − 1),<label>(2)</label></formula><p>in which L(r ) is the length of the circle and A(r ) is the area of the disk. Hence, both equations illustrate the exponentially expansion of the hyperbolic space H 2 ϵ with respect to the radius r . Although hyperbolic space cannot be isometrically embedded into Euclidean space, there exists multiple models of hyperbolic geometry that can be formulated as a subset of Euclidean space and are very insightful to work with, depending on different tasks. In this work, we prefer the Poincaré ball model due to its conformality (i.e., angles are preserved between hyperbolic and Euclidean space) and convenient parameterization <ref type="bibr" target="#b26">[27]</ref>.</p><p>The Poincaré ball model is the Riemannian manifold P n = (D n , д p ), in which D n = {x ∈ R n : ∥x∥ &lt; 1} is the open ndimensional unit ball that is equipped with the metric:</p><formula xml:id="formula_2">д p (x) = 2 1 − ∥x∥ 2 2 д e ,<label>(3)</label></formula><p>where x ∈ D n ; ∥ · ∥ denotes the Euclidean norm; and д e is the Euclidean metric tensor with components I n of R n . The induced distance between two points on D n is given by:</p><formula xml:id="formula_3">d D (x, y) = cosh −1 1 + 2 ∥x − y∥ 2 (1 − ∥x∥ 2 )(1 − ∥y∥ 2 ) .<label>(4)</label></formula><p>In fact, if we adopt the hyperbolic distance function as a matching function to model the relationships between users and items, the hyperbolic distance d D (u, v) between user u and item v could be calculated based on Eqn. (4).</p><p>On a side note, let v j and v k represent the items user i liked and did not like with d D (u i , v j ) and d D (u i , v k ) are their distances to the user i on hyperbolic space, respectively. Our goal is to pull v j close to u i while pushing v k away from u i . If we consider the triplet as a tree with two children v j , v k of parent u i and place u i relatively close to the origin, the graph distance of v j and v k is obviously</p><formula xml:id="formula_4">calculated as d(v j , v k ) = d(u i , v j ) + d(u i , v k ), or we will obtain the ratio d (v j ,v k ) d (u i ,v j )+d (u i ,v k ) = 1. If we embed the triplet in Euclidean space, the ratio d E (v j ,v k ) d E (u i ,v j )+d E (u i ,v k )</formula><p>is constant, which seems not to capture the mentioned graph-like structure. However, in hyperbolic space, the ratio</p><formula xml:id="formula_5">d D (v j ,v k ) d D (u i ,v j )+d D (u i ,v k</formula><p>approaches 1 as the edges are long enough, which makes the distances nearly preserved <ref type="bibr" target="#b31">[32]</ref>.</p><p>Thus, it is worth mentioning our key idea is that for a given triplet, we aim to embed the root (the user) arbitrarily close to the origin and space the children (positive and negative items) around a sphere centered at the parent. Notably, the distances between points grows exponentially as the norm of the vectors approaches 1. Geometrically, if we place the root node of a tree at the origin of D n , the children nodes spread out exponentially with their distances to the root towards the boundary of the ball due to the above mentioned property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gyrovector spaces</head><p>In this section, we make use of Möbius gyrovector spaces operations <ref type="bibr" target="#b10">[11]</ref> to generally design the distance of user-item pairs for further extension.</p><p>Specifically, for c ≥ 0, we denote D n c = {x ∈ R n : c ∥x ∥ 2 &lt; 1}, which is considered as the open ball of radius 1 √ c . Note that if c = 0, we get D n c = R n ; and if c = 1, we retrieve the usual unit ball as D n c = D n . Some widely used Möbius operations of gyrovector spaces are introduced as follows:</p><p>Möbius addition: The Möbius addition of x and y in D n c is defined:</p><formula xml:id="formula_6">x ⊕ c y = (1 + 2c ⟨x, y⟩ + c ∥y∥ 2 )x + (1 − c ∥x ∥ 2 )y 1 + 2c ⟨x, y⟩ + c ∥x ∥ 2 ∥y∥ 2 .<label>(5)</label></formula><p>Möbius scalar multiplication: For c &gt; 0, the Möbius scalar multi-</p><formula xml:id="formula_7">plication of x ∈ D n c \ {0} with r ∈ R is defined: r ⊗ c x = 1 √ c tanh(r tanh −1 ( √ c ∥x ∥)) x ∥x ∥ ,<label>(6)</label></formula><p>and r ⊗ c 0 = 0. Note that when c → 0, we recover the Euclidean addition and scalar multiplication. The Möbius subtraction can also be obtained as x ⊖ c y = x ⊕ c (−y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Möbius exponential and logarithmic maps:</head><p>For any x ∈ D n c , the Möbius exponential map and logarithmic map, given v 0 and y x, are defined:</p><formula xml:id="formula_8">exp c x (v) = x ⊕ c tanh √ c λ c x ∥v ∥ 2 v √ c ∥v ∥ ,<label>(7)</label></formula><p>log c</p><formula xml:id="formula_9">x (y) = 2 λ c x √ c tanh −1 ( √ c ∥(−x) ⊕ c y∥) (−x) ⊕ c y ∥(−x) ⊕ c y ∥ ,<label>(8)</label></formula><p>where λ c x = 2 1−c ∥x ∥ 2 is the conformal factor of (D n c , д c ) in which д c is the generalized hyperbolic metric tensor. We also recover the Euclidean exponential map and logarithmic map as c → 0. Readers can refer to <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38]</ref> for the detailed introduction to Gyrovector spaces.</p><p>We then obtain the generalized distance in Gyrovector spaces:</p><formula xml:id="formula_10">d c (x, y) = 2 √ c tanh −1 ( √ c ∥(−x) ⊕ c y ∥).<label>(9)</label></formula><p>When c → 0, we recover the Euclidean distance since we have lim c→0 d c (x, y) = 2∥x −y∥. When c = 1, we retrieve the Eqn. <ref type="bibr" target="#b3">(4)</ref>. In other words, hyperbolic space resembles Euclidean as it gets closer to the origin, which motivates us to design our loss function in the multi-task learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Formulation</head><p>Our proposed model takes a user (denoted as u i ), a positive (observed) item (denoted as v j ) and a negative (unobserved) item (denoted as v k ) as an input. Each user and item is represented as a one-hot vector which map onto a dense low-dimensional vector by indexing onto an user/item embedding matrix. We learn these vectors with the generalized distance:</p><formula xml:id="formula_11">d c (u, v) = 2 √ c tanh −1 ( √ c ∥(−u) ⊕ c v∥),<label>(10)</label></formula><p>in which an item j that user i liked (positive) is expected to be closer to the user than the ones he did not like (negative). In fact, we would like to learn the user-item joint metric to encode the observed positive feedback. Specifically, the learned metric pulls the positive pairs closer and pushes the other pairs further apart.</p><p>Notably, this process will also cluster the users who like the same items together, and the items that are liked by the same users together, due to the triangle inequalities. Similar to <ref type="bibr" target="#b18">[19]</ref>, for a given user, the nearest neighborhood items are: 1) the items liked by this user previously, and 2) the items liked by other users with similar interests to this user. In other words, we are also able to indirectly observe the relationships between user-user pairs and item-item pairs through the pull-push mechanism of metric learning.</p><p>Pull-and-Push Optimization. To formulate such constraint, we define our pull-and-push loss function as:</p><formula xml:id="formula_12">L P = (i, j)∈S (i,k ) S [m + d 2 D (i, j) − d 2 D (i, k)] + ,<label>(11)</label></formula><p>where j is an item user i liked and k is the one he did not like; S contains all the observed implicit feedback, i.e. positive item-user pairs;</p><p>[z] + = max(0, z) is the standard hinge loss; and m &gt; 0 is the safety margin size. Notably, our loss function does not adopt the ranking loss weight compared to <ref type="bibr" target="#b18">[19]</ref>.</p><p>Distortion Optimization. The ultimate goal when embedding a space into another is to preserve distances while maintaining complex structures/relationships <ref type="bibr" target="#b31">[32]</ref>. Thus, it becomes a challenge when embedding user-item pairs to hyperbolic space with the needs of preserving good structure quality for metric learning. To this end, we consider the two factors of good representations namely local and global factor. Locally, the children items must be spread out on the sphere around the parent user as described, with pull and push forces created by the gradients. Globally, the learned triplets should be separated reasonably from each other. While pull-andpush optimization satisfies the local requirement, we define the distortion optimization function to meet the global requirement as:</p><formula xml:id="formula_13">L D = (i, j)∈S |d D (f (i), f (j)) − d E (i, j)| d E (i, j) + + (i,k ) S |d D (f (i), f (k)) − d E (i, k)| d E (i, k) + ,<label>(12)</label></formula><p>where | · | defines the absolute value; and f (·) is a mapping function f : E → D from Euclidean space E to hyperbolic space D. In this paper, we take f (·) as an identity function. We aim to preserve the distances by minimizing L D for the global factor. Ideally, the lower the distortion, the better the preservation.</p><p>Multi-Task Learning. We then integrate the pull-and-push part (i.e., L P ) and the distortion part (i.e., L D ) into an end-to-end fashion through a multi-task learning framework. The objective function is defined as: min</p><formula xml:id="formula_14">Θ L = L P + γ L D ,<label>(13)</label></formula><p>where Θ is the total parameter space, including all embeddings and variables of the networks; and γ is the multi-task learning weight.  There is an unavoidable trade-off between the precision (learned from the pull-push mechanism) and the distortion as similar to <ref type="bibr" target="#b31">[32]</ref>. Thus, jointly training L P and L D can help to boost the model performance while providing good representations. Indeed, we examine the performance of HyperML with and without the distortion by varying different multi-task learning weight γ in our experiment in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient Conversion.</head><p>The parameters of our model are learned by projected Riemannian stochastic gradient descent (RSGD) <ref type="bibr" target="#b26">[27]</ref> with the form:</p><formula xml:id="formula_15">θ t +1 = R θ t (−η t ∇ R L(θ t )),<label>(14)</label></formula><p>where R θ t denotes a retraction onto D at θ and η t is the learning rate at time t. The Riemannian gradient ∇ R is then calculated from the Euclidean gradient by rescaling ∇ E with the inverse of the Poincaré ball metric tensor as</p><formula xml:id="formula_16">∇ R = (1− ∥θ t ∥ 2 ) 2 4</formula><p>∇ E , in which this scaling factor depends on the Euclidean distance of the point at time t from the origin <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34]</ref>. Notably, one could also exploit full RSGD for optimization to perform the updates instead of using first-order approximation to the exponential map [1, 2, 10, 41].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS 3.1 Experimental Setup</head><p>Datasets. To evaluate our experiments, we use a wide spectrum of datasets with diverse domains and densities. The statistics of the datasets are reported in <ref type="table">Table 1</ref>.</p><p>• MovieLens: A widely adopted benchmark dataset in the application domain of recommending movies to users provided by GroupLens research 1 . We use two configurations, namely MovieLens20M and MovieLens1M. Similar to <ref type="bibr" target="#b32">[33]</ref>, the MovieLens20M datasets are filtered with 100-core setting. • Goodbooks: A large book recommendation dataset contains six million ratings for ten thousand most popular (with most ratings) books. 2 • Yelp: A crowd-sourced platform for local businesses such as restaurants, bars, etc. We use the dataset from the 2018 edition of the Yelp Dataset Challenge. 3 • Meetup: An event-based social network dataset. We use the dataset includes event-user pairs from NYC that was provided by <ref type="bibr" target="#b28">[29]</ref>. • Amazon Reviews: The amazon review datasets that was introduced in <ref type="bibr" target="#b13">[14]</ref>. The subsets 4 are selected based on promoting diversity based on dataset size and domain.</p><p>Evaluation Protocol and Metrics. We experiment on the one-class collaborative filtering setup. We adopt nDCG@10 (normalized discounted cumulative gain) and HR@10 (Hit Ratio) evaluation metrics, which are well-established ranking metrics for recommendation task. Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref>, we randomly select 100 negative samples which the user have not interacted with and rank the ground truth amongst these negative samples. For all datasets, the last item the user has interacted with is withheld as the test set while the penultimate serves as the validation set. During training, we report the test scores of the model based on the best validation scores. All models are evaluated on the validation set at every 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MovieLens20M</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MovieLens1M</head><p>Goodbooks Yelp</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meetup</head><p>Clothing, Shoes, and Jewelry Sports and Outdoors Cell phones and Accessories</p><p>Toys and Games Automotive <ref type="figure">Figure 2</ref>: Two-dimensional hyperbolic embedding of ten benchmark datasets in the Poincaré disk using t-SNE. The images illustrate the embedding of user and item pairs after the convergence (Best viewed in color).</p><p>Compared Baselines. In our experiments, we compare with five well-established and competitive baselines which in turn employ different matching functions: inner product (MF-BPR), neural networks (MLP, NCF) and metric learning (CML, LRML).</p><p>• Matrix Factorization with Bayesian Personalized Ranking (MF-BPR) <ref type="bibr" target="#b30">[31]</ref> is the standard and strong collaborative filtering (CF) baseline for recommender systems. It models the user-item representation using the inner product and explores the triplet objective to rank items. • Multi-layered Perceptron (MLP) is a feedforward neural network that applies multiple layers of nonlinearities to capture the relationship between users and items. We select the best number of MLP layers from {3, 4, 5}. • Neural Collaborative Filtering (NCF) <ref type="bibr" target="#b16">[17]</ref> is a neural network based method for collaborative filtering which models nonlinear user-item interaction. The key idea of NCF is to fuse the last hidden representation of MF and MLP into a joint model. Following <ref type="bibr" target="#b16">[17]</ref>, we use a three layered MLP with a pyramid structure.</p><p>• Collaborative Metric Learning (CML) <ref type="bibr" target="#b18">[19]</ref> is a strong metric learning baseline that learns user-item similarity using the Euclidean distance. CML can be considered a key ablative baseline in our experiments, signifying the difference between Hyperbolic and Euclidean metric spaces. • Latent Relational Metric Learning (LRML) <ref type="bibr" target="#b32">[33]</ref> is also a strong metric learning baseline that learns adaptive relation vectors between user and item interactions to find a single optimal translation vector between each user-item pair.</p><p>Implementation Details. We implement all models in Tensorflow. All models are trained with the Adam <ref type="bibr" target="#b20">[21]</ref> or AdaGrad <ref type="bibr" target="#b8">[9]</ref> optimizer with learning rates from {0.01, 0.001, 0.0001, 0.00001}. The embedding size d of all models is tuned amongst {32, 64, 128} and the batch size B is tuned amongst {128, 256, 512}. The multi-task learning weight γ is empirically chosen from {0, 0.1, 0.25, 0.5, 0.75, 1.0}. For models that optimize the hinge loss, the margin λ is selected from {0.1, 0.2, 0.5}. For NCF, we use a pre-trained model as reported in <ref type="bibr" target="#b16">[17]</ref> to achieve its best performance. All the embeddings and parameters are randomly initialized using the random   <ref type="bibr">1 3</ref> , in which we choose β = 0.01. The reason is that we would like all the embeddings of the metric learning models to be initialized arbitrarily close to the origin of the balls 5 for a fair comparison. For most datasets and baselines, we empirically set the embedding size of 64 and the batch size is 512. We also empirically set the dropout rate ρ = 0.5 to prevent overfitting. For each dataset, the optimal parameters are established by repeating each experiment for N runs and assessing the average results. We have used N = 5 for our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>This section presents our experimental results on all datasets. For all obtained results, the best result is in boldface whereas the second best is underlined. As reported in <ref type="table" target="#tab_2">Table 2</ref>, our proposed model consistently outperforms all the baselines on both HR@10 and nDCG@10 metrics across all benchmark datasets.</p><p>Pertaining to the baselines, we observe that there is no obvious winner among the baseline solutions. In addition, we also observe that the performance of MF-BPR and CML is extremely competitive, i.e. both MF-BPR and CML consistently achieve good results across the datasets. Notably, the performance of MF-BPR is much better than CML on datasets with less number of interactions. For datasets with larger size (i.e., MovieLens20M, MovieLens1M and Goodbooks), the performance of metric learning models perform better in which the gain of CML and LRML on the non-metric learning baselines across large datasets is approximately +0.39% and +0.91% respectively in terms of nDCG. One possible reason is that for small datasets with low interactions (e.g., Automotive with 26K interactions of 0.49% density), a simple model such as MF-BPR should be considered as a priority choice. In addition, the <ref type="bibr" target="#b4">5</ref> The balls are referred as hyperbolic ball for HyperML model and Euclidean ball for CML and LRML model. performance of a careful pre-trained NCF also often achieves competitive results with large datasets but not small ones in most cases. The explanation is because using the dual embedding spaces (since NCF combines MLP and MF) could possibly lead to overfitting if the dataset is not large enough <ref type="bibr" target="#b32">[33]</ref>.</p><p>Remarkably, our proposed model HyperML demonstrates highly competitive results and consistently outperforms the best baseline method. The percentage improvements in terms of nDCG on ten datasets (in the same order as reported in <ref type="table" target="#tab_2">Table 2</ref>) are +0.42%, +0.83%, +0.52%, +5.49%, +4.39%, +2.03%, +32.32%, +10.71%, +4.55% and +21.06% respectively. We also observe similar high performance gains on the hit ratio (HR@10). Note that the hyperbolic spaces expand faster, i.e. exponentially, than Euclidean spaces, in which the forces are generated by the rescaled gradients, pulling and pushing the points with more reasonable distances compared to Euclidean. Therefore, it enables us to achieve very competitive results of our proposed HyperML in the hyperbolic space over other strong Euclidean baselines. Our experimental evidence shows the remarkable recommendation results of our proposed HyperML model on the variety of datasets and the advantage of hyperbolic space over Euclidean space in boosting the performance in metric learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Convergence Analysis</head><p>This section investigates the model convergence analysis of our proposed model to understand the behavior of the embeddings in hyperbolic space.</p><p>Hyperbolic Convergence. <ref type="figure">Figure 2</ref> visualizes the two-dimensional hyperbolic embedding using t-SNE on the test set of ten benchmark datasets after the convergence. We observe that item embeddings form a sphere over the user embeddings. Moreover, since we conduct the analysis on the test set, the visualization of the user/item embeddings in <ref type="figure">Figure 2</ref> demonstrates the ability of HyperML to self-organize and automatically spread out the item embeddings on  the sphere around user embeddings, as mentioned in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. Moreover, the clustering characteristic of observing the user-user and item-item relationships discussed in Section 2 is also captured in <ref type="figure">Figure 2</ref>. It could be seen especially clearly for the MovieLens and Yelp dataset.</p><p>Convergence Comparison. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the comparison between two-dimensional Poincaré embedding (HyperML) and Euclidean embedding (CML, LRML) on the Yelp and Amazon dataset. For the Euclidean embedding, we clip the norm (i.e., the norm of the embeddings is constrained to 1) and initialize all the embeddings very close to the origin, for an analogous comparison.</p><p>At first glance, we notice the difference between the three types of embedding by observing the distribution of user and item embeddings in the spaces after the convergence. While HyperML and CML have the item embeddings gradually assemble to a sphere structure surround user embeddings, the item embeddings of LRML have the opposite movement. The reason is because the motivation behind both CML and HyperML is to create the learned metric through the pull-push mechanism, whereas the motivation of LRML is to additionally learn the translation vector, which establishes the main cause of different visualizations.</p><p>It is worth mentioning that since we initialize all the embeddings very close to the origin, we observe the difference between hyperbolic and Euclidean space that leads to the difference in the convergence of HyperML and CML. While both models form a sphere shape over the user embeddings equally, we observe that the user embeddings of HyperML tend to be located closer to the origin than CML while we get similar spread out observation of items. The explanation is that even with similar forces created by the gradients in the same direction, the different expansion property of the two spaces produces the distances between the triplets more separable, which leads to the boosting performance of the proposed model.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison of Hyperbolic Variants</head><p>In this section, we study the variants of our proposed model: Hy-perML and HyperTS (applied optimization after mapping the user and item embeddings to the tangent space at 0 using the log 0 map). Notably, HyperTS is viable because the tangent space at the origin of the Poincaré ball resembles Euclidean space. <ref type="table" target="#tab_4">Table 3</ref> represents the performance of the variants on the datasets in terms of nDCG@10 and HR@10. In general, we observe both HyperML and HyperTS achieve highly competitive results, boosting the performance over Euclidean metric learning models across Meetup, Clothing, Sports, and Cell phones datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Effect of Scaling Variable</head><p>In this section, we study the effect of the variable c on our proposed HyperML and the CML baseline model. <ref type="table" target="#tab_5">Table 4</ref> represents the performance of HyperML regarding the different value of the scaling variable c comparing to CML in terms of nDCG@10. We observe that HyperML achieves best performance when c = 0.5 on Goodbooks dataset, but c = 2.0 on Games dataset. For other values of c, we notice the oscillated performance of HyperML. As introduced, for c &gt; 0, our ball shrinks to the radius of 1 √ c .</p><p>Without loss of generality, the case of c &gt; 0 can be reduced to c = 1 (the usual unit ball). However, we observe that different scaling variable c would effect the performance differently in practice, which should be set carefully for each dataset. In fact, with c carries values from 0.5 to 8.0, the percentage gain/loss of HyperML over CML varies from +4.87%/−17.39% to +14.17%/−12.71% on Goodbooks and Games dataset, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Accuracy Trade-off with Different Multi-task Learning Weight</head><p>In this section, we study the effect of different multi-task learning weight γ on our proposed HyperML model on Meetup and Automotive dataset. <ref type="figure" target="#fig_4">Figure 4</ref> represents the performance of HyperML when changing the value of the multi-task learning weight γ in terms of HR@10 and nDCG@10. We observe the obvious boost of   the performance when γ increases from 0 to positive values on both two datasets. While for Meetup dataset, HyperML achieves best performance when γ = 0.75, we observe the performance of Hy-perML achieves its best result on Automotive dataset when γ = 1.0.</p><p>For other values of γ , we also observe the oscillated performance of HyperML due to the trade-off. On a side note, when γ = 0, i.e. removing the distortion, we notice the decreasing performance of HyperML compared to CML by -21.48% and -15.48% in terms of nDCG@10 on Meetup and Automotive dataset, respectively. Thus, we conclude that the multi-task learning weight γ as well as the distortion play important roles on boosting the performance, in which the weight γ causes the trade-off between minimizing the distortion and the model's accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Metric Learning Visualization</head><p>In this section, we study the clustering effect of HyperML. <ref type="figure" target="#fig_5">Figure  5</ref> represents the clustering effect in which the 18 colors represent 18 movie genres from the MovieLens1M dataset <ref type="bibr" target="#b5">6</ref> . From the figure, we empirically discover that despite being only trained on implicit <ref type="bibr" target="#b5">6</ref> The colors were assigned to the movie genres in the same order as reported in http://files.grouplens.org/datasets/movielens/ml-1m-README.txt interactions, explicit rating information is surprisingly being discovered in HyperML. Within the hyperbolic space, the metric learning shows cluster structures of items with same genres induced by users, providing insight and achieving similar effect as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>. The visualization supports our previous claim that the nearest neighborhood items tend to be liked by the same users with similar interests. Notably, the t-SNE visualization also illustrates the sphere structure embeddings as introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Across the rich history of recommender systems research, a myriad of machine learning models have been proposed using matching functions to define similarity scores <ref type="bibr">[17-19, 22, 26, 30, 31, 31]</ref>. Traditionally, many works are mainly focused on factorizing the interaction matrix, combining the user-item embeddings using the inner product as a matching function <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>. On the other hand, many approaches in personalized recommender system based on the distance/similarity metric between two points using Euclidean distance have shown their strong competency in improving the model accuracy in different domains <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>To this end, <ref type="bibr" target="#b18">[19]</ref> argued that using inner product formulation lacks expressiveness due to its violation of the triangle inequality. As a result, the authors proposed Collaborative Metric Learning (CML), a strong recommendation baseline based on Euclidean distance. Notably, many recent works have moved into neural models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref>, in which stacked nonlinear transformations have been used to approximate the interaction function.</p><p>Our work is inspired by recent advances in hyperbolic representation learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>. For instance, <ref type="bibr" target="#b33">[34]</ref> proposed training a question answering system in hyperbolic space. <ref type="bibr" target="#b7">[8]</ref> proposed learning word embeddings using a hyperbolic neural network. <ref type="bibr" target="#b12">[13]</ref> proposed a hyperbolic variation of self-attention and the transformer network, and applied it to tasks such as visual question answering and neural machine translation. <ref type="bibr" target="#b10">[11]</ref> proposed recurrent neural networks in hyperbolic space, <ref type="bibr" target="#b2">[3]</ref> proposed a method of embedding graphs in hyperbolic space. <ref type="bibr" target="#b3">[4]</ref> is the most similar work to ours that embeds bipartite user-item graphs in hyperbolic space, but it does not learn the embeddings with metric learning manner. While the advantages of hyperbolic space seem eminent in the wide variety of application domains, there is no work that investigates this embedding space within the context of metric learning in recommender systems. This constitutes the key novelty of our work. A detailed primer on hyperbolic space is given in the technical exposition of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we introduce a new effective and competent recommendation model called HyperML. To the best of our knowledge, HyperML is the first model to explore metric learning in hyperbolic space in recommender system. Additionally, we also introduce a distortion term, which is essential to control good representations in hyperbolic space. Through extensive experiments on ten datasets, we are able to demonstrate the effectiveness of HyperML over other baselines in Euclidean space, even state-of-the-art metric learning models such as CML or LRML. The promising results of HyperML may inspire other future works to explore hyperbolic space in solving recommendation problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of our proposed HyperML. The left figure with the greenish ball represents the hyperbolic unit ball and the pale blue parallelogram illustrates its tangent space; red and vermeil circles represent user embeddings; green and purple circles represent item embeddings. The right figure illustrates an example of a triplet embedding of user (red circle), positive item (green circle) and negative item (orange circle), in which it demonstrates a small tree of one root and two children which is embedded into hyperbolic space with the exponentially expansion property (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. Embeddings comparison on Yelp dataset. LRML CML HyperML (b). Embeddings comparison on Automotive dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison between two-dimensional Poincaré embedding and Euclidean embedding on Yelp and Automotive dataset. The images illustrate the embeddings of LRML, CML and HyperML after convergence (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Performance on Different Multi-task Learning Weight γ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>2D t-SNE item embeddings visualization of hyperbolic metric in MovieLens1M dataset (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>3β 2 2d )</cell></row></table><note>Experimental results (nDCG@10 and HR@10) on ten public benchmark datasets. Best result is in bold face and second best is underlined. Our proposed HyperML achieves very competitive results, outperforming strong recent advanced metric learning baselines such as CML and LRML.uniform initializer U(−α, α). For non-metric learning baselines, we set α = 0.01. For metric learning models, we empirically set α = (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison between HyperML and HyperTS.</figDesc><table><row><cell>Scaling</cell><cell></cell><cell>Goodbooks</cell><cell></cell><cell></cell><cell>Games</cell></row><row><cell cols="3">Variable c HyperML CML</cell><cell>∆(%)</cell><cell cols="2">HyperML CML</cell><cell>∆(%)</cell></row><row><cell>c = 0.5</cell><cell>51.396</cell><cell cols="2">49.010 +4.87%</cell><cell>37.134</cell><cell>32.524 +14.17%</cell></row><row><cell>c = 1.0</cell><cell>51.088</cell><cell cols="2">49.010 +4.24%</cell><cell>35.995</cell><cell>32.524 +10.67%</cell></row><row><cell>c = 2.0</cell><cell>49.631</cell><cell cols="2">49.010 +1.27%</cell><cell>38.578</cell><cell>32.524 +18.61%</cell></row><row><cell>c = 4.0</cell><cell>46.017</cell><cell cols="2">49.010 −6.11%</cell><cell>35.466</cell><cell>32.524 +9.05%</cell></row><row><cell>c = 8.0</cell><cell>40.488</cell><cell cols="2">49.010 −17.39%</cell><cell>28.390</cell><cell>32.524 −12.71%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Effects of the scaling variable c on Goodbooks andGames datasets in terms of nDCG@10.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://grouplens.org/datasets/movielens/ 2 https://github.com/zygmuntz/goodbooks-10k 3 https://www.yelp.com/dataset/challenge 4 Datasets are obtained from http://jmcauley.ucsd.edu/data/amazon/ using the 5-core setting with the domain names truncated in the interest of space.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Riemannian Adaptive Optimization Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stochastic Gradient Descent on Riemannian Manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvere</forename><surname>Bonnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="2217" to="2229" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural Embeddings of Graphs in Hyperbolic Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Benjamin Paul Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Peter</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deisenroth</surname></persName>
		</author>
		<idno>abs/1705.10359</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Scalable Hyperbolic Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">R</forename><surname>Benjamin Paul Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Hardwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabon</forename><surname>Wardrope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Dzogang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saúl</forename><surname>Daolio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vargas</surname></persName>
		</author>
		<idno>abs/1902.08648</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-Margin Classification in Hyperbolic Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunghoon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Demeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Berger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research (Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1832" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a Similarity Metric Discriminatively, with Application to Face Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hyperspherical Variational Auto-Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">R</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Falorsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018</title>
		<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018<address><addrLine>Monterey, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-06" />
			<biblScope unit="page" from="856" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Embedding Text in Hyperbolic Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing</title>
		<meeting>the Twelfth Workshop on Graph-Based Methods for Natural Language Processing<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-06" />
			<biblScope unit="page" from="59" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hyperbolic Entailment Cones for Learning Hierarchical Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
			<biblScope unit="page" from="1632" to="1641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hyperbolic Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="5350" to="5360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Mixed-Curvature Representations in Product Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hyperbolic Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2016-04-11" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI&apos;16)</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence (AAAI&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Outer Product-based Neural Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI&apos;18)</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence (IJCAI&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-03" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast matrix factorization for online recommendation with implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collaborative Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Kang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="0201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-linear Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dor</forename><surname>Kedem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12-03" />
			<biblScope unit="page" from="2582" to="2590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hyperbolic Geometry of Complex Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><forename type="middle">V</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marián</forename><surname>Boguñá</surname></persName>
		</author>
		<idno>abs/1006.5169</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lorentzian Distance Learning for Hyperbolic Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Marc Teva Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="3672" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Poincaré Embeddings for Learning Hierarchical Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
			<biblScope unit="page" from="3776" to="3785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A General Recommendation Model for Heterogeneous Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Anh Nguyen</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2016-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Factorization Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 10th IEEE International Conference on Data Mining</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-12" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>ICDM 2010</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI 2009, Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06-18" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representation Tradeoffs for Hyperbolic Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
			<biblScope unit="page" from="4457" to="4466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
		<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018<address><addrLine>Marina Del Rey, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02-05" />
			<biblScope unit="page" from="583" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Poincare Glove: Hyperbolic Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Tifrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Signed Distancebased Deep Memory Recommender</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyumin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-13" />
			<biblScope unit="page" from="1841" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial Mahalanobis Distance-based Attentive Song Recommender for Automatic Playlist Continuation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renee</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyumin</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-21" />
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A Gyrovector Space Approach to Hyperbolic Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><forename type="middle">Albert</forename><surname>Ungar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Metric Learning with Multiple Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huyen</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Woznica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Kalousis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
			<biblScope unit="page" from="1170" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distance Metric Learning for Large Margin Nearest Neighbor Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-12-05" />
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Gradient descent in hyperbolic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthia</forename><surname>Leimeister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08207</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distance Metric Learning with Application to Clustering with Side-Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-12-09" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">NeuRec: On Nonlinear Transformation for Personalized Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manqing</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3669" to="3675" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
