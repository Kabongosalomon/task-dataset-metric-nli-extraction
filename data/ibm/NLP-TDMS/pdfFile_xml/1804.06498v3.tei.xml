<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Multimodal Subspace Clustering Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018">DECEMBER. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Abavisani</forename><surname>Mahdi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Vishal</forename><forename type="middle">M Patel</forename></persName>
						</author>
						<title level="a" type="main">Deep Multimodal Subspace Clustering Networks</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING</title>
						<imprint>
							<biblScope unit="volume">12</biblScope>
							<biblScope unit="issue">6</biblScope>
							<date type="published" when="2018">DECEMBER. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep multimodal subspace clustering</term>
					<term>subspace clustering</term>
					<term>multimodal learning</term>
					<term>multi-view subspace clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present convolutional neural network (CNN) based approaches for unsupervised multimodal subspace clustering. The proposed framework consists of three main stages -multimodal encoder, self-expressive layer, and multimodal decoder. The encoder takes multimodal data as input and fuses them to a latent space representation. The self-expressive layer is responsible for enforcing the self-expressiveness property and acquiring an affinity matrix corresponding to the data points. The decoder reconstructs the original input data. The network uses the distance between the decoder's reconstruction and the original input in its training. We investigate early, late and intermediate fusion techniques and propose three different encoders corresponding to them for spatial fusion. The selfexpressive layers and multimodal decoders are essentially the same for different spatial fusion-based approaches. In addition to various spatial fusion-based methods, an affinity fusion-based network is also proposed in which the self-expressive layer corresponding to different modalities is enforced to be the same. Extensive experiments on three datasets show that the proposed methods significantly outperform the state-of-the-art multimodal subspace clustering methods.</p><p>Index Terms-Deep multimodal subspace clustering, subspace clustering, multimodal learning, multi-view subspace clustering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M ANY practical applications in image processing, computer vision, and speech processing require one to process very high-dimensional data. However, these data often lie in a low-dimensional subspace. For instance, facial images with variation in illumination <ref type="bibr" target="#b0">[1]</ref>, handwritten digits <ref type="bibr" target="#b1">[2]</ref> and trajectories of a rigidly moving object in a video <ref type="bibr" target="#b2">[3]</ref> are examples where the high-dimensional data can be represented by low-dimensional subspaces. Subspace clustering algorithms essentially use this fact to find clusters in different subspaces within a dataset <ref type="bibr" target="#b3">[4]</ref>. In other words, in a subspace clustering task, given the data from a union of subspaces, the objective is to find the number of subspaces, their dimensions, the segmentation of the data and a basis for each subspace <ref type="bibr" target="#b3">[4]</ref>. This problem has numerous applications including motion segmentation <ref type="bibr" target="#b4">[5]</ref>, unsupervised image segmentation <ref type="bibr" target="#b5">[6]</ref>, image representation and compression <ref type="bibr" target="#b6">[7]</ref> and face clustering <ref type="bibr" target="#b7">[8]</ref>.</p><p>Various subspace clustering methods have been proposed in the literature <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. In particular, methods based on sparse and low-rank representation have gained a lot of attraction in recent years <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. These methods exploit the fact that noiseless data in a union of subspaces are self-expressive, i.e. each data point can be M. Abavisani is with the department of Electrical and Computer Engineering at Rutgers University, Piscataway, NJ USA. email: mahdi.abavisani@rutgers.edu.</p><p>Vishal M. Patel is with the department of Electrical and Computer Engineering at Johns Hopkins University, Baltimore, MD USA. email: vpa-tel36@jhu.edu.  <ref type="figure" target="#fig_0">Fig. 1</ref>. An overview of the proposed deep multimodal subspace clustering framework. Note that the network consists of three blocks: a multimodal encoder, a self-expressive layer, and a multimodal decoder. The weights in the self-expressive layer, Θ s , are used to construct the affinity matrix. We present several models for the multimodal encoder.</p><p>expressed as a sparse linear combination of other data points.</p><p>The self-expressiveness property was also recently investigated in <ref type="bibr" target="#b15">[16]</ref> to develop a deep convolutional neural network (CNN) for subspace clustering. This deep learning-based method was shown to significantly outperform the state-of-the-art subspace clustering methods.</p><p>In the case where the data consists of multiple modalities or views, multimodal subspace clustering methods can be employed to simultaneously cluster the data in the individual modalities according to their subspaces <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Some of the multimodal subspace clustering methods make use of the kernel trick to map the data onto a high-dimensional feature space to achieve better clustering <ref type="bibr" target="#b35">[36]</ref>.</p><p>Motivated by the recent advances in deep subspace clustering <ref type="bibr" target="#b15">[16]</ref> as well as multimodal data processing using CNNs <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, in this paper, we propose a different approach to the problem of multimodal subspace clustering. We present a novel CNNbased autoencoder approach in which a fully-connected layer is introduced between the encoder and the decoder which mimics the self-expressiveness property that has been widely used in various subspace clustering algorithms. <ref type="figure" target="#fig_0">Figure 1</ref> gives an overview of the proposed deep multimodal subspace clustering framework. The self-expressive layer is responsible for enforcing the self-expressiveness property and acquiring an affinity matrix corresponding to the data points. The decoder reconstructs the original input data from the latent features. The network uses the distance between the decoder's reconstruction and the original input in its training.</p><p>For encoding the multimodal data into a latent space, we investigate three different spatial fusion techniques based on late, early and intermediate fusion. These fusion techniques are motivated by the deep multimodal learning methods in supervised learning tasks <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, that provide the representation arXiv:1804.06498v3 <ref type="bibr">[cs.</ref>LG] 4 Jan 2019 of modalities across spatial positions. In addition to the spatial fusion methods, we propose an affinity fusion-based network in which the self-expressive layer corresponding to different modalities is enforced to be the same. For both spatial and the affinity fusion-based methods, we formulate an end-to-end training objective loss.</p><p>Key contributions of our work are as follows:</p><p>• Deep learning-based multimodal subspace clustering framework is proposed in which the self-expressiveness property is encoded in the latent space by using a fully connected layer.</p><p>• Novel encoder network architectures corresponding to late, early and intermediate fusion are proposed for fusing multimodal data.</p><p>• An affinity fusion-based network architecture is proposed in which the self-expressive layer is enforced to have the same weights across latent representations of all the modalities. To the best of our knowledge, this is the first attempt that proposes to use deep learning for multimodal subspace clustering. Furthermore, the proposed method obtains the state-of-the-art results on various multimodal subspace clustering datasets. Code is available at: https://github.com/mahdiabavisani/ Deep-multimodal-subspace-clustering-networks. This paper is organized as follows. Related works on subspace clustering and multimodal learning are presented in Section II. The proposed spatial fusion-based and affinity fusion-based multimodal subspace clustering methods are presented in Section III and IV, respectively. Experimental results are presented in Section V, and finally, Section VI concludes the paper with a brief summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we review some related works on subspace clustering and multimodal learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sparse and Low-rank Representation-based Subspace Clustering</head><p>Let</p><formula xml:id="formula_0">X = [x 1 , · · · , x N ] ∈ R D×N be a collection of N signals {x i ∈ R D } N i=1</formula><p>drawn from a union of n linear subspaces S 1 ∪ S 2 ∪ · · · ∪ S n of dimensions {d } n =1 in R D . Given X, the task of subspace clustering is to find sub-matrices X ∈ R D×N that lie in S with N 1 + N 2 + · · · + N n = N. The sparse subspace clustering (SSC) <ref type="bibr" target="#b20">[21]</ref> and low-rank representationsbased subspace clustering (LRR) <ref type="bibr" target="#b21">[22]</ref> algorithms exploit the fact that noiseless data in a union of subspaces are selfexpressive. In other words, it is assumed that each data point can be represented as a linear combination of other data points. Hence, these algorithms aim to find the sparse or low-rank matrix C by solving the following optimization problem</p><formula xml:id="formula_1">min C C p + λ 2 X − XC 2 F ,<label>(1)</label></formula><p>where . p is the 1 -norm in the case of SSC <ref type="bibr" target="#b20">[21]</ref> and the nuclear norm in the case of LRR <ref type="bibr" target="#b21">[22]</ref>. Here, λ is a regularization parameter. In addition, to prevent the trivial solution C = I, an additional constraint of diag(C) = 0 is added</p><formula xml:id="formula_2">Encoder Decoder … … Z m ⇥e Z m ⇥e ⇥s Z⇥ e Z⇥ e C Ŷ Y⇥ {G(x1), G(x2), · · · , G(xN X )} G(·) {y1, y2, · · · , yN Y } {F (y1), F (y2), · · · , F (yN Y )} F (·) 1 Z⇥ e Z⇥ e ⇥s {G(x1), G(x2), · · · , G(xN X )} G(·) {y1, y2, · · · , yN Y } {F (y1), F (y2), · · · , F (yN Y )} F (·) 1 Input Output Z m ⇥ e Z m ⇥ e ⇥ s Z ⇥ e Z ⇥ e C X X ⇥ {G(x 1 ), G(x 2 ), · · · , G(x N X )} G(·) {y 1 , y 2 , · · · , y N Y } {F (y 1 ), F (y 2 ), · · · , F (y N Y )} F (·) 1 Z m ⇥ e Z m ⇥ e ⇥ s Z ⇥ e Z ⇥ e C X X ⇥ {G(x 1 ), G(x 2 ), · · · G(·) {y 1 , y 2 , · · · , y N Y } {F (y 1 ), F (y 2 ), · · · , F (·) ∼ Fig.</formula><p>2. An overview of the DSC framework proposed in <ref type="bibr" target="#b15">[16]</ref> for unimodal subspace clustering.</p><p>to the above optimization problem in the case of SSC. Once C is found, spectral clustering methods <ref type="bibr" target="#b48">[49]</ref> are applied on the affinity matrix W = |C| + |C| T to obtain the segmentation of the data X.</p><p>Non-linear versions of the SSC and LRR algorithms have also been proposed in the literature <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Subspace Clustering</head><p>The deep subspace clustering network (DSC) <ref type="bibr" target="#b15">[16]</ref> explores the self-expressiveness property by embedding the data into a latent space using an encoder-decoder type network. <ref type="figure">Figure 2</ref> gives an overview of the DSC method for unimodal subsapce clustering. The method optimizes an objective similar to that of (1) but the matrix C is approximated using a trainable dense layer embedded within the network. Let us denote the parameters of the self-expressive layer as Θ s . Note that these parameters are essentially the elements of C in (1). The following loss function is used to train the network</p><formula xml:id="formula_3">miñ Θ Θ s p + λ 1 2 Z Θ e − Z Θ e Θ s 2 F + λ 2 2 X −XΘ , s.t. diag(Θ s ) = 0,<label>(2)</label></formula><p>where Z Θ e denotes the output of the encoder, andXΘ is the reconstructed signal at the output of the decoder. Here, the network parametersΘ consist of encoder parameters Θ e , decoder parameters Θ d and self-expressive layer parameters Θ s . Here, λ 1 and λ 2 are two regularization parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multimodal Subspace Clustering</head><p>A number of multimodal and multiview subspace clustering approaches have been developed in recent years. Bickel et al. introduced an Expectation Maximization (EM) and agglomerative multiview clustering methods in <ref type="bibr" target="#b32">[33]</ref>. White et al. <ref type="bibr" target="#b31">[32]</ref> provided a convex reformulation of multiview subspace learning that as opposed to local formulations enables global learning. Some algorithms use dimensionality reduction methods such as Canonical Correlation Analysis (CCA) to project the multiview data onto a low-dimensional subspace for clustering <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Some other multimodal methods are specifically designed for two views and can not be easily generalized to multiple views <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Kumar et al. <ref type="bibr" target="#b28">[29]</ref> proposed a co-regularization method that enforces the clusterings to be aligned in different views. Zhao et al. <ref type="bibr" target="#b29">[30]</ref> use output of clustering in one view to learn discriminant     DP S0 S1 S2 Visible z = f (DP,S0,S1,S2,Vi) <ref type="bibr" target="#b3">4</ref>. In spatial fusion methods each location of the fusion is related to the input values at the same location. In this especial case, the facial components (i.e. eyes, nose and mouth) are aligned across all the modalities (i.e. DP, S0, S1, S2, Visible). subspaces in another view. A multiview subspace clustering method, called Low-rank Tensor constrained Multiview Subspace Clustering (LT-MSC) was recently proposed in <ref type="bibr" target="#b25">[26]</ref>. In the LT-MSC method, all the subspace representations are integrated into a low-rank tensor, which captures the high order correlations underlying multiview data. In <ref type="bibr" target="#b50">[51]</ref>, a diversityinduced multiview subspace clustering was proposed in which the Hilbert Schmidt independence criterion was utilized to explore the complementarity of multiview representations. Recently, <ref type="bibr" target="#b51">[52]</ref> proposed a constrained multi-view video face clustering (CMVFC) framework in which pairwise constraints are employed in both sparse subspace representation and spectral clustering procedures for multimodal face clustering. A collaborative image segmentation framework, called Multitask Low-rank Affinity Pursuit (MLAP) was proposed in <ref type="bibr" target="#b26">[27]</ref>.</p><formula xml:id="formula_4">{G(x1), G(x2), · · · , G(xN X )} G(·) {y1, y2, · · · , yN Y } {F (y1), F (y2), · · · , F (yN Y )} F (·) 1 Z⇥ e Z⇥ e ⇥s {G(x1), G(x2), · · · , G(xN X )} G(·) {y1, y2, · · · , yN Y } {F (y1), F (y2), · · · , F (yN Y )} F (·)</formula><formula xml:id="formula_5">{G(x1), G(x2), · · · , G(xN X )} G(·) {y1, y2, · · · , yN Y } {F (y1), F (y2), · · · , F (yN Y )} F (·) 1 Z⇥ e Z⇥ e ⇥s {G(x1), G(x2), · · · , G(xN X )} G(·) {y1, y2, · · · , yN Y } {F (y1), F (y2), · · · , F (yN Y )} F (·)</formula><formula xml:id="formula_6">{G(x1), G(x2), · · · , G(xN X )} G(·) {y1, y2, · · · , yN Y } {F (y1), F (y2), · · · , F (yN Y )} F (·) 1 Z⇥ e Z⇥ e ⇥s {G(x1), G(x2), · · · , G(xN X )} G(·) {y1, y2, · · · , yN Y } {F (y1), F (y2), · · · , F (yN Y )} F (·) 1</formula><formula xml:id="formula_7">(a) (b) (c) (d) Z m ⇥ m e Z m ⇥ m e ⇥s Z⇥ e Z⇥ e C X X⇥ {G(x1), G(x2), · · · , G(xN X )} G(·) {y1, y2, · · · , yN Y } {F (y1), F (y2), · · · , F (yN Y )} F (·) 1 Z m ⇥ m e Z m ⇥ m e ⇥s Z⇥ e Z⇥ e C X X⇥ {G(x1), G(x2), · · · , G(xN X )} G(·) {y1, y2, · · · , yN Y } {F (y1), F (y2), · · · , F (yN Y )} F (·)</formula><formula xml:id="formula_8">Spatial Fusion Result Input Modalities Spatial Fusion f (x 1 , x 2 , x 3 , x 4 , x 5 ) Fig.</formula><p>In this method, the sparsity-consistent low-rank affinities from the joint decompositions of multiple feature matrices into pairs of sparse and low-rank matrices are exploited for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Deep Multimodal Learning</head><p>In multimodal learning problems, the idea is to use the complementary information provided by the different modalities to enhance the recognition performance.Supervised deep multimodal learning was first introduced in <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, and has gained a lot of attention in recent years <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b39">[40]</ref>.</p><p>Keila et al. <ref type="bibr" target="#b46">[47]</ref> investigated deep multimodal classification of large-scaled datasets. They, compared a number of multimodal fusion methods in terms of accuracy and computational efficiency, and provided analysis regarding the interpretability of multimodal classification models. Feichtenhofer et al. <ref type="bibr" target="#b47">[48]</ref> proposed a convolutional fusion method for two stream 3D networks. They explored multiple fusion functions within deep architectures and studied the importance of learning the correspondences between spatial and temporal feature maps. Various deep supervised multimodal fusion approaches have also been proposed in the literature for different applications including medical image analysis applications <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> visual recognition <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b39">[40]</ref> and visual question answering <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b42">[43]</ref>. We refer readers to <ref type="bibr" target="#b38">[39]</ref> for more detailed survey of various deep supervised multimodal fusion methods.</p><p>While most of the deep multimodal approaches have reported improvements in the supervised tasks, to the best of our knowledge, there is no deep multimodal learning method specifically designed for unsupervised subspace clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SPATIAL FUSION-BASED DEEP MULTIMODAL SUBSPACE CLUSTERING</head><p>In this section, we present details of the proposed spatial fusion-based networks for unsupervised subspace clustering. Spatial fusion methods find a joint representation that contains complementary information from different modalities. The joint representation has a spatial correspondence to every modality. <ref type="figure">Figure 4</ref> shows a visual example of spatial fusion where five different modalities (DP, S0, S1, S2, Visible) are combined to produce a fused result z. The spatial fusion methods are especially popular in supervised multimodal learning applications <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. We investigate applying these fusion techniques to our problem of deep subspace clustering.</p><p>An essential component of such methods is the fusion function that merges the information from multiple input representations and returns a fused output. In the case of deep networks, flexibility in the choice of fusion network leads to different models. In what follows, we investigate several network designs and spatial fusion functions for multimodal subspace clustering. Then, we formulate an end-to-end training objective for the proposed networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fusion Structures</head><p>We build our deep multimodal subspace clustering networks based on the architecture proposed in <ref type="bibr" target="#b15">[16]</ref> for unimodal subspace clustering. Our framework consists of three main components: an encoder, a fully connected self-expressive layer, and a decoder. We propose to achieve the spatial fusion using an encoder and the fused representation is then fed to a self-expressive layer which essentially exploits the selfexpressiveness property of the joint representation. The joint representation resulting from the output of the self-expressive layer is then fed to a multimodal decoder that reconstructs the different modalities from the joint latent representation.</p><p>For the case of M input modalities, the decoder consists of M branches, each reconstructing one of the modalities. The encoders on the other hand, can be designed such that they achieve early, late or intermediate fusion. Early fusion refers to the integration of multimodal data in the stage of feature level before feeding them to the network. Late fusion, on the other hand, involves the integration of multimodal data in the last stage of the network. The flexibility of deep networks also offers the third type of fusion known as the intermediate fusion, where the feature maps from the intermediate layers of a network are combined to achieve better joint representation. Figures 3 (a), (b) and (c) give an overview of deep multimodal subspace clustering networks with different spatial fusion structures. Note that the multimodal decoder's structure remains the same in all three cases. It is worth mentioning that in the case of intermediate fusion, it is a common practice to aggregate the weak or correlated modalities at earlier stages and combine the remaining strong modalities at the in-depth stages <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fusion Functions</head><p>Assume for a particular data point, x i , there are M feature maps corresponding to the representation of different modalities. A fusion function f : {x 1 , x 2 . · · · , x M } → z fuses the M feature maps and produces an output z. For simplicity we assume that all the input feature maps have the same dimension of R H×W ×d i n , and the output has the dimension of R H×W ×d ou t . In fact, deep network structures offer the design option for having feature maps with the same dimensions. We use z i, j,k and x m i, j,k to denote the value in the spatial position (i, j, k) in the output and the mth input feature map, respectively. Various fusion functions can be used to combine the input feature maps. Below, we investigate a few.</p><formula xml:id="formula_9">1) Sum fusion z = sum(x 1 , x 2 . · · · , x M ):</formula><p>computes the sum of the feature maps at the same special positions as follows</p><formula xml:id="formula_10">z i, j,k = M m=1 x m i, j,k .<label>(3)</label></formula><p>2) Maxpooling function z = max(x 1 , x 2 . · · · , x M ): returns the maximum value of the corresponding location in the input feature maps as follows</p><formula xml:id="formula_11">z i, j,k = Max{x 1 i, j,k , x 2 i, j,k . · · · , x M i, j,k }.<label>(4)</label></formula><p>3) Concatenation function z = cat(x 1 , x 2 . · · · , x M ): constructs the output by concatenating the input feature maps as follows</p><formula xml:id="formula_12">z = [x 1 , x 2 . · · · , x M ],<label>(5)</label></formula><p>where each input has the dimension R H×W ×d i n and the output has the dimension R H×W ×(d i n ×M) . Note that these fusion functions are denoted as "Fusion" in blue boxes in <ref type="figure">Figure 3</ref> (a)-(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. End-to-End Training Objective</head><p>Given N paired data samples</p><formula xml:id="formula_13">{x 1 i , x 2 i , · · · , x M i } N i=1 from M different modalities, define the corresponding data matrices as X m = [x m 1 , x m s , · · · , x m N ], m ∈ {1, · · · , M }.</formula><p>Regardless of the network structure and the fusion function of choice, let Θ M .e denote the parameters of the multimodal encoder. Similarly, let Θ s be the self-expressive layer parameters and Θ M .d be the multimodal decoder parameters. Then the proposed spatial fusion models can be trained end-to-end using the following loss function</p><formula xml:id="formula_14">min Θ Θ s p + λ 1 2 Z Θ M .e − Z Θ M .e Θ s 2 F + λ 2 2 M m=1 X m −X m Θ s.t diag(Θ s ) = 0, (6)</formula><p>where Θ denotes all the training network parameters including Θ M .e , Θ s and Θ M .d . The joint representation is denoted by Z Θ M .e , andX m Θ is the reconstruction of X m . Here, λ 1 and λ 2 are two regularization parameters, and · p can be either 1 or 2 norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. AFFINITY FUSION-BASED DEEP MULTIMODAL SUBSPACE CLUSTERING</head><p>In this section, we propose a new method for fusing the affinities across the data modalities to achieve better clustering. Spatial fusion methods require the samples from different modalities to be aligned (see <ref type="figure">Figure 4</ref>) to achieve better clustering. In contrast, the proposed affinity fusion approach combines the similarities from the self-expressive layer to obtain a joint representation of the multimodal data. This is done by enforcing the network to have a joint affinity matrix. This avoids the issue of having aligned data or increasing the dimensionality of the fused output (i.e. concatenation). The motivation for enforcing a shared affinity matrix is that similar (dissimilar) data in one modality should be similar (dissimilar) in the other modalities as well. <ref type="figure" target="#fig_2">Figure 5</ref> shows an example of the proposed affinity fusion method by forcing the modalities to share the same affinity matrix.</p><p>In the DSC framework <ref type="bibr" target="#b15">[16]</ref>, the affinity matrix is calculated from the self-expressive layer weights as follows</p><formula xml:id="formula_15">W = |Θ T s | + |Θ T s |,</formula><p>where Θ s corresponds to the self-expressive layer weights learned by an end-to-end training strategy <ref type="bibr" target="#b15">[16]</ref>. Thus a shared Θ s results in a common W across the modalities. We enforce the modalities to share a common Θ s while having different encoders, decoders and the latent representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Structure</head><p>For an M modality problem, we propose to stack M parallel DSC networks, where they share a common self-expressive layer. In this model, per each modality one encoder-decoder network is trained. In contrast to the spatial fusion models that only have one joint latent representation, this model results in M distinct latent representations corresponding to M different modalities. The latent representations are connected together by sharing the self-expressive layer. The optimal selfexpressive layer should be able to jointly exploit the selfexpressiveness property across all the M modalities. <ref type="figure">Figure  3 (d)</ref> gives an overview of the proposed affinity fusion-based network architecture.</p><p>Algorithm 1 Spatial and affinity fusion algorithms</p><formula xml:id="formula_16">1: procedure DMSC({X m } M m=1 , λ 1 , λ 2 , 'mode') 2:</formula><p>if mode = Spatial fusion then 3:</p><p>Train the networks using the loss <ref type="bibr" target="#b5">(6)</ref>. <ref type="bibr">4:</ref> else if mode = Affinity fusion then 5:</p><p>Train the networks using the loss <ref type="bibr" target="#b6">(7)</ref>. <ref type="bibr" target="#b5">6</ref>:</p><formula xml:id="formula_17">end if 7:</formula><p>Extract Θ s from the trained networks. <ref type="bibr">8:</ref> Normalize the columns of Θ s as θ si ← θ si θ si ∞ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Form a similarity graph with N nodes and set the weights on the edges by W = |Θ s | + |Θ T s |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>Apply spectral clustering to the similarity graph. 11: end procedure 12: Output: Segmented multimodal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. End-to-End Training</head><p>We propose to find the shared self-expressive layer weights by training the network with the following loss</p><formula xml:id="formula_18">min Θ Θ s p + λ 1 2 M m=1 Z m Θ m e − Z m Θ m e Θ s 2 F + λ 2 2 M m=1 X m −X m Θ m s.t. diag(Θ s ) = 0, (7)</formula><p>where Θ s is the common self-expressive layer weighs. Here, λ 1 and λ 2 are regularization parameters. Z m Θ m e andX m Θ m are respectively the latent space representation and the reconstructed decoder's output corresponding to X m . Θ m denotes the network parameters corresponding to the mth modality and Θ indicates to all the trainable parameters. Minimizing <ref type="bibr" target="#b6">(7)</ref> encourages the networks to learn the latent representations that share the same affinity matrix.</p><p>Algorithm 1 summarizes the proposed spatial fusion and affinity fusion-based subspace clustering methods. Details of different network architectures used in this paper are given in Appendix VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>We evaluate the proposed deep multimodal subspace clustering methods on several real-world multimodal datasets. The following datasets are used in our experiments.</p><p>• Multiview digit clustering using the MNIST <ref type="bibr" target="#b56">[57]</ref> and the USPS <ref type="bibr" target="#b57">[58]</ref> handwritten digits datasets. Here, we view images from the individual datasets as two views of the same digit. These datasets are considered to be spatially related but not aligned. Since the number of parameters in the self-expressive layer of a deep subspace clustering network scales quadratically with the size of the data, we randomly select 200 samples per digit to keep the networks to a tractable size.</p><p>• Heterogeneous face clustering using the ARL Polarimetric face dataset <ref type="bibr" target="#b58">[59]</ref>. The ARL dataset contains five spatially well-aligned modalities (Visible, DP, S0, S1, S2).</p><p>• Face clustering based on the facial regions using the Extended Yale-B dataset <ref type="bibr" target="#b59">[60]</ref>. We extract facial components   (i.e. eyes, nose, mouth) from the images and view them as soft biometrics and use them along with the entire face for clustering. Here, the modalities do not share any direct spatial correspondence. <ref type="figure" target="#fig_3">Figure 6</ref> (a), (b), and (c) show sample images from the digits, ARL and Extended Yale-B datasets, respectively. Table I gives an overview of their details. Note that as opposed to supervised methods, we do not split datasets into training and testing sets for subspace clustering. Similar to <ref type="bibr" target="#b15">[16]</ref>, the parameters of the deep subspace clustering networks are trained using the entire dataset.</p><p>To investigate ability and limitations of different versions of the proposed fusion methods, we evaluate the affinity fusion method along with a wide range of plausible spatial fusion methods based on different structure designs and fusion functions. For the early fusion structure, we consider the concatenation fusion function 1 . As for the intermediate and late fusion structures, we consider all the three presented fusion functions which results in six distinct models. <ref type="table" target="#tab_5">Table II</ref> presents the structural variations we have used for the presented spatial fusion methods and the name we assign to them when reporting their performances. Besides, we compare our methods against the following state-of-the-art multimodal subspace clustering baselines: CMVFC <ref type="bibr" target="#b51">[52]</ref>, TM-MSC <ref type="bibr" target="#b25">[26]</ref>, MSSC <ref type="bibr" target="#b35">[36]</ref>, MLRR <ref type="bibr" target="#b35">[36]</ref>, KMSSC <ref type="bibr" target="#b35">[36]</ref>, and KMLRR <ref type="bibr" target="#b35">[36]</ref>.  Also, to explore the contribution of leveraging information from multiple modalities into the performance of subspace clustering task, we report the performance of subspace clustering methods on the single modalities as well. In particular, we report the classical SSC <ref type="bibr" target="#b20">[21]</ref> and LRR <ref type="bibr" target="#b21">[22]</ref> performances on the individual modalities along with the recently proposed DSC method <ref type="bibr" target="#b15">[16]</ref>. Furthermore, we train an encoder-decoder similar to the network in <ref type="bibr" target="#b15">[16]</ref> but without the self-expressive layer, and extract the latent space representations. These deep features are then fed to the SSC algorithm for clustering. We call this method "AE+SSC". This baseline will show the significance of using an end-to-end deep learning method for subspace clustering. In our tables, we use boldface letters to denote the top performing method and specify the corresponding modalities or datasets in the rows, and subspace clustering methods on the columns.  <ref type="bibr" target="#b56">[57]</ref>, and USPS <ref type="bibr" target="#b57">[58]</ref> digits datasets, (b) ARL polarimetric face dataset <ref type="bibr" target="#b58">[59]</ref>, and (c) Faces and facial components from the Extended Yale-B dataset <ref type="bibr" target="#b59">[60]</ref>. In our experiments, samples from all the modalities are resized to 32 × 32, and rescaled to have pixel values between 0 and 255.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structures:</head><p>We perform all the experiments on different datasets using the same protocol and network architectures to ensure fair and meaningful comparisons (including the networks for the single modality experiments). All the encoders have four convolutional layers, and decoders are stacked three deconvolution layers mimicking the inverse task of the encoder. The network details are given in the Appendix.</p><p>For the spatial fusion experiments, in the case of early fusion, we apply the fusion functions on the pixel intensities, and the rest of the network is similar to that of the single modality deep subspace clustering network. Conducted experiments for the intermediate fusion use a prior knowledge on the importance of the modalities. They integrate weak modalities in the second hidden layer, and then, the combination of them in the third layer. Finally, the fusion of all the weak modalities is combined with the strong modality (for example the visible domain in the ARL dataset) in the fourth layer. In the case of late fusion, all the modalities are fused in the fourth layer of the encoder.</p><p>As discussed earlier, in the affinity fusion method there exists an encoder-decoder and a latent space per number of available modalities. For example, in the case of the ARL dataset with 5 modalities, we have 5 distinct encoders and decoders connected with a shared self-expressive layer. For each modality in the experiments with the shared affinity, we use similar encoder-decoders as in the case of the DSC network <ref type="bibr" target="#b15">[16]</ref> with unimodal experiments.</p><p>Training details: We implemented our method in Python-2 with Tensorflow-1.4 <ref type="bibr" target="#b60">[61]</ref>. We use the adaptive momentumbased gradient descent method (ADAM) <ref type="bibr" target="#b61">[62]</ref> to minimize our loss functions, and apply a learning rate of 10 −3 .</p><p>The input images of all the modalities are resized to 32×32, and rescaled to have pixel values between 0 and 255. In our experiments, the Frobenius norm (i.e. p = 2) is used in the loss functions (2), (6) and <ref type="formula">(7)</ref> while training the networks. Similar to <ref type="bibr" target="#b15">[16]</ref>, for all the methods that have self-expressive layer, we start training on the specified objective functions in each model after a stage of pre-training on the dataset without the self-expressive layer. In particular, for all the proposed deep multimodal subspace clustering methods, and the unimodal DSC networks in the experiments with individual modalities, we pre-train the encoder-decoders for 20k epochs with the following objective</p><formula xml:id="formula_19">min Θ M m=1 X m −X m Θ 2 F ,</formula><p>whereΘ indicates the union of parameters in the encoder and decoder networks. Note that for the unimodal experiments, M = 1.</p><p>We use a batch size of 100 for the pretraining stage of all the experiments. However, once we start training the selfexpressive layer, the method requires all the data points to be fed as a batch. Thus, in the experiments with digits, ARL faces and Yale-B facial components the batch sizes are 2000, 2160 and 2432, respectively.</p><p>We set the regularization parameters as λ 1 = 1 and λ 2 = 1 × 10 K 10 −3 , where K is the number of subjects in the dataset. This experimental rule has been found to be efficient in <ref type="bibr" target="#b15">[16]</ref> as well. A sensibility analysis over the range [10 −4 , 10 4 ] in Section V-E, shows that if λ 1 and λ 2 are kept around the same scale as our selections, the performance of the proposed method is not much sensitive to these parameters for a set of wide ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics:</head><p>We compare the performance of different methods using the clustering accuracy rate (ACC), normalized mutual information (NMI) <ref type="bibr" target="#b62">[63]</ref>, and Adjusted Rand Index (ARI) <ref type="bibr" target="#b63">[64]</ref> metrics.</p><p>In external validation of clustering methods where ground truth labels are available, a correct clustering is usually referred as assigning objects belonging to the same category in the ground truth to the same cluster, and objects belonging to different categories to different clusters. With that, ACC is defined as the number of data points correctly clustered divided by the total number of data points. The ARI metric, in addition to penalizing the misclustered data points, penalizes putting two objects with the same label in different clusters, and is adjusted such that a random clustering will score close to 0. The NMI captures the mutual information between the correct labels and the predicted labels, and is normalized between the range [0,1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Handwritten Digits</head><p>In the first set of experiments, we use the 10 classes (i.e. digits) from the MNIST and the USPS datasets. <ref type="figure" target="#fig_3">Figure 6</ref> (a) shows example images from these datasets. For the experiments with digits, we randomly sample 200 images per class from their training sets to reduce the computations and adjust the imbalance in the tests.</p><p>We randomly bundle the same class samples across the two datasets and assume they present two modalities (views) of a digit. One can see from <ref type="figure" target="#fig_3">Figure 6</ref> (a), that the needed receptive field for recognizing the digits in the MNIST and the USPS datasets is relatively large. Based on this logic, in the experiments with digits, we use large kernels in the encoders. The detailed network settings for these experiments are described in the Appendix. Note that some structures including the late fusion methods in <ref type="table" target="#tab_5">Table II</ref> and the affinity fusion method have more than one branches in some of their layers. <ref type="table" target="#tab_5">Table III</ref> shows the performance of deep subspace clustering per individual digits. This table reveals that the MNIST dataset is easier than the USPS dataset for the subspace clustering task. This observance coincides with the performance of other methods reported in <ref type="bibr" target="#b64">[65]</ref>.</p><p>Note that while the DSC method in <ref type="table" target="#tab_5">Table III</ref> shows thestate-of-the-art performance on both datasets, a successful multimodal method should enhance the performance by leveraging the information across the two modalities. <ref type="table" target="#tab_5">Table IV</ref> compares the performance of the multimodal methods in terms of accuracy, NMI and ARI metrics. We observe that most of the multimodal methods can successfully integrate the complementary information of the datasets in the subspace clustering task and provide a better performance in comparison to their unimodal counterpart. However, the proposed deep multimodal subspace clustering methods perform significantly better than the classical multimodal subspace clustering methods. In particular, the affinity fusion and late-addition methods can segment the digits with an accuracy of 95.15%, and NMI and ARI metric of above 90%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ARL Heterogeneous Face Dataset</head><p>To test our methods on clustering datasets with a large number of subjects, we use the ARL dataset <ref type="bibr" target="#b58">[59]</ref> which consists of facial images from 60 unique individuals in different spectrums and from different distances. This dataset has facial images in the visible domain as well as four different polarimetric thermal domains. Each subject has several wellaligned facial images per each modality. Sample images from this dataset are shown in <ref type="figure" target="#fig_3">Figure 6 (b)</ref>. <ref type="table">Table V</ref> compares the performance of subspace clustering methods on individual modalities in the ARL dataset. As expected, the visible modality shows better performance among the different spectrums. As the samples are well-aligned in this dataset, we see that most of the subspace clustering methods work well across all the modalities. In particular, the LRR method which takes the advantage of aligned data points, provides comparable results to the DSC method. Since the ARL dataset has multiple modalities, beside the early and late fusion structures, we also use an intermediate structure when designing the multimodal encoders. Hence, in this experiment, we add the following intermediate spatial fusion structure to the multimodal methods. Assuming the visible domain is the main modality, we integrate S0, S1 and S2 modalities in the second layer and combine their fused output with the DP samples in the third layer. Finally, we fuse the result with the visible domain at the last layer of the encoders.</p><p>The performances of deep multimodal subspace clustering methods are compared in <ref type="table" target="#tab_5">Table IV</ref>. We observe that most of the methods are able to leverage the complementary information of the different spectrums and provide a more accurate clustering in comparison to the unimodal performances. In particular, the affinity fusion method has the best performance, and late-concat and early-concat methods provide comparable results. This experiment clearly shows that our proposed methods can perform well even with a large number of subjects in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Facial Components</head><p>The Extended Yale-B dataset <ref type="bibr" target="#b59">[60]</ref> consists of 64 frontal images of 38 individuals under varying illumination conditions. This dataset is popular in subspace clustering studies <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b20">[21]</ref>. We crop the facial components (i.e. eyes, nose and mouth), and view them as weak modalities. In the biometrics literature, they are viewed as soft biometrics <ref type="bibr" target="#b65">[66]</ref>. To crop the facial components, we apply a fixed face mask as shown in <ref type="figure" target="#fig_4">Figure 7</ref> on all the facial images. The extracted facial regions are resized to 32 × 32 images. This experiment is especially important as the modalities do not share the spatial correspondence. For example, spatial locations in the mouth modality cannot be projected on the spatial positions in the nose modality. Sample images from this dataset are shown in <ref type="figure" target="#fig_3">Figure 6</ref> (c). The setting in this experiment can examine the proposed methods under the condition of spatially unrelated modalities.</p><p>The performance of subspace clustering methods on the individual facial components is summarized in <ref type="table" target="#tab_5">Table VI</ref>. We observe that the nose and the mouth modalities fail to provide good clustering results. On the other hand, DSC and AE+SSC perform well on the eye and the entire face modalities.</p><p>Since the mouth, nose, and eyes are considered as weak modalities, in the design of the intermediate spatial fusion we CMVFC <ref type="bibr" target="#b51">[52]</ref> TM-MSC <ref type="bibr" target="#b25">[26]</ref> MSSC <ref type="bibr" target="#b35">[36]</ref> MLRR <ref type="bibr" target="#b35">[36]</ref> KMSSC <ref type="bibr" target="#b35">[36]</ref> KMLRR <ref type="bibr" target="#b35">[36]</ref> Early-concat.   combine the two eyes, and the mouth and the nose separately in the second layer of the encoders, and fuse the result of their combinations in the third layer. Finally, we fuse the combined features with the face features in the fourth layer.</p><p>The performance of various multimodal subspace clustering methods are tabulated in <ref type="table" target="#tab_5">Table IV</ref>. It is worth highlighting several interesting observations from the results. As can be seen, the late-mpool and interm-mpool methods fail to segment the data points. That is because this fusion function at each spatial position returns the maximum of the activation values at the same spatial position between its input feature maps. Since the modalities do not share any spatial correspondence in this experiment, this function does not provide good performance. In addition, even though additive and concatenate fusion functions have provided good results in some cases, because of a similar reason their performances are highly related to the structure choices. For example, the additive function provides better performance with the intermediate fusion structure, while the concatenation works better with the late fusion structure choice. However, the affinity fusion provides the state-of-the-art clustering performance of above 99% accuracy, the NMI of 98.89% and ARI metric of 98.38%. This is mainly due to the fact that this method does not rely on the spatial correspondence among the modalities. <ref type="figure" target="#fig_5">Figure 8</ref> compares the affinity matrices of the first four subjects in the Extended Yale-B datasets. The affinity matrices are calculated from the self-expressive layer weights of their corresponding trained networks. The depicted affinity matrices in these figures are the result of a permutation being applied on the matrix so that data points of the same clusters are alongside each other. With this arrangement, a perfect affinity matrix should be block diagonal. <ref type="figure" target="#fig_5">Figure 8 (a)</ref> shows the affinity matrix corresponding to the DSC method for clustering faces. <ref type="figure" target="#fig_5">Figure 8 (b)</ref> shows this matrix for the multimodal subspace clustering with the latempool method. Note that this method fails to cluster the data, and as can be seen, its affinity matrix is not block-diagonal. <ref type="figure" target="#fig_5">Figure 8</ref> (c) and <ref type="figure" target="#fig_5">Figure 8 (d)</ref> show the affinity matrices of DSC <ref type="bibr" target="#b15">[16]</ref> AE+SSC SSC <ref type="bibr" target="#b20">[21]</ref> LRR <ref type="bibr" target="#b21">[22]</ref>  the late-concat and affinity fusion methods, respectively. We observe that both methods provide a solid block diagonal affinity matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Convergence study</head><p>To empirically show the convergence of our proposed method, in <ref type="figure" target="#fig_6">Figure 9</ref>, we show the objective function of the affinity fusion method and its clustering metrics vs iteration plot for solving <ref type="bibr" target="#b6">(7)</ref>. The reported values in <ref type="figure" target="#fig_6">Figure 9</ref> are normalized between zero and one. As can be seen from the figure, our algorithm converges in a few iterations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Regularization parameters</head><p>In this section, we analyze the sensibility of the proposed method to the regularization parameters λ 1 and λ 2 in the loss function <ref type="bibr" target="#b6">(7)</ref>. <ref type="figure" target="#fig_0">Figure 10</ref> shows the influence of these regularization parameters on the performance of the affinity fusion method on the Extended Yale-B dataset.</p><p>In <ref type="figure" target="#fig_0">Figure 10</ref> (a), we fix λ 2 = 1 and report the metrics with various λ 1 s over the range of [10 −4 , 10 4 ]. Similarly, in <ref type="figure" target="#fig_0">Figure 10</ref> (b), we fix λ 1 = 1 and this time change λ 2 in the similar range to analyze the influence of λ 2 on the performance of the method. As can be seen from the figure, in a wide range of values, the final performance of the method is not sensitive to the choice of parameters. The experimental setting suggested in <ref type="bibr" target="#b15">[16]</ref> also performed well in all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Performance with respect to different norms on the selfexpressive layer</head><p>In this section, we compare the performance of the proposed affinity fusion method by changing the p-norm on the selfexpressive layer in the optimization problem <ref type="bibr" target="#b6">(7)</ref>. <ref type="table" target="#tab_5">Table VII</ref> Metric · p p &lt; 0.3 p = 0.3 p = 1 p = reports the clustering metrics for the experiments with p = 0.3, p = 1, p = 1.5 and p = 2. As can be seen from this table, while experiments with p = 1, p = 1.5 and p = 2 have comparable performances, applying the p-norm with p = 0.3 does not provide sufficient result. It is worth mentioning that in our experiments with different norms with 0.3 &lt; p &lt; 1 the method showed instability, and for p &lt; 0.3 the minimization of (7) did not converge. The reason is that the norms with p &lt; 1 are nonconvex, and one might need additional regularizations to keep the optimization tractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We presented novel deep multimodal subspace clustering networks for clustering multimodal data. In particular, we presented two fusion techniques of spatial fusion and affinity fusion. We observed that spatial fusion methods in a deep multimodal subspace clustering task relay on spatial correspondences among the modalities. On the other hand, the proposed affinity fusion that finds a shared affinity across all the modalities provides the state-of-the-art results in all the conducted experiments. This method clusters the images in the Extended Yale-B dataset with an accuracy of 99.22%, normalized mutual information of 98.89% and adjusted rand index of 98.38%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX: NETWORK ARCHITECTURES</head><p>In this section, we provide the details of the network architecture used in the experiments. Note that all the plugged in convolutional layers use relu as well.              </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Different networks corresponding to digits experiments</head><formula xml:id="formula_20">Decoder 1 D1/deconv 1 L-recon 1 D1/deconv 1 1 × 3 × 3 × 15 (1,0) D1/deconv 2 D1/deconv 1 D1/deconv 2 1 × 5 × 5 × 10 (2,1) D1/deconv 3 D1/deconv 2 Recon 1 1 × 7 × 7 × 7 (2,1) Decoder 2 D2/deconv 2 L-recon 2 D2/deconv 1 1 × 3 × 3 × 15 (1,0) D2/deconv 2 D2/deconv 1 D2/deconv 2 1 × 5 × 5 × 10 (2,1) D2/deconv 3 D2/deconv 2 Recon 2 1 × 7 × 7 × 7 (2,1)</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>F (·) 1</head><label>1</label><figDesc>{G(x1), G(x2), · · · , G(xN X )} G(·) {y1, y2, · · · , yN Y } {F (y1), F (y2), · · · , F (yN Y )} Z⇥ e Z⇥ e ⇥s {G(x1), G(x2), · · · , G(xN X )} G(·) {y1, y2, · · · , yN Y } {F (y1), F (y2), · · · , F (yN Y )} F (·) 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Fig. 3 .</head><label>13</label><figDesc>Different network architectures corresponding to (a) early fusion, (b) intermediate fusion, and (c) late fusion.Note that in all the spatial fusion-based networks (a)-(c), the overall structure for the self-expressive layer and the multimodal decoder remain the same. (d) Network architecture corresponding to affinity fusion. In this case, the encoder and decoder are trained separately for each modality, but are forced to have the same self-expressive layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>An example of affinity fusion. Affinities corresponding to different modalities are combined to have only a single shared affinity. This method does not relay on spatial relation across different modalities. Instead, it aggregates the similarities among data points across different modalities and returns a shared affinity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FaceFig. 6 .</head><label>6</label><figDesc>Sample images from (a) MNIST</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Facial components are extracted by applying a fixed mask on the faces in the Extended Yale-B dataset<ref type="bibr" target="#b59">[60]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization of the affinity matrices for first four subjects in the Extended Yale-B dataset calculated from the self-expressive layer weight matrices in (a) unimodal clustering on faces using DSC. (b) The late-mpool method. (c) The late-concat method. (d) The affinity fusion method. Note that (b) shows a failure case of the spatial fusion methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>The affinity fusion method's loss function and the clustering metrics over different training epochs in the Yale-B facial components experiment. The reported values in this figure are normalized between zero and one. This figure shows the convergence of our objective function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>The affinity fusion method's performance through different parameter selections for λ 1 and λ 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE I DETAILS</head><label>I</label><figDesc>OF THE MULTIMODAL DATASETS THAT ARE USED IN THE EXPERIMENTS. NOTE THAT AS OPPOSED TO SUPERVISED METHODS, WE DO NOT SPLIT DATASETS TO TRAINING AND TESTING SETS IN A DEEP SUBSPACE CLUSTERING TASK.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE II SPATIALTABLE III THE</head><label>IIIII</label><figDesc>FUSION VARIATIONS THAT ARE USED IN THE EXPERIMENTS. PERFORMANCE OF SINGLE MODALITY SUBSPACE CLUSTERING METHODS ON DIGITS. EXPERIMENTS ARE EVALUATED BY AVERAGE ACC, NMI AND ARI OVER 5 RUNS. WE USE BOLDFACE FOR THE TOP PERFORMER. COLUMNS SPECIFY THE SINGLE MODALITY SUBSPACE CLUSTERING METHOD, AND ROWS SPECIFY THE MODALITY (MNIST OR USPS) AND CRITERIA.</figDesc><table><row><cell>MNIST</cell><cell>ACC NMI ARI</cell><cell>DSC[16] 92.05 87.07 84.60</cell><cell>AE+SSC 70.1 80.94 62.33</cell><cell>SSC[21] 67.5 71.64 57.03</cell><cell>LRR[22] 67.4 66.51 58.33</cell></row><row><cell>USPS</cell><cell>ACC NMI ARI</cell><cell>72.15 74.73 65.47</cell><cell>69.9 80.98 62.41</cell><cell>37.5 36.61 28.40</cell><cell>44.35 35.18 32.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IV THE</head><label>IV</label><figDesc>PERFORMANCE OF MULTIMODAL SUBSPACE CLUSTERING METHODS. EACH EXPERIMENT IS EVALUATED BY AVERAGE ACC, NMI AND ARI OVER 5</figDesc><table /><note>RUNS. WE USE BOLDFACE FOR THE TOP PERFORMER. COLUMNS OF THIS TABLE SHOW THE MULTIMODAL SUBSPACE CLUSTERING METHOD, AND THE ROWS LIST DATASETS AND CLUSTERING METRICS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>DATASET. EXPERIMENTS ARE EVALUATED BY AVERAGE ACC, NMI AND ARI OVER 5 RUNS. WE USE BOLDFACE FOR THE TOP PERFORMER. COLUMNS SPECIFY THE SINGLE MODALITY SUBSPACE CLUSTERING METHOD, AND ROWS SPECIFY THE FACIA COMPONENTS AND CRITERIA.</figDesc><table><row><cell>Visible</cell><cell>ACC NMI ARI</cell><cell>92.54 97.03 92.54</cell><cell>89.87 96.25 88.08</cell><cell>81.86 94.56 72.32</cell><cell>91.07 97.16 89.94</cell></row><row><cell>DP</cell><cell>ACC NMI ARI</cell><cell>91.81 97.60 91.69</cell><cell>89.08 97.17 87.48</cell><cell>63.2 83.59 47.98</cell><cell>89.4 95.71 85.47</cell></row><row><cell>S0</cell><cell>ACC NMI ARI</cell><cell>62.64 84.20 49.23</cell><cell>55.38 77.62 41.60</cell><cell>21.58 47.83 11.63</cell><cell>57.23 80.44 36.56</cell></row><row><cell>S1</cell><cell>ACC NMI ARI</cell><cell>91.72 97.09 89.55</cell><cell>86.21 96.55 86.16</cell><cell>54.68 78.60 42.69</cell><cell>86.12 95.13 85.62</cell></row><row><cell>S2</cell><cell>ACC NMI ARI</cell><cell>89.68 97.63 89.34</cell><cell>89.26 97.38 88.05</cell><cell>57.92 82.77 43.38</cell><cell>85.88 94.73 84.05</cell></row><row><cell cols="6">TABLE V THE PERFORMANCE OF SINGLE MODALITY SUBSPACE CLUSTERING METHODS ON ARL DATASET. EXPERIMENTS ARE EVALUATED BY AVERAGE ACC, NMI AND ARI OVER 5 RUNS. WE USE BOLDFACE FOR THE TOP PERFORMER. COLUMNS SPECIFY THE SINGLE MODALITY</cell></row><row><cell cols="6">SUBSPACE CLUSTERING METHOD, AND ROWS SPECIFY THE MODALITIES</cell></row><row><cell></cell><cell></cell><cell cols="2">AND CRITERIA.</cell><cell></cell><cell></cell></row><row><cell>Face</cell><cell>ACC NMI ARI</cell><cell>DSC[16] 96.82 94.82 91.31</cell><cell>AE+SSC 72.93 79.10 43.94</cell><cell>SSC[21] 72.78 79.17 42.90</cell><cell>LRR[22] 63.34 70.08 37.38</cell></row><row><cell>Right-eye</cell><cell>ACC NMI ARI</cell><cell>87.62 89.19 75.05</cell><cell>83.34 86.99 61.90</cell><cell>66.84 73.62 39.66</cell><cell>65.35 69.33 38.37</cell></row><row><cell>Left-eye</cell><cell>ACC NMI ARI</cell><cell>80.94 79.58 50.17</cell><cell>72.24 76.48 42.94</cell><cell>63.02 69.08 33.12</cell><cell>63.08 70.13 34.07</cell></row><row><cell>Nose</cell><cell>ACC NMI ARI</cell><cell>67.53 75.23 40.82</cell><cell>51.61 61.64 22.96</cell><cell>41.51 50.78 16.67</cell><cell>39.9 48.73 15.13</cell></row><row><cell>Mouth</cell><cell>ACC NMI ARI</cell><cell>76.86 76.42 43.90</cell><cell>67.42 72.91 40.52</cell><cell>56.07 64.11 25.71</cell><cell>62.92 67.28 33.02</cell></row><row><cell cols="6">TABLE VI THE PERFORMANCE OF SINGLE MODALITY SUBSPACE CLUSTERING METHODS ON EXTENDED YALE-B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>TABLE VII ANALYSIS OF DIFFERENT REGULARIZATION NORMS ON THE SELF-EXPRESSIVE LAYER. OUR EXPERIMENTS WITH p &lt; 0 DID NOT CONVERGED. THE RESULTS ARE 5-FOLD AVERAGE. WE USE BOLDFACE FOR THE TOP PERFORMER.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.5</cell><cell>p = 2</cell></row><row><cell>PUR NMI ARI</cell><cell>× × ×</cell><cell>09.32 18.64 02.38</cell><cell>99.13 98.78 98.20</cell><cell>99.17 98.84 98.29</cell><cell>99.22 98.89 98.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VIII EARLY</head><label>VIII</label><figDesc>-FUSION NETWORKS IN THE DIGITS EXPERIMENTS.</figDesc><table><row><cell>Feature Fusion</cell><cell>Layer Fusion 1</cell><cell>Input Image 1 Image 2</cell><cell>output Fusion 1</cell><cell>Kernel -</cell><cell>(stride, pad) -</cell></row><row><cell>Convolutional layers</cell><cell>Conv 1 Conv 2 Conv 3 Conv 4</cell><cell>Fusion 1 Conv 1 Conv 2 Conv 3</cell><cell>Conv 1 Conv 2 Conv 3 Latent</cell><cell>1 × 7 × 7 × 7 1 × 5 × 5 × 10 1 × 3 × 3 × 15 1 × 1 × 1 × 15</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Self-expressiveness</cell><cell>Θs</cell><cell>Latent</cell><cell>L-recon</cell><cell>4000000 Parameters</cell><cell>-</cell></row><row><cell>Multimodal Decoder</cell><cell>Decoder layers</cell><cell>L-recon</cell><cell>Recon 1 Recon 2</cell><cell>Details in Table XI</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE IX LATE</head><label>IX</label><figDesc>-FUSION NETWORKS IN THE DIGITS EXPERIMENTS.</figDesc><table><row><cell>Branch 1</cell><cell>Layer B1/Conv 1 B1/Conv 2 B1/Conv 3 B1/Conv 4</cell><cell>Input Image 1 B1/Conv 1 B1/Conv 2 B1/Conv 3</cell><cell>output B1/Conv 1 B1/Conv 2 B1/Conv 3 B1/out</cell><cell>Kernel 1 × 7 × 7 × 7 1 × 5 × 5 × 10 1 × 3 × 3 × 15 1 × 1 × 1 × 15</cell><cell>(stride, pad) (2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Branch 2</cell><cell>B2/Conv 1 B2/Conv 2 B2/Conv 3 B2/Conv 4</cell><cell>Image 2 B2/Conv 1 B2/Conv 2 B2/Conv 3</cell><cell>B2/Conv 1 B2/Conv 2 B2/Conv 3 B2/out</cell><cell>1 × 7 × 7 × 7 1 × 5 × 5 × 10 1 × 3 × 3 × 15 1 × 1 × 1 × 15</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Feature Fusion</cell><cell>Fusion 1</cell><cell>B1/out B2/out</cell><cell>Latent</cell><cell>-</cell><cell>-</cell></row><row><cell>Self-expressiveness</cell><cell>Θs</cell><cell>Latent</cell><cell>L-recon</cell><cell>4000000 Parameters</cell><cell>-</cell></row><row><cell>Multimodal Decoder</cell><cell>Decoder layers</cell><cell>L-recon</cell><cell>Recon 1 Recon 2</cell><cell>Details in Table XI</cell><cell></cell></row><row><cell cols="5">TABLE X AFFINITY FUSION NETWORKS IN THE DIGITS EXPERIMENTS.</cell><cell></cell></row><row><cell>Encoder 1</cell><cell>Layer B1/Conv 1 B1/Conv 2 B1/Conv 3 B1/Conv 4</cell><cell>Input Image 1 B1/Conv 1 B1/Conv 2 B1/Conv 3</cell><cell>output B1/Conv 1 B1/Conv 2 B1/Conv 3 Latent 1</cell><cell>Kernel 1 × 7 × 7 × 7 1 × 5 × 5 × 10 1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(stride, pad) (2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Encoder 2</cell><cell>B2/Conv 1 B2/Conv 2 B2/Conv 3 B2/Conv 4</cell><cell>Image 2 B2/Conv 1 B2/Conv 2 B2/Conv 3</cell><cell>B2/Conv 1 B2/Conv 2 B2/Conv 3 Latent 2</cell><cell>1 × 7 × 7 × 7 1 × 5 × 5 × 10 1 × 3 × 3 × 15 1 × 3 × 3 × 15</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Self-expressiveness layer</cell><cell>Common Θs</cell><cell>Latent 1 Latent 2</cell><cell>L-recon 1 L-recon 2</cell><cell>4000000 Parameters</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XI MULTIMODAL</head><label>XI</label><figDesc>DECODER IN THE DIGITS EXPERIMENTS.</figDesc><table><row><cell>Decoder 1</cell><cell>Layer D1/deconv 1 D1/deconv 2 D1/deconv 3</cell><cell>Input L-recon D1/deconv 1 D1/deconv 2</cell><cell>output D1/deconv 1 D1/deconv 2 Recon 1</cell><cell>Kernel 1 × 3 × 3 × 15 1 × 5 × 5 × 10 1 × 7 × 7 × 7</cell><cell>(stride, pad) (1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 2</cell><cell>D2/deconv 2 D2/deconv 2 D2/deconv 3</cell><cell>L-recon D2/deconv 1 D2/deconv 2</cell><cell>D2/deconv 1 D2/deconv 2 Recon 2</cell><cell>1 × 3 × 3 × 15 1 × 5 × 5 × 10 1 × 7 × 7 × 7</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell cols="6">B. Different networks corresponding to ARL experiments</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE XII EARLY</head><label>XII</label><figDesc>-FUSION NETWORKS IN THE ARL EXPERIMENTS.</figDesc><table><row><cell>Feature Fusion</cell><cell>Layer Fusion 1</cell><cell>Input Image 1 Image 2 Image 3 Image 4 Image 5</cell><cell>output Fusion 1</cell><cell>Kernel -</cell><cell>(stride, pad) -</cell></row><row><cell>Convolutional layers</cell><cell>Conv 1 Conv 2 Conv 3 Conv 4</cell><cell>Fusion 1 Conv 1 Conv 2 Conv 3</cell><cell>Conv 1 Conv 2 Conv 3 Latent</cell><cell>1 × 3 × 3 × 5 1 × 1 × 1 × 7 1 × 1 × 1 × 15 1 × 1 × 1 × 15</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Self-expressiveness</cell><cell>Θs</cell><cell>Latent</cell><cell>L-recon</cell><cell>4665600 Parameters</cell><cell>-</cell></row><row><cell>Multimodal Decoder</cell><cell>Decoder layers</cell><cell>L-recon</cell><cell>Recon 1 Recon 2 Recon 3 Recon 4 Recon 5</cell><cell>Details in Table XVI</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE XIII LATE</head><label>XIII</label><figDesc>-FUSION NETWORKS IN THE ARL EXPERIMENTS.</figDesc><table><row><cell>Branch 1</cell><cell>Layer B1/Conv 1 B1/Conv 2 B1/Conv 3 B1/Conv 4</cell><cell>Input Image 1 B1/Conv 1 B1/Conv 2 B1/Conv 3</cell><cell>output B1/Conv 1 B1/Conv 2 B1/Conv 3 B1/out</cell><cell>Kernel 1 × 3 × 3 × 5 1 × 1 × 1 × 7 1 × 1 × 1 × 15 1 × 1 × 1 × 15</cell><cell>(stride, pad) (2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Branch 2</cell><cell>B2/Conv 1 B2/Conv 2 B2/Conv 3 B2/Conv 4</cell><cell>Image 2 B2/Conv 1 B2/Conv 2 B2/Conv 3</cell><cell>B2/Conv 1 B2/Conv 2 B2/Conv 3 B2/out</cell><cell>1 × 3 × 3 × 5 1 × 1 × 1 × 7 1 × 1 × 1 × 15 1 × 1 × 1 × 15</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Branch 3</cell><cell>B3/Conv 1 B3/Conv 2 B3/Conv 3 B3/Conv 4</cell><cell>Image 3 B3/Conv 1 B3/Conv 2 B3/Conv 3</cell><cell>B3/Conv 1 B3/Conv 2 B3/Conv 3 B3/out</cell><cell>1 × 3 × 3 × 5 1 × 1 × 1 × 7 1 × 1 × 1 × 15 1 × 1 × 1 × 15</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Branch 4</cell><cell>B4/Conv 1 B4/Conv 2 B4/Conv 3 B4/Conv 4</cell><cell>Image 4 B4/Conv 1 B4/Conv 2 B4/Conv 3</cell><cell>B4/Conv 1 B4/Conv 2 B4/Conv 3 B4/out</cell><cell>1 × 3 × 3 × 5 1 × 1 × 1 × 7 1 × 1 × 1 × 15 1 × 1 × 1 × 15</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Branch 5</cell><cell>B5/Conv 1 B5/Conv 2 B5/Conv 3 B5/Conv 4</cell><cell>Image 5 B5/Conv 1 B5/Conv 2 B5/Conv 3</cell><cell>B5/Conv 1 B5/Conv 2 B5/Conv 3 B5/out</cell><cell>1 × 3 × 3 × 5 1 × 1 × 1 × 7 1 × 1 × 1 × 15 1 × 1 × 1 × 15</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Feature Fusion</cell><cell>Fusion 1</cell><cell>B1/out B2/out B3/out B4/out B5/out</cell><cell>Latent</cell><cell>-</cell><cell>-</cell></row><row><cell>Self-expressiveness</cell><cell>Θs</cell><cell>Latent</cell><cell>L-recon</cell><cell>4665600 Parameters</cell><cell>-</cell></row><row><cell>Multimodal Decoder</cell><cell>Decoder layers</cell><cell>L-recon</cell><cell>Recon 1 Recon 2 Recon 3 Recon 4 Recon 5</cell><cell>Details in Table XVI</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE XIV INTERMEDIATE</head><label>XIV</label><figDesc>SPATIAL FUSION NETWORKS IN THE ARL EXPERIMENTS.</figDesc><table><row><cell>Layer 1</cell><cell>Layer B1/Conv 1 B2/Conv 1 B3/Conv 1 B4/Conv 1 B5/Conv 1</cell><cell>Input Image 1 Image 2 Image 3 Image 4 Image 5</cell><cell>output B1/Conv 1 B2/Conv 1 B3/Conv 1 B4/Conv 1 B5/Conv 1</cell><cell>Kernel 1 × 3 × 3 × 5 1 × 3 × 3 × 5 1 × 3 × 3 × 5 1 × 3 × 3 × 5 1 × 3 × 3 × 5</cell><cell>(stride, pad) (2,1) (2,1) (2,1) (2,1) (2,1)</cell></row><row><cell>Feature Fusion</cell><cell>B345/Fusion</cell><cell>B3/Conv 1 B4/Conv 1 B5/Conv 1</cell><cell>B345/Fusion</cell><cell>-</cell><cell>-</cell></row><row><cell>Layer 2</cell><cell>B1/Conv 2 B2/Conv 2 B345/Conv 2</cell><cell>B1/Conv 1 B2/Conv 1 B345/Fusion</cell><cell>B1/Conv 2 B2/Conv 2 B345/Conv 2</cell><cell>1 × 1 × 1 × 7 1 × 1 × 1 × 7 1 × 1 × 1 × 7</cell><cell>(2,1) (2,1) (2,1)</cell></row><row><cell>Feature Fusion</cell><cell>B2345/Fusion</cell><cell>B345/Conv 2 B2/Conv 2</cell><cell>B2345/Fusion</cell><cell>-</cell><cell>-</cell></row><row><cell>Layer 3</cell><cell>B1/Conv 3 B2345/Conv 3</cell><cell>B1/Conv 2 B2345/Fusion</cell><cell>B1/Conv 3 B2345/Conv 3</cell><cell>1 × 1 × 1 × 15 1 × 1 × 1 × 15</cell><cell>(1,0) (1,0)</cell></row><row><cell>Feature Fusion</cell><cell>Ball/Fusion</cell><cell>B1/Conv 3 B2345/Conv 3</cell><cell>Ball/Fusion</cell><cell>-</cell><cell>-</cell></row><row><cell>Layer 4</cell><cell>Ball/Conv 4</cell><cell>Ball/Conv 3</cell><cell>Latent</cell><cell>1 × 1 × 1 × 15</cell><cell>(1,0)</cell></row><row><cell>Self-expressiveness</cell><cell>Θs</cell><cell>Latent</cell><cell>L-recon</cell><cell>4665600 Parameters</cell><cell>-</cell></row><row><cell>Multimodal Decoder</cell><cell>Decoder layers</cell><cell>L-recon</cell><cell>Recon 1 Recon 2 Recon 3 Recon 4 Recon 5</cell><cell>Details in Table XVI</cell><cell></cell></row><row><cell cols="6">C. Different networks corresponding to Extended Yale-B ex-</cell></row><row><cell>periments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE XV AFFINITY</head><label>XV</label><figDesc>FUSION NETWORKS IN THE ARL EXPERIMENTS.</figDesc><table><row><cell>Encoder 1</cell><cell>Layer B1/Conv 1 B1/Conv 2 B1/Conv 3 B1/Conv 4</cell><cell>Input Image 1 B1/Conv 1 B1/Conv 2 B1/Conv 3</cell><cell>output B1/Conv 1 B1/Conv 2 B1/Conv 3 Latent 1</cell><cell>Kernel 1 × 3 × 3 × 5 1 × 1 × 1 × 7 1 × 1 × 1 × 15 1 × 1 × 1 × 15</cell><cell>(stride, pad) (2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Encoder 2</cell><cell>B2/Conv 1 B2/Conv 2 B2/Conv 3 B2/Conv 4</cell><cell>Image 2 B2/Conv 1 B2/Conv 2 B2/Conv 3</cell><cell>B2/Conv 1 B2/Conv 2 B2/Conv 3 Latent 2</cell><cell>1 × 3 × 3 × 5 1 × 1 × 1 × 7 1 × 1 × 1 × 15 1 × 1 × 1 × 15</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Encoder 3</cell><cell>B3/Conv 1 B3/Conv 2 B3/Conv 3 B3/Conv 4</cell><cell>Image 3 B3/Conv 1 B3/Conv 2 B3/Conv 3</cell><cell>B3/Conv 1 B3/Conv 2 B3/Conv 3 Latent 3</cell><cell>1 × 3 × 3 × 5 1 × 1 × 1 × 7 1 × 1 × 1 × 15 1 × 1 × 1 × 15</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Encoder 4</cell><cell>B4/Conv 1 B4/Conv 2 B4/Conv 3 B4/Conv 4</cell><cell>Image 4 B4/Conv 1 B4/Conv 2 B4/Conv 3</cell><cell>B4/Conv 1 B4/Conv 2 B4/Conv 3 Latent 4</cell><cell>1 × 3 × 3 × 5 1 × 1 × 1 × 7 1 × 1 × 1 × 15 1 × 1 × 1 × 15</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Encoder 5</cell><cell>B5/Conv 1 B5/Conv 2 B5/Conv 3 B5/Conv 4</cell><cell>Image 5 B5/Conv 1 B5/Conv 2 B5/Conv 3</cell><cell>B5/Conv 1 B5/Conv 2 B5/Conv 3 Latent 5</cell><cell>1 × 3 × 3 × 5 1 × 1 × 1 × 7 1 × 1 × 1 × 15 1 × 1 × 1 × 15</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Self-expressiveness layer</cell><cell>Common Θs</cell><cell>Latent 1 Latent 2 Latent 3 Latent 4 Latent 5</cell><cell>L-recon 1 L-recon 2 L-recon 3 L-recon 4 L-recon 5</cell><cell>4665600 Parameters</cell><cell>-</cell></row><row><cell>Decoder 1</cell><cell>D1/deconv 1 D1/deconv 2 D1/deconv 3</cell><cell>L-recon 1 D1/deconv 1 D1/deconv 2</cell><cell>D1/deconv 1 D1/deconv 2 Recon 1</cell><cell>1 × 1 × 1 × 15 1 × 1 × 1 × 7 1 × 3 × 3 × 5</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 2</cell><cell>D2/deconv 2 D2/deconv 2 D2/deconv 3</cell><cell>L-recon 2 D2/deconv 1 D2/deconv 2</cell><cell>D2/deconv 1 D2/deconv 2 Recon 2</cell><cell>1 × 1 × 1 × 15 1 × 1 × 1 × 7 1 × 3 × 3 × 5</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 3</cell><cell>D3/deconv 2 D3/deconv 2 D3/deconv 3</cell><cell>L-recon 3 D3/deconv 1 D3/deconv 2</cell><cell>D3/deconv 1 D3/deconv 2 Recon 3</cell><cell>1 × 1 × 1 × 15 1 × 1 × 1 × 7 1 × 3 × 3 × 5</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 4</cell><cell>D4/deconv 2 D4/deconv 2 D4/deconv 3</cell><cell>L-recon 4 D4/deconv 1 D4/deconv 2</cell><cell>D4/deconv 1 D4/deconv 2 Recon 4</cell><cell>1 × 1 × 1 × 15 1 × 1 × 1 × 7 1 × 3 × 3 × 5</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 5</cell><cell>D5/deconv 2 D5/deconv 2 D5/deconv 3</cell><cell>L-recon 5 D5/deconv 1 D5/deconv 2</cell><cell>D5/deconv 1 D5/deconv 2 Recon 5</cell><cell>1 × 1 × 1 × 15 1 × 1 × 1 × 7 1 × 3 × 3 × 5</cell><cell>(1,0) (2,1) (2,1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE XVI MULTIMODAL</head><label>XVI</label><figDesc>DECODERS IN THE ARL EXPERIMENTS.</figDesc><table><row><cell>Decoder 1</cell><cell>Layer D1/deconv 1 D1/deconv 2 D1/deconv 3</cell><cell>Input L-recon D1/deconv 1 D1/deconv 2</cell><cell>output D1/deconv 1 D1/deconv 2 Recon 1</cell><cell>Kernel 1 × 1 × 1 × 15 1 × 1 × 1 × 7 1 × 3 × 3 × 5</cell><cell>(stride, pad) (1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 2</cell><cell>D2/deconv 2 D2/deconv 2 D2/deconv 3</cell><cell>L-recon D2/deconv 1 D2/deconv 2</cell><cell>D2/deconv 1 D2/deconv 2 Recon 2</cell><cell>1 × 1 × 1 × 15 1 × 1 × 1 × 7 1 × 3 × 3 × 5</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 3</cell><cell>D3/deconv 2 D3/deconv 2 D3/deconv 3</cell><cell>L-recon D3/deconv 1 D3/deconv 2</cell><cell>D3/deconv 1 D3/deconv 2 Recon 3</cell><cell>1 × 1 × 1 × 15 1 × 1 × 1 × 7 1 × 3 × 3 × 5</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 4</cell><cell>D4/deconv 2 D4/deconv 2 D4/deconv 3</cell><cell>L-recon D4/deconv 1 D4/deconv 2</cell><cell>D4/deconv 1 D4/deconv 2 Recon 4</cell><cell>1 × 1 × 1 × 15 1 × 1 × 1 × 7 1 × 3 × 3 × 5</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 5</cell><cell>D5/deconv 2 D5/deconv 2 D5/deconv 3</cell><cell>L-recon D5/deconv 1 D5/deconv 2</cell><cell>D5/deconv 1 D5/deconv 2 Recon 5</cell><cell>1 × 1 × 1 × 15 1 × 1 × 1 × 7 1 × 3 × 3 × 5</cell><cell>(1,0) (2,1) (2,1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE XVII EARLY</head><label>XVII</label><figDesc>-FUSION NETWORKS IN THE EXTENDED YALE-B EXPERIMENTS.</figDesc><table><row><cell>Feature Fusion</cell><cell>Layer Fusion 1</cell><cell>Input Image 1 Image 2 Image 3 Image 4 Image 5</cell><cell>output Fusion 1</cell><cell>Kernel -</cell><cell>(stride, pad) -</cell></row><row><cell>Convolutional layers</cell><cell>Conv 1 Conv 2 Conv 3 Conv 4</cell><cell>Fusion 1 Conv 1 Conv 2 Conv 3</cell><cell>Conv 1 Conv 2 Conv 3 Latent</cell><cell>1 × 5 × 5 × 10 1 × 3 × 3 × 20 1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Self-expressiveness</cell><cell>Θs</cell><cell>Latent</cell><cell>L-recon</cell><cell>5914624 Parameters</cell><cell>-</cell></row><row><cell>Multimodal Decoder</cell><cell>Decoder layers</cell><cell>L-recon</cell><cell>Recon 1 Recon 2</cell><cell>Details in</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table XXI</head><label>XXI</label><figDesc></figDesc><table><row><cell>Recon 3 Recon 4 Recon 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE XVIII LATE</head><label>XVIII</label><figDesc>-FUSION NETWORKS IN THE EXTENDED YALE-B EXPERIMENTS.</figDesc><table><row><cell>Branch 1</cell><cell>Layer B1/Conv 1 B1/Conv 2 B1/Conv 3 B1/Conv 4</cell><cell>Input Image 1 B1/Conv 1 B1/Conv 2 B1/Conv 3</cell><cell>output B1/Conv 1 B1/Conv 2 B1/Conv 3 B1/out</cell><cell>Kernel 1 × 5 × 5 × 10 1 × 3 × 3 × 20 1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(stride, pad) (2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Branch 2</cell><cell>B2/Conv 1 B2/Conv 2 B2/Conv 3 B2/Conv 4</cell><cell>Image 2 B2/Conv 1 B2/Conv 2 B2/Conv 3</cell><cell>B2/Conv 1 B2/Conv 2 B2/Conv 3 B2/out</cell><cell>1 × 5 × 5 × 10 1 × 3 × 3 × 20 1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Branch 3</cell><cell>B3/Conv 1 B3/Conv 2 B3/Conv 3 B3/Conv 4</cell><cell>Image 3 B3/Conv 1 B3/Conv 2 B3/Conv 3</cell><cell>B3/Conv 1 B3/Conv 2 B3/Conv 3 B3/out</cell><cell>1 × 5 × 5 × 10 1 × 3 × 3 × 20 1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Branch 4</cell><cell>B4/Conv 1 B4/Conv 2 B4/Conv 3 B4/Conv 4</cell><cell>Image 4 B4/Conv 1 B4/Conv 2 B4/Conv 3</cell><cell>B4/Conv 1 B4/Conv 2 B4/Conv 3 B4/out</cell><cell>1 × 5 × 5 × 10 1 × 3 × 3 × 20 1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Branch 5</cell><cell>B5/Conv 1 B5/Conv 2 B5/Conv 3 B5/Conv 4</cell><cell>Image 5 B5/Conv 1 B5/Conv 2 B5/Conv 3</cell><cell>B5/Conv 1 B5/Conv 2 B5/Conv 3 B5/out</cell><cell>1 × 5 × 5 × 10 1 × 3 × 3 × 20 1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Feature Fusion</cell><cell>Fusion 1</cell><cell>B1/out B2/out B3/out B4/out B5/out</cell><cell>Latent</cell><cell>-</cell><cell>-</cell></row><row><cell>Self-expressiveness</cell><cell>Θs</cell><cell>Latent</cell><cell>L-recon</cell><cell>5914624 Parameters</cell><cell>-</cell></row><row><cell>Multimodal Decoder</cell><cell>Decoder layers</cell><cell>L-recon</cell><cell>Recon 1 Recon 2 Recon 3 Recon 4 Recon 5</cell><cell>Details in Table XXI</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>TABLE XIX AFFINITY</head><label>XIX</label><figDesc>FUSION NETWORKS IN THE EXTENDED YALE-B EXPERIMENTS.</figDesc><table><row><cell>Encoder 1</cell><cell>Layer B1/Conv 1 B1/Conv 2 B1/Conv 3 B1/Conv 4</cell><cell>Input Image 1 B1/Conv 1 B1/Conv 2 B1/Conv 3</cell><cell>output B1/Conv 1 B1/Conv 2 B1/Conv 3 Latent 1</cell><cell>Kernel 1 × 5 × 5 × 10 1 × 3 × 20 1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(stride, pad) (2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Encoder 2</cell><cell>B2/Conv 1 B2/Conv 2 B2/Conv 3 B2/Conv 4</cell><cell>Image 2 B2/Conv 1 B2/Conv 2 B2/Conv 3</cell><cell>B2/Conv 1 B2/Conv 2 B2/Conv 3 Latent 2</cell><cell>1 × 5 × 5 × 10 1 × 3 × 3 × 20 1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Encoder 3</cell><cell>B3/Conv 1 B3/Conv 2 B3/Conv 3 B3/Conv 4</cell><cell>Image 3 B3/Conv 1 B3/Conv 2 B3/Conv 3</cell><cell>B3/Conv 1 B3/Conv 2 B3/Conv 3 Latent 3</cell><cell>1 × 5 × 5 × 10 1 × 3 × 3 × 20 1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Encoder 4</cell><cell>B4/Conv 1 B4/Conv 2 B4/Conv 3 B4/Conv 4</cell><cell>Image 4 B4/Conv 1 B4/Conv 2 B4/Conv 3</cell><cell>B4/Conv 1 B4/Conv 2 B4/Conv 3 Latent 4</cell><cell>1 × 5 × 5 × 10 1 × 3 × 3 × 20 1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Encoder 5</cell><cell>B5/Conv 1 B5/Conv 2 B5/Conv 3 B5/Conv 4</cell><cell>Image 5 B5/Conv 1 B5/Conv 2 B5/Conv 3</cell><cell>B5/Conv 1 B5/Conv 2 B5/Conv 3 Latent 5</cell><cell>1 × 5 × 5 × 10 1 × 3 × 3 × 20 1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(2,1) (2,1) (1,0) (1,0)</cell></row><row><cell>Self-expressiveness layer</cell><cell>Common Θs</cell><cell>Latent 1 Latent 2 Latent 3 Latent 4 Latent 5</cell><cell>L-recon 1 L-recon 2 L-recon 3 L-recon 4 L-recon 5</cell><cell>5914624 Parameters</cell><cell>-</cell></row><row><cell>Decoder 1</cell><cell>D1/deconv 1 D1/deconv 2 D1/deconv 3</cell><cell>L-recon 1 D1/deconv 1 D1/deconv 2</cell><cell>D1/deconv 1 D1/deconv 2 Recon 1</cell><cell>1 × 3 × 3 × 30 1 × 3 × 3 × 20 1 × 5 × 5 × 10</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 2</cell><cell>D2/deconv 2 D2/deconv 2 D2/deconv 3</cell><cell>L-recon 2 D2/deconv 1 D2/deconv 2</cell><cell>D2/deconv 1 D2/deconv 2 Recon 2</cell><cell>1 × 3 × 3 × 30 1 × 3 × 3 × 20 1 × 5 × 5 × 10</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 3</cell><cell>D3/deconv 2 D3/deconv 2 D3/deconv 3</cell><cell>L-recon 3 D3/deconv 1 D3/deconv 2</cell><cell>D3/deconv 1 D3/deconv 2 Recon 3</cell><cell>1 × 3 × 3 × 30 1 × 3 × 3 × 20 1 × 5 × 5 × 10</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 4</cell><cell>D4/deconv 2 D4/deconv 2 D4/deconv 3</cell><cell>L-recon 4 D4/deconv 1 D4/deconv 2</cell><cell>D4/deconv 1 D4/deconv 2 Recon 4</cell><cell>1 × 3 × 3 × 30 1 × 3 × 3 × 20 1 × 5 × 5 × 10</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 5</cell><cell>D5/deconv 2 D5/deconv 2 D5/deconv 3</cell><cell>L-recon 5 D5/deconv 1 D5/deconv 2</cell><cell>D5/deconv 1 D5/deconv 2 Recon 5</cell><cell>1 × 3 × 3 × 30 1 × 3 × 3 × 20 1 × 5 × 5 × 10</cell><cell>(1,0) (2,1) (2,1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>TABLE XX INTERMEDIATE</head><label>XX</label><figDesc>SPATIAL FUSION NETWORKS IN THE EXTENDED YALE-B EXPERIMENTS.</figDesc><table><row><cell>Layer 1</cell><cell>Layer B1/Conv 1 B2/Conv 1 B3/Conv 1 B4/Conv 1 B5/Conv 1</cell><cell>Input Image 1 Image 2 Image 3 Image 4 Image 5</cell><cell>output B1/Conv 1 B2/Conv 1 B3/Conv 1 B4/Conv 1 B5/Conv 1</cell><cell>Kernel 1 × 5 × 5 × 10 1 × 5 × 5 × 10 1 × 5 × 5 × 10 1 × 5 × 5 × 10 1 × 5 × 5 × 10</cell><cell>(stride, pad) (2,1) (2,1) (2,1) (2,1) (2,1)</cell></row><row><cell>Feature Fusion</cell><cell>B23/Fusion B45/Fusion</cell><cell>B2/Conv 1 B3/Conv 1 B4/Conv 1 B5/Conv 1</cell><cell>B23/Fusion B45/Fusion</cell><cell>--</cell><cell>--</cell></row><row><cell>Layer 2</cell><cell>B1/Conv 2 B23/Conv 2 B45/Conv 2</cell><cell>B1/Conv 1 B23/Fusion B45/Fusion</cell><cell>B1/Conv 2 B23/Conv 2 B45/Conv 2</cell><cell>1 × 3 × 3 × 20 1 × 3 × 3 × 20 1 × 3 × 3 × 20</cell><cell>(2,1) (2,1) (2,1)</cell></row><row><cell>Feature Fusion</cell><cell>B2345/Fusion</cell><cell>B23/Conv 2 B45/Conv 2</cell><cell>B2345/Fusion</cell><cell>-</cell><cell>-</cell></row><row><cell>Layer 3</cell><cell>B1/Conv 3 B2345/Conv 3</cell><cell>B1/Conv 2 B2345/Fusion</cell><cell>B1/Conv 3 B2345/Conv 3</cell><cell>1 × 3 × 3 × 30 1 × 3 × 3 × 30</cell><cell>(1,0) (1,0)</cell></row><row><cell>Feature Fusion</cell><cell>Ball/Fusion</cell><cell>B1/Conv 3 B2345/Conv 3</cell><cell>Ball/Fusion</cell><cell>-</cell><cell>-</cell></row><row><cell>Layer 4</cell><cell>Ball/Conv 4</cell><cell>Ball/Conv 3</cell><cell>Latent</cell><cell>1 × 3 × 3 × 30</cell><cell>(1,0)</cell></row><row><cell>Self-expressiveness</cell><cell>Θs</cell><cell>Latent</cell><cell>L-recon</cell><cell>5914624 Parameters</cell><cell>-</cell></row><row><cell>Multimodal Decoder</cell><cell>Decoder layers</cell><cell>L-recon</cell><cell>Recon 1 Recon 2 Recon 3 Recon 4 Recon 5</cell><cell>Details in Table XXI</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>TABLE XXI MULTIMODAL</head><label>XXI</label><figDesc>DECODER DETAILS IN THE EXTENDED YALE-B</figDesc><table><row><cell></cell><cell></cell><cell cols="2">EXPERIMENTS.</cell><cell></cell><cell></cell></row><row><cell>Decoder 1</cell><cell>Layer D1/deconv 1 D1/deconv 2 D1/deconv 3</cell><cell>Input L-recon D1/deconv 1 D1/deconv 2</cell><cell>output D1/deconv 1 D1/deconv 2 Recon 1</cell><cell>Kernel 1 × 3 × 3 × 30 1 × 3 × 3 × 20 1 × 5 × 5 × 10</cell><cell>(stride, pad) (1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 2</cell><cell>D2/deconv 2 D2/deconv 2 D2/deconv 3</cell><cell>L-recon D2/deconv 1 D2/deconv 2</cell><cell>D2/deconv 1 D2/deconv 2 Recon 2</cell><cell>1 × 3 × 3 × 30 1 × 3 × 3 × 20 1 × 5 × 5 × 10</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 3</cell><cell>D3/deconv 2 D3/deconv 2 D3/deconv 3</cell><cell>L-recon D3/deconv 1 D3/deconv 2</cell><cell>D3/deconv 1 D3/deconv 2 Recon 3</cell><cell>1 × 3 × 3 × 30 1 × 3 × 3 × 20 1 × 5 × 5 × 10</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 4</cell><cell>D4/deconv 2 D4/deconv 2 D4/deconv 3</cell><cell>L-recon D4/deconv 1 D4/deconv 2</cell><cell>D4/deconv 1 D4/deconv 2 Recon 4</cell><cell>1 × 3 × 3 × 30 1 × 3 × 3 × 20 1 × 5 × 5 × 10</cell><cell>(1,0) (2,1) (2,1)</cell></row><row><cell>Decoder 5</cell><cell>D5/deconv 2 D5/deconv 2 D5/deconv 3</cell><cell>L-recon D5/deconv 1 D5/deconv 2</cell><cell>D5/deconv 1 D5/deconv 2 Recon 5</cell><cell>1 × 3 × 3 × 30 1 × 3 × 3 × 20 1 × 5 × 5 × 10</cell><cell>(1,0) (2,1) (2,1)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that applying max-pooling and additive functions in pixel level features might result in information loss.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lambertian reflectance and linear subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="218" to="233" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Metrics and models for handwritten character recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="page" from="54" to="65" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A multibody factorization method for independently moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="179" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="52" to="68" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multibody grouping via orthogonal subspace decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>IEEE</publisher>
			<biblScope unit="page">252</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of natural images via lossy data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="212" to="225" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiscale hybrid linear models for lossy image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3655" to="3671" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Clustering appearances of objects under varying illumination conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>IEEE</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmenting motions of different types by unsupervised manifold clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and nondegenerate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2790" to="2797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A closed form solution to robust subspace estimation and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structured sparse subspace clustering: A unified optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable sparse subspace clustering by orthogonal matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3918" to="3927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep subspace clustering networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptive subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abavisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Identity, Security, and Behavior Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain adaptive subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abavisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference. BMVA</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust and efficient subspace segmentation via least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="347" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient dense subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="461" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering: Algorithm, theory, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2765" to="2781" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Latent space sparse and lowrank subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="691" to="701" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kernel sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Provable subspace clustering: When lrr meets ssc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Low-rank tensor constrained multiview subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1582" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-task lowrank affinity pursuit for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2439" to="2446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiview clustering via canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Co-regularized multi-view spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A subspace co-training framework for multi-view clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph learning for multiview clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convex multiview subspace learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1673" to="1681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-feature spectral clustering with minimax optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="4106" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust multi-view spectral clustering via low-rank and sparse decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2149" to="2155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spectral clustering with two views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>De Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Learning With Multiple Views</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multimodal sparse and low-rank subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abavisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="168" to="177" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep multimodal learning: A survey on recent advances and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramachandram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="96" to="108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Emonets: Multimodal deep learning approaches for emotion recognition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Froumenty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeep: A deep learning framework using motion features for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep multispectral semantic scene understanding of forested environments using multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="465" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">In2i: Unsupervised multiimage-to-image translation using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abavisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Large margin multi-modal triplet metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="370" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Efficient large-scale multi-modal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02892</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-view clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Diversity-induced multi-view subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Constrained multiview video face clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4381" to="4393" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Moddrop: adaptive multi-modal gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1692" to="1706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A deep metric for multimodal registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gutiérrez-Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multimodal neuroimaging feature learning for multiclass diagnosis of alzheimer&apos;s disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Fulham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1132" to="1140" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>AT&amp;T Labs</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A polarimetric thermal database for face recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Short</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Riggan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Gurton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thielke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gurram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">X</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical association</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">336</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep clustering with convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Joint sparse representation for robust multimodal biometrics recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="126" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">He is currently a Ph.D. candidate in Electrical and Computer Engineering at Rutgers University. During his Ph.D. he has spent time at Microsoft Research &amp; AI, and Tesla&apos;s Autopilot team in designing deep neural networks for various applications. His research interests include signal and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Abavisani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NJ, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Electrical and Computer Engineering (ECE) from Iran University of Science and Technology, Tehran, Iran in 2014, and Rutgers University</orgName>
		</respStmt>
	</monogr>
	<note>S&apos;11] received his M.S. degrees in. computer vision, machine learning and deep learning</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Prior to joining Hopkins, he was an A. Walter Tyson Assistant Professor in the Department of ECE at Rutgers University and a member of the research faculty at the University of Maryland Institute for Advanced Computer Studies (UMIACS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Patel</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Honorable Mention Paper Award at IAPR ICB 2018, two Best Student Paper Awards at IAPR ICPR 2018, and Best Poster Awards at BTAS 2015 and 2016. He is an Associate Editor of the IEEE Signal Processing Magazine, IEEE Biometrics Compendium, and serves on the Information Forensics and Security Technical Committee of the IEEE Signal Processing Society</title>
		<meeting><address><addrLine>College Park, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Department of Electrical and Computer Engineering (ECE) at Johns Hopkins University ; D. in Electrical Engineering from the University of Maryland ; Pi Mu Epsilon, and Phi Beta Kappa</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Walter Tyson Assistant Professorship Award</note>
	<note>He has received a number of awards including the 2016 ONR Young Investigator Award, the 2016 Jimmy Lin Award for Invention. He is a member of Eta Kappa Nu</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
