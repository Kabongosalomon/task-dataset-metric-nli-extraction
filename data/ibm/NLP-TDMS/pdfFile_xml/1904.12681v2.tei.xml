<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Fitting Degree Scoring Network for Monocular 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab of Intelligent Technologies and Systems</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab of Intelligent Technologies and Systems</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>jzhou@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Lab of Intelligent Technologies and Systems</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Fitting Degree Scoring Network for Monocular 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose to learn a deep fitting degree scoring network for monocular 3D object detection, which aims to score fitting degree between proposals and object conclusively. Different from most existing monocular frameworks which use tight constraint to get 3D location, our approach achieves high-precision localization through measuring the visual fitting degree between the projected 3D proposals and the object. We first regress the dimension and orientation of the object using an anchor-based method so that a suitable 3D proposal can be constructed. We propose FQNet, which can infer the 3D IoU between the 3D proposals and the object solely based on 2D cues. Therefore, during the detection process, we sample a large number of candidates in the 3D space and project these 3D bounding boxes on 2D image individually. The best candidate can be picked out by simply exploring the spatial overlap between proposals and the object, in the form of the output 3D IoU score of FQNet. Experiments on the KITTI dataset demonstrate the effectiveness of our framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>2D perception is far from the requirements for people's daily use as people live in a 3D world essentially. In many applications such as autonomous driving <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b13">14]</ref> and vision-based grasping <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27]</ref>, we usually need to reason about the 3D spatial overlap between objects in order to understand the realistic scene and take further action. 3D object detection is one of the most important problems in 3D perception, which requires solving a 9 Degree of Freedom (DoF) problem including dimension, orientation, and location. Although great progress has been made in stereo- <ref type="bibr">Figure 1</ref>. Comparison between our proposed method and tightconstraint-based method. The upper part is the commonly used approach by many existing methods, which neglects the spatial relation between 3D projection and object, and is very sensitive to the error brought by 2D detection. The lower part is our proposed pipeline which reasons about the 3D spatial overlap between 3D proposals and object so that it can get better detection result.</p><p>based <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, RGBD-based <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b34">35]</ref> and point-cloud-based <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">1,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b46">47]</ref> 3D object detection methods, monocular-image-based approaches have not been thoroughly studied yet, and most of existing works focus on the sub-problem, such as orientation estimation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b31">32]</ref>. The primary cause is that under monocular setting, the only cue is the appearance information in the 2D image, and the real 3D information is not available, which makes the problem ill-conditioned. However, in many cases, such as web images, mobile applications <ref type="bibr" target="#b14">[15]</ref>, and gastroscopy, the information of depth or point-cloud is not available or unaffordable. Moreover, in some extreme scenarios, other sensors can be broken. Therefore, considering the rich source of monocular images and the robustness requirements of the system, monocular 3D object detection problem is of crucial importance.</p><p>In monocular 3D object detection problem, dimension and orientation estimation are easier than location estima-tion, because the only available information, appearance, is strongly related to the former two sub-problems. On the contrary, it is not practical to directly regress location using a single image patch, since nearby and faraway objects with same pose are substantially identical in appearance.</p><p>Tight constraint <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25]</ref> is a commonly used method in monocular 3D object detection problem, which solves the location by placing the 3D proposal in the 2D bounding box compactly. However, the tight constraint has two significant drawbacks: 1) Image appearance cue is not used; thus it cannot benefit from a large number of labeled data in the training set. 2) Its performance highly depends on the 2D detection accuracy, as shown in <ref type="figure">Figure 1</ref>.</p><p>Inspired by the observation that people can easily distinguish the quality of 3D detection results through projecting these 3D bounding boxes on the 2D image and checking the relation between projections and object (fitting degree), we believe that exploring the 3D spatial overlap between proposals and ground-truth is the key to solve the location estimation problem. In this paper, we first regress the dimension and orientation of the object using an anchor-based method so that we can construct a suitable 3D proposal. The reason why we emphasize the importance of the regression step is that without an appropriate proposal, checking the fitting degree is impractical. Then, we propose Fitting Quality Network (FQNet) to infer 3D Intersection over Union (IoU) between 3D proposals and object only using 2D information. Our motivation is that though the 3D location is independent of 2D appearance, drawing the projection results on the 2D image can bring additional information for the convolutional neural network (CNN) to better understand the spatial relationship between the original 3D bounding boxes and the object. As long as the network learns the pattern of the projected 3D bounding boxes, it can gain the power of judging the relation between 3D proposals and the object, and achieve high-precision 3D perception. <ref type="figure">Figure 1</ref> gives an illustration of the essential difference between our idea and existing tight constraint-based method. We can see that our method is not sensitive to the error of 2D detection results. To the best of our knowledge, we are the first to solve the monocular 3D detection problem by exploring the fitting degree between 3D proposals and object. We conducted experiments on the challenging KITTI dataset and achieved state-of-the-art monocular 3D object detection performance, which demonstrates the effectiveness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Monocular 3D Object Detection: Monocular 3D object detection is much more difficult than 2D object detection because of the ambiguities arising from 2D-3D mapping. Many methods have taken the first step, which can roughly categorize into two classes: handcrafted approaches and deep learning based approaches.</p><p>Most of the early works belong to the handcrafted approaches, which concentrated on designing efficient handcrafted features. Payet and Todorovic <ref type="bibr" target="#b32">[33]</ref> used image contours as basic features and proposed mid-level features, called bags of boundaries (BOBs). Fidler et al. <ref type="bibr" target="#b17">[18]</ref> extended the Deformable Part Model (DPM) and represented an object class as a deformable 3D cuboid composed of faces and parts. Pepik et al. <ref type="bibr" target="#b33">[34]</ref> included viewpoint information and part-level 3D geometry information in the DPM and achieved robust 3D object representation. Although these handcrafted methods are very carefully designed and perform well on some scenarios, their generalization ability is still limited.</p><p>Deep learning based approaches aim to benefit from end-to-end training and a large amount of labeled data. Chen et al. <ref type="bibr" target="#b7">[8]</ref> generated a set of candidate class-specific object proposals on a ground prior and used a standard CNN pipeline to obtain high-quality object detections. Mousavian et al. <ref type="bibr" target="#b31">[32]</ref> presented MultiBin architecture for orientation regression and tight constraint to solve the 3D translation. Kundu et al. <ref type="bibr" target="#b24">[25]</ref> trained a deep CNN to map image regions to the full 3D shape and pose of all object instances in the image. Apart from these pure monocular methods, there are some other methods which use additional information for training. Xu and Chen <ref type="bibr" target="#b45">[46]</ref> proposed to fuse a monocular depth estimation module and achieved high-precision localization. Chabot et al. <ref type="bibr" target="#b5">[6]</ref> presented Deep MANTA (Deep Many-Tasks) for simultaneous vehicle detection, part localization and visibility characterization, but their method requires part locations and visibility annotations. In this paper, we propose a unified deep learning based pipeline, which does not require additional labels and can be trained end-to-end using a large number of augmented data.</p><p>Box Refinement Techniques: Our work has some similarities to the box refinement techniques, which focused on improving the localization accuracy. In 2D object detection, the most common method is the bounding box regression, which was first proposed by Felzenszwalb et al. <ref type="bibr" target="#b16">[17]</ref> and has been used in many state-of-the-art detectors, such as Faster R-CNN <ref type="bibr" target="#b35">[36]</ref> and SPP-net <ref type="bibr" target="#b21">[22]</ref>. Gidaris and Komodakis <ref type="bibr" target="#b19">[20]</ref> proposed LocNet to further improve the object-specific localization accuracy through assigning probabilities to the boundaries. While in monocular 3D object detection, the work on this level has been limited. Xiao et al. <ref type="bibr" target="#b44">[45]</ref> proposed to localize the corners using a discriminative parts-based detector. Many works also used stronger representations to achieve high-precision localization. For example, Zia et al. <ref type="bibr" target="#b47">[48]</ref> used a fine-grained 3D shape model, Xiang and Savarese <ref type="bibr" target="#b43">[44]</ref> introduced 3D aspectlet based on a piecewise planar object representation and Pero et al. <ref type="bibr" target="#b11">[12]</ref> proposed to use detailed geometric models. In our case, we stick to the 3D bounding box representation and learn the pattern of projected boxes on the 2D image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS-CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our framework only requires a single image as input and can output precise 3D detection results including dimension, orientation, and location of interested objects. <ref type="figure" target="#fig_0">Figure 2</ref> shows the overall pipeline, where we first perform regular 2D detection 1 and then use an anchor-based regression module to regress the dimension and orientation of each object based on the image patch cropped by 2D detection results. For the 3D location, we first use tight constraint to get a seed-candidate, then perform Gaussian dense sampling to generate a large number of candidates within a small range around the seed-candidate. To evaluate these candidates, we train a Fitting Quality Network (FQNet) to infer the 3D IoU between a large number of augmented samples and the ground truth. Therefore, by estimating the fitting degree between the candidates and the object, the candidate with the highest score will be chosen as the 3D detection result. Our framework separates the dimension and orientation estimation process from the location estimation because we consider that these tasks are fundamentally different (orientation and dimension are all appearance-related but the location is not) and the results of dimension and orientation regression have a significant impact on the location estimation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Regression Module</head><p>The input of our regression module is the cropped detection result of 2D object detection, while the output is the dimension and orientation of each object. For dimension, based on appearance, it is not difficult to infer the type of cars, and cars of the same type usually have similar length, width, and height. For orientation, it is a bit more complicated since there are global orientation and local orientation, but intuitively we can be sure that different orientation will show different appearance.</p><p>Dimension Estimation: To regress the dimension as accurate as possible, we propose an idea called anchor cuboid, whose philosophy is similar to MultiBin <ref type="bibr" target="#b31">[32]</ref>. We first perform k-means clustering on the training dataset to find the K cluster centers of the dimension and regard these cluster centers as 3D anchor cuboids. During the regression process, the regression module outputs confidence and offset for each 3D anchor cuboid respectively, so the output is a 4K-dimensional vector ([c i , ∆w i , ∆h i , ∆l i ], i = 1, ..., K), and the final regression result is the anchor cuboid with the highest confidence adding corresponding offsets. We optimize this module using the following loss function:</p><formula xml:id="formula_0">L d = − log σ(c i )+[1−IoU (A i +[∆w i , ∆h i , ∆l i ], G)]</formula><p>(1) where the i 2 means among K anchor cuboid, the i -th anchor cuboid A i has the maximum IoU with the groundtruth cuboid G, and [∆w i , ∆h i , ∆l i ] are the offsets in three different dimensions relative to anchor cuboid A i . The σ(·) is the softmax function:</p><formula xml:id="formula_1">σ(c i ) = e c i K i=1 e ci<label>(2)</label></formula><p>and the function IoU (·, ·) computes the 3D IoU between two center-aligned cuboids:</p><formula xml:id="formula_2">IoU (A, B) = volume(A) ∩ volume(B) volume(A) ∪ volume(B)<label>(3)</label></formula><p>There are two terms in the Equation <ref type="formula" target="#formula_11">(1)</ref>. For the first term, it encourages the module to give the highest confidence to the anchor cuboid which has the maximum IoU with the ground truth dimension G, and provides low confidence for other anchor cuboids at the same time. For the second term, it encourages the module to regress the offset between the best anchor cuboid and ground truth cuboid. Our loss function is volume-driven rather than dimension-driven, and the rationale is that it can take into account the information from three dimensions synthetically and avoid the situation that two dimensions have good estimation while one dimension is not well estimated.</p><p>Orientation Estimation: There are two orientations in the 3D object detection problem, global orientation and local orientation. The global orientation of an object is defined under the world coordinates and will not change with the camera pose, while the local orientation is defined under the camera coordinates and hinges on how the camera shots the object. <ref type="figure" target="#fig_9">Figure 3</ref> gives an illustration of these two kinds of orientation from the bird's view. In this paper, we focus on the estimation of the local orientation, which is evaluated in the KITTI dataset and directly related to the appearance.</p><p>For orientation regression, the range of the output is [−π, π], we use a similar idea to dimension regression and perform k-means clustering on the training set to get K cluster centers. The output is a 2K -dimensional vector ([c i , ∆θ i ], i = 1, ..., K ). We define the loss function as follows: <ref type="bibr">2</ref> It is worth mentioning that the value of i for each object can be computed and saved in the dataset before the training process.  <ref type="figure" target="#fig_9">Figure 3</ref>. In (a), the global orientations of the car are all facing the right, but the local orientation and appearance will change when the car moves from the left to the right. In (b), the global orientations of the car differ, but both the local orientation in the camera coordinates and the appearance remain unchanged. Hence, we can see that the appearance only has a relationship with the local orientation, and we can only regress the local orientation of the car based on the appearance. If we want to compute the global orientation using local orientation, we need to know the ray direction between the camera and the object, which can be calculated using the location of the object in the 2D image, more details are included in the supplementary materials.  where Θ i is the nearest anchor angle comparing to the ground truth local orientation θ G . The first term in the loss function L o is the same as L d which encourages the module to give high confidence to the nearest anchor angle, and the second term uses the cosine function to ensure that the offset θ i can be well regressed.</p><formula xml:id="formula_3">L o = − log σ(c i ) + [1 − cos(Θ i + ∆θ i − θ G )] (4)</formula><p>The idea behind our anchor-based regression is that directly regress a continuous variable is very difficult because the value range is vast. Using an anchor-based method, we can first solve a classification problem to choose the best anchor, and then regress an offset based on the anchor value; hence the value range that we need to regress can be significantly reduced. We show the concrete architecture of our proposed regression module in <ref type="figure" target="#fig_3">Figure 4</ref>.  <ref type="figure">Figure 5</ref>. The detailed architecture of FQNet. During pre-training process, the classification loss is used to train the convolutional layers and fully connected layers. The core innovation of our proposed FQNet is the input part, where our input image has additional artificial information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Location Estimation</head><p>After estimating the dimension and orientation, we can construct a 3D cuboid in 3D spatial space. We set the original coordinate of 8 vertices of the cuboid as:</p><formula xml:id="formula_4">     x = l 2 l 2 − l 2 − l 2 l 2 l 2 − l 2 − l 2 y = 0 0 0 0 −h −h −h −h z = w 2 − w 2 − w 2 w 2 w 2 − w 2 − w 2 w 2<label>(5)</label></formula><p>where l, h, w are the dimensions of the object. Assume that the 3D location is T = [T x , T y , T z ] T in the camera coordinate, according to the law of camera projection, we have</p><formula xml:id="formula_5">K R T 0 T 1     x i y i z i 1     = s   u i v i 1  <label>(6)</label></formula><p>where u i and v i are the 2D projected coordinates of the ith vertices, K is the intrinsic matrix, and R is the rotation matrix given by the global orientation θ:</p><formula xml:id="formula_6">R =   cos θ 0 sin θ 0 1 0 − sin θ 0 cos θ  <label>(7)</label></formula><p>We can use OpenCV toolkit to draw the projected 3D bounding box on the 2D image based on the 2D projected coordinates; thus, our FQNet can learn from these 2D projection patterns and obtain the ability to reason 3D spatial relations.</p><p>Dense Sampling: We use a sampling and evaluation framework. Sampling in the whole 3D space is timeconsuming, so we choose to get an approximate location (seed candidate) first. Many methods can achieve this goal, such as performing uniform sampling and searching the proposal whose 2D projection has the largest overlap with the 2D detection result. Here, we choose to use tight constraint similar in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25]</ref> to locate the seed candidate, because it is fast and relatively more accurate. After computing the location of seed-candidate, we can perform dense sampling within a small range around it. We model the distribution of the transition value in three axes as three independent Gaussian distribution as follows:</p><formula xml:id="formula_7">     ∆x ∼ N (µ x , σ x ) ∆y ∼ N (µ y , σ y ) ∆z ∼ N (µ z , σ z )<label>(8)</label></formula><p>where the mean and variance are all estimated using the 3D localization error in the training set. Thus, the i-th generated sample can be represented as S i (x + ∆x i , y + ∆y i , z + ∆z i , l, h, w, θ).</p><p>FQNet: The goal of FQNet is to evaluate the fitting quality between each sample S i and object. Even though it is challenging for CNN to infer the spatial relation between S i and ground-truth 3D location of the object, after projecting the sample S i on the 2D image, CNN can obtain this ability through learning the pattern that how well the edges and corners of projections are aligned with the specific part of the object. For quantitative analysis, we force the network to regress the 3D IoU between samples and the object. Denote the object image patch as I, the objective of FQNet can be formulated as follows:</p><formula xml:id="formula_8">Θ = arg min Θ ||F(I, S i |Θ) − IoU (I, S i )||<label>(9)</label></formula><p>where Θ denotes the parameters of FQNet. Since we have the ground-truth 3D location of each object in our training set, we can generate an almost unlimited number of samples by adding a known jitter to the original 3D location. One problem is that how can we guarantee that FQNet can capture the pattern of the projection, which we painted on the 2D image manually. To answer this question, we first pre-train our FQNet to execute a classification task, in which it has to decide whether the input image patch contains an artificially painted 3D bounding box projection or not. The architecture of our proposed FQNet and its pretraining version are shown in <ref type="figure">Figure 5</ref>. For the pre-training process, we use the cross-entropy loss for the classification task. For the 3D IoU regression, we use the smooth L1 loss, since it is less sensitive to outliers compared to the L2 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluated our method on the real-world KITTI dataset <ref type="bibr" target="#b18">[19]</ref>. The KITTI object detection benchmark includes 2D Object Detection Evaluation, 3D Object Detection Evaluation and Bird's Eye View Evaluation. There are 7481 training images and 7518 testing images in the dataset, and in each image, the object is annotated with observation angle (local orientation), 2D location, dimension, 3D location, and global orientation. However, only the labels in the KITTI training set are released, so we mainly conducted controlled experiments in the training set. Results are evaluated based on three levels of difficulty: Easy, Moderate, and Hard, which is defined according to the minimum bounding box height, occlusion and truncation grade. There are two commonly used train/val experimental settings: Chen et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> (train/val 1) and Xiang et al. <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> (train/val 2). Both splits guarantee that images from the training set and validation set are from different videos. We focused our ex- periment on the Car object category since, for Pedestrian and Cyclist, there is not enough data to train our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>For regression module, we used the ImageNet <ref type="bibr" target="#b12">[13]</ref> pretrained VGG-16 <ref type="bibr" target="#b37">[38]</ref> model with 224 × 224 input size to initialize the weights of convolutional layers. We used four anchor cuboids during dimension estimation and two anchor angles during orientation estimation. The module is trained with SGD using a fixed learning rate of 10 −4 with a batch size of 8. We performed data augmentation by adding color distortions, flipping images at random, and jittering 2D boxes with a translation of 0 ∼ 0.03x height and width.</p><p>For dense sampling process, we first sampled 1024 samples around the seed candidate. After discarding the samples more than half of which are outside the image plane, we kept 640 samples for evaluation.</p><p>For FQNet, we used the ImageNet pre-trained VGG-M model with 107 × 107 input size to initialize the weights of convolutional layers. The projection was drawn using green color and linewidth of 1. For the classification pre-training process, we sampled 256 positive samples and 256 negative samples to train the network for each iteration with learning rates 10 −4 for convolutional layers and 10 −3 for fully connected layers. For the 3D IoU regression, we mapped the labels from [0, 1] to [−1, 1] for data balance. We also added a random contour context information to the training image patch to increase the robustness of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effectiveness</head><p>To demonstrate that our proposed pipeline is not sensitive to the 2D detection results, we performed jitter to the MS-CNN 2D detection results and got a set of 2D bounding boxes with different Average Precision (AP). We com-  pared the 3D detection performance (3D AP) of our method with the tight-constraint-based baseline, and the results are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. We can see that the tight-constraintbased method is much more sensitive to the 2D detection AP, while our approach is much more robust. We also studied our FQNet module by evaluating the 3D IoU regression performance. After dividing all proposals into ten levels based on the 3D IoU, we computed the average regression error for each level and drew the histogram in <ref type="figure" target="#fig_6">Figure 7</ref>. We can see that for the samples whose 3D IoU is around 0.4 to 0.5, the average estimation error is the lowest, which is about 0.05. Therefore, we can be sure that our FQNet have the ability to evaluate candidates.</p><formula xml:id="formula_9">t/v 1 t/v 2 t/v 1 t/v 2 t/v 1 t/v 2 t/v 1 t/v 2 t/v 1 t/v 2 t/v 1 t/v 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Arts</head><p>We compared our proposed method with 6 recently proposed state-of-the-art 3D object detection methods on the KITTI benchmark, including 3DOP <ref type="bibr" target="#b8">[9]</ref>, Mono3D <ref type="bibr" target="#b7">[8]</ref>, 3DVP <ref type="bibr" target="#b41">[42]</ref>, SubCNN <ref type="bibr" target="#b42">[43]</ref>, Deep3DBox <ref type="bibr" target="#b31">[32]</ref> and 3D-RCNN <ref type="bibr" target="#b24">[25]</ref>. For fair comparisons, we used the detection results reported by the authors. All experiments were conducted on both two validation splits (different models are trained with the corresponding training sets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Orientation and Dimension Evaluation:</head><p>For orientation evaluation, we used the official metric of the KITTI dataset, which is the Average Orientation Similarity (AOS). Our results are summarized in <ref type="table">Table 1</ref>. From the results, we see that our method achieves state-of-the-art method on both train/val 1 and train/val 2 experimental settings. And we can see that especially for the train/val 1 on Easy and Moderate setting, our method has a significant improvement comparing with existing methods.</p><p>For dimension evaluation, we use the average error defined as:</p><formula xml:id="formula_10">E a = 1 N N i=1 (∆w 2 i + ∆h 2 i + ∆l 2 i )<label>(10)</label></formula><p>Since the detection results and the ground truth are not one-to-one equivalents, we have to find the corresponding object in the ground truth which is closest to the detection result for computing E a . Not all methods provided their experimental results, so we only compare our method with 3DOP <ref type="bibr" target="#b8">[9]</ref>, Mono3D <ref type="bibr" target="#b7">[8]</ref>, and Deep3DBox <ref type="bibr" target="#b31">[32]</ref>. Our results are summarized in <ref type="table" target="#tab_1">Table 3</ref>. We can see that our methods have the lowest estimation error with an average dimension estimation error of about 0.15 meters, which demonstrate the effectiveness of our anchor based regression module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Location Evaluation:</head><p>For location evaluation, we first reported our results on the official evaluation metrics  from KITTI Birds Eye View Evaluation, where AP for the birds eye view boxes is evaluated, which are obtained by projecting the 3D boxes to the ground plane and neglect the location precision on the Y-axis. From <ref type="table">Table 2</ref>, we can see that our method outperformed Mono3D <ref type="bibr" target="#b7">[8]</ref> and Deep3DBox <ref type="bibr" target="#b31">[32]</ref> by a significant margin of about 3% improvement. Since 3DOP <ref type="bibr" target="#b8">[9]</ref> is a stereo-based method that can obtain depth information directly, so its performance is much better than pure monocular based methods. We also conducted experiments on 3D Object Detection Evaluation, where the 3D AP metric is used to evaluate the full 3D bounding boxes. From <ref type="table" target="#tab_2">Table 4</ref>, we can see that our method ranks first among pure monocular based methods, and we even outperformed stereo-based 3DOP when 3D IoU threshold is set to 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>Apart from drawing the 3D detection boxes on 2D images, we also projected the 3D detection boxes in the 3D space for better visualization. As shown in <ref type="figure" target="#fig_7">Figure 8</ref>, our approach can fit the object well and achieve high-precision 3D perception in various scenes with only one monocular image as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have proposed a unified pipeline for monocular 3D object detection. By using an anchor-based regression method, we achieved a high-precision dimension and orientation estimation. Then we perform dense sampling in the 3D space and project these samples on a 2D image. Through measuring the relation between the projections and object, our FQNet successfully estimates the 3D IoU and filters the suitable candidate. Both quantitative and qualitative results have demonstrated that our proposed method outperforms the state-of-the-art monocular 3D object detection methods. How to extend our monocular 3D object detection method for monocular 3D object tracking seems to be interesting future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Global Orientation</head><p>Here we elaborate on more details about how to use local orientation to compute global orientation. In Section 3.1, we have explained how to regress the local orientation θ local , based on the appearance of the object. However, in Section 3.2, to calculate the rotation matrix R, the global orientation θ global is required. Denote the ray direction of the object as θ ray , as shown in <ref type="figure">Figure 1</ref>, we have:</p><formula xml:id="formula_11">θ ray = θ local + (−θ global )<label>(1)</label></formula><p>Therefore, we only need to calculate θ ray first, which is approximately proportional to the distance between the object center and the image center:</p><formula xml:id="formula_12">θ ray ≈ k( width 2 − x 2 + x 1 2 )<label>(2)</label></formula><p>where width is the width of the 2D image, x 1 and x 2 are the left and right boundaries of the 2D bounding box of the object (a more precise object center should be the center of the eight projected corners, which is</p><formula xml:id="formula_13">8 i=1 xi 8</formula><p>), and k is the proportionality coefficient (Strictly speaking, the relationship between θ ray and the distance between object center and image center is the arctan() function, and we used proportional function here for a coarse estimate).</p><p>We regressed the value of k using the training data by minimizing the following objective:</p><formula xml:id="formula_14">k = arg min k N i=1 (y i − k( width 2 − x i2 + x i1 2 )) 2 (3)</formula><p>where N is the number of training samples. * Corresponding Author C − <ref type="figure">Figure 1</ref>. Illustration of different orientations in bird view. For θ global , if the rotation is counterclockwise, it will have a negative measure. For θ local , its value is 0 degree when the camera captures the right side of the object. The positive directions of θ local and θray are marked with arrows in the image.</p><p>We plotted the data points in <ref type="figure" target="#fig_0">Figure 2</ref> for better visualization, and the regression result is k = 0.0012408.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tight Constraint</head><p>As mentioned in Section 3.2, we used tight constraint similar in <ref type="bibr">[3,</ref><ref type="bibr">2]</ref> to obtain the location of seed candidate. Here we provide more details about how to solve the location. The basic idea is to fit the projected 3D bounding box in the 2D bounding box tightly, which means the circum-  Using regression module, we have estimated the dimension and orientation of the object, which means the 3D model is fixed and only the location is unknown. Denote the location as T = [t x , t y , t z ] T , we have:</p><formula xml:id="formula_15">K R T 0 T 1 X 3d = X 2d<label>(4)</label></formula><p>and Equation <ref type="formula" target="#formula_15">(4)</ref> can be rewritten into the form of Ax = b:</p><formula xml:id="formula_16">K I RX 3d 0 T 1     t x t y t z 1     = X 2d<label>(5)</label></formula><p>For each boundary of the 2D bounding box (x 1 , y 1 , x 2 , y 2 ), it has to be touched by one projected vertices, so there are 8 4 = 4096 settings in total 1 . For each setting, we can have 4 equations, but we only have 3 unknown, so it is an overdetermined equation which can be solved by least square method. We computed the residual for each setting, discarded the settings in which the projection of the 3D bounding box exceeded the 2D bouding box, and chose the setting with the least error as the solution. <ref type="figure" target="#fig_3">Figure 4</ref> shows the accuracy against latency for different model configurations. In Mean+Tight, mean dimension and mode of the orientation 2 are used as estimation, and location is computed using tight constraint. Mean+Tight+FQNet applies our FQNet to refine the Mean+Tight detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Tradeoffs of the System</head><p>RM+Tight refers to our regression module. RM+Tight+FQNet is the final version of our proposed method. Our method achieved about 30% improvement compared to Mean+Tight baseline, with a computation burden of 0.009s. We found that solving the tight constraints (0.145s) cost much more time than the forward process of RM and FQNet, so exploring a faster way to locate the seed candidate can speed up reasoning process dramatically. All experiments were conducted based on the MS-CNN <ref type="bibr" target="#b4">[5]</ref> 2D detection boxes which costs 0.4s. We also reported the model size of our proposed RM and FQNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contributions of Each Module</head><p>The proposed method can be divided into two modules: regression module (RM) and FQNet. As shown in <ref type="figure" target="#fig_3">Figure  4</ref>, two modules worked together to achieve the final results. We found that 2D regression accuracy had a great effect on the location estimation process. When using Mean+Tight as seed candidate, FQNet only gained 2% improvement. While with RM+Tight, FQNet can achieve 8% improvement. Another experiment also demonstrated that 2D regression accuracy is of great importance: when we replaced RM with ground truth dimension and orientation, GT+Tight can achieve 54.47% in 3D AP. That is why we studied regression module to increase regression accuracy as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Implementation Details</head><p>For the 2D detection method, we used the code provided by MS-CNN <ref type="bibr">[1]</ref> to get 2D bounding boxes, and filtered the results through abandoning the detections whose score is lower than 0.1.</p><p>Our experiments were conducted on the following specifications: i7-4790K CPU, 32GB RAM, and NVIDIA GTX1080Ti GPU using Python and PyTorch toolbox. In our settings, the whole pipeline runs at about 0.3 3 frames per second .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Video Demo</head><p>We refer the reader to the attached video for more visualizations of our results. We note that to create the video no temporal information is used, and all results are obtained using a single monocular image. The sequences are from the KITTI object tracking benchmark, with index of 0001, 0004, and 0007.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The overall pipeline of our proposed monocular 3D object detection method, which only requires a single RGB image as input, and can achieve a 3D perception of the objects in the scene. We show some intermediate results on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>C</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The architecture of our regression module. There are two branches in the fully connected layers, for dimension regression and orientation regression respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>3D detection performance comparison between our with the tight-constriant-based baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Histogram of average regression error for proposals from ten levels of 3D IoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>The visualization result of our monocular 3D object detection method. We draw detection results in both 2D image and 3D space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 .</head><label>2</label><figDesc>Visualization of the relation between θray and the distance to the center of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of the tight constraint. scribed rectangle of the projected 3D bounding box should be as close to the 2D bounding box as possible (Figure 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 .</head><label>4</label><figDesc>Speed versus accuracy on the KITTI validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparisons of the Average Orientation Similarity (AOS) with the state-of-the-art methods on the KITTI dataset. Comparisons of the 2D AP with the state-of-the-art methods on the KITTI Birds Eyed View validation dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Easy train/val 1 train/val 2</cell><cell>test</cell><cell cols="2">Moderate train/val 1 train/val 2</cell><cell>test</cell><cell cols="2">Hard train/val 1 train/val 2</cell><cell>test</cell></row><row><cell>3DOP [9] Mono3D [8] 3DVP [42] SubCNN [43] Deep3DBox [32] 3D-RCNN [25] Our Method</cell><cell>91.58 91.90 ---90.70 97.28</cell><cell>--78.99 94.55 97.50 97.70 97.57</cell><cell>91.44 91.01 86.92 90.67 92.90 89.98 92.58</cell><cell>85.80 86.28 ---89.10 93.70</cell><cell>--65.73 85.03 96.30 96.50 96.70</cell><cell>86.10 86.62 74.59 88.62 88.75 89.25 88.72</cell><cell>76.80 77.09 ---79.50 79.25</cell><cell>--54.67 72.21 80.40 80.70 80.45</cell><cell>76.52 76.84 64.11 78.68 76.76 80.07 76.85</cell></row><row><cell>Method</cell><cell>Easy</cell><cell cols="2">IoU = 0.5 Moderate</cell><cell>Hard</cell><cell></cell><cell>Easy</cell><cell>IoU = 0.7 Moderate</cell><cell>Hard</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparisons of the Average Error of dimension estimation with state-of-the-art methods on the KITTI validation dataset.</figDesc><table><row><cell>3DOP [9] Mono3D [8] Deep3DBox [32]</cell><cell cols="2">55.04 30.50 -</cell><cell>--30.02</cell><cell>41.25 22.39 -</cell><cell>--23.77</cell><cell>34.55 19.16 -</cell><cell>--18.83</cell><cell>12.63 5.22 -</cell><cell>--9.99</cell><cell>9.49 5.19 -</cell><cell>--7.71</cell><cell>7.59 4.13 -</cell><cell>--5.30</cell></row><row><cell>Our Method</cell><cell cols="13">32.57 33.37 24.60 26.29 21.25 21.57 9.50 10.45 8.02 8.59 7.71 7.43</cell></row><row><cell>Method</cell><cell></cell><cell cols="4">train/val 1 train/val 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3DOP [9] Mono3D [8] Deep3DBox [32] Our Method</cell><cell cols="2">0.3527 0.4251 -0.1698</cell><cell>--0.1934 0.1465</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparisons of the 3D AP with the state-of-the-art methods on the KITTI 3D Object validation dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Easy t/v 1 t/v 2</cell><cell cols="2">IoU = 0.5 Moderate t/v 1 t/v 2</cell><cell cols="8">IoU = 0.7 Moderate t/v 2 t/v 1 t/v 2 t/v 1 t/v 2 t/v 1 t/v 2 Hard Easy Hard t/v 1</cell></row><row><cell>3DOP [9] Mono3D [8] Deep3DBox [32]</cell><cell>46.04 25.19 -</cell><cell>--27.04</cell><cell>34.63 18.20 -</cell><cell>--20.55</cell><cell>30.09 15.52 -</cell><cell>--15.88</cell><cell>6.55 2.53 -</cell><cell>--5.85</cell><cell>5.07 2.31 -</cell><cell>--4.10</cell><cell>4.10 2.31 -</cell><cell>--3.84</cell></row><row><cell>Our Method</cell><cell cols="12">28.16 28.98 21.02 20.71 19.91 18.59 5.98 5.45 5.50 5.11 4.75 4.45</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use a popular 2D detection algorithm<ref type="bibr" target="#b4">[5]</ref> to produce the 2D detection results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Beijing National Research Center for Information Science and Technology, China 4 Noah's Ark Lab, Huawei liulj17@mails.tsinghua.edu.cn {lujiwen,jzhou}@tsinghua.edu.cn, {xuchunjing,tian.qi1}@huawei.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This number can be reduced to 64 by using some prior knowledge, such as normally cars won't turn upside down, so vertices 1 won't touch y 1 .2 Most cars in the KITTI dataset are directed towards the street.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">since for different testing image, there are different numbers of objects, so our running speed may fluctuate.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61822603, Grant U1813218, Grant U1713214, Grant 61672306, and Grant 61572271.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depthcn: Vehicle detection using 3d-lidar and convnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asvadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Premebida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">J</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Group-sensitive triplet embedding for vehicle reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2385" to="2399" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beltran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guindel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cruzado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De La Escalera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01195</idno>
		<title level="m">Birdnet: a 3d object detection framework from lidar information</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vision-based intelligent vehicles: State of the art and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fascioli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teulière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding bayesian rooms using composite 3d object models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bowdish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kermgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ai oriented large-scale video management for smart city: Technologies, standards and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overview of the mpegcdvs standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="179" to="194" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d object detection and viewpoint estimation with a deformable 3d cuboid model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Locnet: Improving localization accuracy for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="789" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aligning 3d models to rgb-d images of cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Computer vision for autonomous vehicles: Problems, datasets and state-ofthe-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05519</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02294</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic 3d scene analysis from a moving vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">IJRR</biblScope>
			<biblScope unit="page" from="421" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07916</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3d object detection with rgbd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ojea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09312</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From contours to 3d object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Payet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multi-view and 3d deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2232" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08488</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">-d data. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robotic grasping of novel objects using vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Driemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Subcategoryaware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Object detection by 3d aspectlets and occlusion reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="530" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Localizing 3d cuboids in single-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rt3d: Real-time 3-d vehicle detection in lidar point cloud for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3434" to="3440" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards scene understanding with detailed 3d object representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="188" to="203" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
