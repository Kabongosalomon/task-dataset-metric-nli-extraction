<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast-Slow Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-06-09">9 Jun 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asier</forename><surname>Mujika</surname></persName>
							<email>asierm@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Meier</surname></persName>
							<email>meierflo@inf.ethz.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science ETH</orgName>
								<address>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelika</forename><surname>Steger</surname></persName>
							<email>steger@inf.ethz.ch</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science ETH</orgName>
								<address>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast-Slow Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-06-09">9 Jun 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Processing sequential data of variable length is a major challenge in a wide range of applications, such as speech recognition, language modeling, generative image modeling and machine translation. Here, we address this challenge by proposing a novel recurrent neural network (RNN) architecture, the Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both multiscale RNNs and deep transition RNNs as it processes sequential data on different timescales and learns complex transition functions from one time step to the next. We evaluate the FS-RNN on two character level language modeling data sets, Penn Treebank and Hutter Prize Wikipedia, where we improve state of the art results to 1.19 and 1.25 bits-per-character (BPC), respectively. In addition, an ensemble of two FS-RNNs achieves 1.20 BPC on Hutter Prize Wikipedia outperforming the best known compression algorithm with respect to the BPC measure. We also present an empirical investigation of the learning and network dynamics of the FS-RNN, which explains the improved performance compared to other RNN architectures. Our approach is general as any kind of RNN cell is a possible building block for the FS-RNN architecture, and thus can be flexibly applied to different tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Processing, modeling and predicting sequential data of variable length is a major challenge in the field of machine learning. In recent years, recurrent neural networks (RNNs) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref> have been the most popular tool to approach this challenge. RNNs have been successfully applied to improve state of the art results in complex tasks like language modeling and speech recognition. A popular variation of RNNs are long short-term memories (LSTMs) <ref type="bibr" target="#b17">[18]</ref>, which have been proposed to address the vanishing gradient problem <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17]</ref>. LSTMs maintain constant error flow and thus are more suitable to learn long-term dependencies compared to standard RNNs.</p><p>Our work contributes to the ongoing debate on how to interconnect several RNN cells with the goals of promoting the learning of long-term dependencies, favoring efficient hierarchical representations of information, exploiting the computational advantages of deep over shallow networks and increasing computational efficiency of training and testing. In deep RNN architectures, RNNs or LSTMs are stacked layer-wise on top of each other <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b10">11]</ref>. The additional layers enable the network to learn complex input to output relations and encourage a efficient hierarchical representation of information. In multiscale RNN architectures <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b5">6]</ref>, the operation on different timescales is enforced by updating the higher layers less frequently, which further encourages an efficient hierarchical representation of information. The slower update rate of higher layers leads to computationally efficient implementations and gives rise to short gradient paths that favor the learning of long-term dependencies. In deep transition RNN architectures, intermediate sequentially connected layers are interposed between two consecutive hidden states in order to increase the depth of the transition function from one time step to the next, as for example in deep transition networks <ref type="bibr" target="#b29">[30]</ref> or Recurrent Highway Networks (RHN) <ref type="bibr" target="#b41">[42]</ref>. The intermediate layers enable the network to learn complex non-linear transition functions. Thus, the model exploits the fact that deep models can represent some functions exponentially more efficiently than shallow models <ref type="bibr" target="#b3">[4]</ref>. We interpret these networks as shallow networks that share the hidden state, rather than a single deep network. Despite being the same in practice, this interpretation makes it trivial to convert any RNN cell to a deep RNN by connecting the cells sequentially, see <ref type="figure" target="#fig_1">Figure 2b</ref>.</p><p>Here, we propose the Fast-Slow RNN (FS-RNN) architecture, a novel way of interconnecting RNN cells, that combines advantages of multiscale RNNs and deep transition RNNs. In its simplest form the architecture consists of two sequentially connected, fast operating RNN cells in the lower hierarchical layer and a slow operating RNN cell in the higher hierarchical layer, see <ref type="figure" target="#fig_0">Figure 1</ref> and Section 3. We evaluate the FS-RNN on two standard character level language modeling data sets, namely Penn Treebank and Hutter Prize Wikipedia. Additionally, following <ref type="bibr" target="#b29">[30]</ref>, we present an empirical analysis that reveals advantages of the FS-RNN architecture over other RNN architectures.</p><p>The main contributions of this paper are:</p><p>• We propose the FS-RNN as a novel RNN architecture.</p><p>• We improve state of the art results on the Penn Treebank and Hutter Prize Wikipedia data sets.</p><p>• We surpass the BPC performance of the best known text compression algorithm evaluated on Hutter Prize Wikipedia by using an ensemble of two FS-RNNs.</p><p>• We show empirically that the FS-RNN incorporates strengths of both multiscale RNNs and deep transition RNNs, as it stores long-term dependencies efficiently and it adapts quickly to unexpected input.</p><p>• We provide our code in the following URL https://github.com/amujika/Fast-Slow-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In the following, we review the work that relates to our approach in more detail. First, we focus on deep transition RNNs and multiscale RNNs since these two architectures are the main sources of inspiration for the FS-RNN architecture. Then, we discuss how our approach differs from these two architectures. Finally, we review other approaches that address the issue of learning long-term dependencies when processing sequential data.</p><p>Pascanu et al. <ref type="bibr" target="#b29">[30]</ref> investigated how a RNN can be converted into a deep RNN. In standard RNNs, the transition function from one hidden state to the next is shallow, that is, the function can be written as one linear transformation concatenated with a point wise non-linearity. The authors added intermediate layers to increase the depth of the transition function, and they found empirically that such deeper architectures boost performance. Since deeper architectures are more difficult to train, they equip the network with skip connections, which give rise to shorter gradient paths (DT(S)-RNN, see <ref type="bibr" target="#b29">[30]</ref>). Following a similar line of research, Zilly et al. <ref type="bibr" target="#b41">[42]</ref> further increased the transition depth between two consecutive hidden states. They used highway layers <ref type="bibr" target="#b36">[37]</ref> to address the issue of training deep architectures. The resulting RHN <ref type="bibr" target="#b41">[42]</ref> achieved state of the art results on the Penn Treebank and Hutter Prize Wikipedia data sets. Furthermore, a vague similarity to deep transition networks can be seen in adaptive computation <ref type="bibr" target="#b11">[12]</ref>, where an LSTM cell learns how many times it should update its state after receiving the input to produce the next output.</p><p>Multiscale RNNs are obtained by stacking multiple RNNs with decreasing order of update frequencies on top of each other. Early attempts proposed such architectures for sequential data compression <ref type="bibr" target="#b33">[34]</ref>, where the higher layer is only updated in case of prediction errors of the lower layer, and for sequence classification <ref type="bibr" target="#b8">[9]</ref>, where the higher layers are updated with a fixed smaller frequency. More recently, Koutnik et al. <ref type="bibr" target="#b23">[24]</ref> proposed the Clockwork RNN, in which the hidden units are divided into several modules, of which the i-th module is only updated every 2 i -th time-step. General advantages of this multiscale RNN architecture are improved computational efficiency, efficient propagation of long-term dependencies and flexibility in allocating resources (units) to the hierarchical layers. Multiscale RNNs have been applied for speech recognition in <ref type="bibr" target="#b2">[3]</ref>, where the slower operating RNN pools information over time and the timescales are fixed hyperparameters as in Clockwork RNNs.</p><formula xml:id="formula_0">f F1 f F2 f F k f S h F2 t h F1 t h F k t h F k−1 t h F k t−1 h S t h S t−1 x t y t · · ·</formula><p>In <ref type="bibr" target="#b34">[35]</ref>, multiscale RNNs are applied to make context-aware query suggestions. In this case, explicit hierarchical boundary information is provided. Chung et al. <ref type="bibr" target="#b5">[6]</ref> presented a hierarchical multiscale RNN (HM-RNN) that discovers the latent hierarchical structure of the sequence without explicitly given boundary information. If a parametrized boundary detector indicates the end of a segment, then a summarized representation of the segment is fed to the upper layer and the state of the lower layer is reset <ref type="bibr" target="#b5">[6]</ref>.</p><p>Our FS-RNN architectures borrows elements from both deep transition RNNs and multiscale RNNs. The major difference to multiscale RNNs is that our lower hierarchical layer zooms in in time, that is, it operates faster than the timescale that is naturally given by the input sequence. The major difference to deep transition RNNs is our approach to facilitate long-term dependencies, namely, we employ a RNN operating on a slow timescale.</p><p>Many approaches aim at solving the problem of learning long-term dependencies in sequential data. A very popular one is to use external memory cells that can be accessed and modified by the network, see Neural Turing Machines <ref type="bibr" target="#b12">[13]</ref>, Memory Networks <ref type="bibr" target="#b38">[39]</ref> and Differentiable Neural Computer <ref type="bibr" target="#b13">[14]</ref>.</p><p>Other approaches focus on different optimization techniques rather than network architectures. One attempt is Hessian Free optimization <ref type="bibr" target="#b27">[28]</ref>, a second order training method that achieved good results on RNNs. The use of different optimization techniques can improve learning in a wide range of RNN architectures and therefore, the FS-RNN may also benefit from it.</p><formula xml:id="formula_1">h F1 t = f F1 (h F k t−1 , x t ) h S t = f S (h S t−1 , h F1 t ) h F2 t = f F2 (h F1 t , h S t ) h Fi t = f Fi (h Fi−1 t ) for 3 ≤ i ≤ k</formula><p>The output y t is computed as an affine transformation of h F k t . It is possible to extend the FS-RNN architecture in order to further facilitate the learning of long-term dependencies by adding hierarchical layers, each of which operates on a slower timescale than the ones below, resembling clockwork RNNs <ref type="bibr" target="#b23">[24]</ref>. However, for the tasks considered in Section 4, we observed that this led to overfitting the training data even when applying regularization techniques and reduced the performance at test time. Therefore, we will not further investigate this extension of the model in this paper, even though it might be beneficial for other tasks or larger data sets.</p><p>In the experiments in Section 4, we use LSTM cells as building blocks for the FS-RNN architecture. For completeness, we state the update function f Q for an LSTM Q. The state of an LSTM is a pair (h t , c t ), consisting of the hidden state and the cell state. The function f Q maps the previous state and input</p><formula xml:id="formula_2">(h t−1 , c t−1 , x t ) to the next state (h t , c t ) according to    f t i t o t g t    = W Q h h t−1 + W Q x x t + b Q c t = σ(f t ) ⊙ c t−1 + σ(i t ) ⊙ tanh(g t ) h t = σ(o t ) ⊙ tanh(c t ) ,</formula><p>where f t , i t and o t are commonly referred to as forget, input and output gates, and g t are the new candidate cell states. Moreover, W Q h , W Q x and b Q are the learnable parameters, σ denotes the sigmoid function, and ⊙ denotes the element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>For the experiments, we consider the Fast-Slow LSTM (FS-LSTM) that is a FS-RNN, where each RNN cell is a LSTM cell. The FS-LSTM is evaluated on two character level language modeling data sets, namely Penn Treebank and Hutter Prize Wikipedia, which will be referred to as enwik8 in this section. The task consists of predicting the probability distribution of the next character given all the previous ones. In Section 4.1, we compare the performance of the FS-LSTM with other approaches. In Section 4.2, we empirically compare the network dynamics of different RNN architectures and show the FS-LSTM combines the benefits of both, deep transition RNNs and multiscale RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance on Penn Treebank and Hutter Prize Wikipedia</head><p>The FS-LSTM achieves 1.19 BPC and 1.25 BPC on the Penn Treebank and enwik8 data sets, respectively. These results are compared to other approaches in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> (the baseline LSTM results without citations are taken from <ref type="bibr" target="#b42">[43]</ref> for Penn Treebank and from <ref type="bibr" target="#b14">[15]</ref> for enwik8). For the Penn Treebank, the FS-LSTM outperforms all previous approaches with significantly less parameters than the previous top approaches. We did not observe any improvement when increasing the model size, probably due to overfitting. In the enwik8 data set, the FS-LSTM surpasses all other neural approaches. Following <ref type="bibr" target="#b12">[13]</ref>, we compare the results with text compression algorithms using the BPC measure. An ensemble of two FS-LSTM models (1.20 BPC) outperforms cmix (1.23 BPC) <ref type="bibr" target="#b22">[23]</ref>, the current best text compression algorithm on enwik8 <ref type="bibr" target="#b25">[26]</ref>. However, a fair comparison is difficult. Compression algorithms are usually evaluated by the final size of the compressed data set including the decompressor size. For character prediction models, the network size is usually not taken into account and the performance is measured on the test set. We remark that as the FS-LSTM is evaluated on the test set, it should achieve similar performance on any part of the English Wikipedia. The FS-LSTM-2 and FS-LSTM-4 model consist of two and four cells in the Fast layer, respectively. The FS-LSTM-4 model outperforms the FS-LSTM-2 model, but its processing time for one time step is 25% higher than the one of the FS-LSTM-2. Adding more cells to the Fast layer could further improve the performance as observed for RHN <ref type="bibr" target="#b41">[42]</ref>, but would increase the processing time, because the cell states are computed sequentially. Therefore, we did not further increase the number of Fast cells.</p><p>The model is trained to minimize the cross-entropy loss between the predictions and the training data. Formally, the loss function is defined as</p><formula xml:id="formula_3">L = − 1 n n i=1 log p θ (x i |x 1 , . . . , x i−1 ), where p θ (x i |x 1 , . . . , x i−1 )</formula><p>is the probability that a model with parameters θ assigns to the next character x i given all the previous ones. The model is evaluated by the BPC measure, which uses the binary logarithm instead of the natural logarithm in the loss function. All the hyperparameters used for the experiments are summarized in <ref type="table" target="#tab_2">Table 3</ref>. We regularize the FS-LSTM with dropout <ref type="bibr" target="#b35">[36]</ref>. In each time step, a different dropout mask is applied for the non-recurrent connections <ref type="bibr" target="#b40">[41]</ref>, and Zoneout <ref type="bibr" target="#b1">[2]</ref> is applied for the recurrent connections. The network is trained with minibatch gradient descent using the Adam optimizer <ref type="bibr" target="#b21">[22]</ref>. If the gradients have norm larger than 1 they are normalized to 1. Truncated backpropagation through time (TBPTT) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b9">10]</ref> is used to approximate the gradients, and the final hidden state is passed to the next sequence. The learning rate is divided by a factor 10 for the last 20 epochs in the Penn Treebank experiments, and it is divided by a factor 10 whenever the validation error does not improve in two consecutive epochs in the enwik8 experiments. The forget bias of every LSTM cell is initialized to 1, and all weight matrices are initialized to orthogonal matrices. Layer normalization <ref type="bibr" target="#b0">[1]</ref> is applied to the cell and to each gate separately. The network with the smallest validation error is evaluated on the test set. The two data sets that we use for evaluation are:</p><p>Penn Treebank <ref type="bibr" target="#b26">[27]</ref> The dataset is a collection of Wall Street Journal articles written in English. It only contains 10000 different words, all written in lower-case, and rare words are replaced with "&lt; unk &gt;". Following <ref type="bibr" target="#b28">[29]</ref>, we split the data set into train, validation and test sets consisting of 5.1M, 400K and 450K characters, respectively.</p><p>Hutter Prize Wikipedia <ref type="bibr" target="#b18">[19]</ref> This dataset is also known as enwik8 and it consists of "raw" Wikipedia data, that is, English articles, tables, XML data, hyperlinks and special characters. The data set contains 100M characters with 205 unique tokens. Following <ref type="bibr" target="#b6">[7]</ref>, we split the data set into train, validation and test sets consisting of 90M, 5M and 5M characters, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison of network dynamics of different architectures</head><p>We compare the FS-LSTM architecture with the stacked-LSTM and the sequential-LSTM architectures, depicted in <ref type="figure" target="#fig_1">Figure 2</ref>, by investigating the network dynamics. In order to conduct a fair comparison we chose the number of parameters to roughly be the same for all three models. The FS-LSTM consists of one Slow and four Fast LSTM cells of 450 units each. The stacked-LSTM consists of five LSTM cells stacked on top of each other consisting of 375 units each, which will be  referred to as Stacked-1, ... , Stacked-5, from bottom to top. The sequential-LSTM consists of five sequentially connected LSTM cells of 500 units each. All three models require roughly the same time to process one time step. The models are trained on enwik8 for 20 epochs with minibatch gradient descent using the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> without any regularization, but layer normalization <ref type="bibr" target="#b0">[1]</ref> is applied on the cell states of the LSTMs. The hyperparameters are not optimized for any of the three models.</p><p>The experiments suggest that the FS-LSTM architecture favors the learning of long-term dependencies <ref type="figure" target="#fig_3">(Figure 3</ref>), enforces hidden cell states to change at different rates <ref type="figure" target="#fig_4">(Figure 4</ref>) and facilitates a quick adaptation to unexpected inputs ( <ref type="figure" target="#fig_5">Figure 5</ref>). Moreover, the FS-LSTM achieves 1.49 BPC and outperforms the stacked-LSTM (1.61 BPC) and the sequential-LSTM (1.58 BPC).</p><p>In <ref type="figure" target="#fig_3">Figure 3</ref>, we asses the ability to capture long-term dependencies by investigating the effect of the cell state on the loss at later time points, following <ref type="bibr" target="#b1">[2]</ref>. We measure the effect of the cell state at time t − k on the loss at time t by the gradient ∂Lt ∂c t−k . This gradient is the largest for the Slow LSTM, and it is small and steeply decaying as k increases for the Fast LSTM. Evidently, the Slow cell captures long-term dependencies, whereas the Fast cell only stores short-term information. In the stacked-LSTM, the gradients decrease from the top layer to the bottom layer, which can be explained   by the vanishing gradient problem. The small, steeply decaying gradients of the sequential-LSTM indicate that it is less capable to learn long-term dependencies than the other two models. <ref type="figure" target="#fig_4">Figure 4</ref> gives further evidence that the FS-LSTM stores long-term dependencies efficiently in the Slow LSTM cell. It shows that among all the layers of the three RNN architectures, the cell states of the Slow LSTM change the least from one time step to the next. The highest change is observed for the cells of the sequential model followed by the Fast LSTM cells.</p><formula xml:id="formula_4">f 1 f 5 h 5 t h 5 t−1 h 1 t h 1 t−1 x t y t . . . (a) Stacked f 1 f 2 f 5 h t h t−1 x t y t · · · (b) Sequential</formula><p>In <ref type="figure" target="#fig_5">Figure 5</ref>, we investigate whether the FS-LSTM quickly adapts to unexpected characters, that is, whether it performs well on the subsequent ones. In text modeling, the initial character of a word has the highest entropy, whereas later characters in a word are usually less ambiguous <ref type="bibr" target="#b9">[10]</ref>. Since the first character of a word is the most difficult one to predict, the performance at the following positions should reflect the ability to adapt to unexpected inputs. While the prediction qualities at the first position are rather close for all three models, the FS-LSTM outperforms the stacked-LSTM and sequential-LSTM significantly on subsequent positions. It is possible that new information is incorporated quickly in the Fast layer, because it only stores short-term information, see <ref type="figure" target="#fig_3">Figure 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed the FS-RNN architecture. Up to our knowledge, it is the first architecture that incorporates ideas of both multiscale and deep transition RNNs. The FS-RNN architecture improved state of the art results on character level language modeling evaluated on the Penn Treebank and Hutter Prize Wikipedia data sets. An ensemble of two FS-RNNs achieves better BPC performance than the best known compression algorithm. Further experiments provided evidence that the Slow cell enables the network to learn long-term dependencies, while the Fast cells enable the network to quickly adapt to unexpected inputs and learn complex transition functions from one time step to the next.</p><p>Our FS-RNN architecture provides a general framework for connecting RNN cells as any type of RNN cell can be used as building block. Thus, there is a lot of flexibility in applying the architecture to different tasks. For instance using RNN cells with good long-term memory, like EURNNs <ref type="bibr" target="#b20">[21]</ref> or NARX RNNs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8]</ref>, for the Slow cell might boost the long-term memory of the FS-RNN architecture. Therefore, the FS-RNN architecture might improve performance in many different applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Diagram of a Fast-Slow RNN with k Fast cells. Observe that only the second Fast cell receives the input from the Slow cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Diagram of (a) stacked-LSTM and (b) sequential-LSTM with 5 cells each.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Long-term effect of the cell states on the loss function. The average value of ∂Lt ∂c t−k , which is the effect of the cell state at time t − k on the loss function at time t, is plotted against k for the different layers in the three RNN architectures. For the sequential-LSTM only the first cell is considered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Rate of change of the cell states from one time step to the next. We plot 1 n n i=1 (c t,i − c t−1,i ) 2 averaged over all time steps, where c t,i is the value of the ith unit at time step t, for the different layers of the three RNN architectures. For the sequential-LSTM only the first cell is considered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Bits-per-character at each character position. The left panel shows the average bits-percharacter at each character positions in the test set. The right panel shows the average relative loss with respect to the stacked-LSTM at each character position. For this Figure, a word is considered to be a sequence of lower-case letters of length at least 2 in-between two spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>BPC on Penn Treebank</figDesc><table><row><cell>Model</cell><cell cols="2">BPC Param Count</cell></row><row><cell>Zoneout LSTM [2]</cell><cell>1.27</cell><cell>-</cell></row><row><cell>2-Layers LSTM</cell><cell>1.243</cell><cell>6.6M</cell></row><row><cell>HM-LSTM [6]</cell><cell>1.24</cell><cell>-</cell></row><row><cell cols="2">HyperLSTM -small [15] 1.233</cell><cell>5.1M</cell></row><row><cell>HyperLSTM [15]</cell><cell>1.219</cell><cell>14.4M</cell></row><row><cell>NASCell -small [43]</cell><cell>1.228</cell><cell>6.6M</cell></row><row><cell>NASCell [43]</cell><cell>1.214</cell><cell>16.3M</cell></row><row><cell>FS-LSTM-2 (ours)</cell><cell>1.190</cell><cell>7.2M</cell></row><row><cell>FS-LSTM-4 (ours)</cell><cell>1.193</cell><cell>6.5M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>BPC on enwik8</figDesc><table><row><cell>Model</cell><cell cols="2">BPC Param Count</cell></row><row><cell>LSTM, 2000 units</cell><cell>1.461</cell><cell>18M</cell></row><row><cell cols="2">Layer Norm LSTM, 1800 units 1.402</cell><cell>14M</cell></row><row><cell>HyperLSTM [15]</cell><cell>1.340</cell><cell>27M</cell></row><row><cell>HM-LSTM [6]</cell><cell>1.32</cell><cell>35M</cell></row><row><cell>Surprisal-driven Zoneout [32]</cell><cell>1.31</cell><cell>64M</cell></row><row><cell>RHN -depth 5 [42]</cell><cell>1.31</cell><cell>23M</cell></row><row><cell>RHN -depth 10 [42]</cell><cell>1.30</cell><cell>21M</cell></row><row><cell>Large RHN -depth 10 [42]</cell><cell>1.27</cell><cell>46M</cell></row><row><cell>FS-LSTM-2 (ours)</cell><cell>1.290</cell><cell>27M</cell></row><row><cell>FS-LSTM-4 (ours)</cell><cell>1.277</cell><cell>27M</cell></row><row><cell>Large FS-LSTM-4 (ours)</cell><cell>1.245</cell><cell>47M</cell></row><row><cell>2 × Large FS-LSTM-4 (ours)</cell><cell>1.198</cell><cell>2 × 47M</cell></row><row><cell>cmix v13 [23]</cell><cell>1.225</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameters for the character-level language model experiments.</figDesc><table><row><cell></cell><cell cols="2">Penn Treebank</cell><cell></cell><cell>enwik8</cell><cell></cell></row><row><cell></cell><cell cols="2">FS-LSTM-2 FS-LSTM-4</cell><cell cols="2">FS-LSTM-2 FS-LSTM-4</cell><cell>Large FS-LSTM-4</cell></row><row><cell>Non-recurrent dropout</cell><cell>0.35</cell><cell>0.35</cell><cell>0.2</cell><cell>0.2</cell><cell>0.25</cell></row><row><cell>Cell zoneout</cell><cell>0.5</cell><cell>0.5</cell><cell>0.3</cell><cell>0.3</cell><cell>0.3</cell></row><row><cell>Hidden zoneout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.05</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell>Fast cell size</cell><cell>700</cell><cell>500</cell><cell>900</cell><cell>730</cell><cell>1200</cell></row><row><cell>Slow cell size</cell><cell>400</cell><cell>400</cell><cell>1500</cell><cell>1500</cell><cell>1500</cell></row><row><cell>TBPTT length</cell><cell>150</cell><cell>150</cell><cell>150</cell><cell>150</cell><cell>100</cell></row><row><cell>Minibatch size</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell></row><row><cell>Input embedding size</cell><cell>128</cell><cell>128</cell><cell>256</cell><cell>256</cell><cell>256</cell></row><row><cell>Initial Learning rate</cell><cell>0.002</cell><cell>0.002</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>Epochs</cell><cell>200</cell><cell>200</cell><cell>35</cell><cell>35</cell><cell>50</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Fast-Slow RNNWe propose the FS-RNN architecture, seeFigure 1. It consists of k sequentially connected RNN cells F 1 , . . . , F k on the lower hierarchical layer and one RNN cell S on the higher hierarchical layer. We call F 1 , . . . , F k the Fast cells, S the Slow cell and the corresponding hierarchical layers the Fast and Slow layer, respectively. S receives input from F 1 and feeds its state to F 2 . F 1 receives the sequential input data x t , and F k outputs the predicted probability distribution y t of the next element of the sequence.Intuitively, the Fast cells are able to learn complex transition functions from one time step to the next one. The Slow cell gives rise to shorter gradient paths between sequential inputs that are distant in time, and thus, it facilitates the learning of long-term dependencies. Therefore, the FS-RNN architecture incorporates advantages of deep transition RNNs and of multiscale RNNs, see Section 2.Since any kind of RNN cell can be used as building block for the FS-RNN architecture, we state the formal update rules of the FS-RNN for arbitrary RNN cells. We define a RNN cell Q to be a differentiable function f Q (h, x) that maps a hidden state h and an additional input x to a new hidden state. Note that x can be input data or input from a cell in a higher or lower hierarchical layer. If a cell does not receive an additional input, then we will omit x. The following equations define the FS-RNN architecture for arbitrary RNN cells F 1 , . . . , F k and S.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Julian Zilly for many helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">János</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Pezeshki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01305</idno>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio, Aaron Courville and Chris Pal. Zoneout: Regularizing rnns by randomly preserving hidden activations</title>
		<meeting><address><addrLine>Nicolas Ballas, Nan Rosemary Ke</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Philemon Brakel, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning deep architectures for ai. Foundations and trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01704</idno>
		<title level="m">Hierarchical multiscale recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2067" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Revisiting narx recurrent neural networks for long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural networks for long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><forename type="middle">El</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nips</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">409</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COGNITIVE SCIENCE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen Netzen. PhD thesis, diploma thesis, institut für informatik, lehrstuhl prof. brauer, technische universität münchen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The human knowledge compression contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://prize.hutter1.net" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Discovering multiscale dynamical features with hierarchical echo state networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Jacobs University Bremen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Tunable efficient unitary neural networks (eunn) and their application to rnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tena</forename><surname>Dubček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Peurifoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Skirlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Soljačić</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryon</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cmix</surname></persName>
		</author>
		<ptr target="http://www.byronknoll.com/cmix.html" />
		<imprint>
			<date type="published" when="2017-05-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">A clockwork rnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies in narx recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsungnan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lee</forename><surname>Tino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1329" to="1338" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="http://mattmahoney.net/dc/text.html" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2017" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning recurrent neural networks with hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Son</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kombrink</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jancernocký</forename></persName>
		</author>
		<ptr target="http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6026</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The utility driven dynamic error propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Fallside</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge Department of Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamil</forename><surname>Rocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Kornuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07675</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Surprisal-driven zoneout. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning complex, extended sequences using the principle of history compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="242" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A hierarchical recurrent encoder-decoder for generative context-aware query suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Vahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="553" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalization of backpropagation with application to a recurrent gas market model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="339" to="356" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Complexity of exact gradient computation algorithms for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<idno>NU-CCS-89-27</idno>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>Boston</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Northeastern University, College of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report Technical Report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03474</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Recurrent highway networks</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
