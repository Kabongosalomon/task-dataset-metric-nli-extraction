<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploit the Connectivity: Multi-Object Tracking with TrackletNet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaoang</forename><surname>Wang</surname></persName>
							<email>gaoang@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renshu</forename><surname>Gu</surname></persName>
							<email>renshugu@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
							<email>hwang@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploit the Connectivity: Multi-Object Tracking with TrackletNet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-object tracking (MOT) is an important and practical task related to both surveillance systems and moving camera applications, such as autonomous driving and robotic vision. However, due to unreliable detection, occlusion and fast camera motion, tracked targets can be easily lost, which makes MOT very challenging. Most recent works treat tracking as a re-identification (Re-ID) task, but how to combine appearance and temporal features is still not well addressed. In this paper, we propose an innovative and effective tracking method called TrackletNet Tracker (TNT) that combines temporal and appearance information together as a unified framework. First, we define a graph model which treats each tracklet as a vertex. The tracklets are generated by appearance similarity with CNN features and intersection-over-union (IOU) with epipolar constraints to compensate camera movement between adjacent frames. Then, for every pair of two tracklets, the similarity is measured by our designed multi-scale TrackletNet. Afterwards, the tracklets are clustered <ref type="bibr" target="#b32">[34]</ref> into groups which represent individual object IDs. Our proposed TNT has the ability to handle most of the challenges in MOT, and achieve promising results on MOT16 and MOT17 benchmark datasets compared with other state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-object tracking is an important topic in computer vision and machine learning field. This technique can be used in many tasks, such as traffic flow counting from surveillance cameras, human behavior prediction and autonomous driving assistance. However, due to noisy detections and occlusions, tracking multiple objects in a long time range is very challenging. To address such problems, many methods follow the tracking-by-detection framework, i.e., tracking is applied as an association approach given the detection results. Built upon the tracking-by-detection framework, multiple cues can be combined together into the <ref type="bibr">Figure 1</ref>. Our TNT framework for multi-object tracking. Given the detections in different frames, detection association is computed to generate Tracklets for the Vertex Set V . After that, each two tracklets are put into a novel TrackletNet to measure the connectivity, which formed the similarity on the Edge Set E. A graph model G can be derived from V and E. Finally, the tracklets with the same ID are grouped into one cluster using the graph partition approach. tracking scheme. 1) Appearance feature of each detected object <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b35">37]</ref>. With a well-embedded appearance, features should be similar if they are from the same object, while they can be very different if they are from distinct objects. 2) Temporal relation for locations among frames in a trajectory <ref type="bibr" target="#b21">[22]</ref>. With slow motion and high frame rate of cameras, we can assume that the trajectories of objects are smooth in time domain. 3) Interaction cue among different target objects which considers the relationship among neighboring targets <ref type="bibr" target="#b26">[28]</ref>. As a result, we should take into account all these cues as an optimization problem.</p><p>In this paper, the proposed TrackletNet Tracker (TNT) takes advantages of the above useful cues together into a unified framework based on an undirected graph model <ref type="bibr">[23]</ref>. Each vertex in our graph model represents one tracklet and the edge between two vertices measures the connectivity of two tracklets. Here, the tracklet is defined as a small piece of consecutive detections of an object. Due to the unreliable detections and occlusions, the entire trajectory of an object may be divided into several distinct tracklets. Given the graph representation, tracking can be regarded as a clustering approach that groups the tracklets into one big cluster.</p><p>To generate the tracklets, i.e., vertices of the graph, we associate detections among consecutive frames based on intersection-over-union (IOU) and the similarity of appearance features. However, the IOU criterion becomes unreliable because the position of detection may shift a lot when camera is moving or revolving. In such situation, epipolar geometry is adopted to compensate camera movement and predict the position of bounding boxes in the next frame. To estimate the connectivity on the edge of the graph between two vertices, the TrackletNet is designed for measuring the continuity of two input tracklets, which combines both trajectory and appearance information. The flowchart of our tracking method TNT is shown in <ref type="figure">Figure 1</ref>.</p><p>Specifically, we propose the following contributions: 1) We build a graph-based model that takes tracklets, instead of detected objects, as the vertices, to better utilize the temporal information and greatly reduce the computational complexity.</p><p>2) To the best of our knowledge, this is the first work to adopt epipolar geometry in tracklet generation to compensate camera movement.</p><p>3) A CNN architecture, called multi-scale TrackletNet, is designed to measure the connectivity between two tracklets. This network combines trajectory and appearance information into a unified system. 4) Our model outperforms state-of-the-art methods in multi-object tracking for both MOT16 and MOT17 benchmarks, and it can be also easily applied to other different scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Graph Model based Tracking. Most of the recent multiobject tracking approaches are based on tracking-bydetection schemes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">40]</ref>. Given detection results, we would like to associate detections across frames and estimate object locations when unreliable detection or occlusion occurs. Many tracking methods are based on graph models <ref type="bibr" target="#b31">[33,</ref><ref type="bibr">23,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b34">36]</ref> and solve the tracking problem by minimizing the total cost. In <ref type="bibr" target="#b31">[33,</ref><ref type="bibr">23,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b16">17]</ref>, the detected objects are treated as the vertices in the graph models, while in <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b34">36]</ref>, the graph vertices are based on tracklets. For detection-based graph models, there are two major disadvantages. First, one of the important assumptions in graph models is the conditional independence of the vertices. However, detections are not conditional independent from frame to frame if we want to track an object in a long run. The temporal information is not well utilized. Second, detection-based graph usually comes with a very high-dimensional affinity matrix, which makes it very hard to find the global minimum solution in the optimization. However, for tracklet-based graph models, it can better utilize the information from a short trajectory to measure the relationship between vertices, but the mis-association should be carefully handled in the tracklet generation step.</p><p>Tracking by RNN. Besides graph models, recurrent neural networks (RNN)-based tracking also plays an important role in recent years <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>. One advantage of RNN-based tracking is the ability of online prediction. However, along with the propagation of RNN block, the relation between two faraway detections becomes very weak. Without direct connections, the performance of RNN-based methods degrades in the long run and sometimes can be easily affected by unreliable detections.</p><p>Tracking by Feature Fusion. Features are very important in the tracking-by-detection framework. There are two types of features that are used in common, i.e., appearance features and temporal features. For appearance features, many works adopt CNN-based features from Re-ID tasks <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b31">33]</ref>. However, histogram-based features, like color histograms, HOG, and LBP, are still powerful if no training data is provided <ref type="bibr" target="#b32">[34]</ref>. As for temporal features, the location, size, and motion of bounding boxes are commonly used. Given the appearance features and temporal features, the tracker can fuse them together using human defined weights <ref type="bibr" target="#b39">[41,</ref><ref type="bibr">23,</ref><ref type="bibr" target="#b32">34]</ref>. Although <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b19">20]</ref> propose RNN-based networks to combine features together, it is still empirical and difficult to determine the weight of each feature.</p><p>End-to-End Tracking. Another category of tracking is based on end-to-end frameworks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, where we input raw video sequences and output object trajectory. In other words, the detection and tracking are trained jointly in a single-stage network. One major advantage of this framework is that the errors will not be accumulated from detection to tracking. The temporal information across frames can help improve the detection performance, while reliable detections can also feedback reliable tracking. However, such a framework requires a lot of training data. Without enough training data, overfitting becomes a severe problem. Unlike detection based training, tracking annotations for video sequences are usually hard to get, which becomes the major limitation of the end-to-end tracking framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tracklet Graph Model</head><p>We use tracklets as the vertices in our graph model. Unlike the detection-based graph models, which are computational expensive and not well utilizing temporal informa-tion, we propose a tracklet-based graph model, which treats the tracklet as the vertex and measures the similarity between tracklets. From the tracklet, we can infer the object moving trajectory for a longer time, and we can also measure how the embedded features of the detections change along the time. Moreover, the number of tracklets is much less than the number of detections, which makes the optimization more efficiently.</p><p>In the following section, we will discuss in detail about the model parameters and optimization by tracklet clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Definition</head><formula xml:id="formula_0">G(V, E) Vertex Set. A finite set V in which every element v ∈ V</formula><p>represents a tracklet of one object across multiple frames, i.e., a set of consecutive detections of the same object along time. For each detection, we define the bounding box with five parameters, i.e., the center of the bounding box (x t , y t ), the width and height (w t , h t ), and the frame index t. Besides the bounding box of the detection, we also extract an appearance feature <ref type="bibr" target="#b28">[30]</ref> for each detected object at frame t. Note that because of unreliable detections, an entire trajectory of an object may be divided into multiple pieces of tracklets. The tracklet generation is explained in detail in Section 4.1.</p><p>Edge Set. A finite set E in which every element e ∈ E represents an edge between two tracklets u, w ∈ V that are not far away in the time domain, i.e., min tu∈T (u),tw∈T (w) |t u − t w | ≤ δ t , where T (u) is the set of frame indices of the tracklet u. For tracklets that are far away, the edge is not considered between them since not enough information can be utilized for measuring their relationship.</p><p>A connectivity measure p e , represents the similarity of the two tracklets connected by the edge e ∈ E. The edge cost is defined as</p><formula xml:id="formula_1">c = log 1 − p e p e .<label>(1)</label></formula><p>Moreover, the connectivity is defined to be 0 if two tracklets have overlap in the time domain since they must belong to distinct objects. This is because an object cannot appear in two tracklets at the same time. The connectivity is measured by our designed TrackletNet, which will be introduced in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Tracklet Clustering</head><p>After the tracklet graph is built, we acquire the object trajectories by clustering the graph into different sub-graphs. The tracklets in each sub-graph can represent the same object. We will explain some details of our tracklet clustering in the following paragraphs.</p><p>Feasible Solutions. Given a tracklet graph G(V, E), we hope to partition G into disjoint sub-graphs G[s τ ], and each sub-graph represents a distinct object. Here ∀s τ ⊆ V , τ represents the object ID. Thus, every tracklet u ∈ s τ is from the same object τ and any two tracklets u ∈ s τ , w ∈ s τ from two different sub-graphs are from different objects τ and τ . For the graph partition problem, the global optimal solution cannot be easily guaranteed. But we can still define the feasible solutions as follows.</p><formula xml:id="formula_2">• Each sub-graph G[s τ ] should be a connected graph, i.e., ∀τ , ∀u, w ∈ s τ , ∃P ∈ G[s τ ], s.t., u, w ∈ P , where P is a path inside G[s τ ]. • The cost on the edge inside each sub-graph should have a finite value, i.e., ∀τ , ∀u, w ∈ s τ , if ∃e ∈ E for u, w, p e = 0.</formula><p>Objective Function. The objective function is defined to minimize the total clustering cost on all graph edges. We define π(u, w) ∈ {±1} as the clustering label for tracklets u and w. If u and w are partitioned into one sub-graph, π(u, w) is set to be +1; otherwise, π(u, w) is set to be −1.</p><p>The objective function is defined as follows,</p><formula xml:id="formula_3">O = min π∈{±1} u,w∈V u∈N (w) π(u, w) · c(u, w),<label>(2)</label></formula><p>where N (w) represents the set of neighboring tracklets of w with edge shared in the graph.</p><p>Clustering. The graph partition is formulated as a clustering problem. However, the minimum cost of graph cut problem defined by Equation <ref type="formula" target="#formula_3">(2)</ref> is APX-hard <ref type="bibr" target="#b22">[24]</ref>. Besides, the number of clusters is unknown in advance. In this work, we exploit a greedy search-based clustering method proposed by <ref type="bibr" target="#b32">[34]</ref> to minimize the cost. Five clustering operations, i.e., assign, merge, split, switch, and break, are used. The advantage of adopting different types of clustering operation is to avoid being stuck at the local minimum as much as possible in the optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed TrackletNet Tracker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Tracklet Generation with Epipolar Constraints</head><p>As defined in Section 3, a tracklet contains consecutively detected objects with bounding box information and appearance features with dimension d ap . To simplify the generation of tracklets, we associate two consecutive detections based on IOU and appearance similarity in adjacent frames with a high association threshold to guarantee the mis-association as small as possible <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b33">35]</ref>.</p><p>However, the association accuracy can still be affected by the fast motion of the camera. For example, as shown in the <ref type="figure" target="#fig_0">Figure 2</ref>(a)(b), the target detection in the t-th frame has a large IOU with another detection in the (t + 1)-th frame. As a result, the detection may easily get mis-associated.</p><p>This issue can be well solved by epipolar geometry (EG) <ref type="bibr" target="#b7">[8]</ref>, i.e., x t Fx t+1 = 0 for any matched static feature point x in two frames, where F is the fundamental matrix. First, if we assume the target is static or has slow motion, then the four corner points x i,t of the target detection bounding box in the t-th frame should lie on the corresponding epipolar lines in the (t + 1)-th frame, i.e., the predicted target bounding box in the (t + 1)-th frame should intersect with the four epipolar lines as much as possible as shown in <ref type="figure" target="#fig_0">Figure 2(c)</ref>. Second, we also assume the size of the bounding box does not have much change in adjacent frames, then the optimal predicted bounding box can be obtained, which is shown in red in <ref type="figure" target="#fig_0">Figure 2(d)</ref>.</p><p>Followed by the above two assumptions, we can predict the target bounding box location in the (t + 1)-th frame by formulating an optimization problem. Define four corner points of the target bounding box in the t-th frame as x i,t , where i ∈ {1, 2, 3, 4}, like the example shown in <ref type="figure" target="#fig_0">Figure 2(a)</ref>. Similarly, we define x i,t+1 , i ∈ {1, 2, 3, 4}, as the bounding box in the (t + 1)-th frame. Then we can define the cost function as follows,</p><formula xml:id="formula_4">f (x i,t+1 ) = 4 i=1 x i,t Fx i,t+1 2 2 + (x 3,t+1 − x 1,t+1 ) − (x 3,t − x 1,t ) 2 2 ,<label>(3)</label></formula><p>where the first term guarantees the predicted bounding box should intersect with four corresponding epipolar lines as much as possible, while the second term is the target size constraint. One example of predicted bounding box, as shown in <ref type="figure" target="#fig_0">Figure 2(d)</ref>, is well aligned with the true target in (t + 1)-th frame. Then, in the detection association, IOU is calculated between predicted bounding boxes and detection bounding boxes in the (t + 1)-th frame. Fundamental matrix F can be estimated by the RANSAC <ref type="bibr" target="#b5">[6]</ref> algorithm with matched SURF points <ref type="bibr" target="#b0">[1]</ref> between two consecutive frames.</p><p>The optimization of the cost function in Equation <ref type="formula" target="#formula_4">(3)</ref> can be reformulated into a Least Square problem and solved efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-Scale TrackletNet</head><p>To measure the connectivity between two tracklets, we aggregate different types of information, including temporal and appearance features via the designed multi-scale TrackletNet. The architecture of the proposed TrackletNet is shown in <ref type="figure">Figure 3</ref>.</p><p>For each frame t, a vector consisting of the bounding box parameters, i.e., (x t , y t , w t , h t ), concatenated by an embedded appearance feature extracted from the FaceNet <ref type="bibr" target="#b28">[30]</ref>, is used to represent an individual detection from a tracklet. Considering two tracklets with edge-shared in the graph, we concatenate the embedded feature of each detection from these two tracklets inside a time window with a fixed size T . Then the feature space in the time window of the two tracklets is (4 + d ap ) × T . As for frames between the two target tracklets, we use a (4 + d ap ) dimensional interpolated vector instead at each missing frame t. Besides, zeropadding is used for frames after the second tracklet. To better represent the time duration of input tracklets, two binary masks are used as individual channels with (4 + d ap ) × T dimension for each input tracklet. For each frame t, if the detection exists, then we set the t-th column of the binary mask to be all 1 vector; otherwise we set 0 vector instead. As a result, the size of the input tensor of the TrackletNet is B × (4 + d ap ) × T × 3, where B is the batch size and 3 indicates the number of channels, one for the embedded feature space and the other two for the binary masks.</p><p>TrackletNet contains three convolution layers Conv1, Conv2, Conv3, one average pooling layer AvgPool, and two fully connected layers FC1, FC2. For each convolution layer, four different sizes of kernels are used, i.e., 1 × 3, 1 × 5, 1 × 9, 1 × 13. Note that our convolution is only in the time domain, which can measure the continuity for each dimension of the feature. Different sizes of kernels will look for feature changes in different scales. The large kernels have the ability to measure the continuity of two tracklets even if they are far away in the time domain, while small kernels can focus on appearance difference if input tracklets are in small pieces. Each convolution is followed by <ref type="figure">Figure 3</ref>. Architecture of Multi-scale TrackletNet. First, we extract embedded features from two input tracklets, which include 4D location features and 512D appearance features along the time window of 64 frames. The input tensor has three channels, i.e., one for tracklet embedded features and the other two for binary masks, where white color represents 1 and black color represents 0. Four types of 1D convolution kernels are applied for feature extraction in three convolution layers. For each convolution layer, max pooling is adopted for down-sampling in the time domain. Average pooling is conducted on the dimensions of the appearance feature after Conv3. Then two fully connected layers are conducted to get the final output. one max pooling layer which down-samples by 2 in the time domain. After Conv3, we take the average pooling on appearance feature dimensions. AvgPool plays a role of the weighted majority vote on the discontinuity of all appearance dimensions. Then we concatenate all features and use two fully connected layers for the final output. The output is defined as the similarity between the two input tracklets, which ranges from zero to one.</p><p>There are some important properties of the TrackletNet, which are listed as follows.</p><p>• TrackletNet focuses on the continuity of the embedded features along the time. Because of the independence among different feature dimensions, no convolution is conducted across the dimensions of the embedded features. In other words, the convolution kernels only capture the dependency along time.</p><p>• Binary masks of the input tensor play a role as the tracklet indicator, telling the temporal locations of the tracklets. They help the network learn if the discontinuity of two tracklets is caused by frames without detection or the abrupt changes of the tracklets.</p><p>• The network integrates object Re-ID, temporal and spatial dependency as one unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>We use MOT16 and MOT17 <ref type="bibr" target="#b20">[21]</ref> datasets to train and evaluate our tracking performance. For MOT16 dataset, there are 7 training video sequences and 7 testing video sequences. The benchmark also provides public deformable part models (DPM) <ref type="bibr" target="#b4">[5]</ref> detections for both training and testing data. MOT17 has the same video sequences as MOT16 but provides more accurate ground truth in the evaluation. In addition to DPM, Faster-RCNN <ref type="bibr" target="#b23">[25]</ref> and scale dependent pooling (SDP) <ref type="bibr" target="#b36">[38]</ref> detections are also provided for evaluating the tracking performance. The number of trajectories in the training data is 546 and the number of total frames is 5, 316.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Our proposed multi-scale TrackletNet is purely trained on MOT dataset. The extracted appearance feature has 512 dimensions, i.e., d ap = 512. The time window T is set to 64 and the batch size B is set to 32. We use Adam optimizer with a learning rate of 10 −3 at the beginning. We decrease the learning rate by 10 times for every 2, 000 steps until it reaches 10 −5 . As mentioned above, the MOT dataset is quite small for training a complex neural network. However, the framework of our proposed TNT is carefully designed to avoid over-fitting. In addition, augmentation approaches are used for generating the training data, i.e., tracklets, as follows.</p><p>Bounding box randomization. Instead of using the ground truth bounding boxes for training, we randomly disturb the size and location of bounding boxes by a factor α sampled from the normal distribution N (0, 0.05 2 ). Since the detection results could be very noisy, this randomization will make sure the data from training and testing are as similar as possible. For each embedded detection before TrackletNet, the four parameters, i.e., (x, y, w, h), are normalized by the size of the frame image to ensure the input of TrackletNet keeps the same scale in different datasets.</p><p>Tracklet generation. Here, we randomly divide the trajectory of each object into small pieces of tracklets as follows. For each frame, we sample a random number from the uniform distribution, if it is smaller than a threshold, then we set this frame as the breaking frame. Then we split the entire trajectory based on the breaking frames into tracklets.</p><p>In the training stage, we randomly generate tracklets with augmentations mentioned above. For each training data, two tracklets are randomly selected as the input if they can satisfy the condition of the edge defined in the graph model in Section 3.1. If they are from the same object, the training label is set to be 1; otherwise, 0 is assigned as the label. To make it no bias, positive and negative pairs are sampled equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Feature Map Visualization</head><p>To better understand the effectiveness of our proposed TrackletNet, we also plot two examples of feature maps as shown in <ref type="figure" target="#fig_1">Figure 4</ref>. For each column (a) and (b) in <ref type="figure" target="#fig_1">Figure 4</ref>, the top figure shows the spatial locations of the two input tracklets in the 64-frame time window. Blue and green colors represent two tracklets respectively. The bottom figure shows the corresponding feature map in the time-channel plane after the max pooling of Conv3 with kernel size 5. The horizontal axis represents the time domain which aligns with the figures in the top row, while the vertical axis represents different channels in the feature map. For the example shown in (a), most higher values of the feature map are on the left side since the connection between the two tracklets is on the left part of the time window. As for (b), higher values in the feature map are on the middle side of the time window, which also matches the situation of the two input tracklets. From the feature map, we can see that the connection part of the input tracklets has strong activation, which is critical for the connectivity measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Tracking Performance</head><p>Quantitative results on MOT16 and MOT17 datasets. We also provide our quantitative results on MOT16 and MOT17 benchmark datasets compared with other state-ofthe-art methods, which are shown in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table">Table 2</ref>. Note that we use IDF1 <ref type="bibr" target="#b24">[26]</ref> and MOTA as major factors to evaluate the reliability of a tracker. As mentioned in <ref type="bibr" target="#b24">[26]</ref>, there are several weaknesses of MOTA metric, which is very sensitive to the detection threshold. Instead, IDF1 score compares ground truth trajectory and computes trajectory by a bipartite graph, which reflects how long of an object has been correctly tracked. We can see that our IDF1 score is much higher than other state-of-the-art methods. For other metrics shown in the table, we are also among the top rankings.</p><p>Qualitative results for different scenarios. With the trained model on the MOT dataset, we also test our proposed tracker on other scenarios without any fine-tuning. Promising results are also achieved. <ref type="figure" target="#fig_2">Figure 5</ref> shows some qualitative tracking results using our proposed tracker on other applications, like 3D pose estimation and UAV applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Study</head><p>Occlusion Handling. Occlusion is one of the major challenges in MOT. Our framework can easily handle both partial and full occlusions even with a very long time range. When a person is occluded, the detection as well as appearance features are unreliable. During generating the track- <ref type="bibr" target="#b19">[20]</ref> 48  lets, when we test that there is a large change in appearance, we just stop detection association even the detection result is available. After several or tens of frames, when the same person appears again from occlusion, a new tracklet will be assigned to the person. Then the connectivity between these two tracklets will be measured to distinguish whether they are the same person. Once they are confirmed with the same ID, we can easily fill out the missing detections with linear interpolation. <ref type="figure" target="#fig_3">Figure 6</ref> shows qualitative results for handling occlusions. The first row of <ref type="figure" target="#fig_3">Figure 6</ref> is from the MOT17-08 sequence. At frame 566, the person with a red bounding box is fully occluded by a statue. But it can be correctly tracked after it appears again at frame 604. The second row is one example of the MOT17-01 sequence, the person with the red bounding box goes across five other pedestrians, but the IDs of all targets keep consis-    eration, we run detection association on MOT17-10 and MOT17-13 with the Faster-RCNN detector because these two sequences have large camera motion. <ref type="table" target="#tab_2">Table 3</ref> shows the results with/without epipolar geometry. Two types of error rates are evaluated, i.e., false discovery rate (FDR) and false negative rate (FNR), which are defined as follows,</p><formula xml:id="formula_5">Tracker IDF1 ↑ MOTA ↑ MT ↑ ML ↓ FP ↓ FN ↓ IDsw. ↓ Frag ↓ GCRA</formula><formula xml:id="formula_6">FDR = FP TP + FP , FNR = FN TP + FN ,<label>(4)</label></formula><p>where TP, FP and FN represent true positive, false positive and false negative, respectively. From <ref type="table" target="#tab_2">Table 3</ref>, we can see that FDR is quite small in both cases, which means only a small portion of incorrect associations is involved in the tracklet generation. It shows the effectiveness of our tracklet-based graph model. On the other hand, FNR largely drops with epipolar geometry adopted, especially for the MOT17-13 sequence, which reflects the effectiveness of the proposed tracklet generation strategy.</p><p>Robustness to Appearance Features. Another major advantage of our TrackletNet is the ability to address overfitting learning of appearance features. Different from <ref type="bibr" target="#b19">[20]</ref>, our TrackletNet is trained only on MOT dataset without using additional tracking datasets, but we can still achieve very good performance. This is because of the dimension independence of appearance features in training the network with convolutions only conducted in the time domain. As a result, the complexity of the network is largely reduced, which also decreases the effect of overfitting.</p><p>To test the model robustness to appearance features, we disturb the appearance features with Gaussian noise on MOT17-02 sequence. The compared baseline method is using the Bhattacharyya distance of appearance features between the input pair of tracklets as the edge cost in the graph, which is commonly used in person Re-ID tasks. The comparison results are shown in <ref type="table" target="#tab_3">Table 4</ref> with Gaussian noise using different standard deviations (Std). From the table, we can see that the baseline method degrades largely with the increasing of noise level, while the tracking performance is not affected much for TNT. This is because TNT measures the temporal continuity of features as the similarity rather than using feature distance itself, which can largely suppress unreliable detections or noise in tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we propose a novel multi-object tracking method TNT based on a tracklet graph model, including tracklet vertex generation with epipolar geometry and connectivity edge measurement by a multi-scale Tracklet-Net. Our TNT outperforms other state-of-the-art methods on MOT16 and MOT17 benchmarks. We also show some qualitative results on different scenarios and applications using TNT. Robustness of TNT is further discussed with handling occlusions.</p><p>However, fast camera motion is still a challenge in 2D tracking. In our future work, we are going to convert 2D tracking to 3D tracking with the help of visual odometry. Once the 3D location of the object in the world coordinate can be estimated, the trajectory of the object should be much smoother than the 2D case.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>An example of EG-based detection association. (a) t-th frame with target detection (blue). (b) (t+1)-th frame with new detections (yellow). The target detection from t-th frame (blue dash-box) has a larger IOU with a different candidate detection in (t+1)-th frame (right yellow box). (c) examples of candidate predicted bounding boxes (red dash-boxes) intersected with epipolar lines (green dash-lines). (d) the predicted bounding box (red) in (t+1)-th frame overlapped with the correct detection (yellow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Examples of feature maps. For each column, the top figure shows the spatial locations of the two input tracklets in the 64-frame time window. The bottom figure is the corresponding feature map after the max pooling of Conv3 with the kernel size 5, which aligns with the figure in the top row in the time domain.We can see that the connection part of the input tracklets in the time domain have strong activations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Tracking in different scenarios. (a) Tracking on campus pose estimation dataset. 3D human pose can be further estimated using the tracking results. (b) Tracking for UAV applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Occlusion handling in different MOT sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Tracking performance on the MOT16 test set. Best in bold, second best in blue.</figDesc><table><row><cell></cell><cell>.6</cell><cell>48.2</cell><cell>12.9%</cell><cell>41.1%</cell><cell>5,104</cell><cell>88,586</cell><cell>821</cell><cell>1,117</cell></row><row><cell>oICF [14]</cell><cell>49.3</cell><cell>43.2</cell><cell>11.3%</cell><cell>48.5%</cell><cell>6,651</cell><cell>96,515</cell><cell>381</cell><cell>1,404</cell></row><row><cell>MOTDT [18]</cell><cell>50.9</cell><cell>47.6</cell><cell>15.2%</cell><cell>38.3%</cell><cell>9,253</cell><cell>85,431</cell><cell>792</cell><cell>1,858</cell></row><row><cell>LMP [33]</cell><cell>51.3</cell><cell>48.8</cell><cell>18.2%</cell><cell>40.1%</cell><cell>6,654</cell><cell>86,245</cell><cell>481</cell><cell>595</cell></row><row><cell>MCjoint [13]</cell><cell>52.3</cell><cell>47.1</cell><cell>20.4%</cell><cell>46.9%</cell><cell>6,703</cell><cell>89,368</cell><cell>370</cell><cell>598</cell></row><row><cell>NOMT [3]</cell><cell>53.3</cell><cell>46.4</cell><cell>18.3%</cell><cell>41.4%</cell><cell>9,753</cell><cell>87,565</cell><cell>359</cell><cell>504</cell></row><row><cell>DMMOT [42]</cell><cell>54.8</cell><cell>46.1</cell><cell>17.4%</cell><cell>42.7%</cell><cell>7,909</cell><cell>89,874</cell><cell>532</cell><cell>1,616</cell></row><row><cell>TNT (Ours)</cell><cell>56.1</cell><cell>49.2</cell><cell>17.3%</cell><cell>40.3%</cell><cell>8,400</cell><cell>83,702</cell><cell>606</cell><cell>882</cell></row><row><cell>Tracker</cell><cell>IDF1 ↑</cell><cell>MOTA ↑</cell><cell>MT ↑</cell><cell>ML ↓</cell><cell>FP ↓</cell><cell>FN ↓</cell><cell>IDsw. ↓</cell><cell>Frag ↓</cell></row><row><cell>MHT DAM [15]</cell><cell>47.2</cell><cell>50.7</cell><cell>20.8%</cell><cell>36.9%</cell><cell>22,875</cell><cell>252,889</cell><cell>2,314</cell><cell>2,865</cell></row><row><cell>FWT [9]</cell><cell>47.6</cell><cell>51.3</cell><cell>21.4%</cell><cell>35.2%</cell><cell>24,101</cell><cell>247,921</cell><cell>2,648</cell><cell>4,279</cell></row><row><cell>HAM SADF17 [39]</cell><cell>51.1</cell><cell>48.3</cell><cell>17.1%</cell><cell>41.7%</cell><cell>20,967</cell><cell>269,038</cell><cell>1,871</cell><cell>3,020</cell></row><row><cell>EDMT17 [2]</cell><cell>51.3</cell><cell>50.0</cell><cell>21.6%</cell><cell>36.3%</cell><cell>32,279</cell><cell>247,297</cell><cell>2,264</cell><cell>3,260</cell></row><row><cell>MOTDT17 [18]</cell><cell>52.7</cell><cell>50.9</cell><cell>17.5%</cell><cell>35.7%</cell><cell>24,069</cell><cell>250,768</cell><cell>2,474</cell><cell>5,317</cell></row><row><cell>jCC [12]</cell><cell>54.5</cell><cell>51.2</cell><cell>20.9%</cell><cell>37.0%</cell><cell>25,937</cell><cell>247,822</cell><cell>1,802</cell><cell>2,984</cell></row><row><cell>DMAN [42]</cell><cell>55.7</cell><cell>48.2</cell><cell>19.3%</cell><cell>38.3%</cell><cell>26,218</cell><cell>263,608</cell><cell>2,194</cell><cell>5,378</cell></row><row><cell>TNT (Ours)</cell><cell>58.0</cell><cell>51.9</cell><cell>23.5%</cell><cell>35.5%</cell><cell>37,311</cell><cell>231,658</cell><cell>2,294</cell><cell>2,917</cell></row></table><note>Table 2. Tracking performance on the MOT17 test set. Best in bold, second best in blue.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>tent along the time. The last row shows the person with a yellow bounding box is crossing the street from MOT17-06 sequence captured with a moving camera. Although it is occluded by several other pedestrians, it can be still effectively tracked in a long run.</figDesc><table><row><cell cols="4">Video Seq. EG Involved FDR (%) FNR (%)</cell></row><row><cell>MOT17-10</cell><cell>×</cell><cell>2.4 2.4</cell><cell>6.5 5.9</cell></row><row><cell>MOT17-13</cell><cell>×</cell><cell>3.6 3.4</cell><cell>12.4 9.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Effectiveness of Tracklet Generation with Epipolar Ge-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ometry. To check the effectiveness of EG in tracklet gen-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The effectiveness of tracklet generation with EG.</figDesc><table><row><cell>Noise (Std)</cell><cell>Method</cell><cell cols="3">IDF1 MOTA IDsw.</cell></row><row><cell>σ = 0.05</cell><cell>Baseline TNT</cell><cell>31.7 34.1</cell><cell>22.4 22.5</cell><cell>23 20</cell></row><row><cell>σ = 0.1</cell><cell>Baseline TNT</cell><cell>31.1 34.1</cell><cell>22.1 22.3</cell><cell>26 21</cell></row><row><cell>σ = 0.2</cell><cell>Baseline TNT</cell><cell>20.6 34.0</cell><cell>19.0 22.5</cell><cell>80 20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The robustness of TNT compared with the baseline method to disturbed appearance features.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhancing detection model for multiple hypothesis tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2143" to="2152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3038" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d traffic scene understanding from movable platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1012" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusion of head and full-body detectors for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">T-cnn: Tubelets with convolutional neural networks for object detection from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="817" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Motion segmentation &amp; multiple object tracking by correlation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A multi-cut formulation for joint segmentation and tracking of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhongjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06317</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online multi-person tracking using integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kieritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hübner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="122" to="130" />
		</imprint>
	</monogr>
	<note>13th IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4696" to="4704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-object tracking with neural gating using bilinear lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="200" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiple object tracking by efficient graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="445" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Real-time multiple people tracking with deeply learned candidate selection and person re-identification. ICME</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haizhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zijie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Online video object detection using association lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04555</idno>
		<title level="m">Trajectory factory: Tracklet cleaving and reconnection by deep siamese bi-gru for multiple object tracking</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-target tracking by discrete-continuous energy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler ; A. Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2054" to="2068" />
		</imprint>
	</monogr>
	<note>Online multi-target tracking using recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimization, approximation, and complexity classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer and system sciences</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="440" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multicamera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Features for multi-target multi-camera tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10859</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01909</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-toend learning of motion, appearance and interaction cues for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Subgraph decomposition for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5033" to="5041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiperson tracking by multicut and deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="100" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3539" to="3548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single-camera and inter-camera vehicle tracking and 3d speed estimation based on fusion of visual and semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop (CVPRW) on the AI City Challenge</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Closedloop tracking-by-detection for rov-based multiple fish tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision for Analysis of Underwater Imagery (CVAUI), 2016 ICPR 2nd Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiple target tracking based on undirected hierarchical relation hypergraph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1282" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3645" to="3649" />
		</imprint>
	</monogr>
	<note>Image Processing (ICIP</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2129" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Online multi-object tracking with historical appearance matching and scene adaptive detection filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boragule</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding highlevel semantics by modeling traffic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3056" to="3063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multitarget, multi-camera tracking by hierarchical clustering: Recent progress on dukemtmc project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09531</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="366" to="382" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
