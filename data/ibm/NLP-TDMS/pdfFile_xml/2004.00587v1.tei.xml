<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Symmetry and Group in Attribute-Object Compositions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
							<email>yongluli@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Symmetry and Group in Attribute-Object Compositions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attributes and objects can compose diverse compositions. To model the compositional nature of these general concepts, it is a good choice to learn them through transformations, such as coupling and decoupling. However, complex transformations need to satisfy specific principles to guarantee the rationality. In this paper, we first propose a previously ignored principle of attribute-object transformation: Symmetry. For example, coupling peeled-apple with attribute peeled should result in peeled-apple, and decoupling peeled from apple should still output apple. Incorporating the symmetry principle, a transformation framework inspired by group theory is built, i.e. SymNet. SymNet consists of two modules, Coupling Network and Decoupling Network. With the group axioms and symmetry property as objectives, we adopt Deep Neural Networks to implement SymNet and train it in an endto-end paradigm. Moreover, we propose a Relative Moving Distance (RMD) based recognition method to utilize the attribute change instead of the attribute pattern itself to classify attributes. Our symmetry learning can be utilized for the Compositional Zero-Shot Learning task and outperforms the state-of-the-art on widely-used benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Attributes describe the properties of generic objects, e.g. material, color, weight, etc. Understanding the attributes would directly facilitate many tasks that require deep semantics, such as scene graph generation <ref type="bibr" target="#b15">[16]</ref>, object perception <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref>, human-object interaction detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. As side information, attributes can also be employed in zero-shot learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Going along with the road of conventional classification setting, some works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b29">30]</ref> address attribute More? <ref type="bibr">(a)</ref> (b) <ref type="figure">Figure 1</ref>. Except for the compositionality and contextuality, attribute-object compositions also have the symmetry property. For instance, a peeled-apple should not change after "adding" the peeled attribute. Similarly, an apple should keep the same after "removing" the peeled attribute because it does not have it.</p><p>recognition with the typical discriminative models for objects and achieve poor performance. This is because attributes cannot be well expressed independently of the context <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>  <ref type="figure">(Fig. 1(a)</ref>). Subsequently, researchers rethink the nature of attributes and treat them as linear operations <ref type="bibr" target="#b25">[26]</ref> to operate these two general concepts, e.g. "adding" attribute to object (coupling) or "removing" attribute from objects (decoupling). Though such new insight has promoted this field, the current "add-remove" system is not complete and lacks an axiomatics foundation to satisfy the specific principles of nature. In this paper, we rethink the physical and linguistic properties of attributeobject, and propose a previously ignored but important principle of attribute-object transformations: symmetry, which would promote attribute-object learning. Symmetry depicts the invariance under transformations, e.g. a circle has rotational symmetry under the rotation without changing its appearance. The transformation that "adding" or "removing" attributes should also satisfy the symmetry: An object should remain unchanged if we "add" an attribute which it already has, or "remove" an attribute which it does not have. For instance, a peeled-apple keeps invariant if we "add" attribute peeled upon it. Similarly, "removing" peeled from apple would still result in apple. As shown in <ref type="figure">Fig. 1(b)</ref>, except the compositionality and contextuality, the symmetry property should also be satisfied to guarantee the rationality. In view of this, we first introduce the symmetry and propose SymNet to depict it. In this work, we aim to bridge attribute-object learning and group theory. Because the elegant properties of group theory would largely help in a more principled way, given its great theoretical potential. Thus, to cover the principles existing in transformations theoretically, the principles from group theory are borrowed to model the symmetry. In detail, we define three transformations {"keep", "add", "remove"} and an operation to perform three transformations upon objects, to construct a "group". To implement these, SymNet adopts Coupling Network (CoN) and Decoupling Network (DecoN) to perform coupling/adding and decoupling/removing. On the other hand, to meet the fundamental requirements of group theory, symmetry and the group axioms closure, associativity, identity element, invertibility element are all implemented as the learning objectives of SymNet. Naturally, SymNet considers the compositionality and contextuality during coupling and decoupling of various attributes and objects. All above principles will be learned under a unified model in an end-to-end paradigm.</p><p>With symmetry learning, we can apply SymNet to address the Compositional Zero-Shot Learning (CZSL), whose target is to classify the unseen compositions composed of seen attributes and objects. We adopt a novel recognition paradigm, Relative Moving Distance (RMD) <ref type="figure" target="#fig_0">(Fig. 2</ref>). That is, given a specific attribute, an object would be manipulated by the "add" and "remove" transformations parallelly in latent space. When those transformations meet the symmetry principle: if the input object already has the attribute, the output after addition should be close to the original input object, and the object after removal should be far from the input. Contrarily, if the object does not have the given attribute, the object after removal should be closer to the input than the object after addition. Thus, attribute classification can be accomplished concurrently by comparing the relative moving distances between the input and two outputs. With RMD recognition, we can utilize the robust attribute change to classify the attributes, instead of only relying on the dramatically unstable visual attribute patterns. Extensive experiments show that our method achieves significant improvements on CZSL benchmarks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>The main contributions of this paper are: 1) We propose a novel property of attribute-object composition transformation: symmetry, and design a framework inspired by group theory to learn it under the supervision of group axioms. 2) Based on symmetry learning, we propose a novel method to infer the attributes based on Relative Moving Distance. 3) We achieve substantial improvements in attribute-object composition zero-shot learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Attribute. Visual attribute was introduced into computer vision to reduce the gap between visual patterns and object concepts, such as reducing the difficulty in object recognition <ref type="bibr" target="#b7">[8]</ref> or acting as an intermediate representation for zero-shot learning <ref type="bibr" target="#b14">[15]</ref>. After that, attribute has been widely applied in recognition of face <ref type="bibr" target="#b19">[20]</ref>, people <ref type="bibr" target="#b0">[1]</ref>, pedestrian <ref type="bibr" target="#b5">[6]</ref> or action <ref type="bibr" target="#b34">[35]</ref>, person Re-ID <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31]</ref>, zeroshot learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr">40]</ref>, caption generation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref> and so on. Therefore, attribute recognition is a fundamental problem to promote the visual concept understanding.</p><p>The typical approach for attribute recognition is to train a multi-label discriminative model same as object classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b29">30]</ref>, which ignores the intrinsic properties of attributes, such as compositionality and contextuality. Farhadi et al. <ref type="bibr" target="#b7">[8]</ref> propose a visual feature selection method to recognize the attributes, under the consideration of cross-category generalization. Later, some works start to consider the properties by exploiting the attribute-attribute or attribute-object correlations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23]</ref>. Considering the contextuality of attributes, Nagarajan et al. <ref type="bibr" target="#b25">[26]</ref> regard attributes as linear transformations operated upon object embeddings, and Misra et al. <ref type="bibr" target="#b24">[25]</ref> map the attributes into model weight space to attain better representations. Compositional Zero-Shot Learning. CZSL is a crossing filed of compositional learning and zero-shot learning. In the CZSL setting, test compositions are unseen during training, while each component is seen in both training set and test set. Chen et al. <ref type="bibr" target="#b3">[4]</ref> construct linear classifiers for unseen compositions with tensor completion of weight vectors. Misra et al. <ref type="bibr" target="#b24">[25]</ref> consider that the model space is more smooth, thus project the attributes or objects into model space by training binary linear SVMs for the corresponding components. To deal with the CZSL task, it composes the attribute and object embeddings in model space as composition representation. Wang et al. <ref type="bibr" target="#b36">[37]</ref> address the attribute-object compositional problem via conditional embedding modification which relies on attribute word embedding <ref type="bibr" target="#b23">[24]</ref> transformation. Nan et al. <ref type="bibr" target="#b26">[27]</ref> map the image features and word vectors <ref type="bibr" target="#b31">[32]</ref> into embedding space with the reconstruction constraint. Nagarajan et al. <ref type="bibr" target="#b25">[26]</ref> regard attributes as linear operations for object embedding and map the image features and transformed object embeddings into a shared latent space. However, linear and explicit matrix transformation may be insufficient to represent various attribute concepts of different complexity, e.g. representing "red" and "broken" as matrices with the same capacity. Previous methods usually ignored or incompletely considered the natural principles within the coupling and decoupling of attributes and objects. In light of this, we propose a unified framework inspired by group theory to learn these important principles such as symmetry.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><formula xml:id="formula_0">&lt; if ≥ if Peeled ?  Yes  No Figure 2.</formula><p>Overview of our proposed method. We construct a "group" to learn the symmetry and operate the composition learning.</p><p>Thus we can utilize it to obtain a deeper understanding of attribute-object, e.g., to address the CZSL task <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42]</ref>. To learn the symmetry in transformations, we need a comprehensive framework to cover all principles. Inspired by the group theory, we define a unified model named SymNet. We define G = {T e , T + , T − } which contains identity ("keep"), coupling ("add") and decoupling ("remove") transformations (Sec. 3.1) for each specific attribute and utilize Deep Neural Networks to implement them (Sec. 3.2). To depict symmetry theoretically, it is a natural choice to adopt group theory as the close associations between symmetry and group in physics and mathematics. Since a group should satisfy the group axioms, i.e., closure, associativity, identity element, and invertibility element, we construct the learning objectives based on these axioms to train the transformations (Sec. 3.3). In addition, SymNet also satisfies the commutativity under conditions. With the above constraints, we can naturally guarantee compositionality and contextuality. Symmetry allows us to use a novel method, Relative Moving Distance, to identify whether an object has a certain attribute with the help of T + and T − (Sec. 3.4) for CZSL task (Sec. 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Group Definition</head><p>To depict the symmetry, we need to first define the transformations. Naturally, we need two reciprocal transformations to "add" and "remove" the attributes. Further, we need an axiomatic system to restrain the transformations and keep the rationality. Thus, we define three transformations G = {T e , T + , T − } and an operation "·". In practice, it is difficult to strictly follow the theory considering the physical and linguistic truth. For example, the operation between attribute transformations "peeled · broken" is odd. Thus the "operation" here is defined to be operated upon object only.</p><p>Definition 1. Identity transformation T e keep the attributes of object. Coupling transformation T + couples a specific attribute with an object. Decoupling transformation T − decouples a specific attribute from an object. Definition 2. Operation "·" performs transformations {T e , T + , T − } upon object. Noticeably, operation "·" is not the dot product and we use this notation to maintain the consistence with group theory.</p><p>More formally, for object o ∈ O and attribute a i , a j ∈ A, a i = a j , where O denotes object set and A denotes attribute set, operation "·" performs transformations in G upon an object/image embedding:</p><formula xml:id="formula_1">f i o · T + (a j ) = f ij o , f ij o · T − (a j ) = f i o , f i o · T e = f i o ,<label>(1)</label></formula><p>where f i o means o has one attribute a i and f ij o means o has two attributes a i , a j . Here we do not sign a specific object category and use o for simplicity. Definition 3. G has the symmetry property if and only if ∀a i , a j ∈ A, a i = a j :</p><formula xml:id="formula_2">f i o = f i o · T + (a i ), f i o = f i o · T − (a j ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Group Implementation</head><p>In practice, when performing T e upon f i o , we directly use f i o as the f i o ·T e to implement the identity transformation for  <ref type="figure">Figure 3</ref>. The structure of CoN and DecoN. They take the attribute embedding to assign a specific attribute a j . f i o , f ij o are the object embeddings extracted from ResNet-18 <ref type="bibr" target="#b8">[9]</ref>. simplicity. For other two transformations T + , T − , we propose SymNet consists of two modules: Coupling Network (CoN) and Decoupling Network (DecoN). CoN and DecoN have the same structure but independent weights and are trained with different tasks. As seen in <ref type="figure">Fig. 3</ref>, CoN and De-coN both take the image embedding f i o of an object and the embedding of attribute a j as inputs, and output the transformed object embedding. We use the attribute category word embeddings such as GloVe <ref type="bibr" target="#b31">[32]</ref> or onehot vector to represent the attributes. f i o is extracted by an ImageNet <ref type="bibr" target="#b4">[5]</ref> pre-trained ResNet <ref type="bibr" target="#b8">[9]</ref> from image I, i.e. f i o = F res (I). Intuitively, attributes affect objects in different ways, e.g. "red" changes the color, "wooden" changes the texture. In CoN and DecoN, we use an attribute-as-attention strategy, i.e. using att = g(a j ) as attention, where g(·) means two fully-connected (FC) and a Softmax layers. We concatenate f i o • att + f i o with original a j as the input and use two FC layers to perform the transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Group Axioms as Objectives</head><p>According to group theory, SymNet should satisfy four group axioms: closure, associativity, identity element, and invertibility. Under certain conditions, attribute-object also satisfy commutativity. Besides, SymNet must obey the symmetry property of the attribute transformations.</p><p>In practice, we use Deep Neural Networks to implement transformations. Thus, we can construct training objectives to approach the theoretic transformations following the axioms. Considering the actual characteristics of attributeobject compositions, we slightly adjust the axioms to construct the objectives. Besides, there are two situations with different forms of axioms: 1) coupling or decoupling an attribute a i that the object f i o already has, or 2) coupling or decoupling an attribute a j that object f i o does not have. Symmetry. First of all, SymNet should satisfy the symmetry property as depicted in Eq. 2, i.e.,</p><formula xml:id="formula_3">f i o = f i o ·T + (a i ), f i o = f i o · T − (a j )</formula><p>. The symmetry is essential to keep the semantic meaning during coupling and decoupling. For example, given a peeled-egg, adding the attribute peeled again should not change the object state. Similarly, a cup without attribute broken should remain unchanged after removing broken. Thus, we construct the symmetry loss:</p><formula xml:id="formula_4">L sym = f i o − f i o · T + (a i ) 2 + f i o − f i o · T − (a j ) 2 . (3)</formula><p>where a i , a j ∈ A, i = j. We use L 2 norm loss to measure the distance between two embeddings. Closure. For all elements in set G, their operation results should also be in G. In SymNet, for the attribute</p><formula xml:id="formula_5">a i that f i o has, f i o · T + (a i ) · T − (a i ) should be equal to f i o · T − (a i ). For the attribute a j that f i o does not have, f i o · T − (a j ) · T + (a j ) should be equal to f i o · T + (a j )</formula><p>. Thus, we construct:</p><formula xml:id="formula_6">L clo = f i o · T + (a i ) · T − (a i ) − f i o · T − (a i ) 2 + f i o · T − (a j ) · T + (a j ) − f i o · T + (a j ) 2 ,<label>(4)</label></formula><p>Identity Element. The properties of identity element T e are automatically satisfied since we implement T e as a skip</p><formula xml:id="formula_7">connection, i.e. f i o ·T * (a i )·T e = f i o ·T e ·T * (a i ) = f i o ·T * (a i ) where T * denotes any element in G. Invertibility Element. According to the definition, T + is the invertibility element of T − , vice versa. For the attribute a i that f i o has, f i o ·T − (a i )·T + (a i ) should be equal to f i o ·T e = f i o . For the attribute a j that f i o does not have, f i o · T + (a j ) · T − (a j ) should be equal to f i o ·T e = f i o .</formula><p>Therefore, we have:</p><formula xml:id="formula_8">L inv = f i o · T + (a j ) · T − (a j ) − f i o · T e 2 + f i o · T − (a i ) · T + (a i ) − f i o · T e 2 .<label>(5)</label></formula><p>Associativity. In view of the practical physical meaning of attribute-object compositions, we only define the operation "·" that operates a transformation upon an object embedding in Sec. 3.1, but do not define the operation between transformations. Therefore, we relax the constraint here and do not construct an objective according to associativity in practice.</p><p>Commutativity. Because of the speciality of attributeobject, SymNet satisfies the commutativity when coupling and decoupling multiple attributes. Thus,</p><formula xml:id="formula_9">f i o · T + (a i ) · T − (a j ) should be equal to f i o · T − (a j ) · T + (a i ): L com = f i o · T + (a i ) · T − (a j )− f i o · T − (a j ) · T + (a i ) 2 .<label>(6)</label></formula><p>Although above definitions do not strictly follow the theory, but the loosely conducted axiom objectives still contribute to the robustness and effectiveness a lot (ablation study in Sec. <ref type="bibr">3.5)</ref> and open a door to a more theoretical way. The last but not the least, CoN and DecoN need to keep the semantic consistency, i.e. before and after the transformation, the object category should not change. Hence we use a cross-entropy loss L o cls for the object recognition of the input and output embeddings of CoN and Decon. In the same way, before and after coupling and decoupling, the attribute changes provide the attribute classification loss L a cls . We use typical visual pattern-based classifiers consisting of FC layers for the object and attribute classifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relative Moving Distance</head><p>As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, we utilize the Relative Moving Distance (RMD) based on the symmetry property to operate the attribute recognition. Given an image embedding f x o of an object with unknown attribute a x , we first input it to both CoN and DecoN with all kinds of attributes word embeddings {a 1 , a 2 , ..., a n } where n is the number of attributes. Two transformers would take attribute embeddings as conditions and operate the coupling and decoupling in parallel, then output 2n transformed em-</p><formula xml:id="formula_10">beddings {f x o · T + (a 1 ), f x o · T + (a 2 ), ..., f x o · T + (a n )} and {f x o · T − (a 1 ), f x o · T − (a 2 ), ..., f x o · T − (a n )}.</formula><p>We compute the distances between f x o and the transformed embeddings:</p><formula xml:id="formula_11">d i + = f x o − f x o · T + (a i ) 2 , d i − = f x o − f x o · T − (a i ) 2 .<label>(7)</label></formula><p>To compare two distances, we define Relative Moving Distance as d i = d i − − d i + and perform binary classification for each attribute <ref type="figure" target="#fig_4">(Fig. 4)</ref></p><formula xml:id="formula_12">: 1) If d i ≥ 0, i.e. f x o · T + (a i ) is closer to f i o than f x o ·T − (a i ), we tend to believe f x o has attribute a i . 2) If d i &lt; 0, i.e. f x o ·T − (a i )</formula><p>is closer, we tend to predict that f x o does not have attribute a i . Previous zero/few-shot learning methods usually classify the instances via measuring the distance between the embedded instances and fixed points like prototype/label/centroid embeddings. Differently, Relative Moving Distance compares the distance before and after applying the coupling and decoupling operation. Training. To enhance the RMD-based classification performance, we further use a triplet loss function. Let X denote the set of attributes that f x o has, the loss can be described as:</p><formula xml:id="formula_13">L tri = X i [d i + − d i − + α] + + A−X j [d j − − d j + + α] + ,<label>(8)</label></formula><p>where α=0.5 is triplet margin. d i + should be less than d i − for the attributes that f x o has and greater than d i − for the attributes f x o does not have. The total loss of SymNet is</p><formula xml:id="formula_14">L total =λ 1 L sym + λ 2 L axiom + λ 3 L a cls + λ 4 L o cls + λ 5 L tri ,<label>(9)</label></formula><p>where L axiom = L clo + L inv + L com . Inference. In practice, for n attribute categories, we use</p><formula xml:id="formula_15">RMDs d = {d i } n i=1 as the attribute scores, i.e. S a = {S i a } n i=1 = {d i } n i=1</formula><p>and obtain attribute probability with Sigmoid function: p i a = Sigmoid(S i a ). Notably, we also consider the scale and use a factor γ to adjust the score before Sigmoid. Our method can be operated in parallel, i.e., we simultaneously compute the RMD values of n attributes. We input [B, n, 300] sized tensor where B is the mini-batch size and 300 is the object embedding size. CoN and DecoN would output two [B, n, 300] sized embeddings  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussion: Composition Zero-Shot Learning</head><p>With robust and effective symmetry learning for attribute-object, we can further apply SymNet to CZSL <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42]</ref>. The goal of CZSL is to infer the unseen attribute-object pairs in test set, i.e. a prediction is true positive if and only if both attribute and object classifications are accurate. The pair candidates are available during testing, thus the predictions of impossible pairs can be masked.</p><p>We propose a novel method to address this task based on Relative Moving Distance (RMD). With Relative Moving Distance d i = d i − − d i + , the probabilities of attribute category are computed as p i a = Sigmoid(d i ). For object category, we input the object embedding to 2-layer FC with Softmax to obtain the object scores S o = {S j o } m j=1 , where m is the number of object categories. The object category probability p j = Sof tmax(S j o ) and p o = {p i o } m j=1 . We then use p ij ao to represent the probability of an attributeobject pair in test set which is composed of the i-th attribute category and j-th object. The pair probabilities are given by p ij ao = p i a × p j o . The impossible compositions would be masked according to the benchmarks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data and Metrics</head><p>Our experiments are conducted on MIT-States <ref type="bibr" target="#b11">[12]</ref> and UT-Zappos50K <ref type="bibr" target="#b41">[42]</ref>. MIT-States contains 63440 images covering 245 objects and 115 attributes. Each image is attached with one single object-attribute composition label and there are 1262 possible pairs in total. We follow the setting of <ref type="bibr" target="#b24">[25]</ref> and use 1262 pairs/34562 images for training and 700 pairs/19191 images as the test set. UT-Zappos50K is a fine-grained dataset with 50025 images of shoes annotated with shoe type-material pairs. We follow the setting and split from <ref type="bibr" target="#b25">[26]</ref>, using 83 object-attribute pairs/24898 images as train set and 33 pairs/4228 images for testing. The training and testing pairs are non-overlapping for both datasets, i.e. the test set contains unseen attribute-object pairs composed of seen attributes and objects. We report the Top-1, 2, 3 accuracies on the unseen test set as evaluation metrics. We also evaluate our model under the generalized CZSL setting of TMN <ref type="bibr" target="#b32">[33]</ref>, since the "open world" setting from <ref type="bibr" target="#b25">[26]</ref> brings biases towards unseen pairs <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines</head><p>We compare SymNet with baselines following <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b25">[26]</ref>, as well as previous state-of-the-arts. If not specified, the adopted methods are based on ResNet-18 backbone. Visual Product trains two simple classifiers for attributes and objects independently and fuses the outputs by multiplying their margin probabilities: P (a, o) = P (a)P (o). The classifiers can be either linear SVMs <ref type="bibr" target="#b24">[25]</ref> or single layer softmax regression models <ref type="bibr" target="#b25">[26]</ref>. LabelEmbed (LE) is proposed by <ref type="bibr" target="#b24">[25]</ref>. It combines the word vectors <ref type="bibr" target="#b31">[32]</ref> of attribute and object and uses 3-layer FC to transform the pair embedding into a transform matrix. The classification score is the product of transform matrix and visual feature: T (e a , e b ) φ(I). It has three variants:</p><p>1. LabelEmbed Only Regression (LEOR) <ref type="bibr" target="#b24">[25]</ref> changes the target to minimize the Euclidean distance between T (e a , e b ) and the weight of pair SVM classifier w ab . 2. LabelEmbed With Regression (LE+R) <ref type="bibr" target="#b24">[25]</ref> combines the losses of LE and LEOR aforementioned. 3. LabelEmbed+ <ref type="bibr" target="#b25">[26]</ref> embeds the attribute, object vectors, and image features into a semantic space and also optimizes the input representations during training. AnalogousAttr <ref type="bibr" target="#b3">[4]</ref> trains linear classifiers for seen compositions and uses tensor completion to generalize to the unseen pairs. We report the reproduced results from <ref type="bibr" target="#b25">[26]</ref>. Red Wine <ref type="bibr" target="#b24">[25]</ref> uses SVM weights as the attribute or object embeddings to replace the word vectors in LabelEmbed. AttrOperator <ref type="bibr" target="#b25">[26]</ref> regards attributes as linear transformations and object word vectors <ref type="bibr" target="#b31">[32]</ref> after transformation as pair embeddings. It takes the pair with the closest distance to the image feature as the recognition result. Besides the top-1 accuracy directly reported in <ref type="bibr" target="#b25">[26]</ref>, we evaluate the top-2, 3 accuracies with the open-sourced code. TAFE-Net <ref type="bibr" target="#b36">[37]</ref> uses word vectors <ref type="bibr" target="#b23">[24]</ref> of attribute-object pair as task embedding of its meta learner. It generates a binary classifier for each existing composition. We report the results based on VGG-16 which is better and more complete than the result based on ResNet-18. GenModel <ref type="bibr" target="#b26">[27]</ref> projects the visual features of images and semantic language embeddings of pairs into a shared latent space. The prediction is given by comparing the distance between visual features and all candidate pair embeddings. TMN <ref type="bibr" target="#b32">[33]</ref> adopts a set of small FC-based modules and configure them via a gating function in a task-driven way. It can <ref type="bibr">Method</ref>  <ref type="table">Top-1  Top-2 Top-3 Top-1 Top-2 Top-3</ref> Visual Product <ref type="bibr">[</ref> be generalized to unseen pairs via re-weighting these primitive modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIT-States UT-Zappos</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>For two datasets, we use ImageNet pre-trained ResNet-18 <ref type="bibr" target="#b8">[9]</ref> as the backbone to extract image features and do not fine-tune it following previous methods. We use the 300dimensional pre-trained GloVe <ref type="bibr" target="#b31">[32]</ref> vectors as the word embeddings. The 512-dimensional ResNet-18 feature is first transformed to 300-dimensional by a single FC. The main modules of our SymNet, CoN and DecoN, have the same structures but independent weights as depicted in <ref type="figure">Fig. 3</ref>: two FC layers of sizes 768/300 with Sigmoid convert the attribute embedding to 300-dimensional attention and be multiplied to the input image representation. The representation after attention is concatenated to the attribute embedding and then compressed to the original dimension by the other two 300-sized FC layers. Each hidden FC in CoN and DecoN is followed by BatchNorm and ReLU layers.</p><p>For each training image, we randomly sample another image with the same object label but different attribute as the negative sample to compute the losses (Sec. 3.3). We train SymNet with SGD optimizer on single NVIDIA GPU. We use cross-validation to determine the hyper-parameters, e.g., learning rate, weights, epochs. For MIT-States, the model is trained with learning rate 5e-4 and batch size 512 for 320 epochs. The loss weights are λ 1 = 0.05, λ 2 = 0.01, λ 3 = 1, λ 4 = 0.01, λ 5 = 0.03. For UT-Zappos, the model is trained with learning rate 1e-4 and batch size 256 for 600 epochs. The loss weights are λ 1 = 0.01, λ 2 = 0.03, λ 3 = 1, λ 4 = 0.5, λ 5 = 0.5. Notably, the weights on two datasets are different. Because MIT-States has diverse attributes and objects, while UT-Zappos contains similar fine-grained shoes. Different range and scale lead to distinct embedding spaces and different parameters for RMD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Compositional Zero-Shot Learning</head><p>To evaluate the symmetry learning in compositional zero-shot task, we conduct experiments on widely-used benchmarks: MIT-States <ref type="bibr" target="#b11">[12]</ref> and UT-Zappos <ref type="bibr" target="#b41">[42]</ref>.  <ref type="table">Table 3</ref>. Attribute learning results on two benchmarks. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> (the scores with * are reproduced by <ref type="bibr" target="#b25">[26]</ref>, the others are from <ref type="bibr" target="#b24">[25]</ref>). SymNet outperforms all baselines on two benchmarks. Although we use a simple product to compose the attribute and object scores, we still achieve 2.1% and 3.8% improvements over the state-of-the-art <ref type="bibr" target="#b26">[27]</ref> on two benchmarks respectively. On UT-Zappos, most previous approaches do not surpass the Visual Product baseline, while ours outperforms it by 2.2%. To further evaluate our SymNet, we additionally conduct the comparison on generalized CZSL setting from recent state-of-the-art TMN <ref type="bibr" target="#b32">[33]</ref>.</p><p>The results are shown in Tab. 2. SymNet also outperforms previous methods significantly, which strongly proves the effectiveness of our method. Attribute Learning. We also compare the attribute accuracy alone on two benchmarks in Tab. 3. We reproduce the results of AttrOperator <ref type="bibr" target="#b25">[26]</ref> with its open-sourced code. For all methods involved, the individual attribute and object accuracy do not consider the relations between attributes and objects. The object recognition module of our method is a simple 3-layer MLP classifier with the visual image features from ResNet-18 backbone. SymNet outperforms previous methods by a large margin, i.e. 3.8% on MIT-States and 8.3% on UT-Zappos. Our RMD-based attribute recognition is particularly effective. In addition, our object classification performance is comparable to AttrOperator <ref type="bibr" target="#b25">[26]</ref> and GenModel <ref type="bibr" target="#b26">[27]</ref>. Accordingly, the main contribution of the CZSL improvement of SymNet comes from attribute learning rather than object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Image Retrieval after Attribute Manipulation</head><p>To qualitatively evaluate SymNet, we further report the image retrieval results after attribute manipulation. We first train SymNet on MIT-States or UT-Zappos, then use trained CoN and DeCoN to manipulate the image embeddings. For an image with pair label (a, o), we remove the attribute a with DeCoN and add an attribute b with CoN, then we retrieve the top-5 nearest neighbors of the manipulated embeddings. This task is much more difficult than the normal attribute-object retrieval <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref> because of the com-  plex semantic manipulation and recognition. Retrieval results are shown in <ref type="figure" target="#fig_6">Fig. 5</ref>, where the imaged on the left are original ones and right are the nearest neighbors after manipulation. SymNet is capable of retrieving a certain number of correct samples among top-5 nearest neighbors, especially in a fine-grained dataset like UT-Zappos. This suggests that our model has well exploited the learned symmetry in attribute transformation and learned the contextuality and compositionality of attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visualization in Latent Space</head><p>To verify the robustness and principles in transformations, we use t-SNE <ref type="bibr" target="#b21">[22]</ref> to visualize the image embeddings before or after transformations in latent space in <ref type="figure">Fig. 6</ref>. Specifically, we first visualize the group axioms related transformations: 1) Closure is verified by comparing</p><formula xml:id="formula_16">{f i o · T + (a i ) · T − (a i ) v.s. f i o · T − (a i )} and {f i o · T − (a j ) · T + (a j ) v.s. f i o · T + (a j )}. 2) Invertibility is verified by comparing {f i o · T + (a j ) · T − (a j ) v.s. f i o · T e } and {f i o · T − (a i ) · T + (a i ) v.s. f i o · T e }. 3) Commutativity is verified by comparing {f i o · T + (a i ) · T − (a j ) v.s. f i o · T − (a j ) · T + (a i )}.</formula><p>The results are shown in <ref type="figure">Fig. 6 (a,b)</ref>. We observe that SymNet can robustly operate the transformations and the axiom objectives are well satisfied during embedding transformations.</p><p>Then, to verify the symmetry property, we visualize the sample embeddings in Relative Moving Space in <ref type="figure">Fig. 6(c,d)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Ablation Study</head><p>To evaluate different components of our method, we design ablation studies and report the results in Tab. 4. Objectives. To evaluate the objectives constructed from</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIT-States</head><p>Closure UT-Zappos   group axioms and the core principle symmetry, we conduct tests of these objectives by removing them. In Tab. 4, Sym-Net shows obvious degradations without the constraints of these principles. This is in line with our assumption that a transformation framework that covers the essential principles can largely promote compositional learning.</p><formula xml:id="formula_17">Commutativity f i o f i o ⋅ T + (a i ) ⋅ T − (a j ) f i o ⋅ T − (a j ) ⋅ T + (a i ) f i o ⋅ T − (a j ) ⋅ T + (a j ) f i o ⋅ T + (a j ) f i o ⋅ T + (a i ) ⋅ T − (a i ) f i o ⋅ T − (a i ) o =</formula><formula xml:id="formula_18">f i o f i o ⋅ T + (a j ) ⋅ T − (a j ) f i o ⋅ T − (a i ) ⋅ T + (a i ) f i o ⋅ T + (a j ) f i o ⋅ T − (a i ) (b) Invertibility MIT-States UT-Zappos Symmetry o = pasta, a i = cooked, a j = ruffled o = sandals, a i = nubuck, a j = nylon f i o f i o ⋅ T + (a j ) f i o ⋅ T − (a j )</formula><p>Attention. Removing the attention module drops 1.9% and 3.6% accuracy on two benchmarks.</p><p>Distance Metrics. SymNet with other distance metrics, i.e., L 1 and cosine distances, perform much worse than L 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose the symmetry property of attribute-object compositions. Symmetry reveals profound principles in composition transformations. To an object, giving it an attribute it already has, or erasing an attribute it does not have, would all result in the same object. To learn the symmetry, we construct a framework inspired by group theory to couple and decouple attribute-object compositions, and use group axioms and symmetry as the learning objectives. When applied to CZSL, our method achieves state-of-the-art performance. In the future, we consider to study the transformation with varying degrees, e.g., not-peeled, half-peeled and totally-peeled and apply SymNet to GAN-related tasks. In this section, we report normal attribute-object image retrieval results of our method in <ref type="figure" target="#fig_9">Fig. 7</ref>, which contains the indomain attributes or unseen compositions for UT-Zappos <ref type="bibr" target="#b41">[42]</ref> and MIT-States <ref type="bibr" target="#b11">[12]</ref> and out-of-domain retrieval for Visual Genome <ref type="bibr" target="#b12">[13]</ref>. We follow the settings of <ref type="bibr" target="#b25">[26]</ref>: 1) In-domain attributes or unseen compositions: we train SymNet on MIT-States or UT-Zappos and query the attributes or unseen pairs upon the test set of each dataset. 2) Out-of-domain retrieval: with SymNet only trained on MIT-States, we conduct retrieval on the large-scale Visual Genome <ref type="bibr" target="#b12">[13]</ref> with over 100K images, which is non-overlapping with the training set of MIT-States.</p><p>SymNet performs robustly on both in-domain and out-of-domain retrievals. Our model is capable of recognizing the images with queried attributes and pairs in most cases. When querying an attribute, the model accurately retrieves images across various objects, e.g. for MIT-States, the top-5 retrievals of fresh vary among fresh-egg, fresh-milk and fresh-flower. In out-of-domain retrieval, our SymNet also shows its robustness. Though it has never seen the images in Visual Genome, the model generalizes well on the target domain and returns correct retrievals, e.g. dark objects and unripe lemon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualized Transformations</head><p>In addition, we also provide more visualized transformations of the attribute-object compositions via t-SNE <ref type="bibr" target="#b21">[22]</ref> in <ref type="figure" target="#fig_10">Fig. 8</ref> and <ref type="figure">Fig. 9</ref>. We observe that the proposed {T e , T + , T − } can robustly operate the transformations. The axiom objectives and relative moving distance (RMD) rules are well satisfied during the embedding transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIT-States</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Closure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UT-Zappos</head><p>Commutativity </p><formula xml:id="formula_19">f i o f i o ⋅ T + (a i ) ⋅ T − (a j ) f i o ⋅ T − (a j ) ⋅ T + (a i ) f i o ⋅ T − (a j ) ⋅ T + (a j ) f i o ⋅ T + (a j ) f i o ⋅ T + (a i ) ⋅ T − (a i ) f i o ⋅ T − (a i ) o =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of Dataset</head><p>Comparatively, accuracy on MIT-States is much lower than UT-Zappos as MIT-States has much more object and attribute categories and suffers from noisy samples and data insufficiency.</p><p>Besides, the synonyms and near-synonyms in attributes greatly affect the results. For example, SymNet recognizes 20.4% samples with attribute ancient as old, while the visual properties of these two attributes can barely be distinguished. These results are basically correct from the human perspective but mistaken according to the benchmark. To explore this phenomenon on MIT-States, we manually select 13 sets of near-synonyms from MIT-States 1 , which are chosen according</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIT-States</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>gives an overview of our approach. Our goal is to learn the symmetry within attribute-object compositions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparison between typical method and our Relative Moving Distance (RMD) based recognition. Previous methods mainly try to adjust the decision boundary in latent space. Our RMD based approach moves the embedding point with T+ and T− and classifies by comparing their moving distances. after transformation. Then we can compute RMDs {d i } n i=1 at the same time. Our method has approximately the same speed as the typical FC classifier. The inference speed from features to RMD is about 41.0 FPS and the FC classifier speed is about 45.8 FPS. The gap can be further omitted if considering the overhead of the feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Image Retrieval on MIT-States, UT-Zappos. We conduct the retrieval after the attribute manipulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>flat shoes, a i = canvas, a j = cotton o = ceiling, a i = cracked, a j = draped Closure (a) Closure and Commutativity MIT-States UT-Zappos Invertibility o = eggs, a i = frozen, a j = diced a i = faux leather, a j = patent leather o = mid-calf boots</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>o 2 Figure 6 .</head><label>26</label><figDesc>= fence, a i = broken o = clogs and mules shoes,a i = suede f i o f i o ⋅ T + (a i ) f i o ⋅ T − (a i )(d) Symmetry-Visualization of symmetry and the group axioms by t-SNE<ref type="bibr" target="#b21">[22]</ref>. The points with colors in a same dotted box should be close.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Additional image retrievals on MIT-States, UT-Zappos (in-domain) and Visual Genome (out-of-domain).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>rope, a i = frayed, a j = loose Closure o = fish, a i = frozen, a j = cooked o = flats shoes, a i = canvas, a j = satin o = mid-calf boots o = wave, a i = large, a j = tiny o = bread, a i = moldy, a j = fresh a i = full grain leather, a j = calf hair o = slipper, a i = leather, a j = faux fur a i = faux leather, a j = cotton o = loafters shoes (a) Closure and Commutativity MIT-States UT-Zappos o = lightbulb, a i = shattered, a j = small o = pot, a i = steaming, a j = coiled o = flats shoes, a i = canvas, a j = satin a i = leather faux, a j = suede o = plastic, a i = thin, a j = molten o = ice, a i = windblown, a j = crushed o = sandals, a i = nubuck, a j = calf hair a i = suede, a j = syntheticInvertibility ⋅ T + (a j ) ⋅ T − (a j ) f i o ⋅ T − (a i ) ⋅ T + (a i ) f i o ⋅ T + (a j ) f i o ⋅ T − (a j )o = high knee boots o = mid-calf boots (b) Invertibility Visualizations of the transformations to verify group axioms. The points with colors in a same dotted box should be close.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 Figure 9 .</head><label>29</label><figDesc>UT-Zappos o = river, a i = burnt, a j = engraved o = bathroom, a i = cluttered, a j = wide o = flats shoes, a i = canvas, a j = calf hair o = ankle boots o = pants, a i = crinkled, a j = short o = room, a i = dark, a j = clean o = heels shoes, a i = calf hair, a j = cotton a i = patent leather, a j = rubberSymmetry f i o f i o ⋅ T + (a j ) f i o ⋅ T − (a j )a i = full-grain leather, a j = calf hair o = flats shoes (a) Symmetry-1 MIT-States UT-Zappos o = fence, a i = broken o = fabric, a i = brushed o = loafters shoes, a i = canvas o = copper, a i = coiled o = ballon, a i = huge o = ankle boots, a i = sheepskin o = clogs and mules shoes, a i = suede Symmetry ⋅ T + (a i ) f i o ⋅ T − (a i ) o = flats shoes, a i = patent leather (b) Symmetry-Visualizations of the transformations to verify symmetry property. The points with colors in a same dotted box should be close.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1. Results of CZSL on MIT-States and UT-Zappos.</figDesc><table><row><cell>25]</cell><cell>9.8/13.9  *</cell><cell>16.1</cell><cell>20.6</cell><cell>49.9  *</cell><cell>/</cell><cell>/</cell></row><row><cell cols="2">LabelEmbed (LE) [25] 11.2/13.4  *</cell><cell>17.6</cell><cell>22.4</cell><cell>25.8  *</cell><cell>/</cell><cell>/</cell></row><row><cell>-LEOR [25]</cell><cell>4.5</cell><cell>6.2</cell><cell>11.8</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>-LE + R [25]</cell><cell>9.3</cell><cell>16.3</cell><cell>20.8</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>-LabelEmbed+ [26]</cell><cell>14.8*</cell><cell>/</cell><cell>/</cell><cell>37.4*</cell><cell>/</cell><cell>/</cell></row><row><cell>AnalogousAttr [4]</cell><cell>1.4</cell><cell>/</cell><cell>/</cell><cell>18.3</cell><cell>/</cell><cell>/</cell></row><row><cell>Red Wine [25]</cell><cell>13.1</cell><cell>21.2</cell><cell>27.6</cell><cell>40.3</cell><cell>/</cell><cell>/</cell></row><row><cell>AttOperator [26]</cell><cell>14.2</cell><cell>19.6</cell><cell>25.1</cell><cell>46.2</cell><cell>56.6</cell><cell>69.2</cell></row><row><cell>TAFE-Net [37]</cell><cell>16.4</cell><cell>26.4</cell><cell>33.0</cell><cell>33.2</cell><cell>/</cell><cell>/</cell></row><row><cell>GenModel [27]</cell><cell>17.8</cell><cell>/</cell><cell>/</cell><cell>48.3</cell><cell>/</cell><cell>/</cell></row><row><cell>SymNet (Ours)</cell><cell>19.9</cell><cell>28.2</cell><cell>33.8</cell><cell>52.1</cell><cell>67.8</cell><cell>76.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Composition Learning. The results of CZSL are shown in Tab. 1, where the first five rows are baselines from</figDesc><table><row><cell>Model</cell><cell>1</cell><cell cols="2">Val AUC 2</cell><cell>3</cell><cell>1</cell><cell cols="2">Test AUC 2</cell><cell>3</cell><cell>Seen Unseen HM</cell></row><row><cell>AttOperator [26]</cell><cell cols="6">2.5 6.2 10.1 1.6 4.7</cell><cell cols="2">7.6</cell><cell>14.3</cell><cell>17.4</cell><cell>9.9</cell></row><row><cell>Red Wine [25]</cell><cell cols="6">2.9 7.3 11.8 2.4 5.7</cell><cell cols="2">9.3</cell><cell>20.7</cell><cell>17.9</cell><cell>11.6</cell></row><row><cell cols="7">LabelEmbed+ [26] 3.0 7.6 12.2 2.0 5.6</cell><cell cols="2">9.4</cell><cell>15.0</cell><cell>20.1</cell><cell>10.7</cell></row><row><cell>GenModel [27]</cell><cell cols="6">3.1 6.9 10.5 2.3 5.7</cell><cell cols="2">8.8</cell><cell>24.8</cell><cell>13.4</cell><cell>11.2</cell></row><row><cell>TMN [33]</cell><cell cols="8">3.5 8.1 12.4 2.9 7.1 11.5 20.2</cell><cell>20.1</cell><cell>13.0</cell></row><row><cell>SymNet (Ours)</cell><cell cols="8">4.3 9.8 14.8 3.0 7.6 12.3 24.4</cell><cell>25.2</cell><cell>16.1</cell></row><row><cell cols="9">Table 2. Results of generalized CZSL on MIT-States. All methods</cell></row><row><cell cols="9">(Sec. 4.2) use ResNet-18 [9] as the backbone.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MIT-States</cell><cell></cell><cell>UT-Zappos</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell cols="6">Attribute Object Attribute Object</cell></row><row><cell cols="3">AttrOperator [26]</cell><cell cols="2">14.6</cell><cell></cell><cell>20.5</cell><cell></cell><cell>29.7</cell><cell>67.5</cell></row><row><cell cols="2">GenModel [27]</cell><cell></cell><cell cols="2">15.1</cell><cell></cell><cell>27.7</cell><cell></cell><cell>18.4</cell><cell>68.1</cell></row><row><cell>SymNet</cell><cell></cell><cell></cell><cell cols="2">18.9</cell><cell></cell><cell>28.8</cell><cell></cell><cell>38.0</cell><cell>65.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>T − (a i ) should be far from f i o . We can also find that the relative moving distance rules are all satisfied, i.e. the symmetry is well learned by our SymNet.</figDesc><table><row><cell>: 1) for the sample f i o which do not have attribute o · T + (a j ) should be far from f i a j , f i o . On the contrary, f i o · T − (a j ) are relatively close to f i o because of the symme-try. 2) For the sample f i o with attribute a i , f i o ·T + (a i ) should be close to f i o and f i o ·</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">{cracked, shattered, splintered}; {chipped, cut}; {dirty, grimy}; {eroded, weathered}; {huge, large}; {melted, molten}; {ancient, old}; {crushed, pureed, mashed}; {ripped, torn}; {crinkled, crumpled, ruffled, wrinkled}; {small, tiny; damp, wet}</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Describing people: A poselet-based approach to attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An empirical study and analysis of generalized zeroshot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inferring analogous attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Chao-Yeh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pedestrian attribute recognition at far distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instaboost: Boosting instance segmentation via probability map guided copypasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sharing features between objects and their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discovering states and transformations in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visruth</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagnik</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by betweenclass attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Hake: Human activity knowledge engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06539</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving person reidentification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond holistic object recognition: Enriching image understanding with part states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A joint learning framework for attribute models and object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundararajan</forename><surname>Sellamanickam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From red wine to red tomato: Composition with context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attributes as operators: factorizing unseen attribute-object compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognizing unseen attribute-object pair with generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiong</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coco attributes: Attributes for people, animals, and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint semantic and latent attribute modelling for cross-class transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Massimiliano Pontil, and Tiejun Huang</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Task-driven modular networks for zero-shot compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Zero-shot video object segmentation via attentive graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tafe-net: Task-aware feature embeddings for low shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Caltech-ucsd birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Srda: Generating instance segmentation annotation via scanning, reasoning and domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic jitter: Dense supervision for visual comparisons via synthetic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On this new benchmark, our model achieves 3.03% improvement on attribute accuracy and 0.66% improvement on CZSL accuracy. We also apply this strategy to AttrOperator [26], obtain improvement of 2.25% on attribute recognition and 0.28% on CZSL recognition. Comparing to AttrOperator</title>
	</analytic>
	<monogr>
		<title level="m">to the similarity in both linguistic meanings and visual patterns</title>
		<imprint/>
	</monogr>
	<note>We then regard the attributes within each set as equal, i.e., predicting the near-synonym is also considered correct. our model suffers more from the synonym problem</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
