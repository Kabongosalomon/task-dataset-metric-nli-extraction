<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MUTAN: Multimodal Tucker Fusion for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
							<email>hedi.ben-younes@lip6.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7606</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universités</orgName>
								<orgName type="institution" key="instit2">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6, 75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<addrLine>248 rue du Faubourg Saint-Antoine</addrLine>
									<postCode>75012</postCode>
									<settlement>Heuritech, Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Cadene</surname></persName>
							<email>remi.cadene@lip6.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7606</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universités</orgName>
								<orgName type="institution" key="instit2">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6, 75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<email>matthieu.cord@lip6.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7606</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universités</orgName>
								<orgName type="institution" key="instit2">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6, 75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
							<email>nicolas.thome@cnam.fr</email>
							<affiliation key="aff2">
								<orgName type="department">Conservatoire National des Arts et Métiers</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MUTAN: Multimodal Tucker Fusion for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bilinear models provide an appealing framework for mixing and merging information in Visual Question Answering (VQA) tasks. They help to learn high level associations between question meaning and visual concepts in the image, but they suffer from huge dimensionality issues.</p><p>We introduce MUTAN, a multimodal tensor-based Tucker decomposition to efficiently parametrize bilinear interactions between visual and textual representations. Additionally to the Tucker framework, we design a low-rank matrix-based decomposition to explicitly constrain the interaction rank. With MUTAN, we control the complexity of the merging scheme while keeping nice interpretable fusion relations. We show how our MUTAN model generalizes some of the latest VQA architectures, providing state-of-theart results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multimodal representation learning for text and image has been extensively studied in recent years. Currently, the most popular task is certainly Visual Question Answering (VQA) <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b1">2]</ref>. VQA is a complex multimodal task which aims at answering a question about an image. A specific benchmark has been first proposed <ref type="bibr" target="#b19">[19]</ref>, and large scale datasets have been recently collected <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">31]</ref>, enabeling the development of more powerful models.</p><p>To solve this problem, precise image and text models are required and, most importantly, high level interactions between these two modalities have to be carefully encoded into the model in order to provide the correct answer. This projection from the unimodal spaces to a multimodal one is supposed to extract and model the relevant correlations between the two spaces. Besides, the model must have the ability to understand the full scene, focus its attention on the relevant visual regions and discard the useless information regarding the question. * Equal contribution <ref type="figure">Figure 1</ref>: The proposed MUTAN model uses a Tucker decomposition of the image/question correlation tensor, which enables modeling rich and accurate multi-modal interactions. For the same input image, we show the result of the MUTAN fusion process when integrated into an attention mechanism <ref type="bibr" target="#b28">[28]</ref>: we can see that the regions with larger attention scores (in red) indicate a very fine understanding of the image and question contents, enabling MUTAN to properly answer the question (see detailed maps in experiments section).</p><p>Bilinear models are powerful approaches for the fusion problem in VQA because they encode full second-order interactions. They currently hold state-of the-art performances <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">8]</ref>. The main issue with these bilinear models is related to the number of parameters, which quickly becomes intractable with respect to the input and output dimensions. Therefore, current bilinear approaches must be simplified or approximated by reducing the model complexity: in <ref type="bibr" target="#b4">[5]</ref>, the authors sacrifice trainability by using a handcrafted multi-modal projection, while a global tensor rank constraint is applied in <ref type="bibr" target="#b8">[8]</ref>, reducing correlations to a simple element-wise product.</p><p>In this work, we introduce a new architecture called MU-TAN ( <ref type="figure">Figure 2</ref>), which focuses on modeling fine and rich interactions between image and textual modalities. Our approach is based on a Tucker decomposition <ref type="bibr" target="#b24">[24]</ref> of the cor-relation tensor, which is able to represent full bilinear interactions, while maintaining the size of the model tractable. The resulting scheme allows us to explicitly control the model complexity, and to choose an accurate and interpretable repartition of the learnable parameters.</p><p>In the next section, we provide more details on related VQA works and highlight our contributions. The MUTAN fusion model, based on a Tucker decomposition, is presented in section 3, and successful experiments are reported in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The main task in multimodal visual and textual analysis aims at learning an alignment between feature spaces <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b18">18]</ref>. Thus, the recent task of image captioning aims at generating linguistic descriptions of images <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b28">28]</ref>. Instead of explicitly learning an alignment between two spaces, the goal of VQA <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">19]</ref> is to merge both modalities in order to decide which answer is correct. This problem requires modeling very precise correlations between the image and the question representations. Attention. Attention mechanisms <ref type="bibr" target="#b28">[28]</ref> have been a real breakthrough in multimodal systems, and are fundamental for VQA models to obtain the best possible results. <ref type="bibr" target="#b30">[30]</ref> propose to stack multiple question-guided attention mechanisms, each one looking at different regions of the image.</p><p>[22] and <ref type="bibr" target="#b14">[14]</ref> extract bounding boxes in the image and score each one of them according to the textual features. In <ref type="bibr" target="#b17">[17]</ref>, word features are aggregated with an attention mechanism guided by the image regions and, equivalently, the region visual features are aggregated into one global image embedding. This co-attentional framework uses concatenations and sum pooling to merge all the components. On the contrary, <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b8">[8]</ref> developed their own fusion methods that they use for global and attention-based strategies.</p><p>In this paper, we use the attentional modeling, proposed in <ref type="bibr" target="#b4">[5]</ref>, as a tool that we integrate in our new fusion strategy for both the global fusion and the attentional modeling. Fusion strategies. Early works have modeled interactions between multiple modalities with first order interactions. The IMG+BOW model in <ref type="bibr" target="#b21">[21]</ref> is the first to use a concatenation to merge a global image representation with a question embedding, obtained by summing all the learnt word embeddings from the question. In <ref type="bibr" target="#b22">[22]</ref>, (image, question, answer) triplets are scored in an attentional framework. Each local feature is given a score corresponding to its similarity with textual features. These scores are used to weight region multimodal embeddings, obtained from a concatenation between the region's visual features and the textual embeddings. The hierarchical co-attention network <ref type="bibr" target="#b17">[17]</ref>, after extracting multiple textual and visual features, merges them with concatenations and sums.</p><p>Second order models are a more powerful way to model interactions between two embedding spaces. Bilinear interactions have shown great success in deep learning for fine-grained classification <ref type="bibr" target="#b16">[16]</ref>, and Multimodal language modeling <ref type="bibr" target="#b10">[10]</ref>. In VQA, a simple element-wise product between the two vectors is performed in <ref type="bibr" target="#b1">[2]</ref>. <ref type="bibr" target="#b6">[7]</ref> also uses an element-wise product in a more complex iterative global merging scheme. In <ref type="bibr" target="#b14">[14]</ref>, they use the element-wise product aggregation in an attentional framework. To go deeper in bilinear interactions, Multimodal Compact Bilinear pooling (MCB) <ref type="bibr" target="#b4">[5]</ref> uses an outer product q⊗v between visual v and textual q embeddings. The count-sketch projection <ref type="bibr" target="#b2">[3]</ref> Ψ is used to project q⊗v on a lower dimensional space. Interestingly, nice count-sketch properties are capitalized to compute the projection without having to explicitly compute the outer product. However, interaction parameters in MCB are fixed by the count-sketch projection (randomly chosen in {0; −1; 1}), limiting its expressive power for modeling complex interactions between image and questions. In contrast, our approach is able to model rich second order interaction with learned parameters.</p><p>In the recent Multimodal Low-rank Bilinear (MLB) pooling work <ref type="bibr" target="#b8">[8]</ref>, full bilinear interactions between image and question spaces are parametrized by a tensor. Again, to limit the number of free parameters, this tensor is constrained to be of low rank r. The MLB strategy reaches state-of-the-art performances on the well-known VQA database <ref type="bibr" target="#b1">[2]</ref>. Despite these impressive results, the low rank tensor structure is equivalent to a projection of both visual and question representations into a common r-dimensional space, and to compute simple element-wise product interactions in this space. MLB is thus essentially designed to learn a powerful mono-modal embedding for text and image modalities, but relies on a simple fusion scheme in this space.</p><p>In this work, we introduce MUTAN, a multimodal fusion scheme based on bilinear interactions between modalities. To control the number of model parameters, MUTAN reduces the size of the mono-modal embeddings, while modeling their interaction as accurately as possible with a full bilinear fusion scheme. Our submission therefore encompasses the following contributions:</p><p>-New fusion scheme for VQA relying on a Tucker tensor-based decomposition, consisting in a factorization into three matrices and a core tensor. We show that the MU-TAN fusion scheme generalizes the latest bilinear models, i.e. MCB <ref type="bibr" target="#b4">[5]</ref> and MLB <ref type="bibr" target="#b8">[8]</ref>, while having more expressive power.</p><p>-Additional structured sparsity constraint the core tensor to further control the number of model parameters. This acts as a regularizer during training and prevents overfitting, giving us more flexibility to adjust the input/output projections.</p><p>-State-of-the-art results on the most widely used dataset <ref type="figure">Figure 2</ref>: MUTAN fusion scheme for global Visual QA. The prediction is modeled as a bilinear interaction between visual and linguistic features, parametrized by the tensor T . In MUTAN, we factorise the tensor T using a Tucker decomposition, resulting in an architecture with three intra-modal matrices W q , W v and W o , and a smaller tensor T c . The complexity of T c is controlled via a structured sparsity constraint on the slice matrices of the tensor.</p><p>for Visual QA <ref type="bibr" target="#b1">[2]</ref>. We also show that MUTAN outperforms MCB <ref type="bibr" target="#b4">[5]</ref> and MLB <ref type="bibr" target="#b8">[8]</ref> in the same setting, and that performances can be further improved when combined with MLB, validating the complementarity potential between the two approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MUTAN Model</head><p>Our method deals with the problem of Visual Question Answering (VQA). In VQA, one is given a question q ∈ Q about an image v ∈ I, and the goal is to provide a meaningful answer. During training, we aim at learning a model such that the predicted answerâ matches the correct one a . More formally, denoting as Θ the whole set of parameters of the model, the predicted outputâ can be written as:</p><formula xml:id="formula_0">a = arg max a∈A p Θ (a|v, q)<label>(1)</label></formula><p>The general architecture of the proposed approach is shown in <ref type="figure">Figure 2</ref>. As commonly done in VQA, images v and questions q are firstly embedded into vectors and the output is represented as a classification vector y . In this work, we use a fully convolutional neural network <ref type="bibr" target="#b5">[6]</ref> (ResNet-152) to describe the image content, and a GRU recurrent network <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b3">4]</ref> for the question, yielding representations v ∈ R dv for the image and q ∈ R dq for the question. Vision and language representations v and q are then fused using the operator T (explained below) to produce a vector y, providing (through a softmax function) the final answer in Eq. (1). This global merging scheme is also embedded into a visual attention-based mechanism <ref type="bibr" target="#b8">[8]</ref> to provide our final MUTAN architecture.</p><p>Fusion and Bilinear models The issue of merging visual and linguistic information is crucial in VQA. Complex and high-level interactions between textual meaning in the question and visual concepts in the image have to be extracted to provide a meaningful answer.</p><p>Bilinear models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">8]</ref> are recent powerful solutions to the fusion problem, since they encode fully-parametrized bilinear interactions between the vectors q and v:</p><formula xml:id="formula_1">y = (T × 1 q) × 2 v<label>(2)</label></formula><p>with the full tensor T ∈ R dq×dv×|A| , and the operator × i designing the i-mode product between a tensor and a matrix (here a vector). Despite their appealing modeling power, fullyparametrized bilinear interactions quickly become intractable in VQA, because the size of the full tensor is prohibitive using common dimensions for textual, visual and output spaces. For example, with d v ≈ d q ≈ 2048 and |A| ≈ 2000, the number of free parameters in the tensor T is ∼ 10 10 . Such a huge number of free parameters is a problem both for learning and for GPU memory consumption <ref type="bibr" target="#b0">1</ref> .</p><p>In MUTAN, we factorize the full tensor T using a Tucker decomposition. We also propose to complete our decomposition by structuring the second tensor T c (see gray box in <ref type="figure">Fig. 2</ref>) in order to keep flexibility over the input/output dimensions while keeping the number of parameters tractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Tucker decomposition</head><p>The Tucker decomposition <ref type="bibr" target="#b24">[24]</ref> of a 3-way tensor T ∈ R dq×dv×|A| expresses T as a tensor product between factor matrices W q , W v and W o , and a core tensor T c in such a way that:</p><formula xml:id="formula_2">T = ((T c × 1 W q ) × 2 W v ) × 3 W o (3) with W q ∈ R dq×tq , W v ∈ R dv×tv and W o ∈ R |A|×to , and T c ∈ R tq×tv×to . Interestingly, Eq.</formula><p>(3) states that the weights in T are functions of a restricted number of param- <ref type="bibr">[1,tq]</ref>,m∈ <ref type="bibr">[1,tv]</ref>,n∈ <ref type="bibr">[1,to]</ref> </p><formula xml:id="formula_3">eters ∀i ∈ [1, d q ], j ∈ [1, d v ], k ∈ [1, d o ]: T [i, j, k] = l∈</formula><formula xml:id="formula_4">T c [l, m, n]W q [i, l]W v [j, m]W o [k, n] T is usually summarized as T = T c ; W q , W v , W o .</formula><p>A comprehensive discussion on Tucker decomposition and tensor analysis may be found in <ref type="bibr" target="#b12">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multimodal Tucker Fusion</head><p>As we parametrize the weights of the tensor T with its Tucker decomposition of the Eq. (3), we can rewrite Eq. (2) as follows:</p><formula xml:id="formula_5">y = T c × 1 q W q × 2 v W v × 3 W o<label>(4)</label></formula><p>This is strictly equivalent to encode a full bilinear interaction of projections of q and v into a latent pair representation z, and to use this latent code to predict the correct answer.</p><formula xml:id="formula_6">If we defineq = q W q ∈ R tq andṽ = v W v ∈ R tv , we have: z = (T c × 1q ) × 2ṽ ∈ R to (5)</formula><p>z is projected into the prediction space y = z W o ∈ R |A| and p = softmax(y). In our experiments, we use non-</p><formula xml:id="formula_7">linearitiesq = tanh(q W q ) andṽ = tanh(v W v )</formula><p>in the fusion, as in <ref type="bibr" target="#b8">[8]</ref>, providing slightly better results. The multimodal Tucker fusion is depicted in <ref type="figure">Figure 2</ref>.</p><p>Interpretation Using the Tucker decomposition, we have separated T into four components, each having a specific role in the modeling. Matrices W q and W v project the question and the image vectors into spaces of respective dimensions t q and t v . These dimensions directly impact the modeling complexity that will be allowed for each modality. The higher t q (resp. t v ) will be, the more complex the question (resp. image) modeling will be. Tensor T c is used to model interactions betweenq andṽ. It learns a projection from all the correlationsq[i]ṽ[j] to a vector z of size t o . This dimension controls the complexity allowed for the interactions between modalities. Finally, the matrix W o scores this pair embedding z for each class in A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Tensor sparsity</head><p>To further balance between expressivity and complexity of the interactions modeling, we introduce a structured sparsity constraint based on the rank of the slice matrices in T c . When we perform the t o bilinear combinations betweenq andṽ of Eq. (5), each dimension k ∈ 1, t o in z can be written as:</p><formula xml:id="formula_8">z[k] =q T c [:, :, k]ṽ<label>(6)</label></formula><p>The correlations between elements ofq andṽ are weighted by the parameters of T c [:, :, k]. We might benefit from the introduction of a structure in each of these slices. This structure can be expressed in terms of rank constraints on the slices of T c . We impose the rank of each slice to be equal to a constant R. Thus we express each slice T c [:, :, k] as a sum of R rank one matrices:</p><formula xml:id="formula_9">T c [:, :, k] = R r=1 m k r ⊗ n k r<label>(7)</label></formula><p>with m k r ∈ R tq and n k r ∈ R tv Eq. (6) becomes:</p><formula xml:id="formula_10">z[k] = R r=1 q m k r ṽ n k r (8) We can define R matrices M r ∈ R tq × to (resp. N r ∈ R tv × to ) such as ∀k ∈ 1, d o , M r [:, k] = m k r (resp. N r [:, k] = n k r )</formula><p>. The structured sparsity on T c can then be written as:</p><formula xml:id="formula_11">z = R r=1 z r (9) z r = (q M r ) * (ṽ N r )<label>(10)</label></formula><p>Interpretation Adding this rank constraint on T c leads to expressing the output vector z as a sum over R vectors z r . To obtain each of these vectors, we projectq andṽ into a common space and merge them with an elementwise product. Thus, we can interpret z as modeling an OR interaction over multiple AND gates (R in MUTAN) between projections ofq andṽ. z[k] can described in terms of logical operators as:</p><formula xml:id="formula_12">z r [k] = q similar to m k r AND ṽ similar to n k r (11) z[k] = z 1 [k] OR ... OR z R [k]<label>(12)</label></formula><p>This decomposition gives a very clear insight of how the fusion is carried out in our MUTAN model. In our experiments, we will show how different r's in 1, R behave, depending on the type of question. We will exhibit some cases where some r's specialize over specific question types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Unification and Discussion</head><p>In this subsection, we show how two state of the art models, namely Multimodal Low-rank bilinear pooling <ref type="bibr" target="#b8">[8]</ref> (MLB) and Multimodal Compact Bilinear pooling <ref type="bibr" target="#b4">[5]</ref> (MCB), can be seen as special cases of our Multimodal Tucker Fusion. Each of these models use a different type of bilinear interaction between q and v, hence instantiating a specific parametrization of the weight tensor T . These parameterizations actually consist in a Tucker decomposition with specific constraints on the elements T c , W q , W v and W o . More importantly, when we cast MCB and MLB into the framework of Tucker decompositions, we show that the structural constraints imposed by these two models state that some parameters are fixed, while they are free to be learnt in our full Tucker fusion. This is illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. We show in color the learnt parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Multimodal Compact Bilinear (MCB)</head><p>We can show that the Multimodal Compact Bilinear pooling <ref type="bibr" target="#b4">[5]</ref> can be written as a bilinear model where the weight tensor T mcb is decomposed into its Tucker decomposition, with specific structures on the decompositions' elements. As was noticed in <ref type="bibr" target="#b8">[8]</ref>, all the learnt parameters in MCB are located after the fusion. The combinations of dimensions from q and from v that are supposed to interact with each other are randomly sampled beforehand (through h).</p><p>To compensate for the fact of fixing the parameters s q , s v and h, they must set a very high t o dimension (typically 16,000). This set of combinations is taken as a feature vector for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Multimodal Low-rank Bilinear (MLB)</head><p>The low-rank bilinear interaction corresponds to a canonical decomposition of the tensor T such as its rank is equal to R. It is well-known that the low-rank decomposition of a tensor is a special case of the Tucker decomposition, such as</p><formula xml:id="formula_13">T mlb = I R ; W q , W v , W o where t q = t v = t o = R.</formula><p>Two major constraints are imposed when reducing Tucker decomposition to low-rank decomposition. First, the three dimensions t q , t v and t o are structurally set to be equal. The dimension of the space in which a modality is projected (t q and t v ) quantifies the model's complexity. Our intuition is that since the image and language spaces are different, they may require to be modeled with different levels of complexity, hence different projection dimensions. The second constraint is on the core tensor, which is set to be the identity. A dimension k ofq mlb is only allowed to interact with the same dimension ofṽ mlb , which might be restrictive. We will experimentally show the beneficial effect of removing these constraints.</p><p>We would like to point out the differences between MLB and the structured sparsity per slice presented in 3.3. There are two main differences between the two approaches. First, our rank reduction is made on the core tensor of the Tucker decomposition T c , while in MLB they constrain the rank of the global tensor T . This lets us keep different dimensionalities for the projected vectorsq andṽ. The second difference is we do not reduce the tensor on the third mode, but only on the first two modes corresponding to the image and question modalities. The implicit parameters in T c are correlated inside a mode-3 slice but independent between the slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>VQA Dataset The VQA dataset <ref type="bibr" target="#b1">[2]</ref> is built over images of MSCOCO <ref type="bibr" target="#b15">[15]</ref>, where each image was manually annotated with 3 questions. Each one of these questions is then answered by 10 annotators, yielding a list of 10 groundtruth answers. The dataset is composed of 248,349 pairs (image, question) for the training set, 121,512 for validation and 244,302 for testing. The ground truth answers are given for the train and val splits, and one must submit their predictions to an evaluation server to get the scores on teststd split. Note that the evaluation server makes it possible to submit multiple models per day on test-dev, which is a subsample of test-std. The whole submission on test-std can only be done five times. We focus on the open-ended task, where the ground truth answers are given in free natural language phrases. This dataset comes with its evaluation metric, presented in <ref type="bibr" target="#b1">[2]</ref>. When the model predicts an answer for a visual question, the VQA accuracy is given by:</p><formula xml:id="formula_14">min 1,</formula><p># humans that provided that answer 3</p><p>If the predicted answer appears at least 3 times in the ground truth answers, the accuracy for this example is considered to be 1. Intuitively, this metrics takes into account the consensus between annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MUTAN Setup</head><p>We first resize our images to be of size (448, 448). We use ResNet152 <ref type="bibr" target="#b5">[6]</ref> as our visual feature extractor, which produces feature maps of size 14×14×2048. We keep the 14 × 14 tiling when attention models are used (section 4.2). Otherwise, the image is represented as the average of 14 × 14 vectors at the output of the CNN (section 4.1). To represent questions, we use a GRU <ref type="bibr" target="#b3">[4]</ref> initialized with the parameters of a pretrained Skip-thoughts model <ref type="bibr" target="#b11">[11]</ref>. Each model is trained to predict the most common answer in the 10 annotated responses. |A| is fixed to the 2000 most frequent answers as in <ref type="bibr" target="#b8">[8]</ref>, and we train our model using ADAM <ref type="bibr" target="#b9">[9]</ref> (see details in supplementary material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fusion Scheme Comparison</head><p>To point out the performance variation due to the fusion modules, we first compare MUTAN to state-of-the-art bilinear models, under the same experimental framework. We do not use attention models here. Several merging scheme results are presented in <ref type="table" target="#tab_0">Table 1</ref>: Concat denotes a baseline where v and q are merged by simply concatenating them. For MCB <ref type="bibr" target="#b4">[5]</ref> and MLB <ref type="bibr" target="#b8">[8]</ref>, we use the available code <ref type="bibr">2 3</ref> to train models on the same visual and linguistic features. We choose an output dimension of 16,000 for MCB and 1,200 for MLB, as indicated in the respective articles. MU-TAN noR designates the MUTAN model without the rank sparsity constraint. We choose all the projection dimensions to be equal to each other: t q = t v = t o = 160. These parameters are chosen considering the results on val split. Finally, our MUTAN 4 designates the full Tucker decomposition with rank sparsity strategy. We choose all the projection dimensions to be equal to each other: t q = t v = t o = 360, and a rank R = 10. These parameters were chosen so that MUTAN and MUTAN noR have the same number of parameters. As we can see in <ref type="table" target="#tab_0">Table 1</ref>, MUTAN noR performs better than MLB, which validates the fact that modeling full bilinear interactions between low dimensional projections yields a more powerful representation than having strong mono-modal transformations with a simple fusion scheme (element-wise product). With the structured sparsity, MUTAN obtains the best results, validating our intuition of having a nice tradeoff between the projection dimensions and a reasonable number of useful bilinear interaction parameters in the core tensor T c . Finally, a naive late fusion MUTAN+MLB further improves performances (about +1pt on test-dev). It validates the complementarity between the two types of tensor decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">State-of-the-art comparison</head><p>To compare the performance of the proposed approach to state-of-the-art works, we associate the MUTAN fusion with recently introduced techniques for VQA, which are described below.</p><p>Attention mechanism We use the same kind of multiglimpse attention mechanisms as the ones presented in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b8">[8]</ref>. We use MUTAN to score the region embeddings according to the question vector, and compute a global visual vector as a sum pooling weighted by these scores.</p><p>Answer sampling (Ans. Sampl.) Each (image,question) pair in the VQA dataset is annotated with 10 ground truth answers, corresponding to the different annotators. In those 10, we keep only the answers occuring more than 3 times, and randomly choose the one we ask our model to predict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data augmentation (DAVG)</head><p>We use Visual Genome <ref type="bibr" target="#b13">[13]</ref> as a data augmentation to train our model, keeping only the example whose answer is in our vocabulary. This triples the size of our training set.</p><p>Ensembling MUTAN (5) consist in an ensemble of five models trained on train+val splits. We use 3 attentional MUTAN architectures with one trained with additional Visual Genome data. The 2 other models are instances of MLB, which can be seen as a special case of MUTAN. Details about the ensembling will be provided in the supplementary material.</p><p>Results State-of-the-art comparison results are gathered in <ref type="table" target="#tab_2">Table 2</ref>  MCB <ref type="bibr" target="#b4">[5]</ref> and MLB <ref type="bibr" target="#b8">[8]</ref> have a strong edge over other methods with a less powerful fusion scheme. MUTAN outperforms all the previous methods with a large margin on test-dev and test-std. This validates the relevance of the proposed fusion scheme, which models precise interactions between modalities. The good performances of MUTAN (5) also confirms its complementarity with MLB, already seen in section 4.1 without attention mecanism: MLB learns informative mono-modal projections, wheras MUTAN is explicitly devoted to accurately models bilinear interactions. Finally, we can notice that the performance improvement of MUTAN in this enhanced setup is conform to the performance gap reported in section 4.1, showing that the benefit of the fusion scheme directly translates for the whole VQA task.</p><p>Finally, we also evaluated an ensemble of 3 models based on the MUTAN fusion scheme (without MLB), that we denote as MUTAN (3). This ensemble also outperforms state-of-the-art results. We can point out that this improvement is reached with is an ensembling of 3 models, which is smaller than the previous state-of-the-art MLB results containing an ensembling of 7 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Further analysis</head><p>Experimental setup In this section, we study the behavior of MUTAN under different conditions. Here, we examine under different aspects the fusion between q and v with the Tucker decomposition of tensor T . As we did previously, we don't use the attention mechanism in this section. We only consider a global visual vector, computed as the average of the 14 × 14 region vectors given by our CNN. We also don't use the answer sampling, asking our model to always predict the most frequent answer of the 10 ground Impact of a plain tensor The goal is to see how important are all the parameters in the core tensor T c , which model the correlations between projections of q and v. We train multiple MUTAN noR, where we fix all projection dimensions to be equal t q = t v = t o = t and t ranges from 20 to 220. In <ref type="figure" target="#fig_2">Figure 4</ref>, we compare these MUTAN noR with a model trained with the same projection dimension, but where T c is replaced by the identity tensor 5 . One can see that MUTAN noR gives much better results than identity tensor, even for very small core tensor dimensions. This shows that MUTAN noR is able to learn powerful correlations between modalities 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of rank sparsity</head><p>We want to study the impact of introducing the rank constraint in the core tensor T c . We fix the input dimensions t q = 210 and t v = 210, and vary the output dimension t o for multiple rank constraints R. As we can see in <ref type="figure">Figure 5</ref>, controlling the rank of slices in T c allows to better model the interactions between the unimodal spaces. The different colored lines show the behavior of MUTAN for different values of R. Comparing R = 60 (blue line) and R = 20 (green line), we see that a lower rank allows to reach higher values of t o without overfitting. The number of parameters in the fusion is lower, and the accuracy on the val split is higher. Qualitative observations In MUTAN, the vector z that encodes the (image,question) pair is expressed as a sum over R vectors z r . We want to study the R different latent projections that have been learnt during training, and assess whether the representations have captured different semantic properties of inputs. We quantify the differences between each of the R spaces using the VQA question types.</p><p>We first train a model on the train split, with R = 20, and measure its performance on the val set. Then, we set to 0 all of the z r vectors except one, and evaluate this ablated system on the validation set. In <ref type="figure">Figure 6</ref>, we compare the full system to the R ablated systems for 4 different ques- tion types. The dotted line shows the accuracy of the full system, while the different bars show the accuracy of the ablated system for each R. Depending on the question type, we observe 3 different behaviors of the ranks. When the question type's answer support is small, we observe that each rank has learnt enough to reach almost the same accuracy as the global system. This is the case for questions starting by "Is there", whose answer is almost always "yes" or "no". Other question types require information from all the latent projections, as in the case of "What is the man". This leads to cases where all projections perform equally and significantly worst when taken individually than when combined to get the full model. At last, we observe that specific projections contribute more than others depending on the question type. For example, latent variable 16 performs well on "what room is", and is less informative to answer questions starting by "what sport is". The opposite behavior is observed for latent variable 17.</p><p>We run the same kind of analysis for the MUTAN fusion in the attention mechanism. In <ref type="figure" target="#fig_4">Figure 7</ref>, we show for two images the different attentions that we obtain when turning off all the projections but one. For the first image, we can see that a projection focuses on the elephant, while another focuses on the woman. Both these visual informations are necessary to answer the question "Where is the woman ?". The same behavior is observed for the second image, where a projection focuses on the smoke while another gives high attention to the train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced our MUTAN strategy for the VQA task. Our main contribution is a multimodal fusion between visual and textual information using a bilinear framework. Our model combines a Tucker decomposition with a low-rank matrix constraint. It is designed to control the full bilinear interaction's complexity. MUTAN factorizes the interaction tensor into interpretable elements, and allows an easy control of the model's expressiveness. We also show how the Tucker decomposition framework generalizes the most competitive VQA architectures. MUTAN is evaluated on the most recent VQA dataset, reaching stateof-the-art.</p><p>Image As in <ref type="bibr" target="#b4">[5]</ref> (MCB) or <ref type="bibr" target="#b8">[8]</ref> (MLB), we preprocess the images before training our VQA models as follow. We load and rescal the image to 448. It is important to notice that we keep the proportion. Thus, 448 will be the size of the smaller edge. Then, we crop the image at the center to have a region of size 448 × 448. We normalize the image using the ImageNet normalization. Finally, we feed the image to a pretrained ResNet-152 and extract the features before the last Rectified Linear Unit (ReLU).</p><p>Question We use almost the same preprocessing as <ref type="bibr" target="#b4">[5]</ref> or <ref type="bibr" target="#b8">[8]</ref> for the questions. We keep the questions which are associated to the 2000 most occuring answers. We convert the questions characters to lower case and remove all the ponctuations. We use the space character to split the question into a sequence of words. Then, we replace all the words which are not in the vocabulary of our pretrained Skipthoughts model by a special "unknown" word ("UNK"). Finally, we pad all the sequences of words with zero-padding to match the maximum sequence length of 26 words. We use TrimZero as in <ref type="bibr" target="#b8">[8]</ref> to avoid the zero values from the padding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization details</head><p>Algorithm It is important to notice that we use the classical implementation of Adam 1 with a learning rate of 10 −4 unlike in <ref type="bibr" target="#b4">[5]</ref> or <ref type="bibr" target="#b8">[8]</ref>. In fact, we tried RMSPROP, SGD Nesterov and Adam with or without learning rate decay. We found that Adam without learning rate decay was more convenient and lead to the same accuracy.</p><p>Batch size During the optimization process, we use a batch size of 512 for the models without an attention modeling. For the others, we use a batch size of 100, because the models are more memory consuming.</p><p>Early stopping As in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b8">[8]</ref>, we use early stopping as a regularizer. During our training process, we save the * Equal contribution 1 https://github.com/torch/optim/blob/master/ adam.lua model parameters after each epoch. To evaluate our model on the evaluation server, we chose the best epoch according to the Open Ended validation accuracy computed on the val split when available.</p><p>As in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b8">[8]</ref>, for the models trained on the trainval split, we use the test-dev split as a validation set and are obliged to submit several times on the evaluation server. Note that we are limited to 10 submissions per day. In practice, we submit 3 to 4 times per models for epochs associated to training accuracies between 63% to 70%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble details</head><p>In table 3, we report several single models which compose our two ensembles. MUTAN(3) is made of a MUTAN trained on the trainval split with 2 glimpses, an other MU-TAN with 3 glimpses and a third MUTAN with 2 glimpses trained on the trainval split with the visual genome data augmentation. All three have been trained with the same hyper-parameters besides the number of glimpses. MUTAN(5) is made of the three same MUTAN models of MUTAN(3) and two MLB models which can be viewed as a special case of our Multimodal Tucker Fusion. The first MLB has 2 glimpses and was trained on the trainval split. It has been made available by the authors of [8] 2 . The second MLB has 4 glimpses and was trained by ourself on the trainval split with the visual genome data augmentation.</p><p>The final results of both ensembles are obtained by averaging the features extracted before the final Softmax layer of all their models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scores details</head><p>In   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Tensor design strategies. (a) MCB: W q and W v are fixed diagonal matrices, T c is a sparse fixed tensor, only the output factor matrix W o is learnt; (b) MLB: the 3 factor matrices are learnt but the core tensor is T c set to identity; (c) MU-TAN: W q , W v , W o and T c are learnt. The full bilinear interaction T c is structured with a low-rank (R) decomposition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The intramodal projection matrices W mcb q and W mcb v are diagonal matrices where the non-zero coefficients take their values in {−1; 1}: W mcb q = Diag (s q ) and W mcb v = Diag (s v ), where s q ∈ R dq and s v ∈ R dv are random vectors sampled at the instanciation of the model but kept fixed afterwards. The core tensor T c is sparse and its values follow the rule: T mcb c [i, j, k] = 1 if h(i, j) = k (and 0 else), where h : 1, d q × 1, d v → 1, d o is randomly sampled at the beginning of training and no longer changed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The improvements given by MUTAN noR over a model trained with the identity tensor as a fusion operator betweenq andṽ. truth responses. All the models are trained on the VQA train split, and the scores are reported on val.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Accuracy on VQA val in function of t o . Each colored dot shows the score of a MUTAN model trained on train. The yellow labels indicate the number of parameters in the fusion. (a) "Is there" (b) "What room is"(c) "What is the man" (d) "What sport is" Visualizing the performances of ablated systems according to the R variables. Full system performance is denoted in dotted line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>(a) Question: Where is the woman ? -Answer: on the elephant (b) Question: Where is the smoke coming from ? -Answer: train The original image is shown on the left. The center and right images show heatmaps obtained when turning off all the projections but one, for two different projections. Each projection focuses on a specific concept needed to answer the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>No. Other All All Concat 8.9 79.25 36.18 46.69 58.91 56.92 MCB 32 80.81 35.91 46.43 59.40 57.39 MLB 7.7 82.02 36.61 46.65 60.08 57.91 MUTAN noR 4.9 81.44 36.42 46.86 59.92 57.94 MUTAN 4.9 81.45 37.32 47.17 60.17 58.16 MUTAN+MLB 17.5 82.29 37.27 48.23 61.02 58.76 Comparison between different fusion under the same setup on the test-dev split. Θ indicates the number of learnable parameters (in million).</figDesc><table><row><cell>test-dev</cell><cell>val</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Firstly, we can notice that bilinear models, i.e. 37.32 43.12 57.99 58.24 Ask Your Neur. [20] 78.39 36.45 46.28 58.39 58.43</figDesc><table><row><cell></cell><cell>test-dev</cell><cell>test-</cell></row><row><cell></cell><cell></cell><cell>std</cell></row><row><cell></cell><cell>Y/N No. Other All</cell><cell>All</cell></row><row><cell cols="3">SMem 2-hop [27] 80.87 SAN [30] 79.3 36.6 46.1 58.7 58.9</cell></row><row><cell>D-NMN [1]</cell><cell cols="2">81.1 38.6 45.5 59.4 59.4</cell></row><row><cell>ACK [26]</cell><cell cols="2">81.01 38.42 45.23 59.17 59.44</cell></row><row><cell>MRN [7]</cell><cell cols="2">82.28 38.82 49.25 61.68 61.84</cell></row><row><cell>HieCoAtt [17]</cell><cell cols="2">79.7 38.7 51.7 61.8 62.1</cell></row><row><cell>MCB (7) [5]</cell><cell cols="2">83.4 39.8 58.5 66.7 66.5</cell></row><row><cell>MLB (7) [8]</cell><cell cols="2">84.57 39.21 57.81 66.77 66.89</cell></row><row><cell>MUTAN (3)</cell><cell cols="2">84.54 39.32 57.36 67.03 66.96</cell></row><row><cell>MUTAN (5)</cell><cell cols="2">85.14 39.81 58.52 67.42 67.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: MUTAN performance comparison on the test-dev</cell></row><row><cell>and test-standard splits VQA dataset; (n) for an ensemble</cell></row><row><cell>of n models.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>table 3 ,</head><label>3</label><figDesc>we provide the scores for each answer type processed on the val and test-dev splits. In table 4, we provide the same scores for test-dev and test-standard.</figDesc><table><row><cell>test-dev</cell><cell>val</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison between different fusion under the same setup on the test-dev split. Θ indicates the number of learnable parameters (in million).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">test-dev</cell><cell></cell><cell></cell><cell>test-std</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Y/N</cell><cell>No.</cell><cell>Other</cell><cell>All</cell><cell>Y/N</cell><cell>No.</cell><cell>Other</cell><cell>All</cell></row><row><cell>SMem 2-hop [27]</cell><cell>80.87</cell><cell>37.32</cell><cell>43.12</cell><cell>57.99</cell><cell>80.0</cell><cell>37.53</cell><cell>43.48</cell><cell>58.24</cell></row><row><cell>Ask Your Neur. [20]</cell><cell>78.39</cell><cell>36.45</cell><cell>46.28</cell><cell>58.39</cell><cell>78.24</cell><cell>36.27</cell><cell>46.32</cell><cell>58.43</cell></row><row><cell>SAN [30]</cell><cell>79.3</cell><cell>36.6</cell><cell>46.1</cell><cell>58.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.9</cell></row><row><cell>D-NMN [1]</cell><cell>81.1</cell><cell>38.6</cell><cell>45.5</cell><cell>59.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.4</cell></row><row><cell>ACK [26]</cell><cell>81.01</cell><cell>38.42</cell><cell>45.23</cell><cell>59.17</cell><cell>81.07</cell><cell>37.12</cell><cell>45.83</cell><cell>59.44</cell></row><row><cell>MRN [7]</cell><cell>82.28</cell><cell>38.82</cell><cell>49.25</cell><cell>61.68</cell><cell>82.39</cell><cell>38.23</cell><cell>49.41</cell><cell>61.84</cell></row><row><cell>HieCoAtt [17]</cell><cell>79.7</cell><cell>38.7</cell><cell>51.7</cell><cell>61.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>62.1</cell></row><row><cell>MCB (7) [5]</cell><cell>83.4</cell><cell>39.8</cell><cell>58.5</cell><cell>66.7</cell><cell>83.2</cell><cell>39.5</cell><cell>58.0</cell><cell>66.5</cell></row><row><cell>MLB (7) [8]</cell><cell>84.54</cell><cell>39.21</cell><cell>57.81</cell><cell>66.77</cell><cell>84.61</cell><cell>39.07</cell><cell>57.79</cell><cell>66.89</cell></row><row><cell>MUTAN (3)</cell><cell>84.57</cell><cell>39.32</cell><cell>57.36</cell><cell>67.03</cell><cell>84.39</cell><cell>38.70</cell><cell>58.20</cell><cell>66.96</cell></row><row><cell>MUTAN (5)</cell><cell>85.14</cell><cell>39.81</cell><cell>58.52</cell><cell>67.42</cell><cell>84.91</cell><cell>39.79</cell><cell>58.35</cell><cell>67.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>MUTAN performance comparison on the test-dev and test-standard splits VQA dataset; (n) for an ensemble of n models.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A tensor with 8 billion float32 scalars approximately needs 32Go to be stored, while top-grade GPUs hold about 24Go each.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/jnhwkim/cbp 3 https://github.com/jnhwkim/MulLowBiVQA 4 https://github.com/cadene/vqa.pytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">This is strictly equivalent to MLB<ref type="bibr" target="#b8">[8]</ref> without attention. However, we are fully aware that it takes between 1000 and 2000 dimensions of projection to be around the operating point of MLB. With our experimental setup, we just focus on the effect of adding parameters to our fusion scheme.<ref type="bibr" target="#b5">6</ref> Notice that for each t, MUTAN noR has t 3 parameters. For instance, for t = 220, MUTAN adds 10.6M parameters over identity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/jnhwkim/MulLowBiVQA/tree/ master/model</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>Preprocessing details</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1545" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finding frequent items in data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farach-Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages and Programming</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="693" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<title level="m">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal Residual Learning for Visual QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hadamard Product for Low-rank Bilinear Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual question answering with question representation update (qru)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4655" to="4663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2623" to="2631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards a visual turing challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Semantics (NIPS workshop)</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ask your neurons: A deep learning approach to visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02697</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2953" to="2961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ask me anything: free-form visual question answering based on knowledge from external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual7W: Grounded Question Answering in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
