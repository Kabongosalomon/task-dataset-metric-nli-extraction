<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Clearing the Skies: A deep network architecture for single-image rain removal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabin</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
						</author>
						<title level="a" type="main">Clearing the Skies: A deep network architecture for single-image rain removal</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Rain removal</term>
					<term>deep learning</term>
					<term>convolutional neu- ral networks</term>
					<term>image enhancement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a deep network architecture called DerainNet for removing rain streaks from an image. Based on the deep convolutional neural network (CNN), we directly learn the mapping relationship between rainy and clean image detail layers from data. Because we do not possess the ground truth corresponding to real-world rainy images, we synthesize images with rain for training. In contrast to other common strategies that increase depth or breadth of the network, we use image processing domain knowledge to modify the objective function and improve deraining with a modestly-sized CNN. Specifically, we train our DerainNet on the detail (high-pass) layer rather than in the image domain. Though DerainNet is trained on synthetic data, we find that the learned network translates very effectively to real-world images for testing. Moreover, we augment the CNN framework with image enhancement to improve the visual results. Compared with state-of-the-art single image deraining methods, our method has improved rain removal and much faster computation time after network training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As the most common bad-weather condition, the effects of rain can degrade the visual quality of images and severely affect the performance of outdoor vision systems. Under rainy conditions, rain streaks create not only a blurring effect in images, but also haziness due to light scattering. Effective methods for removing rain streaks are required for a wide range of practical applications, such as image enhancement and object tracking. We present the first deep convolutional neural network (CNN) tailored to this task and show how the CNN framework can obtain state-of-the-art results. <ref type="figure" target="#fig_1">Figure 1</ref> shows an example of a real-world testing image degraded by rain and our de-rained result.</p><p>In the last few decades, many methods have been proposed for removing the effects of rain on image quality. These methods can be categorized into two groups: video-based methods and single-image based methods. We briefly review these approaches to rain removal, then discuss the contributions of our proposed DerainNet.  A. Related work: Video v.s. single-image based rain removal</p><p>Due to the redundant temporal information that exists in video, rain streaks can be more easily identified and removed in this domain <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. For example, in <ref type="bibr" target="#b0">[1]</ref> the authors first propose a rain streak detection algorithm based on a correlation model. After detecting the location of rain streaks, the method uses the average pixel value taken from the neighboring frames to remove streaks. In <ref type="bibr" target="#b1">[2]</ref>, the authors analyze the properties of rain and establish a model of visual effect of rain in frequency space. In <ref type="bibr" target="#b2">[3]</ref>, the histogram of streak orientation is used to detect rain and a Gaussian mixture model is used to extract the rain layer. In <ref type="bibr" target="#b3">[4]</ref>, based on the minimization of registration error between frames, phase congruency is used to detect and remove the rain streaks. Many of these methods work well, but are significantly aided by the temporal content of video. In this paper we instead focus on removing rain from a single image.</p><p>Compared with video-based methods, removing rain from individual images is much more challenging since much less information is available for detecting and removing rain streaks. Single-image based methods have been proposed to deal with this challenging problem, but success is less noticeable than in video-based algorithms, and there is still much room for improvement. To give three examples, in <ref type="bibr" target="#b4">[5]</ref> rain streak detection and removal is achieved using kernel regression and a non-local mean filtering. In <ref type="bibr" target="#b5">[6]</ref>, a related work based on deep learning was introduced to remove static raindrops and dirt spots from pictures taken through windows. However, focusing on a specific application this method uses a different physical model from the one in this paper. As our later comparisons show, this physical model limits its ability to transfer to rain streak removal. In <ref type="bibr" target="#b6">[7]</ref>, a generalized lowrank model; both single-image and video rain removal can be achieved through this the spatial and temporal correlations learned by this method.</p><p>Recently, several methods based on dictionary learning have been proposed <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b11">[12]</ref>. In <ref type="bibr" target="#b8">[9]</ref>, the input rainy image is first decomposed into its base layer and detail layer. Rain streaks and object details are isolated in the detail layer while the structure remains in the base layer. Then sparse coding dictionary learning is used to detect and remove rain streaks from the detail layer. The output is obtained by combining the de-rained detail layer and base layer. A similar decomposition strategy is also adopted in method <ref type="bibr" target="#b11">[12]</ref>. In this method, both rain streaks removal and non-rain component restoration is achieved by using a hybrid feature set. In <ref type="bibr" target="#b9">[10]</ref>, a selflearning based image decomposition method is introduced to automatically distinguish rain streaks from the detail layer. In <ref type="bibr" target="#b10">[11]</ref>, the authors use discriminative sparse coding to recover a clean image from a rainy image. A drawback of methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> is that they tend to generate over-smoothed results when dealing with images containing complex structures that are similar to rain streaks, as shown in <ref type="figure" target="#fig_7">Figure 9</ref>(c), while method <ref type="bibr" target="#b10">[11]</ref> usually leaves rain streaks in the de-rained result, as shown in <ref type="figure" target="#fig_7">Figure 9</ref>(d). Moreover, all four dictionary learning based frameworks <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b11">[12]</ref> require significant computation time. More recently, patch-based priors for both the clean and rain layers have been explored to remove rain streaks <ref type="bibr" target="#b12">[13]</ref>. In this method, the multiple orientations and scales of rain streaks are addressed by pre-trained Gaussian mixture models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contributions of our DerainNet approach</head><p>As mentioned, compared to video-based methods, removing rain from a single image is significantly more difficult. This is because most existing methods <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> only separate rain streaks from object details by using low level features, for example by learning a dictionary to for object representation. When an object's structure and orientation are similar with that of rain streaks, these methods have difficulty simultaneously removing rain streaks and preserving structural information. Humans on the other hand can easily distinguish rain streaks within a single image using high-level features such as context information. We are therefore motivated to design a rain detection and removal algorithm based on the deep convolutional neural network (CNN) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. CNN's have achieved success on several low level vision tasks, such as image denoising <ref type="bibr" target="#b15">[16]</ref>, super-resolution <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, image deconvolution <ref type="bibr" target="#b18">[19]</ref>, image inpainting <ref type="bibr" target="#b19">[20]</ref> and image filtering <ref type="bibr" target="#b20">[21]</ref>. We show that the CNN can also provide excellent performance for single-image rain removal.</p><p>In this paper, we propose "DerainNet" for removing rain from single-images, which we base on the deep CNN. To our knowledge, this is the first approach based on deep learning to directly address this problem. Our main contributions are threefold: 1) DerainNet learns the nonlinear mapping function between clean and rainy detail (i.e., high resolution) layers directly and automatically from data. Both rain removal and image enhancement are performed to improve the visual effect. We show significant improvement over three recent state-of-the-art methods. Additionally, our method has significantly faster testing speed than the competitive approaches, making it more suitable for realtime applications. 2) Instead of using common strategies such as increasing neurons or stacking hidden layers to effectively and efficiently approximate the desired mapping function, we use image processing domain knowledge to modify the objective function and improve the de-rain quality. We show how better results can be obtained without introducing more complex network architecture or more computing resources. 3) Because we lack access to the ground truth for realworld rainy images, we synthesize a dataset of rainy images using real-world clean images, which we can take as the ground truth. We show that, though we train on synthesized rainy images, the resulting network is very effective when testing on real-world rainy images. In this way, the model can be learned with easy access to an unlimited amount of training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DERAINNET: DEEP LEARNING FOR RAIN REMOVAL</head><p>We illustrate the proposed DerainNet framework in <ref type="figure" target="#fig_2">Figure  2</ref>. As discussed in more detail below, we decompose each image into a low-frequency base layer and a high-frequency detail layer. The detail layer is the input to the CNN for rain removal. To further improve visual quality, we introduce an image enhancement step to sharpen the results of both layers since the effects of heavy rain naturally leads to a hazy effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training on high-pass detail layers</head><p>We denote the input rainy image and corresponding clean image as I and J respectively. Initially, a goal may be to train a network architecture h P (·) that minimizes</p><formula xml:id="formula_0">L = 1 N N n=1 f W (I n ) − J n 2 F ,<label>(1)</label></formula><p>where W are the network parameters and F is the Frobenius norm and n indexes the image. However, we found that the result obtained by directly training in the image domain is not satisfactory. In <ref type="figure">Figure 3</ref>(a), we show an example of a synthetic rainy image. Note that this image is used in the training process. In <ref type="figure">Figure 3</ref>(b) we see that even when this image is used as a training sample, the de-rained image still exhibits clear rain streaks when zoomed in. <ref type="figure">Figure 3</ref>(b) implies that the desired mapping function was not learned well when training on the image domain, i.e., the model under-fit the data. It is natural to ask whether it is necessary to train a more complex model to further improve the capacity of the network. As is well known, there are two ways to improve a network's capacity in the deep learning domain. One way is to increase the depth of network <ref type="bibr" target="#b21">[22]</ref> by stacking more hidden layers. Usually, more hidden layers can help to obtain high-level features. However, the de-rain problem is a low-level image task and the deeper structure is not necessarily better for this image processing problems. Furthermore, training a feed-forward network with more layers suffers from gradient vanishing unless other training strategies or more complex network structures are introduced. As shown in <ref type="figure">Figure 3</ref>(c), when we add network depth to improve the modeling ability, the result actually becomes worse. The other approach is to increase the breadth of network <ref type="bibr" target="#b22">[23]</ref> by using more neurons in each hidden layer. However, to avoid over-fitting, this strategy requires more training data and computation time that may be intolerable under normal computing condition.</p><p>To effectively and efficiently tackle the de-rain problem, we instead use a priori image processing knowledge to modify the objective function rather than increase the complexity of the problem. Conventional end-to-end procedures directly uses image patches to train the model by finding a mapping function f that transforms the input to output <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Motivated by <ref type="figure">Figure 3</ref>, rather than directly train on the image, we first decompose the image into the sum of a "base" layer and a "detail" layer by using a low-pass filter,</p><formula xml:id="formula_1">J = J base + J detail .<label>(2)</label></formula><p>Using on image processing techniques, we found that after applying an appropriate low-pass filters such as <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>, low-pass versions of both the rainy image I base and the clean image J base are smooth and are approximately equal, as shown in <ref type="figure">Figure 4</ref>. In other words, both the rain streaks and the object's details remain in the high-pass detail layer and I base ≈ J base . This implies that the base layer portion can be removed from the training process, significantly simplifying the mapping needed to be learned by the CNN. Thus, we rewrite the objective function in (1) as</p><formula xml:id="formula_2">L = 1 N N n=1 f W (I n detail ) − J n detail 2 F .<label>(3)</label></formula><p>This directly lead us to train the CNN network on the detail layer instead of the image domain. Moreover, training on the detail layer has several advantages. First, after subtracting the base layer, the detail layer is sparser than the image since most regions in the detail layer are close to zero. As shown in <ref type="figure">Figure 5</ref>, the detail layer has many more pixels that are close to zero than the image itself. Taking advantage of the sparsity of the detail layer is a widely used technique in existing deraining methods <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. In the context of a neural network, training a CNN on the detail layer also follows the procedure of mapping an input patch to an output patch, but since the mapping range has been significantly decreased, the regression problem is significantly easier to handle for a deep learning model. Thus, training on the detail layer instead of the image domain can improve learning the network weights and thus the de-raining result without a large increase in training data or computational resources.</p><p>A second advantage of training on sparse data is that it can improve the convergence of the CNN. As we show in our experiments ( <ref type="figure" target="#fig_1">Figure 17</ref>), training on the detail layer converges much faster than training on the image domain. A third advantage is that decomposing an image into base and detail layers is widely used by the wider image enhancement community <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. These enhancement procedures are tailored to this decomposition and can be easily embedded into our architecture to further improve image quality, which we describe in Section II-D.</p><p>We therefore first decompose the image into a base layer by using a low-pass filter and a detail layer; the detail layer is equal to the difference between the image and the base layer. We use the guided filtering method of <ref type="bibr" target="#b23">[24]</ref> as the low-pass filter because it is simple and fast to implement. In this paper, the guidance image is the input image itself. However, the choice of low-pass filter is not limited to guided filtering; other filtering approaches were also effective in our experiments, such as bilateral filtering <ref type="bibr" target="#b24">[25]</ref> and rolling guidance filtering <ref type="bibr" target="#b25">[26]</ref>. Results with these filters were nearly identical, so we choose <ref type="bibr" target="#b23">[24]</ref> for its low computational complexity.</p><p>After this decomposition we train the CNN on the detail layer image instead of raw image itself according to Eq. (3). This step represents the CNN portion of <ref type="figure" target="#fig_2">Figure 2</ref>. In <ref type="figure">Figure  3</ref>(d) we show an example of the de-rained image using this training approach. In terms of rain streak removal, the result is clearly better than the same CNN structure trained on the image domain shown in <ref type="figure">Figure 3</ref>(b). This conclusion is further supported by our experiments below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Our convolutional neural network</head><p>Our network structure can be expressed as three operations:  <ref type="formula">)</ref> show the detail layer input and output of the network. In (f) we show four of the 512 convolutional output of (e) in the first layer. These appear to be producing different "views" of the rain. In (g) we show four of the 512 layers that are combined to produced the three layer RGB output in (f). The intensities of the images in the second row have been amplified for better visualization. where l indexes layer number, * indicates the convolution operation and b l is the bias. We define σ(·) to be the nonlinear hyperbolic tangent function and f 0 (I detail ) = I detail . We use two hidden layers in our DerainNet architecture and Eq. <ref type="formula">(5)</ref> is the output of the cleaned detail layer.</p><formula xml:id="formula_3">f l (I detail ) = σ(W l * f l−1 (I detail ) + b l ), l = 1, 2 (4) f W (I detail ) = W l * f l−1 (I detail ) + b l , l = 3, (5) (a) Rainy image (b) Our result (c) W 1 (d) W 3 (e) I detail (f) f 1 (I detail ) (g) f 2 (I detail ) (h) De-rained f W (I detail )</formula><p>To better understand the effects of the network f W , we show the learned weights and intermediate results from the hidden layers in <ref type="figure">Figure 6</ref>. The first hidden layer performs feature extraction on the input detail layer, which is similar to the common strategy used for image restoration of extracting and representing image patches by a set of dictionary elements. Thus, W 1 contains some filters that look like edge detectors that align with the direction of rain streaks and object edges. The second hidden layer performs the rain streaks removal and f 2 (I detail ) looks smoother than f 1 (I detail ). The third layer performs reconstruction and enhances the smoothed details with respect to image content. As can be seen in <ref type="figure">Figure  6</ref>, f W (I detail ) contains clear details with most of the rain removed. The intermediate results show that the CNN is effective at feature extraction and helps to recognize and remove rain streaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training</head><p>We use stochastic gradient descent (SGD) to minimize the objective function in Eq. (3). Since it is extremely difficult to obtain a large number of clean/rainy image pairs from realworld data, we synthesize rain using Photoshop 1 to create our training dataset. We randomly collected a total of 350 clean outdoor images from the UCID dataset <ref type="bibr" target="#b28">[29]</ref>, the BSD dataset <ref type="bibr" target="#b29">[30]</ref> and Google image search which we used to synthesize rainy images. Each clean image was used to generate 14 rainy images of different streak orientations and intensity. An example is shown in <ref type="figure" target="#fig_5">Figure 7</ref>. Thus we create a dataset containing 350 × 14 = 4900 rainy images, each having a corresponding ground truth clean image. We randomly selected one million 64 × 64 clean/rainy patch pairs from this synthesized data as training samples. A 56×56 output is generated to avoid border effects caused by convolution. In each iteration, t, the CNN weight and bias are updated using back-propagation,</p><formula xml:id="formula_4">W t+1 = W t − α(f W (I detail i ) − J detail i ) T ∂f W (I detail i ) ∂W , b t+1 = b t − α(f W (I detail i ) − J detail i ) T ,<label>(6)</label></formula><p>where α is the learning rate and (I detail i , J detail i ) is the ith patch pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Combining CNN with image enhancement</head><p>After training the network, the de-rained image can be obtained by directly adding the output detail layer to the base </p><formula xml:id="formula_5">O = I base + f w (I detail ),<label>(7)</label></formula><p>where O is the de-rained output. However, when dealing with heavy rain the result unsurprisingly looks hazy, as shown in <ref type="figure" target="#fig_6">Figure 8</ref>(b). Fortunately, we can easily embed image enhancement technology into our framework to create a better visual result. Different mature and advanced image enhancement algorithms can be directly adopted in this framework as postprocessing. In this paper, we use the non-linear function <ref type="bibr" target="#b30">[31]</ref> to enhance the base layer, and boost the detail layer by simply multiplying the output of the CNN by two to magnify the details,</p><formula xml:id="formula_6">O enhanced = (I base ) enhanced + 2f w (I detail ),<label>(8)</label></formula><p>where O enhanced is the de-rained output with enhancement and (I base ) enhanced is the enhanced base layer. <ref type="figure" target="#fig_6">Figure 8(c)</ref> shows the de-rained result with image enhancement. As shown in the intermediate results in <ref type="figure" target="#fig_6">Figures 8(d)</ref>-(g), virtually all of rain removal is being performed on the detail layer by the CNN, while the image enhancement on the base layer improves the global contrast and leads to a better visual result than shown in <ref type="figure" target="#fig_6">Figure 8</ref>(b) without using enhancement.</p><p>(a) Ground truth (b) Rainy image (c) Method <ref type="bibr" target="#b9">[10]</ref> (d) Method <ref type="bibr" target="#b10">[11]</ref> (e) Method <ref type="bibr" target="#b12">[13]</ref> (f) Our result </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>To evaluate our DerainNet framework, we test on both synthetic and real-world rainy images. As mentioned previously, both testing frameworks are performed using the network trained on synthesized rainy images. We compare with three recent high quality de-raining methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Software implementations of these methods were provided in Matlab by the authors. We use the default parameters reported in these three papers. All experiments are performed on a PC with Intel Core i5 CPU 4460, 8GB RAM and NVIDIA Geforce GTX 750. Our network contains two hidden layers and one output layer as described in Section II-B. We set kernel sizes s 1 = 16, s 2 = 1 and s 3 = 8, respectively. The number of feature maps for each hidden layer are n 1 = n 2 = 512. We set the learning rate to α = 0.01. More visual results and our Matlab implementation can be found at http://smartdsp.xmu.edu.cn/derainNet.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Synthesized data</head><p>We first evaluate the results of testing on newly synthesized rainy images. In our first results, we synthesize new rainy images by selecting from the set of 350 clean images from our database. <ref type="figure" target="#fig_7">Figure 9</ref> shows visual comparisons for one such synthesized test image. As can be seen, method <ref type="bibr" target="#b9">[10]</ref> exhibits over-smoothing of the rope and method <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> leaves significant rain streaks in the result. This is because <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> are algorithms based on low-level image features. When the rope's orientation and magnitude is similar with that of rain, methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> cannot efficiently distinguish the rope from rain streaks. However, as shown in the last result, the multiple convolutional layers of DerainNet can identify and remove rain while preserving the rope. <ref type="figure" target="#fig_1">Figure 10</ref> shows visual comparisons for four more synthesized rainy image using different rain streak orientations and magnitudes. Since the ground truth is known, we use (a) Ground truth (b) Synthesized image (c) Method <ref type="bibr" target="#b9">[10]</ref> (d) Method <ref type="bibr" target="#b10">[11]</ref> (e) Method <ref type="bibr" target="#b12">[13]</ref> (f) DerainNet <ref type="figure" target="#fig_1">Fig. 10</ref>. Example results on synthesized rainy images "umbrella", "rabbit", "girl" and "bird." These rainy images were for testing and not used for training. the the structure similarity index (SSIM) <ref type="bibr" target="#b31">[32]</ref> for quantitative evaluation. A higher SSIM value indicates a de-rained image that is closer to the ground truth in terms of image structural properties. (For the ground truth, the SSIM equals 1.) For a fair comparison, the image enhancement operation is not implemented by our algorithm for these synthetic experiments.</p><p>As is again evident in these results, method <ref type="bibr" target="#b9">[10]</ref> oversmooths the results and methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> leave rain streaks, both of which are addressed by our algorithm. Moreover, we see in <ref type="table" target="#tab_1">Table I</ref> that our method has the highest SSIM values, in agreement with the visual effect. Also shown in <ref type="table" target="#tab_1">Table I</ref> is the performance of the three methods on 100 newly-synthesized testing images using our synthesizing strategy. In <ref type="table" target="#tab_1">Table II</ref> we show the number of images for which the algorithm on the row outperformed the algorithm on the column for these 100 images.</p><p>In <ref type="table" target="#tab_1">Table I</ref> we also show results applying the same trained algorithms for each method on 12 newly synthesized rainy images (called Rain12) <ref type="bibr" target="#b12">[13]</ref> that are generated using photorealistic rendering techniques <ref type="bibr" target="#b32">[33]</ref>. This clearly highlights the generalizability of DerainNet to new scenes; whereas the other algorithms either decrease the performance or leave it </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real-world data</head><p>Since we do not possess the ground truth corresponding to real-world rainy images, we test DerainNet on real-world data using the network trained on the 4900 synthesized images from the previous section. In <ref type="figure" target="#fig_1">Figure 11</ref> we show the results of all algorithms with and without enhancement, where enhancement of <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b12">[13]</ref> are performed as post-processing, and for DerainNet is performed as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. In our quantitative comparison below, we use enhancement for all results, but note that the relative performance between algorithms was similar without using enhancement. We show results on three more real-world rainy images in <ref type="figure" target="#fig_1">Figure 12.</ref> (a) Rainy image (b) Method <ref type="bibr" target="#b9">[10]</ref> (c) Method <ref type="bibr" target="#b10">[11]</ref> (d) Method <ref type="bibr" target="#b12">[13]</ref> (e) Our result (f) Rainy enhanced (g) Method <ref type="bibr" target="#b9">[10]</ref> enhanced (h) Method <ref type="bibr" target="#b10">[11]</ref> enhanced (i) Method <ref type="bibr" target="#b12">[13]</ref> enhanced (j) Our result enhanced <ref type="figure" target="#fig_1">Fig. 11</ref>. Comparison of algorithms on a real-world "soccer" image with and without enhancement.</p><p>Although we use synthetic data to train our DerainNet, we see that this is sufficient for learning a network that is effective when applied to real-world images.</p><p>In <ref type="figure" target="#fig_1">Figure 12</ref>, the proposed method arguably shows the best visual performance on simultaneously removing rain and preserving details. Since the ground truth is unavailable in these examples, we cannot definitively say which algorithm performs quantitatively the best. Instead, we use a referencefree measure called the Blind Image Quality Index (BIQI) <ref type="bibr" target="#b33">[34]</ref> for quantitative evaluation. This index is designed to provide a score of the quality of an image without reference to ground truth. A lower value of BIQI indicates a higher quality image. However, as with all reference-free image quality metrics, BIQI is arguably not always subjectively correct. Still, as <ref type="table" target="#tab_1">Table III</ref> indicates, our method has the lowest BIQI on 100 newly obtained real-world testing images. This gives additional evidence that our method outputs an image with greater improvement.</p><p>To provide realistic feedback and quantify the subjective evaluation of DerainNet, we also constructed an independent user study. In this experiment, we use the de-rained results (with enhancement) of the same 100 real-world images scored with BIQI. For each image, we randomly order the outputs of the four algorithms, as well as the original rainy image, and display them on a screen. We then separately asked 20 participants to rank each image from 1 to 5 subjectively according to quality, with the instructions being that visible rain should decrease the quality and clarity should increase quality (1 represents the worst quality image and 5 represents the best quality image). We show the average scores in <ref type="table" target="#tab_1">Table  IV</ref> from these 2000 trials. As is evident, methods <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b10">[11]</ref> do not make a clear improvement over the original image. Method <ref type="bibr" target="#b6">[7]</ref> does clearly improve the rainy image, but the proposed method is subjectively superior to all images. This small-scale experiment gives additional support along with BIQI and our own subjective assessment that DerainNet improves the de-raining on real-world images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameter settings</head><p>In this section, we test different parameters setting to study their impact on performance. We use the same training data as previously. The testing data includes the same 100 newlysynthesized images as well as the new Rain12 images <ref type="bibr" target="#b12">[13]</ref>. 1) Kernel size: First, we test the impact of different kernel sizes. The default kernel sizes for the three levels are 16, 1 and 8; we denote this network as 16-1-8. We fix the kernel size of the second layer and reduce the kernel sizes of first and third layers to 4-1-2 and 8-1-4. We then performed experiments by instead increasing the kernel size of second layer to 16-3-8 and 16-5-8. <ref type="table" target="#tab_4">Table V</ref> shows the average SSIM values for these different kernel sizes. As can be seen, larger kernel sizes can generate better results. This is because more structure and texture can be modeled using a large kernel. On the contrary, from our experiments we find that increasing the kernel size of the second layer brings only limited improvement. This is because the second layer performs a non-linear operation for rain removal and the 1×1 kernel can achieve promising results. Thus, we choose 16-1-8 as the default setting of kernel size. 2) Network width: Intuitively, if we increase the network width by increasing the number of kernels, n 1 and n 2 , the performance should improve. We train three models by using the values: n 1 , n 2 ∈ {64, 128, 256} and compare them to our default setting of n 1 = n 2 = 512. <ref type="table" target="#tab_1">Table VI</ref> shows the average SSIM values for these four models. As can be seen, better performance can be achieved by increasing the width of the network. However, increasing the number of kernels improves the performance at the cost of running time since more convolutional operations are required. Thus we choose n 1 = n 2 = 512 as the default setting of network width.</p><p>(a) Rainy images (b) Method <ref type="bibr" target="#b9">[10]</ref> (c) Method <ref type="bibr" target="#b10">[11]</ref> (d) Method <ref type="bibr" target="#b12">[13]</ref> (e) Our results  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Network depth:</head><p>We also test the performance of using deeper structures by adding more non-linear layers. We train and test on 3 networks with depths 3, 5 and 10. As shown in <ref type="table" target="#tab_1">Table VII</ref>, for the de-raining problem, increasing the network depth does not bring better results using a feed-forward network structure. This is a results of gradient vanishing, which may perhaps be addressed by designing a more complex network structure (with increased computation time). However, our DerainNet generates high quality results with only 3 layers as a result of our proposed detail training strategy, and so the complexity and computation time of the model can be significantly reduced. Therefore, we adopt three layers as the default setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with another potential deep learning method</head><p>The proposed DerainNet combines image domain knowledge as pre-processing before the CNN step. As mentioned <ref type="bibr" target="#b5">[6]</ref> proposed directly using a CNN to removing dirt and drops from a window <ref type="bibr" target="#b5">[6]</ref>. (This is the only other related deep learning approach we are aware of.) As motivated in Section II-A and <ref type="figure">Figure 3</ref>, directly training on the image domain has drawbacks. We show a few other examples on real and synthesized data in <ref type="figure" target="#fig_1">Figure 13</ref>. As is evident from these examples as well, directly training on the image domain has drawbacks that are effectively addressed by our approach. We note that both approaches have virtually identical computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Impact of image enhancement step</head><p>In this section we assess the impact of image enhancement on our algorithm. We adopt three processing strategies for real-world data. Specifically, we conduct de-raining without any enhancement, de-raining with the enhancement as a postprocessing step after reconstruction, and simultaneous deraining and enhancement as proposed in <ref type="figure" target="#fig_2">Figure 2</ref>. <ref type="figure" target="#fig_1">Figure 14</ref> shows one example of these different processing strategies. As can be seen, rain streaks are removed by the CNN alone, while the enhancement step further improves the visual quality. We also use the BIQI metric to evaluate the three strategies by testing on collected real-world images, as shown in <ref type="table" target="#tab_1">Table VIII</ref>. Although the visual quality is similar with post-processing, the overall BIQI shows the best quantitative performance of our "mid-processing" approach.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Impact of the selected low-pass filter</head><p>Though we choose the guided filter <ref type="bibr" target="#b23">[24]</ref> to separate the base and detail layers for training the CNN, we found that the framework of <ref type="figure" target="#fig_2">Figure 2</ref> is effective using other low-pass filters as well. <ref type="figure" target="#fig_1">Figure 15</ref> shows one example of a de-raining result using different low-pass filters: guided filtering <ref type="bibr" target="#b23">[24]</ref>, bilateral filtering <ref type="bibr" target="#b24">[25]</ref> and rolling guidance filtering <ref type="bibr" target="#b25">[26]</ref>. As can be seen, though the low and high frequency decompositions look significantly different, the de-raining result is qualitatively (a) Clean image (b) Rainy image (c) Base layer <ref type="bibr" target="#b23">[24]</ref> (d) Base layer <ref type="bibr" target="#b24">[25]</ref> (e) Base layer <ref type="bibr" target="#b25">[26]</ref> (f) Detail layer <ref type="bibr" target="#b23">[24]</ref> (g) Detail layer <ref type="bibr" target="#b24">[25]</ref> (h) Detail layer <ref type="bibr" target="#b25">[26]</ref> (i) De-rained (f) (j) De-rained (g) (k) De-rained (h) (l) Result of (c)+(i) (m) Result of (d)+(j) (n) Result of (e)+(k) <ref type="figure" target="#fig_1">Fig. 15</ref>. Impact of three low-pass filters <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. SSIM values of (f), (j) and (n) are 0.9181, 0.9160 and 0.9158, respectively. Intensities of detail layer images have been amplified for better visualization. similar and the three SSIM values of the de-rained results are almost the same. DerainNet is able to recognize and remove rain as long as it is isolated to the detail layer. The method proposed in <ref type="bibr" target="#b9">[10]</ref> also applies this decomposition strategy using the bilateral filtering, but used in a different model. We make a comparison with method <ref type="bibr" target="#b9">[10]</ref> using bilateral filtering for our CNN as well. To ensure the low-pass filter removes all of the rain streaks, we change the default parameters of the bilateral filtering in <ref type="bibr" target="#b9">[10]</ref>. Specifically, we change the window size from 5 to 15 and intensity-domain standard deviations from 0.1 to 1. The difference in filtering operations between our method and <ref type="bibr" target="#b9">[10]</ref> is that method <ref type="bibr" target="#b9">[10]</ref> implements the pre-processing in the Y channel of YUV color space, while our method implements it in the RGB color space. <ref type="figure" target="#fig_1">Figure 16</ref> shows the both intermediate and final de-rained results. As can be seen, both methods isolate the rain to the (a) Rainy image (b) Base layer (Y channel) (c) Detail layer (Y channel) (d) De-rained (c) <ref type="bibr" target="#b9">[10]</ref> (e) De-rained result <ref type="bibr" target="#b9">[10]</ref> (f) Clean image (g) Base layer (RGB) (h) Detail layer (RGB) (i) Our de-rained (h) (j) Our de-rained result <ref type="figure" target="#fig_1">Fig. 16</ref>. Comparison with method <ref type="bibr" target="#b9">[10]</ref> by using the bilateral filtering approach as pre-processing for the respective models. SSIM values of (e) and (j) are 0.77 and 0.87, respectively. Intensities of detail layer images have been amplified for better visualization. high-pass portion for de-raining, but <ref type="bibr" target="#b9">[10]</ref> fails to completely remove rain streaks in <ref type="figure" target="#fig_1">Figure 16</ref>(c), while our result in <ref type="figure" target="#fig_1">Figure  16</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Training convergence and testing runtime</head><p>Training required approximately two days to run. In <ref type="figure" target="#fig_1">Figure  17</ref> we show the training convergence as a function of the number of backpropagations. While training requires a nontrivial amount of computation time, DerainNet is able to process new images very efficiently compared with current state-of-the-art de-raining methods. <ref type="table" target="#tab_1">Table IX</ref> shows the average running time for three different image sizes, each averaged over 10 testing images. (Note that these results do not factor in training time, but are for applying these methods to new data.) Since methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> are based on dictionary learning and method <ref type="bibr" target="#b12">[13]</ref> is based on Gaussian mixture model learning, complex optimizations are still required to de-rain new images, leading to a slower computation time. Our method has significantly faster running time since the testing procedure is completely feed-forward after network training. For even larger images, such as those taken by a typical camera, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> take from several minutes to over an hour to process a new image, while our method requires roughly half a minute based on a parallel GPU implementation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>We have presented a deep learning architecture called DerainNet for removing rain from individual images. Using a convolutional neural network on the high frequency detail content, our approach learns the mapping function between clean and rainy image detail layers. Since we do not possess the ground truth clean images corresponding to real-world rainy images, we synthesize clean/rainy image pairs for network learning, and showed how this network still transfers well to real-world images. We showed that deep learning with convolutional neural networks, a technology widely used for high-level vision task, can also be exploited to successfully deal with natural images under bad weather conditions. We also showed that DerainNet noticeably outperforms other stateof-the-art methods with respect to image quality and computational efficiency. In addition, by using image processing domain knowledge, we were able to show that we do not need a very deep (or wide) network to perform this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>X</head><label></label><figDesc>. Fu, J. Huang, X. Ding and Y. Liao are with Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University. (*Corresponding to: dxh@xmu.edu.cn) J. Paisley is with the Department of Electrical Engineering, Columbia University, New York, NY 10027 USA. This work was supported in part by the National Natural Science Foundation of China under Grants 61571382, 61571005, 81301278, 61172179 and 61103121, in part by the Guangdong Natural Science Foundation under Grant 2015A030313007, in part by the Fundamental Research Funds for the Central Universities under Grants 20720160075, 20720150169 and 20720150093, and in part by the Research Fund for the Doctoral Program of Higher Education under Grant 20120121120043.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Input rainy image (b) Our result An example real-world rainy image and our de-rained result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The proposed DerainNet framework for single-image rain removal. The intensities of the detail layer images have been amplified for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(a) 3 Fig. 3 .</head><label>33</label><figDesc>Rainy image (b) Image domain, depth = 3 (c) Image domain, depth = 10 (d) Detail layer domain, depth = CNN learning options: (b) directly on image domain with depth = 3 (equivalent to retraining [6] on new data), (c) directly on image domain with depth = 10, and (d) on high-frequency detail layer with depth = 3. The first row shows the full image and the second row a zoomed-in region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>|J base − I base | Example base and detail layers of two synthesized images. We use the guided filtering<ref type="bibr" target="#b23">[24]</ref> as the low-pass filter to generate the results.(a) Clean image J (b) Rainy image I (c) J detail (d) Sparsity of detail layer. The detail layers are obtained by J detail = J − J base and I detail = I − I base .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>An example of synthesized rainy images. The top left is the clean image and the remaining are various images synthesized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization of intermediate results with enhancement. Intensities of detail layers have been amplified for better visualization. layer,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Results on synthesized rainy image "dock". Row 2 shows corresponding enlarged parts of red boxes in Row 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Three more results on real-world rainy images: (top-to-bottom) "Buddha," "street," "cars." All algorithms use image enhancement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Top: Two zoomed-in regions of images synthesized with rain. Bottom: Real world rainy image (no enhancement).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Impact of image enhancement of different processing strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 17 .</head><label>17</label><figDesc>The training convergence curve of DerainNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(i) has considerably better success. The final results are shown in Figures 16(e) and (j).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 6. Visualization of intermediate results. The first row shows our de-raining result and the trained weights W 1 (512 kernels of size 16 × 16 × 3) and W 3 (3 kernels of size 8 × 8 × 512, one for each color channel). For W 3 we visualize these three kernels as RGB images across the 512 dimensions. Since the 512 kernels W</figDesc><table /><note>2 are 1 × 1 × 512, we do not show them. The second row shows the corresponding hidden layer activations. (e) and (h</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>MEASUREMENT RESULTS USING SSIM ON SYNTHESIZED TEST IMAGES.</figDesc><table><row><cell>Images</cell><cell>Ground truth</cell><cell>Rainy image</cell><cell cols="2">Method [10] Method [11]</cell><cell>Method [13]</cell><cell>Ours</cell></row><row><cell>dock</cell><cell>1</cell><cell>0.86</cell><cell>0.84</cell><cell>0.88</cell><cell>0.90</cell><cell>0.92</cell></row><row><cell>umbrella</cell><cell>1</cell><cell>0.75</cell><cell>0.83</cell><cell>0.81</cell><cell>0.86</cell><cell>0.88</cell></row><row><cell>rabbit</cell><cell>1</cell><cell>0.72</cell><cell>0.74</cell><cell>0.77</cell><cell>0.79</cell><cell>0.85</cell></row><row><cell>girl</cell><cell>1</cell><cell>0.93</cell><cell>0.80</cell><cell>0.94</cell><cell>0.91</cell><cell>0.94</cell></row><row><cell>bird</cell><cell>1</cell><cell>0.57</cell><cell>0.63</cell><cell>0.62</cell><cell>0.75</cell><cell>0.82</cell></row><row><cell>100 new images</cell><cell>1</cell><cell>0.79 ± 0.13</cell><cell>0.73 ± 0.07</cell><cell>0.84 ± 0.09</cell><cell>0.82 ± 0.10</cell><cell>0.89 ± 0.06</cell></row><row><cell>Rain12 [13]</cell><cell>1</cell><cell>0.91 ± 0.05</cell><cell>0.81 ± 0.07</cell><cell>0.88 ± 0.05</cell><cell>0.91 ± 0.03</cell><cell>0.92 ± 0.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II #</head><label>II</label><figDesc>TIMES</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(ROW) BEAT (COL)</cell><cell></cell></row><row><cell></cell><cell cols="2">[10] [11]</cell><cell cols="2">[13] Ours</cell></row><row><cell>[10]</cell><cell>−</cell><cell>5</cell><cell>4</cell><cell>0</cell></row><row><cell>[11]</cell><cell>107</cell><cell>−</cell><cell>76</cell><cell>6</cell></row><row><cell>[13]</cell><cell>108</cell><cell>36</cell><cell>−</cell><cell>7</cell></row><row><cell>Ours</cell><cell>112</cell><cell>102</cell><cell>105</cell><cell>−</cell></row><row><cell cols="5">unchanged, DerainNet still shows improvement.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV AVERAGE</head><label>IV</label><figDesc>SCORES OF USER STUDY.</figDesc><table><row><cell cols="3">Images Input Method [10]</cell><cell>Method [11]</cell><cell cols="2">Method [13] Ours</cell></row><row><cell>Scores</cell><cell>1.51</cell><cell>1.46</cell><cell>1.73</cell><cell>2.57</cell><cell>4.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V AVERAGE</head><label>V</label><figDesc>SSIM OF DIFFERENT KERNEL SIZES.</figDesc><table><row><cell>Kernel sizes</cell><cell>4-1-2</cell><cell></cell><cell>8-1-4</cell><cell>16-1-8 (default)</cell></row><row><cell>SSIM</cell><cell cols="2">0.84 ± 0.06</cell><cell cols="2">0.87 ± 0.05</cell><cell>0.89 ± 0.06</cell></row><row><cell cols="2">Kernel sizes</cell><cell></cell><cell>16-3-8</cell><cell>16-5-8</cell></row><row><cell></cell><cell>SSIM</cell><cell cols="2">0.89 ± 0.06</cell><cell>0.90 ± 0.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>MEASUREMENT RESULTS OF BIQI ON REAL-WORLD TEST IMAGES.</figDesc><table><row><cell></cell><cell></cell><cell>Images</cell><cell cols="2">Input</cell><cell></cell><cell>Method [10]</cell><cell>Method [11]</cell><cell>Method [13]</cell><cell>Ours</cell></row><row><cell></cell><cell></cell><cell>soccer</cell><cell cols="2">57.96</cell><cell></cell><cell>42.70</cell><cell>53.86</cell><cell>35.64</cell><cell>33.35</cell></row><row><cell></cell><cell></cell><cell>Buddha</cell><cell cols="2">49.06</cell><cell></cell><cell>39.55</cell><cell>50.13</cell><cell>39.90</cell><cell>28.10</cell></row><row><cell></cell><cell></cell><cell>street</cell><cell cols="2">37.70</cell><cell></cell><cell>40.67</cell><cell>38.32</cell><cell>38.98</cell><cell>34.08</cell></row><row><cell></cell><cell></cell><cell>cars</cell><cell cols="2">27.84</cell><cell></cell><cell>40.08</cell><cell>21.17</cell><cell>31.70</cell><cell>24.18</cell></row><row><cell></cell><cell></cell><cell cols="4">100 test images 33.00 ± 13.19</cell><cell>39.63 ± 7.66</cell><cell>31.43 ± 9.81</cell><cell>34.60 ± 7.78</cell><cell>29.86 ± 6.98</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">AVERAGE SSIM OF DIFFERENT NETWORK WIDTH.</cell></row><row><cell>Width</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell></cell><cell cols="2">512 (default)</cell></row><row><cell>SSIM</cell><cell cols="2">0.80 ± 0.05 0.82 ± 0.05</cell><cell cols="2">0.85 ± 0.05</cell><cell cols="2">0.89 ± 0.06</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VII</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">AVERAGE SSIM OF DIFFERENT NETWORK DEPTH.</cell></row><row><cell></cell><cell>Depth</cell><cell>3 (default)</cell><cell>5</cell><cell>10</cell><cell></cell></row><row><cell></cell><cell>SSIM</cell><cell cols="2">0.89 ± 0.06 0.84 ± 0.05</cell><cell cols="3">0.79 ± 0.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII BIQI</head><label>VIII</label><figDesc>RESULTS FOR THREE ENHANCEMENT STRATEGIES.</figDesc><table><row><cell>Images</cell><cell>No enhance</cell><cell>Post-enhance</cell><cell>Proposed</cell></row><row><cell>soccer</cell><cell>34.69</cell><cell>32.06</cell><cell>33.35</cell></row><row><cell>Buddha</cell><cell>37.88</cell><cell>29.72</cell><cell>28.10</cell></row><row><cell>street</cell><cell>37.47</cell><cell>34.33</cell><cell>34.08</cell></row><row><cell>cars</cell><cell>31.56</cell><cell>26.08</cell><cell>24.18</cell></row><row><cell cols="2">100 test images 31.86 ± 7.89</cell><cell>29.98 ± 7.82</cell><cell>29.86 ± 6.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IX COMPARISON</head><label>IX</label><figDesc>OF RUNNING TIME (SECONDS).</figDesc><table><row><cell>Image size</cell><cell cols="2">Method [10] Method [11]</cell><cell>Method [13]</cell><cell>Ours</cell></row><row><cell>250 × 250</cell><cell>68</cell><cell>53</cell><cell>196</cell><cell>1.3</cell></row><row><cell>500 × 500</cell><cell>76</cell><cell>230</cell><cell>942</cell><cell>2.8</cell></row><row><cell>750 × 750</cell><cell>99</cell><cell>782</cell><cell>1374</cell><cell>5.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.photoshopessentials.com/photo-effects/rain/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detection and removal of rain from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of rain and snow in frequency space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Barnum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="256" to="274" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rain or snow detection in image sequences through use of a histogram of orientation of streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bossu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Tarel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="348" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Utilizing local phase information to remove rain from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Santhaseelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="89" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single-image deraining using an adaptive nonlocal means filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Restoring an image taken through a window covered with dirt or rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A generalized low-rank appearance model for spatio-temporally correlated rain streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Context-aware single image rain removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic single image-based rain streaks removal via image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1742" to="1755" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-learning based image decomposition with applications to single image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="93" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Removing rain from a single image via discriminative sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual depth guided color image rain streaks removal using sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1430" to="1455" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rain streak removal using layer priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processin Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shepard convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep edge-aware filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rolling guidance filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Local edge-preserving multiscale decomposition for high dynamic range image tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LLSURE: local linear surebased edge-preserving image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="90" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">UCID: an uncompressed color image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Storage and Retrieval Methods and Applications for Multimedia</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A contrast enhancement framework with JPEG artifacts suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Photorealistic rendering of rain streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="996" to="1002" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A two-step framework for constructing blind image quality indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="516" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
