<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MILDNet: A Lightweight Single Scaled Deep Ranking Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-03-15">March 15, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudha</forename><surname>Vishvakarma</surname></persName>
							<email>anirudhav@fynd.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Fynd (Shopsense Retail Technologies Pvt. Ltd.) Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MILDNet: A Lightweight Single Scaled Deep Ranking Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-03-15">March 15, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Computer Vision</term>
					<term>Image Re- trieval</term>
					<term>Visual Search</term>
					<term>Recommender Systems</term>
					<term>Feature Ex- traction</term>
					<term>E-Commerce</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-scale deep CNN architecture <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> successfully captures both fine and coarse level image descriptors for visual similarity task, but they come up with expensive memory overhead and latency. In this paper, we propose a competing novel CNN architecture, called MILDNet, which merits by being vastly compact (about 3 times). Inspired by the fact that successive CNN layers represent the image with increasing levels of abstraction, we compressed our deep ranking model to a single CNN by coupling activations from multiple intermediate layers along with the last layer. Trained on the famous Street2shop dataset [4], we demonstrate that our approach performs as good as the current state-of-the-art models with only one third of the parameters, model size, training time and significant reduction in inference time. The significance of intermediate layers on image retrieval task has also been shown to be performing on popular datasets Holidays, Oxford, Paris <ref type="bibr" target="#b4">[5]</ref>. So even though our experiments are done on ecommerce domain, it is applicable to other domains as well. We further did an ablation study to validate our hypothesis by checking the impact on adding each intermediate layer.</p><p>With this we also present two more useful variants of MILD-Net, a mobile model (12 times smaller) for on-edge devices and a compactly featured model (512-d feature embeddings) for systems with less RAMs and to reduce the ranking cost. Further we present an intuitive way to automatically create a tailored in-house triplet training dataset, which is very hard to create manually. This solution too can also be deployed as an all-inclusive visual similarity solution. Finally, we present our entire production level architecture which currently powers visual similarity at Fynd.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Instance-level-image retrieval (content based -CBIR) also known as visual similarity is a technique used for various tasks like visual recommendation, visual search, etc. and has * The code is open-sourced at https://github.com/gofynd/mildnet proved to be a useful application and an active research topic for decades. The need of learning both fine level and coarse level abstractions complexly grained in the image pixel makes it a challenging task.</p><p>Especially for an Ecommerce platform, where showcasing products in the right manner adds lot to the users' experience on product discovery. Hence, it is a critical feature to recommend users the products similar to what they are viewing as it readily captures their current intent (as opposed to similar users' taste using collaborative filtering or using past activities of user), leads to more engagement (CTR) and hence conversion (CR). For ecommerce, two basic use cases where CBIR is usually applied are:</p><p>1. Visual Search: Users upload a picture of someone who has worn the product (e.g. their friend or some celebrity) on the platform and a set of top visual similar items are presented from the entire dataset of the platform. Conversion rate could be very high if a good match is found this way.</p><p>2. Visual Recommendation: While user is browsing through any product on app, the top visual similar products to that product is readily displayed to user to capture his/her immediate interest. It can directly increase CTR.</p><p>Naively one can use text/meta details based search, but this data is not usually detailed and readily available. Also, the notion of similarity, especially for ecommerce domain, is not just a function of meta details of the product but is engraved in the visual appearance complexly present in the pixels of the product image. Capturing these minute details makes a big difference to the outcomes of such a feature. The task is not trivial as product comes in various variety even within the same class. Also, the notion of similarity is abstract, vague and debatable since it is usually relative. Let's take an example of a woman dress, the dress could be in variety of shapes (maxi, a-line, halter etc.), lengths (long, medium etc.), collar types (v neck, round neck, peter pan etc.), sleeve types (sleeveless, bell, puff, one-shoulder etc.), patterns (checked, striped, printed etc.), colors (red, maroon, pink etc.), etc. which the model should have an eye on. While the product image could be with or w/o a background and worn by a model of different complexion and hair colors in different poses, or a mannequin, or folded, or laid flat, are a few at- <ref type="figure">Figure 1</ref>: Basic visual similarity pipeline where all the products in a database are mapped on an n-dimensional space using a feature extractor. Nearest neighbours are then the top most visual similar items.</p><p>tributes our system should be ignoring. The solution hence is expected to understand fine-grained as well as coarse grained difference of details in two product images.</p><p>The visual similarity pipeline (see <ref type="figure">Figure 1</ref>) generally consists of a feature extractor which takes image pixels as input and produce a feature vector/embeddings representing the visual attributes needed for the task. The feature embeddings of all the images in the database when mapped to an n-dimension space places them in such a manner that visually similar products are always nearer. We can then use a technique like k Nearest Neighbour to quickly find k visual similar products to a query image.</p><p>Handcrafted features (SIFT <ref type="bibr" target="#b11">[12]</ref>, HOG <ref type="bibr" target="#b10">[11]</ref>, etc.) have been tried as the feature extractor but failed to give high accuracy. A deep CNN is hence a suitable candidate as it can be expected to capture all these details while also being robust. Deep learning has achieved great feat in classification tasks, the features extracted from these trained networks are still found useful for other tasks including image retrieval. Using features from a single CNN trained on a classification task has been tried but fails to give high accuracy, due to the strong invariance encoded in its architecture. While training on classification task, a deep CNN generally encode strong invariance which grows higher towards the top layers, making it hard to learn the fine-grained image visual similarity. Current state of the arts uses an ensemble of 3 CNNs, also known as multi-scale architecture, first introduced by Jiang et. al <ref type="bibr" target="#b0">[1]</ref>. Each CNN here is expected to capture different level of abstraction from the image. They are trained on image triplets containing a query image, a positive image (similar to query) and a negative image (relatively dissimilar to query image).</p><p>Triplet training strategy is used where a triplet loss function is used to penalize the network whenever query image is closer to negative than positive images while training (see <ref type="figure" target="#fig_0">Figure  2</ref>). This enforces the network to learn relevant features to keep similar images together. The improvement in learning and results comes up with a trade-off of larger model size and latency on inference.</p><p>While multi-scale deep CNN architecture can learn complex relations required for visual similarity task, they also come up with high computation cost and latency. Since an image retrieval task might require generating thousands of image in real-time at production, it's great if the model can be light and fast as well. We counter the use of extra CNNs by arguing that a multi-layered CNN while convolving image or the feature maps of previous layers, already has been seen to automatically learn different level of detail (from edges, gradients, to eyes, nose, to faces) progressively at different layers <ref type="bibr" target="#b5">[6]</ref>. The lower first few layers of a CNN captures more local patterns of objects like lines and corners, and as we go deeper, the later layers starts recognizing more complex features. Since multiple level of features are already present in multiple layers of a CNN, using only the feature from the last layer hasn't found the best utilization of its learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>We here present a novel deep CNN architecture, MILD-Net (Multi-Intermediate Layers Descriptors Net), which uses only 1 CNN containing multiple skip connections to capture features from different layers of a single deep CNN model. Activations from a convolutional layers can be interpreted as local features describing particular image regions. We aggregated this local features inspired by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> to convert them into powerful global descriptors. Qili et. al <ref type="bibr" target="#b8">[9]</ref> used a very similar approach to competitively perform in the Alibaba large-scale search challenge. We used global average pooling to aggregate features from each layer inspired by the work of Artem et. al <ref type="bibr" target="#b9">[10]</ref>. Rigorous experiments and comparison with multi-scale network architectures mentioned earlier on popular Street2shop clothing similarity data <ref type="bibr" target="#b3">[4]</ref> has shown that our model captures as good as features as current state-of-art, but with 3 times smaller network size (around 80 MBs) and inference latency. We used accuracy and recall as the metrics to compare. To validate the impact of each skip connections we progressively trained CNN with addition of each intermediate layer starting from no skip connection. We also have experimented multiple variants of MILDNet in which one variant reduces inference latency and network size further by roughly 3 times, while the other reduces the ranking cost by 2 times. Another variant is a mobile version is presented which tradesoff 5.4% top test accuracy to be around 11 times lesser in size than the current state-of-the-art models. This model can be deployed client side in an application to reduce the server load. Further we demonstrate a novel way to create in-house tailored catalog triplet data semi-automatically, which is hard to create manually. Since similarity is a factor of various attributes and relative similarity is rather subjective, using this approach can create a data directly from product catalog capturing one's notion of visual similarity. Coupling the architecture of MILDNet with the tailored training data from direct catalog can boost the overall performance for an ecommerce app. Finally, we explain our production deployment strategy and methods to optimize such a system. Hence, the contribution of this work is multi-fold and also comprehensive. Embeddings are tuned to make sure that positive images are always closer to the query images than the negative in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Since the task of image retrieval is not trivial, various studies are done in the past to solve it. The best results are ob-tained from Content-based image retrieval (CBIR) systems where distinctions are made based on the visual details of the image rather than the meta details. The basic approach is always getting embeddings from passing images pixels through a function and then searching for similar image in the embedding space. So, the main component most studied is hence the feature extractor.</p><p>Global features generalizes entire object (contour representations, shape descriptors, and texture features) are seen to be extracted using Histogram Oriented Gradients (HOG <ref type="bibr" target="#b10">[11]</ref>). While local features describes the image patches (like the texture in image patch) has been extracted from SIFT <ref type="bibr" target="#b11">[12]</ref>, OA-SIS <ref type="bibr" target="#b12">[13]</ref> and local distance learning <ref type="bibr" target="#b13">[14]</ref> learn fine-grained image similarity ranking models on top of the hand-crafted features. These hand-crafted features works fast but lacks expressive power.</p><p>Encoding hand-crafted features into bag-of-words (BoW) histograms had been a traditional approach <ref type="bibr" target="#b14">[15]</ref>. A further compact representation is found to be achieved using Vector Locally Aggregated Descriptor (VLAD) <ref type="bibr" target="#b15">[16]</ref> which achieves good results while requiring less storage. Other approaches like Fisher Vectors <ref type="bibr" target="#b17">[18]</ref>, and, more recently, triangular embedding <ref type="bibr" target="#b18">[19]</ref>, have also shown state-of-the-art for hand-crafted features like SIFT.</p><p>Deep convolutional features have been used for image retrieval in various prior works. Razavian et al. <ref type="bibr" target="#b16">[17]</ref> was among the first to investigate the use of CNN features for various computer vision tasks, including image retrieval. But the performance lagged behind that of simple SIFT-based methods which can be tackled by additionally incorporating spatial information. Qualitative examples of retrieval using deep features extracted from fully-connected layers have been studied <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref> which showed to outperform SIFT-like features. Gong et al. <ref type="bibr" target="#b20">[21]</ref> improved it by introducing Multi-scale Orderless Pooling (MOP) where different fragments of image as passed through CNN and the activations from the fullyconnected layer is aggregated by VLAD <ref type="bibr" target="#b15">[16]</ref>. This introduced complexity of computing the full DCNN pipeline not only on the original image but also on a large number of multi-scale patches and further apply two levels of PCA dimensionality reduction. The works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> evaluated image retrieval descriptors obtained by the max pooling aggregation of the last convolutional layer. Artem et. al <ref type="bibr" target="#b9">[10]</ref> showed that using sum pooling to aggregate features on the last convolutional layer leads to a much better performance. Hence, inspired by this work we chose global average pooling for aggregating the deep CNN features in our proposed solution.</p><p>While CNN features from last convolutional layer showed good results, but have less discriminability for instance-levelimage retrieval since local characteristics of objects are not preserved. On the other hand features from intermediate layers have these local characteristics. Wan et. al <ref type="bibr" target="#b6">[7]</ref> did a comprehensive study on applying CNN features to real-world image retrieval with model retraining and similarity learning. Encouraging experimental results show that CNN features are effective in bridging the semantic gap between low-level visual features and high-level concepts. Several cutting-edge studies <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> suggested that mid-level features extracted from intermediate layers could obtain better performance than features extracted from the final layer. Joe et al. <ref type="bibr" target="#b7">[8]</ref> experimented on all intermediate layers of GoogleNet and selected the best performant layer as the best representation of the image. However, this approach is inefficient to the largescale image dataset because the best layer of different objects may be various. Recently the work of Qili et. al <ref type="bibr" target="#b8">[9]</ref> demonstrated great results using a novel representation policy that encodes feature vectors extracted from different layers, called as multi-level-image representation. Our work is closely related to theirs but consists of experimentation on a different dataset, comparison with different architectures, loss function and aggregation step.</p><p>Image similarity using Triplet Networks has been studied in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref>. The work by Jiang et. al <ref type="bibr" target="#b0">[1]</ref> introduced a novel multi-scale CNN architecture including Alexnet + 2 low resolution paths which demonstrated state-of-the art results for capturing both the global visual properties and the image semantics. A triplet-based hinge loss ranking function is used to characterize fine-grained image similarity relationships. Later Devashish et. al <ref type="bibr" target="#b1">[2]</ref> improved the results further by using VGG16 instead of Alexnet. Recently, Rishabh et. al <ref type="bibr" target="#b2">[3]</ref> showed state-of-the-art performance by using VGG19 as base convnet along with the use of a Siamese network with contrastive loss function <ref type="bibr" target="#b26">[27]</ref>. In this paper, we will compare our model performance with these 3 works on the dataset Street2Shop which was made available by Kiapour et. al <ref type="bibr" target="#b3">[4]</ref> (see <ref type="figure" target="#fig_1">Figure 3</ref>). This dataset contains curated, human labelled dataset containing wild/street image as query images and catalog/shop images as matching images. In most of the experiments we used triplet loss function which is a prediction errorbased loss function, widely used for representation learning and metric learning in deep networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Our top performing model instead uses contrastive loss function <ref type="bibr" target="#b26">[27]</ref> inspired by <ref type="bibr" target="#b2">[3]</ref> which is rather a distance-based loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Used</head><p>We have experimented our system on their ability of retrieving ecommerce product based on its visual appearance. For this we gathered triplet pairs consisting of a query image, a positive image (relatively similar to query image) and a negative image (relatively dissimilar to query image). The query image can either be • Wild Image: where people wearing the cloth in everyday uncontrolled settings. • Catalog Image: model wearing cloth in controlled settings as shown in an ecommerce app.</p><p>While the positive and negative images can also be • In-class: same product category as query image • Out-of-class: other product category than query image Generally wild query images and both in-class and out of class positive and negative images are used for Visual Search feature where users can upload a picture of someone wearing a piece of cloth and find similar products from the entire catalog. While catalog query image with in-class positive and negatives are used for Visual Recommendation feature where on a product page a list of visually similar products are displayed to users in an ecommerce platform. So, for experimentation we used wild query image with catalog in-class and out-of-class positives and negatives. While for production deployment we used a mixture of both wild and catalog query image with majorly in-class positive and negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data for Experimentation</head><p>We used Exact Street2Shop <ref type="bibr" target="#b3">[4]</ref> dataset which is an extensive dataset which contains 20,000 wild images, 4,00,000 catalog images and exact street-to-shop pairs as meta-data (established manually). The retrieval sets is present for 11 fashion clothing categories, but only the category tops is rigorously experimented and presented here by us. It also contains bounding boxes around the object of interest within the wild image. We sampled the query image, q of the triplet from the wild images (without cropping). Positive images, p of the triplet was always the ground truth match from catalog images. Negative images, n of the triplet were images of other catalog items. Finally, a training dataset of 70733 image and validation dataset of 25460 images is extracted and used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data for Production Deployment</head><p>For the production level model, the data should also consist catalog query images as that is what primarily will be queried in our app. Also, we wanted to make sure that the results should be tailored to our needs from visual recommendation system. So we decided to bootstrap our own data using an engineered way to tune the results (Section 7.1). Using results from this engineered model, we can now easily sample plenty of both in-class and out-of-class catalog triplet samples from our entire database. We used 30% in-class to 70% out-ofclass triplets in case of catalog image triplets, while 30% wild image triplets to 70% catalog image triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>We trained and evaluated our model as a triplet-based network architecture where 3 images (query q i , positive p i and negative n i ) are passed independently into three identical deep ranking architectures with shared architecture and parameters. The triplets capture relative similarity notion of query image with positive image as compared to negative image. The system in this manner learns to create a feature embedding of dimension d, which is sufficient to capture both the fine-grained and coarse visual details of the image. During training the loss function makes sure that the resulting embeddings of query image is closer to positive images than the negative images. After getting these visual features, the problem now turn into a nearest neighbour search problem, where images with the closest embeddings to the query image gives visual similar items. Hence the main components of this system is 1. Feature Extractor (CNN Architecture), 2. Loss Function (Hinge Loss), 3. Nearest Neighbour search (kNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Multi-scale deep ranking architecture</head><p>Comparison of our proposed model MILDNet is done with 3 popular deep ranking architectures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. All the them are a type of a multi-scale neural network architecture first introduced by Jiang et. al <ref type="bibr" target="#b0">[1]</ref>. The underlying architecture involves a combination of 3 separate CNN:</p><p>• ConvNet which is usually Alexnet <ref type="bibr" target="#b14">[15]</ref> and recently VGG16 <ref type="bibr" target="#b29">[30]</ref>, VGG19 <ref type="bibr" target="#b29">[30]</ref>. This CNN captures the image semantics and encodes strong invariance which can be harmful for fine-grained image similarity tasks. Also, the convnet is not trained from scratch and is pretrained on the popular ImageNet ILSVRC-2012 dataset <ref type="bibr" target="#b30">[31]</ref>, which contains roughly 1000 images in each of 1000 categories.</p><p>• Two shallower networks, receives down-sampled images to encode lesser invariance and capture the visual appearance.</p><p>The features from these three networks are concatenated and passed through a few fully connected layers to finally obtain a feature map which captures both the global visual properties and the image semantics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">MILDNet</head><p>Rather than using complimentary CNNs to compensate the partial learning of the convnet in multi-scale architecture, we planned on using the features from few of the intermediate layers. As from many studies <ref type="bibr" target="#b5">[6]</ref>, it is found that deep CNN capture different level of abstraction at different level of layers, this seemed intuitive. Our top performing variant of MILDNet uses VGG16 as the base Convnet, pretrained on ImageNet dataset. We extract features from 4 additional level than the last layer, just after the presence of max-pooling layers to limit the features extracted to most important ones. Artem et. al <ref type="bibr" target="#b9">[10]</ref> have studied the performance of different aggregation methods for converting the CNN activations into global features. Inspired by their work, we used global average pooling to flatten the features and concatenate them to obtain a 1472-d feature vector. This is then passed through an FC -Dropout -FC layer to finally give the desired feature embedding of 2048 dimension. This way we significantly reduce the model size to the size of roughly a single VGG16 model. Also, while training we froze the first 10 layers since the initial layers contains very local and generic features and need not be retrained. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Summary</head><p>We tried out and compared five architectures in our experiments. Three of which are from recent research work on Multi-scale deep ranking architecture, and the rest 2 are variants of our proposed network, MILDNet. <ref type="table" target="#tab_0">Table 1</ref> shows the basic details of these five architectures. Finally, the feature vector can be summarized as a function f, which on passing input image I, outputs the image embedding x i in the embedding space.</p><p>x</p><formula xml:id="formula_0">i = f (I, W )<label>(1)</label></formula><p>where W contains all the weights and biases learned to optimize the mapping of images in embedding space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss</head><p>Loss functions are the one which penalizes neural networks when they induce error while gradient descent tries to reduce the loss by adjusting the weights. In case of triplet image similarity where the base architecture have given 3 embedding feature vectors q, p and n, loss functions make sure to keep the similar images together and dissimilar images apart in the embedding space. We tried two such loss functions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Hinge Loss</head><p>Hinge Loss function makes sure that in embedding space, vector q is always relatively closer to p than n in terms of euclidean distance. Let's say that D( x, y) denotes the euclidean distance of vector x and y in the embedding space, then the hinge loss function can be written as</p><formula xml:id="formula_1">L = max(0, D( q, p) 2 − D( q, n) 2 + m)<label>(2)</label></formula><p>where m is the extra margin ¿ 0, by which these vectors should be at least away and is decided empirically. We have used m = 1 in our experiments. As can be seen from the equation, when p is farther to q than n, the loss becomes positive by c plus the difference. This penalizes the model and weights are changed accordingly. Hence, hinge loss tries to keep the similar images closer relative to the dissimilar images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Contrastive loss function</head><p>Contrastive loss function <ref type="bibr" target="#b22">[23]</ref> is another distance-based loss function which also maps similar vectors to nearby points and dissimilar vectors to distant points. This loss function applies on a pair of samples rather like conventional learning systems which sums the loss over samples. The contrastive loss function can be written as</p><formula xml:id="formula_2">L( q, p) = 1/2 * D( q, p) 2<label>(3)</label></formula><p>L( q, n) = 1/2 * (max(0, m − D( q, n))) 2 <ref type="bibr" target="#b3">(4)</ref> where D is the euclidean distance between two vectors and m is the extra margin ¿ 0 which is decided empirically. We have used m = 1 in our experiments. When similar images are passed, the network is proportionally penalized by their distance. While for dissimilar images, the network is penalized only when distance is greater than the margin m. The paper also points that by simply minimizing the euclidean distance over the set of all similar pairs should lead to a collapsed solution. Contrastive Loss function have recently shown to have performed better than Hinge loss on the same task and dataset using multi-scale architecture <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Nearest Neighbour Search</head><p>Finally, when we utilize the model to get a ranked list of visually similar images from the entire dataset, we need to search through the embedding space. We chose approximate nearest neighbour search for this task. The time complexity of exact Nearest Neighbour is O(n 2 ), which is reduced to O(n*k) by approximate nearest neighbours by allowing some errors. By definition, it works by finding a point p ∈ P which isapproximate nearest neighbor of the query q, that p ' ∈ P, d(p, q) (1 + )d(p ' ,q). This definition says that instances which are only a factor of away from the real nearest neighbors can be considered as nearest neighbors. We used Annoy (Approximate Nearest Neighbours Oh Yeah) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">33]</ref>, a library open-sourced by Spotify to aid us in this task. It is an algorithm based on random projections and trees. To compute the nearest neighbors it splits the set of points into half and does this recursively until each set is having k items where value of k is set empirically. To get better results, a priority queue sorted by the minimum margin for the path from the root node is used to search the tree. For every set multiple trees are build. If k items in the trees are found, duplicates are removed and for these instances the distances are computed on the original dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment Details</head><p>In all of our experiments, we used triplets extracted from the tops category of the famous Street2shop dataset, which finally gave us training dataset of 70733 image and validation dataset of 25460 images. We made sure that the triplets in validation dataset contains unique query images from the training dataset. These images are resized by their respective model architecture input size and rescaled by 1/255 to normalize. We chose Keras <ref type="bibr" target="#b32">[34]</ref> with backend of Tensorflow <ref type="bibr" target="#b33">[35]</ref> as the deep learning library. Training images are augmented using Keras's real-time augmentator, ImageDataGenerator class. We used following augmentations:</p><p>• We kept a seed to make sure same augmentation applies in all experiments, so we can compare results and to make it reproducible.</p><p>To conceptualized our models on Google Colaboratory (also known as Colab), which is an open-sourced preconfigured ML playground. We ran all our experiments on Google Cloud ML Engine, which is a serive to train and deploy models at scale. Our experiments used one of 1. Single NVIDIA Tesla K80 GPU 2. Single NVIDIA Tesla V100 GPU 3. Cluster of four NVIDIA Tesla K80 GPUs 4.Single cloud TPU. We ran around a 100 training jobs, tried different architectures or combination of losses and other hyperparameters, out of which top 8 top performing results of a few architectures are presented here. For optimizer, we have tried both RMSProp algorithm and stochastic gradient descent (SGD) with a momentum. The best result we got is by using SGD with a momentum of 0.9 and learning rate of 0.001. We used a batch size of 96 in most of the experiments.</p><p>For monitoring, we used Google Tensorboard as well as a very handy log and metrics monitoring tool Hyperdash <ref type="bibr" target="#b35">[37]</ref>.</p><p>For visualization of dataset and results, as well as for reporting results and plotting graph we used Google Colaboratory notebooks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results &amp; Conclusion</head><p>We present here the results of 8 of our experiments, each of which varies on the architecture used or the loss function. For evaluation, the major metrics for us was triplet test data accuracy and average inference time. The details of the underlying architectures and loss functions are presented in Section 4. The <ref type="table" target="#tab_0">Table 1</ref> shows the basic details of the 8 experiments we carried. Details of these 8 experiments are below:</p><p>• Multiscale-Alexnet: Multiscale model with base convnet as Alexnet and 2 shallow networks. We couldn't find a good implementation of Alexnet on Tensorflow, so we used Theano to train this network.  <ref type="table" target="#tab_2">Table 2</ref> shows training and validation triplet accuracy percentage of all the experiments, while the graphs 6 and 7 shows the trend of the training and validation triplet accuracy respectively. The test dataset consists of 25460 images consists of triplets from Street2shop dataset, having unique query images than the training data. Our model came second and only lags behind the top performing model (Ranknet) by only 1.29% but with only one third of parameters and model size. This shows our model even being light has sufficient learning potential for this task. The model are further compared on term training and inference speed shown in <ref type="table" target="#tab_3">Table 3</ref>. A significant drop of training time and inference speed is shown by our proposed architecture. Further by using MobileNet architecture as base CNN instead of VGG16, we even brought down the inference speed to only 1ms by trading off the accuracy to 89.60%. The training is done on a 24GB NVIDIA Tesla K80 GPU on Google Cloud ML Engine. The inference speed is tested on a 11GB GeForce GTX 1080 Ti GPU. <ref type="figure">Figure 6</ref>: Trend of training triplet accuracy for different models. <ref type="figure">Figure 7</ref>: Trend of test triplet accuracy for different models. We evaluated our models on test sets which were split apart from the complete dataset in the beginning. The test sets contain the same number of categories as that in the training set and also the class distribution was similar to that of the training data. The similar distribution also ensures that a generalized performance of the model is being measured.</p><p>Finally, to ascertain the efficacy of skip connections, we did 4 more experiments where we started with no skip connection and gradually added aggregated embeddings from each intermediate layer as mentioned in the architecture of MILD-Net (see <ref type="figure" target="#fig_3">Figure 5</ref>. <ref type="table" target="#tab_4">Table 4</ref> shows the performance of these experiments. The results show a trend of increasing learning ability and inference accuracy with the contribution from each intermediate layer. For our training data we found that the second last layer "block4 pool" contributed the most. But as been studied here <ref type="bibr" target="#b4">[5]</ref>, the layer with the most contribution varies on the scale of input as well as the task. So to be comprehensive we considered all the 4 intermediate layers in our proposed solution.</p><p>We also visualized the embeddings space obtained from our top performing model MILDNet with contrastive loss function. The 2048-d embeddings are projected to 2D using t-SNE <ref type="bibr" target="#b34">[36]</ref>, which is a distributed stochastic neighbor embedding algorithm. The visualization showed that by training on Street2shop training data the model learned to recognize pat-   terns and shapes present in the image. However, it failed to ignore backgrounds in the images which could be because query images were wild images while others were catalog images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Production Details</head><p>As discussed earlier in dataset creation for production (Section 3.2), we used both wild images triplets from street-toshop dataset, and in-house automatically curated catalog image triplets from results of a unique and intuitive visual similarity model. Here we will first demonstrate our initial model which gave us enough catalog triplet data to train our final MILDNet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Configurable and Engineered Visual Recommendation Model</head><p>We identified three visual attributes which captures our notion of visual similarity:</p><p>• Structure: the shape of the product</p><p>• Pattern: the pattern/texture on the material of the product</p><p>• Color: the primary color of the product To capture these details from an image, we picked 3 deep classification models from our in-house models repository. Since, the second last layer of trained models contains automatically learned complex features necessary for the underlying classification task, we chose the following ways to extract hints of these features:  • Color: LAB color space is used rather than RGB as they are closer resemblance to human perception. Histograms on LAB color space are stacked together to get color feature vector.</p><p>Using 2048 features representing structure, 1024 features representing pattern and 540 features from color histograms, we created kNN models using Annoy library. Finally, we created a pipeline (see <ref type="figure" target="#fig_7">Figure 10</ref>) which we got after fine-tuning to get the results we desire. This was our initial solution that we made live in a week span. The results from this solution is further used as training data after our experiments on MILDNet came positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Infra Details</head><p>The Fynd catalog comprises thousands of products in around 500 categories, and each day around 10k insertions can happen. For this we decided to create a batch processing of data to find our visual similars of all products in the data once every day. Our system is roughly expected to first fetch each product, infer final feature vectors from either MILDNet or our initial model, then do a k nearest neighbour search to find top 10 visual similar products in the dataset. Our production setup (see <ref type="figure">Figure 11</ref>) takes roughly around 15 mins on let's say 6 worker clusters every day. It consists of following critical components:</p><p>• Databases: We use MongoDb database hosted on Amazon ec2 instances to store the product details (input db) and results (output db).</p><p>• Batching and Pruning search space: Since the database is huge and the results are only expected to be in-class, we decided to break it in batches. To cleverly create batches, we partitioned the entire dataset by two meta-data filed of the product, namely gender (men/women/girl/boy) and category key (last level of category among around 400 categories). This hugely eased the complexity and processing needed by the system by reducing the search space.</p><p>• Cluster: To process the data in batches we used Google Cloud Dataproc clusters which works on Apache Spark framework in a distributed manner. The cluster consists of 1 high memory master, while number of workers are decided on run time based on the number of partitions to process. <ref type="figure">Figure 11</ref>: The production flow of visual similarity pipeline at Fynd. Items from database are partitioned on gen-der+category keys and processed in batches on Spark cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Optimizations</head><p>Following are some of the optimization which are made possible by processing the data in partitions and also due to the high learning ability of MILDNet networks:</p><p>• Only those partitions are processed which has a new item added than the last run.</p><p>• Only those existing items are processed which are one among the nearest neighbour of new items.</p><p>• Since the costliest process is getting features for new product images, the features once inferred are stored in a database for future requirements.</p><p>• For mobile deployment, a variant of MILDNet where MobileNet with features from 5 intermediate layers is used instead of VGG16. We observed this to reduce the accuracy by around 5.4%.</p><p>• For real-time faster retrieval, we used a variant of MILD-Net with only 512-d embedding instead of 1024-d embeddings by changing the number of features in last dense layer. We observed this to reduce the accuracy by only 2.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Summary</head><p>We have presented here a fresh take of using a single model with skip connections instead of using 3 CNN models to capture the notion of visual similarity. Experimenting on the famous Street-to-shop dataset we achieved equivalent accuracy and recall while reducing the model size and inference latency by 3 times. We also observed the effect of sequentially adding each skip connection. Different variants are also made with easy indexing using only 512-d final embedding and MobileNet variant with further 3 times lesser size <ref type="bibr">(20 MBs)</ref>. Since latency of such a system in live settings plays a key role, this could really boost the performance of such a system. Further we introduced a way to automatically create in-house tailored visual similarity catalog triplet data which is hard to create manually. Lastly, we demonstrated our entire production pipeline which caters batch processing entire ecommerce catalog dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Base Flow for triplets training. Each of the query, positive and negative images are passed through the CNN architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Samples of triplets from Street2Shop data. Each row contains query, similar/positive and dissimilar/negative images. Query image is mostly a wild image, while positive and negative are catalog images. Last row is an example of out of class triplet pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Multi-scale network architecture consists of 3 independent CNNs: 1. Convnet(AlexNet/VGG16/VGG19) pretrained on ImageNet dataset 2. Shallow Network 3. Shallow network. The convnet captures the image semantics while shallow networks focus on the visual appearance. Embeddings from them are passed to a fully connected layer to get 4096-d embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>MILDNet architecture only uses the Convnet(VGG16/MobileNet) pretrained on ImageNet dataset. Embeddings from multiple intermediate layers are aggregated on concatenate to get 1472 embeddings. This are passed through FC -dropout -FC layers to finally get 2048-d embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>horizontal flip: True, flips the image horizontally • vertical flip: True, flips the image vertically • zoom range: 0.2, zoom the image by scale of ±0.2 • shear range: 0.2, shear the image by a factor of ±0.2 • rotation range: 30, rotate the image by an angle of ±30 • • fill mode: nearest, fills extra pixels after the distortion on the basis of nearest points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Image Retrieval Samples of MILDNet. Note that since we only trained on tops category of Street2shop dataset, the second row doesn't contain only bag. But the model successfully shows understanding of both fine and coarse grained visual details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>t-SNE Visualisation of Street2Shop test data (only catalog images) made using features extracted from MILD-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Engineered Visual Recommendation Model. Features are extracted from pretrained 1. category classification model 2. pattern classification model 3. LAB colorspace histogram. Impact of these features on results are tuned to capture our notion of visual similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell cols="3">Model Architecture Comparison</cell></row><row><cell>Model</cell><cell cols="2">Size (MBs) Total Params (M)</cell></row><row><cell>Ranknet[3]</cell><cell>260</cell><cell>68.31</cell></row><row><cell>Visnet[2]</cell><cell>253</cell><cell>66.54</cell></row><row><cell>Multi-scale-Alexnet[1]</cell><cell>240</cell><cell>63.00</cell></row><row><cell>MILDNet(VGG16)</cell><cell>83</cell><cell>21.93</cell></row><row><cell>MILDNet(MobileNet)</cell><cell>20</cell><cell>5.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Model Accuracy Results</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Loss Type Max Training Accuracy (%) Max Test Accuracy (%)</cell></row><row><cell>Ranknet</cell><cell>Contrastive</cell><cell>97.89</cell><cell>94.98</cell></row><row><cell>MILDNet-Contrastive</cell><cell>Contrastive</cell><cell>95.69</cell><cell>93.69</cell></row><row><cell>Visnet-LRN2D</cell><cell>Hinge</cell><cell>92.87</cell><cell>93.39</cell></row><row><cell>MILDNet</cell><cell>Hinge</cell><cell>92.94</cell><cell>92.50</cell></row><row><cell>Visnet</cell><cell>Hinge</cell><cell>92.97</cell><cell>92.42</cell></row><row><cell>MILDNet-512-No-Dropout</cell><cell>Hinge</cell><cell>92.8</cell><cell>91.15</cell></row><row><cell>Multiscale-Alexnet</cell><cell>Hinge</cell><cell>87.72</cell><cell>90.80</cell></row><row><cell>MILDNet-MobileNet</cell><cell>Hinge</cell><cell>89.90</cell><cell>89.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Model Performance Results</figDesc><table><row><cell>Model</cell><cell>Loss Type</cell><cell>Avg. Training Speed (mins/epoch)</cell><cell>Avg. Inference Speed (ms)</cell></row><row><cell>Ranknet</cell><cell>Contrastive</cell><cell>212</cell><cell>3.6</cell></row><row><cell>MILDNet-Contrastive</cell><cell>Contrastive</cell><cell>85</cell><cell>2.6</cell></row><row><cell>Visnet-LRN2D</cell><cell>Hinge</cell><cell>197</cell><cell>3.2</cell></row><row><cell>MILDNet</cell><cell>Hinge</cell><cell>88</cell><cell>2.6</cell></row><row><cell>Visnet</cell><cell>Hinge</cell><cell>243</cell><cell>3.2</cell></row><row><cell>MILDNet-512-No-Dropout</cell><cell>Hinge</cell><cell>91</cell><cell>2.6</cell></row><row><cell>Multiscale-Alexnet</cell><cell>Hinge</cell><cell>165</cell><cell>3.0</cell></row><row><cell>MILDNet-MobileNet</cell><cell>Hinge</cell><cell>70</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">MILDNet Ablation Study</cell><cell></cell><cell></cell></row><row><cell>Skip Layers</cell><cell>Total Params(M)</cell><cell>Max Training Accuracy(%)</cell><cell>Max Test Accuracy(%)</cell></row><row><cell>No skip</cell><cell>19.96</cell><cell>70.35</cell><cell>71.15</cell></row><row><cell>block4 pool</cell><cell>21.01</cell><cell>92.45</cell><cell>91.05</cell></row><row><cell>block3 pool, block4 pool</cell><cell>21.53</cell><cell>92.05</cell><cell>91.01</cell></row><row><cell>block2 pool, block3 pool, block4 pool</cell><cell>21.80</cell><cell>92.12</cell><cell>91.18</cell></row><row><cell>block2 pool, block3 pool, block4 pool, block5 pool</cell><cell>21.93</cell><cell>92.94</cell><cell>92.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>•</head><label></label><figDesc>Structure: A product category classification model (42 categories like shirts, tshirts, sneakers, chinos etc.) retrained on InceptionV3 architecture. Feature vector from second last layer extracted containing 2048 features. • Pattern: A pattern classification model (7 pattern classes like solid, checked, melange, etc.) retrained on Incep-tionV3 architecture. Feature vector from the third last layer extracted containing 1024 features.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Fine-grained Image Similarity with Deep Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. 13861393</title>
		<meeting>CVPR. 13861393</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devashish</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Narumanchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H A</forename><surname>Ananya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod</forename><surname>Kompalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnendu</forename><surname>Chaudhury</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Retrieving Similar E-Commerce Images Using Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishab</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudha</forename><surname>Vishvakarma</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Where to Buy It: Matching Street Clothing Photos in Online Shops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xufeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploiting local features from deep networks for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename></persName>
		</author>
		<idno type="DOI">53-61.10.1109/CVPRW.2015.7301272</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ECCV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning for content-based image retrieval: a comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia<address><addrLine>Orlando, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">157166</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting local features from deep networks for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition Workshops<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-level image representation for large-scale image-based instance retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAAI Transactions on Intelligence Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3339</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aggregating deep convolutional features for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gradients for Human Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">886893</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11501157</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">11091135</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image retrieval and classification using local distance functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">14701477</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">17041716</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">512519</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Largescale image retrieval with compressed fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">33843391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Triangulation embedding and democratic aggregation for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">33103317</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision -ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">584599</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">392407</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">From generic to specific deep representations for visual recognition. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5774</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Visual instance retrieval with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<idno>abs/1412.6574</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting local features from deep networks for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition Workshops<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep convolutional features for image based retrieval and scene categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06033</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simultaneous feature learning and hash coding with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">32703278</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cvpr</title>
		<imprint>
			<publisher>Ieee Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-domain image retrieval with a dual attribute-aware ranking network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision</title>
		<meeting>IEEE Int. Conf. Computer Vision<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">10621070</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FaceNet: a unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">815823</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bernhardsson</surname></persName>
		</author>
		<ptr target="https://github.com/spotify/annoyaccessed" />
	</analytic>
	<monogr>
		<title level="j">Annoy library on Github</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/kerasaccessed" />
	</analytic>
	<monogr>
		<title level="j">Keras library on Github</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Tensorflow: Largescale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Laurens van der, and Geoffrey Hinton Visualizing data using tSNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Artoul</surname></persName>
		</author>
		<ptr target="https://github.com/hyperdashio/hyperdash-sdk-pyaccessed" />
		<title level="m">Hyperdash library on Github</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
