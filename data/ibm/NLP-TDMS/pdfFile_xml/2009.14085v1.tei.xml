<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Localize to Classify and Classify to Localize: Mutual Guidance in Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Avignon</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">ATERMES company</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Bretagne Sud</orgName>
								<orgName type="institution" key="instit2">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">IUF</orgName>
								<address>
									<region>Inria</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Localize to Classify and Classify to Localize: Mutual Guidance in Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3[0000−0001−6093−1729]</term>
					<term>Elisa FROMONT 1</term>
					<term>4[0000−0003−0133−3491]</term>
					<term>Sbastien LEFEVRE 2[0000−0002−2384−8202]</term>
					<term>and</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most deep learning object detectors are based on the anchor mechanism and resort to the Intersection over Union (IoU) between predefined anchor boxes and ground truth boxes to evaluate the matching quality between anchors and objects. In this paper, we question this use of IoU and propose a new anchor matching criterion guided, during the training phase, by the optimization of both the localization and the classification tasks: the predictions related to one task are used to dynamically assign sample anchors and improve the model on the other task, and vice versa. Despite the simplicity of the proposed method, our experiments with different state-of-the-art deep learning architectures on PASCAL VOC and MS COCO datasets demonstrate the effectiveness and generality of our Mutual Guidance strategy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Supervised object detection is a popular task in computer vision that aims at localizing objects through bounding boxes and assigning each of them to a predefined class. Deep learning-based methods largely dominate this research field and most recent methods are based on the anchor mechanism <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Anchors are predefined reference boxes of different sizes and aspect ratios uniformly stacked over the whole image. They help the network to handle object scale and shape variations by converting the object detection problem into an anchor-wise bounding box regression and classification problem. Most stateof-the-art anchor-based object detectors resort to the Intersection over Union (IoU) between the predefined anchor boxes and the ground truth boxes (called IoU anchor in the following) to assign the sample anchors to an object (positive anchors) or a background (negative anchors) category. These assigned anchors are then used to minimize the bounding box regression and classification losses during training.</p><p>This IoU anchor -based anchor matching criterion is reasonable under the assumption that anchor boxes with high IoU anchor are appropriate for localization and classification. However, in reality, the IoU anchor is insensitive to objects' content/context, thus not "optimal" to be used, as such, for anchor matching. In <ref type="figure" target="#fig_0">Figure 1</ref>, we show several examples where IoU anchor does not well reflect the matching quality between anchors and objects: anchors A and anchors B have exactly the same IoU anchor but possess very different matching qualities. For example, on the first line of <ref type="figure" target="#fig_0">Figure 1</ref>, anchors A covers a more representative and informative part of the object than anchors B; On the second line, anchors B contains parts of a nearby object which hinders the prediction on the jockey/left person.</p><p>Deep learning-based object detection involves two sub-tasks: instance localization and classification. Predictions for these two tasks tell us "where" and "what" objects are on the image respectively. During the training phase, both tasks are jointly optimized by gradient descent, but the static anchor matching strategy does not explicitly benefit from the joint resolution of the two tasks, which may then yield to a task-misalignment problem, i.e., during the evaluation phase, the model might generate predictions with correct classification but imprecisely localized bounding boxes as well as predictions with precise localization but wrong classification. Both predictions significantly reduce the overall detection quality.</p><p>To address these two limitations of the existing IoU anchor -based strategy, we propose a new, adaptive anchor matching criterion guided by the localization and by the classification tasks mutually, i.e., resorting to the bounding box regression prediction, we dynamically assign training anchor samples for optimizing classification and vice versa. In particular, we constrain anchors that are well-localized to also be well-classified (Localize to Classify), and those well-classified to also be well-localized (Classify to Localize). These strategies lead to a content/contextsensitive anchor matching and avoid the task-misalignment problem. Despite the simplicity of the proposed strategy, Mutual Guidance brings consistent Average Precision (AP) gains over the traditional static strategy with different deep learn-ing architectures on PASCAL VOC <ref type="bibr" target="#b12">[13]</ref> and MS COCO <ref type="bibr" target="#b13">[14]</ref> datasets, especially on strict metrics such as AP75. Our method is expected to be more efficient on applications that require a precise instance localization, e.g., autonomous driving, robotics, outdoor video surveillance, etc.</p><p>The rest of this paper is organized as follows: in Section 2, we discuss some representative related work in object detection. Section 3 provides implementation details of the proposed Mutual Guidance. Section 4 compares our dynamic anchor matching criterion to the traditional static criterion with different deep learning architectures on different public object detection datasets, and discusses reasons for the precision improvements. Section 5 brings concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Modern CNN-based object detection methods can be divided into two major categories: two-stage detectors and single-stage ones. Both categories give similar performance with a small edge in accuracy for the former and in efficiency for the latter. Besides, both categories of detectors are massively based on the anchor mechanism which usually resorts to IoU anchor for evaluating the matching quality between anchors and objects when assigning training labels and computing the bounding box regression and classification losses for a training example. Our method aims to improve this anchor matching criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anchor-based object detection</head><p>Two-stage object detectors. Faster RCNN <ref type="bibr" target="#b0">[1]</ref> defines the generic paradigm for two-stage object detectors: it first generates a sparse set of Regions of Interest (RoIs) with a Region Proposal Network (RPN), then classifies these regions and refines their bounding boxes. The RoIs are generated by the anchor mechanism. Multiple improvements have been proposed based on this framework: R-FCN <ref type="bibr" target="#b1">[2]</ref> suggests position-sensitive score maps to share almost all computations on the entire image; FPN <ref type="bibr" target="#b2">[3]</ref> uses a top-down architecture and lateral connections to build high-level semantic feature maps at all scales; PANet <ref type="bibr" target="#b3">[4]</ref> enhances the multi-scale feature fusion by adding bottom-up path augmentation to introduce accurate localization signals in lower layers; Libra RCNN <ref type="bibr" target="#b4">[5]</ref> proposes the Balanced Feature Pyramid to further integrate multi-scale information into FPN; TridentNet <ref type="bibr" target="#b15">[15]</ref> constructs a parallel multi-branch architecture and adopts a scale-aware training scheme for training object scale specialized detection branches. Cascade RCNN <ref type="bibr" target="#b5">[6]</ref> further extends the two-stage paradigm into a multi-stage paradigm, where a sequence of detectors are trained stage by stage.</p><p>Single-stage object detectors. SSD <ref type="bibr" target="#b6">[7]</ref> and YOLO <ref type="bibr" target="#b16">[16]</ref> are the fundamental methods for single-stage object detection. From this basis, many other works have been proposed: FSSD <ref type="bibr" target="#b9">[10]</ref> aggregates contextual information into the detector by concatenating features of different scales; RetinaNet <ref type="bibr" target="#b8">[9]</ref> proposes the Focal Loss to tackle the imbalanced classification problem that arises when trying to  separate the actual object to detect from the massive background; RFBNet <ref type="bibr" target="#b10">[11]</ref> proposes Receptive Field Block, which takes the relationship between the size and the eccentricity of the reception fields into account; RefineDet <ref type="bibr" target="#b17">[17]</ref> introduces an additional stage of refinement for anchor boxes; M2Det <ref type="bibr" target="#b11">[12]</ref> stacks multiple thinned U-shape modules to tackle the so-called appearance-complexity variations. While these methods introduce novel architectures to improve results for the object detection task, they all rely on the standard IoU anchor -based matching. We identify this component as a possible limitation and propose a novel matching criterion, that could be adapted to any existing deep architecture for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Anchor-free object detection</head><p>The idea of anchor-free object detection consists in detecting objects not from predefined anchors boxes, but directly from particular key-points <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21]</ref> or object centres <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref>. However, these methods do not lead to a substantial accuracy advantage compared to anchor-based methods. The main idea of our Mutual Guidance could also be applied to this class of object detectors, and the experimental results with anchor-free detectors are included in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>As already sketched in the introduction, in order to train an anchor-based object detector, the predefined anchors should be assigned as positive ("it is a true object") or negative ("it is a part of the background") according to an evaluation of the matching between the anchors and the ground truth objects. Then, the bounding box regression loss is optimized according to the positive anchors, and the instance classification loss is optimized according to the positive as well as the negative anchors. When training an anchor-based single-stage object detector with a static anchor matching strategy, the IoU between predefined anchor boxes and ground truth boxes (IoU anchor ) is the usual matching criterion. As shown in the IoU anchor column of <ref type="figure" target="#fig_1">Figure 2</ref>, anchors with more than 50% of IoU anchor are labelled as "positive", those with less than 40% of IoU anchor are labelled as "negative", the rest are "ignored anchors". Note that at least one anchor should be assigned as positive, hence if there is no anchor with more than 50% of IoU anchor , the anchor with the highest IoU anchor is considered.</p><p>The proposed Mutual Guidance consists of two components: Localize to Classify and Classify to Localize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Localize to Classify</head><p>If an anchor is capable to precisely localize an object, this anchor must cover a good part of the semantically important area of this object and thus could be considered as an appropriate positive sample for classification. Drawing on this, we propose to leverage the IoU between regressed bounding boxes (i.e., the network's localization predictions) and ground truth boxes (noted IoU regressed ) to better assign the anchor labels for classification. Inspired by the usual IoU anchor , we compare IoU regressed to some given thresholds (discussed in the next paragraph) and then define anchors with IoU regressed greater than a high threshold as positive samples, and those with IoU regressed lower than a low threshold as negative samples (see IoU regressed column of <ref type="figure" target="#fig_1">Figure 2)</ref>.</p><p>We now discuss a dynamic solution to set the thresholds. A fixed threshold (e.g., 50% or 40%) does not seem optimal since the network's localization ability gradually improves during the training procedure and so does the IoU regressed for each anchor, leading to the assignment of more and more positive anchors which destabilizes the training. To address this issue, we propose a dynamic thresholding strategy. Even though the IoU anchor is not the best choice to accurately indicate the matching quality between anchors and objects, the number of assigned positive and ignored anchors does reflect the global matching conditions (brought by the size and the aspect ratio of the objects to detect), thus these numbers could be considered as reference values for our dynamic criterion. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, while applying the IoU anchor -based anchor matching strategy with the thresholds being 50% and 40%, the number of positive anchors (N p ) and ignored anchors (N i ) are noted (N p = 6 and N i = 3 for the boat). We then use these numbers to label the N p highest IoU regressed anchors as positive, and the following N i anchors as ignored. More formally, we exploit the N p -th largest IoU regressed as our positive anchor threshold, and the (N p +N i )-th largest IoU regressed as our ignored anchor threshold. Using this, our Localize to Classify anchor matching strategy evolves with the network's localization capacity and maintains a consistent number of anchor samples assigned to both categories (positive/negative) during the whole training procedure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classify to localize</head><p>As with the Localize to Classify process, the positive anchor samples in Classify to Localize are assigned according to the network's classification predictions (noted Classif score). Specifically, Classif score is the predicted classification score for the object category, e.g., the Classif score of <ref type="figure" target="#fig_1">Figure 2</ref> indicates the classification score for the boat category.</p><p>Nevertheless, this Classif score is not effective enough to be used directly for assigning good positive anchors for the bounding box regression optimization. It is especially true at the beginning of the training process, when the network's weights are almost random values and all predicted classification scores are close to zero. The IoU regressed is optimized on the basis of the IoU anchor , therefore we have IoU regressed ≥ IoU anchor in most cases (even at the beginning of the training), and this property helps to avoid such cold start problem and ensures training stability. Symmetrically to the Localize to Classify strategy, we now propose a Classify to Localize strategy based on an IoU amplif ied defined as:</p><formula xml:id="formula_0">IoU amplif ied = (IoU anchor ) σ−p σ<label>(1)</label></formula><p>where σ is a hyper-parameter aiming at adjusting the degree of amplification, p represents the mentioned Classif score. Inspired by the focal loss <ref type="bibr" target="#b8">[9]</ref>, we chose eq. 1 as the simplest one able to amplify the IoU of anchors according to the correct classification predictions p. Its behavior is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>The IoU amplif ied is always higher than the IoU anchor , and the amplification is proportional to the predicted Classif score. In particular, the amplification is stronger for smaller σ (note that σ should be larger than 1), and disappears when σ becomes large. Similarly to the Localize to Classify strategy, we apply a dynamic thresholding strategy to keep the number of assigned positive samples for the localization task and for the classification task consistent, e.g., we assign in <ref type="figure" target="#fig_1">Figure 2</ref>, the top 6 anchors with the highest IoU amplif ied as positive samples. Note that there is no need for selecting ignored or negative anchors for the localization task since the background does not have an associated ground truth box.</p><p>As discussed in Section 1, IoU anchor is not sensitive to the content or the context of an object. Our proposed Localize to Classify and Classify to Localize, however, attempt to adaptively label the anchor samples according to their visual content and context information. Considering anchor F and anchor H in <ref type="figure" target="#fig_1">Figure 2</ref>, one can tell that anchor H is better than anchor F for recognizing this boat, even with a smaller IoU anchor . Using both our strategies, anchor H has been promoted to positive thanks to its excellent prediction quality on both tasks whereas anchor F has been labelled as negative even though it has a large IoU anchor .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">About the task-misalignment problem</head><p>Since Localize to Classify and Classify to Localize are independent strategies, they could possibly assign contradictory positive/negative labels (e.g, the anchor C in <ref type="figure" target="#fig_1">Figure 2</ref> is labelled negative for the classification task but positive for the bounding box regression task). This happens when one anchor entails a good prediction on one task and a poor prediction on the other (i.e. they are misaligned predictions). Dealing with such contradictory labels, as we do with Mutual Guidance, does not harm the training process. On the contrary, our method tackles the task-misalignment problem since the labels for one task are assigned according to the prediction quality on the other task, and vice versa. This mechanism forces the network to generate aligned predictions: if the classification prediction from one anchor is good while its localization prediction is bad, the Mutual Guidance will give a positive label on the localization task to this anchor, to constrain it to be better at localizing as well while giving a negative label (i.e. background) on the classification task to avoid misaligned predictions. In fact, the predicted classification score of this mislocalized anchor should be low enough for the anchor to be suppressed by the NMS procedure in the inference phase. The same reasoning holds for a good localization prediction with a bad classification one.</p><p>On the contrary, if a network always assigns similar positive/negative labels (as done in standard IoU anchor -based methods) to both tasks during training, one cannot guarantee that there will be no misalignment of the localization and the classification predictions at inference time. Keeping anchors (after NMS) with misaligned predictions is harmful for strict evaluation metrics such as AP75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Network architecture and parameters. In order to test the generalization performance of the proposed method, we implement our method on the single-stage object detectors FSSD <ref type="bibr" target="#b9">[10]</ref>, RetinaNet <ref type="bibr" target="#b8">[9]</ref> and RFBNet <ref type="bibr" target="#b10">[11]</ref> using both ResNet-18 <ref type="bibr" target="#b26">[26]</ref> or VGG-16 <ref type="bibr" target="#b27">[27]</ref> as backbone networks in our experiments. Note that RFBNet is not implemented with ResNet-18 as backbone since the two architectures are not compatible. The backbone networks are pre-trained on ImageNet-1k classification dataset <ref type="bibr" target="#b28">[28]</ref>. We adopt the Focal Loss <ref type="bibr" target="#b8">[9]</ref> and Balanced L1 Loss <ref type="bibr" target="#b4">[5]</ref> as our instance classification and bounding box regression loss functions respectively for all experiments. The input image resolution is fixed to 320 × 320 pixels for all experiments (single scale training and evaluation). Unless specified, all other implementation details are the same as in <ref type="bibr" target="#b10">[11]</ref>. Following the results of <ref type="figure" target="#fig_2">Figure 3</ref>, we decided to fix our only new hyper-parameter σ to 2 for all experiments. σ is used to set the degree of amplification when computing IoU amplif ied in Eq. <ref type="bibr" target="#b0">(1)</ref>. It needs to be greater than 1 for the exponent to be positive and lower than 3 since this does not bring any amplification as shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Datasets and evaluation metrics. Extensive experiments are performed on two benchmark datasets: PASCAL VOC <ref type="bibr" target="#b12">[13]</ref> and MS COCO <ref type="bibr" target="#b13">[14]</ref>. PASCAL VOC dataset has 20 object categories. Similarly to previous works, we utilize the combination of VOC2007 and VOC2012 trainval sets for training, and rely on the VOC2007 test for evaluation. MS COCO dataset contains 80 classes. Our experiments on this dataset are conducted on the train2017 and val2017 set for training and evaluation respectively. For all datasets, we use the evaluation metrics introduced in the MS COCO benchmark: the Average Precision (AP) averaged over 10 IoU thresholds from 0.5 to 0.95, but also AP50, AP75, AP s , AP m , AP l . AP50 and AP75 measure the average precision for a given IoU threshold (50% and 75%, respectively). The last three aim at focusing on small (area &lt; 32 2 ), medium (32 2 &lt; area &lt; 96 2 ) and large (area &gt; 96 2 ) objects respectively. Since the size of the objects greatly varies between MS COCO and PASCAL VOC, these size-dependent measures are ignored when experimenting with PASCAL VOC dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Experiments on PASCAL VOC. We evaluate the effectiveness of both components (Localize to Classify and Classify to Localize) of our proposed approach w.r.t. the usual IoU anchor -based matching strategy when applied on the same deep learning architectures. The results obtained on the PASCAL VOC dataset are given in <ref type="table" target="#tab_1">Table 1</ref>. Both proposed anchor matching strategies consistently boost the performance of the "vanilla" networks and their combination (Mutual Guidance) leads to the best AP and all other evaluation metrics.</p><p>In particular, we observe that the improvements are small on AP50 (around 0.5%) but significant on AP75 (around 3%), which means that we obtain more precise detections. As analysed in Section 3.3, this comes from the task-misalignment problem faced with the usual static anchor matching methods. This issue leads to retain well-classified but poorly-localized predictions and suppress well-localized but poorly-classified predictions, which in turns results in a significant drop of the AP score at strict IoU thresholds, e.g., AP75. In Mutual Guidance, however, training labels for one task are dynamically assigned according to the prediction quality on the other task and vice versa. This connection makes the classification and localization tasks consistent along all training phases and as such avoids this task-misalignment problem. We also notice that Localize to Classify alone brings, for all five architectures, a higher improvement than Classify to Localize alone. We hypothesize two possible reasons for this: 1) most object detection errors come from wrong classification instead of imprecise localization, so the classification task is more difficult than the localization task and thus, there is more room for the improvement on this task; 2) the amplification proposed in Eq. (1) may not be the most appropriate one to take advantage of the classification task for optimizing the bounding box regression task.</p><p>Experiments on MS COCO. We then conduct experiments on the more difficult MS COCO <ref type="bibr" target="#b13">[14]</ref> dataset and report our results in <ref type="table">Table 2</ref>. Note that according to the scale range defined by MS COCO, APs of small, medium and large objects are listed. In this dataset also, our Mutual Guidance strategy consistently brings some performance gains compared to the IoU anchor -based baselines. We notice that our AP gains on large objects is significant (around 2%). This is because larger objects generally have more matched positive anchors, which offers more room for improvements to our method. Since the Mutual guidance strategy only involves the training phase, and since there is no difference between IoU anchor -  <ref type="table">Table 2</ref>. AP performance of different architectures for object detection on MS COCO dataset using 2 different anchor matching strategies: the usual IoU anchor -based one and our complete approach marked as Mutual Guidance. The best score for each architecture is in bold.</p><p>based and our method during the evaluation phase, these improvements can be considered cost-free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative analysis</head><p>Label assignment visualization. Here, we would like to explore the reasons for the performance improvements by visualizing the difference in the label assignment between the IoU anchor -based strategy and the Mutual Guidance strategy during training. Some examples are shown in <ref type="figure">Figure 4</ref>. White dotted-line boxes represent ground truth boxes; Red anchor boxes are assigned as positive by IoU anchor -based strategy, while considered as negative or ignored by Localize to Classify (the top two lines in <ref type="figure">Figure 4</ref>) or Classify to Localize (the bottom two lines in <ref type="figure">Figure 4</ref>); Green anchor boxes are assigned as positive by Localize to Classify but negative or ignored by IoU anchor -based; Yellow anchor boxes are assigned as positive by Classify to Localize but negative or ignored by IoU anchorbased. From these examples, we can conclude that the IoU anchor -based strategy only assigns the "positive" label to anchors with sufficient IoU with the ground truth box, regardless of their content/context, whereas our proposed Localize to Classify and Localize to Classify strategies dynamically assign "positive" labels to anchors covering semantic discriminant parts of the object (e.g., upper body of a person, main body of animals), and assign "negative" labels to anchors with complex background, occluded parts, or anchors containing nearby objects. We believe that our proposed instance-adaptive strategies make the label assignment more reasonable, which is the main reason for performance increase.</p><p>Detection results visualization. <ref type="figure" target="#fig_3">Figure 5</ref> illustrates on a few images from the PASCAL VOC dataset the different behaviours shown by our Mutual Guidance method and the baseline anchor matching strategy. As analysed in Section 3.3, <ref type="figure">Fig. 4</ref>. Visualization of the difference in the label assignment during training phase (images are resized to 320 × 320 pixels). Red, yellow and green anchor boxes are positive anchors assigned by IoU anchor -based, Localize to Classify and Classify to Localize respectively. Zoom in to see details.</p><p>we can find misaligned predictions (good at classification but poor at localization) from IoU anchor -based anchor matching strategy. As shown in the figure, our method gives better results when different objects are close to each other in the image, e.g. "man riding a horse" or "man riding a bike". With the usual IoU anchor -based anchor matching strategy, the instance localization and classification tasks are optimized independently of each other. Hence, it is possible that, during the evaluation phase, the classification prediction relies on one object whereas the bounding box regression targets the other object. However, such a problem is rarer with the Mutual Guidance strategy. Apparently, our anchor matching strategies introduce interactions between both tasks and makes the predictions of localization and classification aligned, which substantially eliminated such false positive predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we question the use of the IoU between predefined anchor boxes and ground truth boxes as a good criterion for anchor matching in object detection and study the interdependence of the two sub-tasks (i.e. localization and classification) involved in the detection process. We propose a Mutual Guidance mechanism, which provides an adaptive matching between anchors and objects by assigning anchor labels for one task according to the prediction quality on the other task and vice versa. We assess our method on different architectures and different public datasets and compare it with the traditional static anchor matching strategy. Reported results show the effectiveness and generality of this Mutual Guidance mechanism in object detection.</p><p>Abstract. The principle of Mutual Guidance in object detection is to assign labels of one task according to the prediction on the other task, and vice versa. Apparently, it is not limited to anchor-based methods, but applicable for any object detector that performs localization and classification tasks. Here we introduce the application of Mutual Guidance in anchor-free methods. Experiments conducted on PASCAL VOC dataset demonstrate the consistent precision improvements brought by our method on this category of detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Summary of Mutual Guidance</head><p>In order to realize a content/context-sensitive label assignment and avoid the task-misalignment problem, we propose the Mutual Guidance mechanism in object detection, which can be summarised as follows:</p><p>-When assigning labels for the classification task:</p><p>• If the prediction of localization is good, the label is assigned as positive (i.e., "object") to encourage good predictions on both tasks; • If the prediction of localization is poor, the label is assigned as negative (i.e., "background") to avoid misaligned false positive detection; -When assigning labels for the localization task:</p><p>• If the prediction of classification is good, the label is assigned as positive (i.e., we optimize this sample) to encourage good predictions on both tasks; • If the prediction of classification is poor, the label is assigned as negative (i.e., we do not optimize this sample) to avoid unnecessary optimizations.  When applying Mutual Guidance to FCOS, identical to the implementation in anchor-based methods, two strategies are applied: Localize to Classify and Classify to Localize. Moreover, the same dynamic thresholding strategy is applied. Specifically, for Localize to Classify, we firstly note the number of positive samples assigned by the original strategy (N p ), then label the N p highest IoU regressed points as positive; for Classify to Localize, we amplify each point's centerness score according to its classification prediction, and label the N p highest amplified centerness score points as positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Applying Mutual Guidance to FCOS</head><p>Experiments are performed on the PASCAL VOC dataset. ResNet-18 <ref type="bibr" target="#b26">[26]</ref> and VGG-16 <ref type="bibr" target="#b27">[27]</ref> are adopted as backbone networks in our experiments. Unless specified, all other implementation details are the same as in our paper. Experimental results are listed in <ref type="table" target="#tab_1">Table 1</ref>. Same as with anchor-based methods, our Mutual Guidance strategy significantly boosts the detection precision for FCOS, especially on the AP75 metric.</p><p>We then conduct qualitative analysis on the label assignment difference and detection result difference between the original FCOS strategy and Mutual Guidance. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the original strategy (red points) assigns positive to points in the central region of the object box regardless of their content/context, whereas the Localize to Classify (yellow points) and Classify to Localize (green points) strategies adaptively assign positive to points with representative semantic information, and assign negative to points on background or nearby objects. Since the labels assigned by the original FCOS strategy are always the same for localization and classification tasks, the task-misalignment problem exists in FCOS as well. Several misaligned false positive detections are observed in <ref type="figure" target="#fig_1">Figure  2</ref>, however, the proposed Mutual Guidance provides more accurate detection. <ref type="figure" target="#fig_0">Fig. 1</ref>. Visualization of the difference in the label assignment during training phase (images are resized to 320×320 pixels). The ground truth object in each image is marked by white dotted-line box. Red, yellow and green points are positive samples assigned by original FCOS strategy, Localize to Classify and Classify to Localize respectively. Zoom in to see details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual guidance</head><p>Original strategy Mutual guidance <ref type="figure" target="#fig_1">Fig. 2</ref>. Examples of detection results using the original FCOS label assignment strategy (odd lines) and our proposed Mutual Guidance one (even lines). The results are given for all images after applying a Non-Maximum Suppression process with a IoU threshold of 50%. Zoom in to see details.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>arXiv:2009.14085v1 [cs.CV] 29 Sep 2020 t Anchors A and anchors B have the same IoU with ground truth box but different visual semantic information. The ground truth in each image is marked as dotted-line box. Better viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of different anchor matching strategies for the boat image resorting to IoU anchor (static), IoU regressed (Localize to Classify) and IoU amplif ied (Classify to Localize). Anchors A-M are predefined anchor boxes around the boat in the picture (only F and H are visualized as examples). Better viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of IoU amplif ied with different σ values (1, 2 or 3). IoU amplif ied = IoU anchor when Classif score = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Examples of detection results using an IoU anchor -based anchor matching strategy (odd lines) and our proposed Mutual Guidance one (even lines). The results are given for all images after applying a Non-Maximum Suppression process with a IoU threshold of 50%. Zoom in to see details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>prediction fashion. On each pixel of feature maps, it classifies the category of this sample point and regresses the four distances to the target bounding box borders. When assigning training labels in FCOS, firstly the corresponding detection layer is selected according to the scale of the object to detect, then positive samples (for both localization and classification tasks) are assigned to all the points inside the ground truth box. Based on that, [?] proposes to only sample the points in the central region of the ground truth box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>IoU anchor -based 50.3% 75.5% 53.7% Localize to Classify 51.8% 76.1% 55.9% Classify to Localize 51.0% 76.1% 54.3% Mutual Guidance 52.1% 76.2% 55.9% IoU anchor -based 54.1% 80.1% 58.3% Localize to Classify 56.0% 80.3% 60.6% Classify to Localize 54.4% 79.9% 58.5% Mutual Guidance 56.2% 80.4% 61.4% IoU anchor -based 51.1% 75.8% 54.8% Localize to Classify 53.4% 76.5% 57.2% Classify to Localize 51.9% 75.9% 55.8% Mutual Guidance 53.5% 76.9% 57.4% IoU anchor -based 55.2% 80.2% 59.6% Localize to Classify 57.4% 81.1% 62.6% Classify to Localize 56.2% 80.1% 61.7% Mutual Guidance 57.7% 81.1% 62.9% IoU anchor -based 55.6% 80.9% 59.6% Localize to Classify 57.2% 80.9% 61.6% Classify to Localize 55.9% 80.8% 60.2% Mutual Guidance 57.9% 81.5% 62.6% Comparison of different anchor matching strategies (the usual IoU anchorbased, proposed Localize to Classify, Classify to Localize and Mutual Guidance) for object detection. Experiments are conducted on the PASCAL VOC dataset. The best score for each architecture is in bold.</figDesc><table><row><cell>Model</cell><cell>Matching strategy AP AP 50 AP 75</cell></row><row><cell>FSSD with</cell><cell></cell></row><row><cell>ResNet-18 backbone</cell><cell></cell></row><row><cell>FSSD with</cell><cell></cell></row><row><cell>VGG-16 backbone</cell><cell></cell></row><row><cell>RetinaNet with</cell><cell></cell></row><row><cell>ResNet-18 backbone</cell><cell></cell></row><row><cell>RetinaNet with</cell><cell></cell></row><row><cell>VGG-16 backbone</cell><cell></cell></row><row><cell>RFBNet with</cell><cell></cell></row><row><cell>VGG-16 backbone</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>ModelMatching strategy AP AP 50 AP 75 APs APm AP l FSSD with ResNet-18 backboneIoU anchor -based 26.1% 42.8% 26.7% 8.6% 29.1% 41.0% Mutual Guidance 27.0% 42.9% 28.2% 9.5% 29.7% 43.0% FSSD with VGG-16 backbone IoU anchor -based 31.1% 48.9% 32.7% 13.3% 37.2% 44.7% Mutual Guidance 32.0% 49.3% 33.9% 13.7% 37.8% 46.4% IoU anchor -based 27.8% 44.5% 28.6% 10.4% 31.6% 42.6% Mutual Guidance 28.7% 44.9% 29.9% 11.0% 32.2% 44.8% IoU anchor -based 32.3% 50.3% 34.0% 14.3% 37.9% 46.7% Mutual Guidance 33.6% 50.8% 35.7% 15.4% 38.9% 48.8% IoU anchor -based 33.4% 51.6% 35.1% 14.2% 38.3% 49.1% Mutual Guidance 34.6% 52.0% 36.8% 15.8% 39.0% 51.1%</figDesc><table><row><cell>RetinaNet with</cell></row><row><cell>ResNet-18 backbone</cell></row><row><cell>RetinaNet with</cell></row><row><cell>VGG-16 backbone</cell></row><row><cell>RFBNet with</cell></row><row><cell>VGG-16 backbone</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Fully Convolutional One-Stage Object Detection (FCOS)<ref type="bibr" target="#b24">[24]</ref> is one of the most representative anchor-free methods, which solves object detection in a per-pixelModel Matching strategy AP AP 50 AP 75 FCOS with ResNet-18 backbone original strategy 49.2% 73.7% 52.1% Mutual Guidance 51.1% 74.4% 54.3% FCOS with VGG-16 backbone original strategy 53.9% 78.4% 57.4% Mutual Guidance 55.9% 79.4% 60.2% Comparison of different label assignment strategies (the original one and Mutual Guidance) for FCOS. Experiments are conducted on the PASCAL VOC dataset. The best score for each architecture is in bold.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Heng ZHANG 1,3[0000−0001−6093−1729] , Elisa FROMONT 1,4[0000−0003−0133−3491] , Sbastien LEFEVRE 2[0000−0002−2384−8202] , and Bruno AVIGNON 3 1 Univ Rennes, IRISA, France 2 Univ Bretagne Sud, IRISA, France 3 ATERMES company, France 4 IUF, Inria, France</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Applying Mutual Guidance to Anchor-free detectors</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<editor>Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M., Garnett, R.</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">R-FCN: object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Libra R-CNN: towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9905</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">FSSD: feature fusion single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1712.00960</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11215</biblScope>
			<biblScope unit="page" from="404" to="419" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">M2det: A single-shot object detector based on multi-level feature pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="9259" to="9266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<editor>Fleet, D.J., Pajdla, T., Schiele, B., Tuytelaars, T.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="6053" to="6062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="765" to="781" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIV</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cornernet-lite: Efficient keypoint based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1904.08900</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="6568" to="6577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno>abs/1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for singleshot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FCOS: fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="9626" to="9635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
