<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Epipolar Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<email>ruiyan@alumni</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
							<email>shoou-i.yu@fb.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Facebook Reality Labs Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Epipolar Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A common approach to localize 3D human joints in a synchronized and calibrated multi-view setup consists of two-steps: (1) apply a 2D detector separately on each view to localize joints in 2D, and (2) perform robust triangulation on 2D detections from each view to acquire the 3D joint locations. However, in step 1, the 2D detector is limited to solving challenging cases which could potentially be better resolved in 3D, such as occlusions and oblique viewing angles, purely in 2D without leveraging any 3D information. Therefore, we propose the differentiable "epipolar transformer", which enables the 2D detector to leverage 3Daware features to improve 2D pose estimation. The intuition is: given a 2D location p in the current view, we would like to first find its corresponding point p in a neighboring view, and then combine the features at p with the features at p, thus leading to a 3D-aware feature at p. Inspired by stereo matching, the epipolar transformer leverages epipolar constraints and feature matching to approximate the features at p . Experiments on InterHand and Human3.6M <ref type="bibr" target="#b12">[13]</ref> show that our approach has consistent improvements over the baselines. Specifically, in the condition where no external data is used, our Human3.6M model trained with ResNet-50 backbone and image size 256×256 outperforms state-ofthe-art by 4.23mm and achieves MPJPE 26.9 mm. Code is available 1 . arXiv:2005.04551v1 [cs.CV] 10 May 2020</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In order to estimate the 3D pose of a human body or hand, there are two common settings. The first setting is single-view 3D pose estimation <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref>, where the algorithm directly estimates 3D pose from a single image. This is extremely challenging due to the ambiguity in depth when only one view is available. The second setting, which is the focus of our paper, is multi-view 3D pose estima- * Equal contribution <ref type="bibr" target="#b0">1</ref>  ' <ref type="figure">Figure 1</ref>: Overview of the proposed epipolar transformer, which enables 2D detectors to leverage 3D-aware features for more accurate pose estimation. For a query vector (e.g., with length 256) on the intermediate deep feature maps of the reference view (H×W×256), we extract K samples along the corresponding epipolar line in the source view. Dot-product and softmax are used to compute similarity between the query and sampled vectors, which in turn is used to compute the corresponding feature. The corresponding feature is then fused with the reference view feature to arrive at a 3D-aware feature for the reference view. tion, where the algorithm can leverage multiple synchronized and geometrically calibrated views to resolve depth ambiguity. A common framework <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16]</ref> to resolve depth ambiguity and accurately estimate the 3D position of joints follows a two-step process: (1) apply a 2D pose detector on each view separately to localize joints in 2D, and (2) perform robust triangulation based on camera calibration and 2D detections from each view to acquire the 3D position of joints. Robust triangulation is required as the prediction of the 2D pose detector could be incorrect or missing due to occlusions. One main disadvantage of this framework is that the detection in step 1 predicts keypoint positions in-dependently from all other views. Thus, challenging cases that could potentially be better resolved in 3D, such as occlusions and viewing the scene from oblique angles, are all resolved by the detector in 2D without utilizing any 3D information. This could lead to inaccurate detections that are inconsistent in 3D, or the network might require more capacity and training data to resolve these challenging cases.</p><p>To this end, we propose the fully differentiable "epipolar transformer" module, which enables a 2D detector to gain access to 3D information in the intermediate layers of the 2D detector itself, and not only during the final robust triangulation phase. The epipolar transformer augments the intermediate features of a 2D detector for a given view (reference view) with features from neighboring views (source view), thus making the intermediate features 3D-aware as shown in <ref type="figure">Figure 1</ref>. To compute the 3D-aware intermediate feature at location p in the reference view, we first find the point corresponding to p in the source view: p , and then fuse the feature at p with the feature at p to get a 3D-aware feature. However, we do not know where the correct p is, so in order to approximate the feature at p , we first leverage the epipolar line generated by p in the source view to limit the potential locations of p . Then, we compute the similarity between the feature at p and the features sampled along the epipolar line. Finally, we perform a weighted sum over the features along the epipolar line as an approximation of the feature at p . The weights used for the weighted sum are the feature similarity. In order to fuse the features at p and p , we propose multiple methods inspired by non-local networks <ref type="bibr" target="#b33">[34]</ref>. Note that the aforementioned operation is done densely for all locations in an intermediate feature map, so the final output of our module is a set of 3D-aware intermediate features that have the same dimensions as the input feature map.</p><p>Since the epipolar transformer is fully differentiable and outputs features with the same dimensions as the input, it can be flexibly inserted into desired locations of a 2D pose detection network and trained end-to-end. The inputs to our network are geometrically calibrated and synchronized multi-view images, and the output of the network is 2D joint locations, which can then further be triangulated to compute 3D joint locations. Note that even though our network outputs 2D joint locations, our network has access to both 2D and 3D features, thus empowering it to leverage more information to achieve more accurate 2D predictions.</p><p>To evaluate the performance of our epipolar transformer, we have conducted experiments on Human3.6M <ref type="bibr" target="#b12">[13]</ref> and InterHand. On Human3.6M <ref type="bibr" target="#b12">[13]</ref>, we achieve 26.9 mm mean per-joint position error when using the ResNet-50 backbone on input images of resolution 256×256 and trained without external data. This outperforms the stateof-the-art, Qiu et al. <ref type="bibr" target="#b27">[28]</ref> from ICCV'19 by 4.23 mm. InterHand is an internal multi-view hand dataset where we also consistently outperform the baselines.</p><p>In sum, the strengths of our method are as follows. 1. The epipolar transformer can easily be added into existing network architectures given it is fully differentiable and the output feature dimensions are the same as the input. 2. The epipolar transformer contains minimal learnable parameters (parameter size is C-by-C, where C is input feature channel size). 3. The epipolar transformer is interpretable because one can analyze the feature similarity along the epipolar line to gauge whether the matching is successful. 4. The network learned with the epipolar transformer could generalize to new multi-camera setups that are not included in the training data as long as if the intrinsics and extrinsics are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-view 3D Human Pose Estimation: There are many methods proposed for multi-view human pose estimation. Pavllo et al. <ref type="bibr" target="#b25">[26]</ref> proposed estimating 3D human pose in video via dilated temporal convolutions over 2D keypoints. Rhodin et al. <ref type="bibr" target="#b28">[29]</ref> proposed to leverage multi-view constraints as weak supervision to enhance a monocular 3D human pose detector when labeled data is limited. Our method is most similar to Qiu et al. <ref type="bibr" target="#b27">[28]</ref> and Iskakov et al. <ref type="bibr" target="#b14">[15]</ref>, thus we provide a more detailed comparison in the following paragraphs. Qiu et al. <ref type="bibr" target="#b27">[28]</ref> proposed to fuse features from other views through learning a fixed attention weight for all pairs of pixels for each pair of views. The advantage of this method is that camera calibration is no longer required. However, the disadvantages are (1) more data from each view is needed to train this attention weight, <ref type="bibr" target="#b1">(2)</ref> there are significantly more weights to learn when the number of views and the image resolution increases, and (3) during test time, if the multi-camera setup changes, then the attention learned during training time is no longer applicable. On the other hand, although the proposed epipolar transformer relies on camera calibration, it only adds minimal learnable parameters. This makes it significantly easier to train, and thus has less demand on the number of training images per view ( <ref type="table">Table 4</ref>). Furthermore, the network trained with the epipolar transformer can be applied to an unseen multi-camera setup without additional training as long as if the calibration parameters are provided.</p><p>Iskakov et al. <ref type="bibr" target="#b14">[15]</ref> proposed to learn 3D pose directly via differentiable triangulation <ref type="bibr" target="#b10">[11]</ref>. One key difference between their learnable triangulation and ours is that Iskakov et al. <ref type="bibr" target="#b14">[15]</ref> fuses features with 3D voxel feature maps, which is more computationally expensive and memory intensive than our method that fuses 3D-aware features in 2D feature maps ( <ref type="table">Table 7)</ref>.</p><p>Multi-view Hand Pose Estimation: Most 3D hand pose estimation works focus on either monocular RGB images or monocular depth images <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>. In contrast, there are fewer works on multiview 3D hand pose estimation, especially two-hand pose estimation due to the difficulty of obtaining multi-view hand data annotations. Simon et al. <ref type="bibr" target="#b29">[30]</ref> proposed to iteratively boost single image 2D hand keypoint detection performance using multi-view bootstrapping. Garcia et al. <ref type="bibr" target="#b7">[8]</ref> introduced a first-person two-hand action dataset with RGB-D and 3D hand pose annotations. Unfortunately, only the right hand is annotated. To showcase our epipolar transformer on the application of multi-view two-hand pose estimation, we use our internal InterHand dataset.</p><p>Epipolar Geometry in Deep Neural Networks: Prasad et al. <ref type="bibr" target="#b26">[27]</ref> applied epipolar constraints to depth regression with the essential matrix. Yang et al. <ref type="bibr" target="#b37">[38]</ref> proposed to use symmetric epipolar distance for data-adaptive interest points. MONET <ref type="bibr" target="#b15">[16]</ref> used epipolar divergence for multiview semi-supervised keypoint detection. Different from the above methods, we leverage the epipolar geometry for deep feature fusion.</p><p>Attention Mechanism: Vaswani et al. <ref type="bibr" target="#b32">[33]</ref> first proposed a transformer for sequence modeling based solely on attention mechanisms. Non-local networks <ref type="bibr" target="#b33">[34]</ref> were introduced for capturing long-term dependencies in videos for video classification. Our approach is named epipolar attention, because we compute attention weights along the epipolar line based on feature similarity and use these weights to fuse features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Epipolar Transformer</head><p>Our epipolar transformer consists of two main components: the epipolar sampler and the feature fusion module. Given a point p in the reference view, the epipolar sampler will sample features along the corresponding epipolar line in the source view. The feature fusion module will then take (1) all the features at the sampled locations in the source view and (2) the feature at p in the reference view to produce a final 3D-aware feature. Note that this is done densely for all locations of an intermediate feature map from the reference view. We now provide the details to the two components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Epipolar Sampler</head><p>We first define the notations used to describe the epipolar sampler. Given two images captured at the same time but from different views, namely, reference view I and source view I , we denote their projection matrices as M, M ∈ R 3×4 and camera centers as C, C ∈ R 4 in homogeneous coordinates. As illustrated in <ref type="figure">Figure 1</ref>, assuming the camera centers do not overlap, the epipolar line l corresponding to a given query pixel p = (x, y, 1) in I can be deterministically located on I as follows <ref type="bibr" target="#b10">[11]</ref>.</p><formula xml:id="formula_0">l = [M C] × M M + p,<label>(1)</label></formula><p>where M + is the pseudo-inverse of M , and [·] × represents the skew symmetric matrix. p's corresponding point in the source view: p , should lie on the epipolar line: l T p = 0. Given the epipolar line l of the source view, the epipolar sampler uniformly samples K locations (64 in our experiments) along the visible portion of the epipolar line, i.e., the intersection of I and l. The sampled locations form a set P with cardinality K. The epipolar sampler samples sub-pixel locations (real value coordinates) via bilinear interpolation. For query points whose epipolar lines do not intersect with I at all, we simply skip them. Please also see supplementary materials for details on how to handle image transformations for the epipolar transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Fusion Module</head><p>Ideally, if we knew the ground-truth p in the source view that corresponds to p in the reference view, then all we need to do is sample the feature at p in the source view: F src (p ), and then combine it with the feature at p in the reference view: F ref (p). However, we do not know the ground-truth p . Therefore, inspired by Transformer <ref type="bibr" target="#b32">[33]</ref> and non-local networks <ref type="bibr" target="#b33">[34]</ref>, we approximate F src (p ) by a weighted sum of all the features along the epipolar line as follows:</p><formula xml:id="formula_1">F src (p) = p ∈P sim(p, p )F src (p )<label>(2)</label></formula><p>where the pairwise function sim(·, ·) computes the similarity score between two vectors. More specifically, it is the dot-product followed by a softmax function. Once we have the feature from the source view: F src (p), we now need to fuse it with the feature in the reference view: F ref (p). One straightforward way to fuse the features is motivated by the residual block <ref type="bibr" target="#b11">[12]</ref>, where the feature from the source view goes through a transformation W z before being added to the features of the reference view as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (b) and the following equation:</p><formula xml:id="formula_2">F fused (p) = F ref (p) + W z (F src (p))<label>(3)</label></formula><p>The weights W z are 1 × 1 convolutions in our experiments. We refer to this method as the Identity Gaussian architecture. Note that the output F fused is of the same shape as the input F ref , thus this property enables us to insert the epipolar transformer module into different stages of many existing networks.</p><p>We also explore the Bottleneck Embedded Gaussian architecture, which was popularized by non-local networks <ref type="bibr" target="#b33">[34]</ref>, as the feature fusion module as shown in <ref type="figure">Figure</ref>   the reference view and source view goes through an embedded Gaussian kernel, where the channel size is downsampled by a factor of two, and the output is up-sampled back, so that the shape of the fused feature still matches the input's shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We have conducted our experiments on two largescale pose estimation datasets with multi-view images and ground-truth 3D pose annotations: an internal hand dataset InterHand, and a publicly available human pose dataset Hu-man3.6M <ref type="bibr" target="#b12">[13]</ref>.</p><p>InterHand dataset: InterHand is an internal hand dataset that is captured in a synchronized multi-view studio with 34 color cameras and 46 monochrome cameras. Only the color cameras were used in our experiments. We captured 23 subjects doing various one and two handed poses. We then annotated the 3D location of the hand for 7.6K unique timestamps, which led to 257K annotated 2D hands when we projected the 3D annotations to all 34 2D views. 248K images were used for training, and 9K images were used for testing. For each hand, 21 keypoints were annotated, so there are 42 unique points for two hands.</p><p>Human3.6M <ref type="bibr" target="#b12">[13]</ref>: Human3.6M <ref type="bibr" target="#b12">[13]</ref> is one of the largest 3D human pose benchmarks captured with four cameras and has 3.6M 3D annotations available. The cameras are located at the corners of a rectangular room and therefore have larger baselines. This leads to a major difference compared to InterHand -the viewing angle difference between the cameras in Human3.6M <ref type="bibr" target="#b12">[13]</ref> are significantly larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metric:</head><p>During training, we use Mean Squared Error (MSE) between the predicted and ground truth heatmaps as the loss. A ground truth heatmap is generated by applying a 2D Gaussian centered on a joint. To estimate the accuracy of 3D pose prediction, we adopt the MPJPE (Mean Per Joint Position Error) metric. It is one of the most popular evaluation metrics, which is referred to as Protocol #1 in <ref type="bibr" target="#b20">[21]</ref>. It is calculated by the average of the L2 distance between the ground-truth and predictions of each joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation study on InterHand Dataset</head><p>We have performed a series of ablation studies on the InterHand dataset to better understand the epipolar transformer, and at the same time to understand the effect of different design choices.</p><p>We trained a single-stage Hourglass network <ref type="bibr" target="#b22">[23]</ref> with the epipolar transformer included. To predict the 3D joint positions, we run the 2D detector trained with the epipolar transformer on all the views, and then perform triangulation with RANSAC to get the final 3D joint locations. During prediction time, our 2D detector requires features from the source view, which is randomly selected from the pool of cameras that were used as the source view during training. We downsampled the images to resolution 512×336 for training and testing. <ref type="figure" target="#fig_1">Figure 3</ref> visualizes some predictions from our model.</p><p>Feature fusion module design: First, we compare the Bottleneck Embedded Gaussian ( <ref type="figure" target="#fig_0">(Figure 2 (a)</ref>) popularized in the non-local networks <ref type="bibr" target="#b33">[34]</ref> with Identity Gaussian <ref type="figure" target="#fig_0">(Figure 2 (b)</ref>). As shown in <ref type="table">Table 1</ref>, Identity Gaussian performs slightly better. We hypothesize that, unlike video classification <ref type="bibr" target="#b33">[34]</ref>, pose estimation needs accurate correspondences, so down-sampling in Bottleneck Embedded Gaussian could be detrimental to the performance.   Max or softmax? We use softmax to obtain the weights along the epipolar line. Another option is to use max because our goal is to find a single "correct" point on the epipolar line. Shown in <ref type="table">Table 1</ref>, max performs slightly worse than softmax. We hypothesize that it is because softmax produces gradients for all samples on the epipolar line, which helps training.</p><p>Viewing angle: We now study how the viewing angle difference of the selected neighboring camera affects the performance of the epipolar transformer. The intuition is that, if the source view has a viewing angle very similar to the reference view, the features from the two views may be too similar, thus not very informative for our fusion module. However, if the views are too far apart, the feature matching becomes harder. There is also a higher chance that a point is occluded in one of the views, which makes matching even harder. Therefore, we experiment with four viewing angle settings during training: 6 • , 12 • , 24 • and 42 • . The source view is randomly selected from ten cameras whose viewing angles are closest to the chosen viewing angle. <ref type="figure">Figure 4</ref> shows examples of the source views with different viewing angles from an example reference view. As also shown in <ref type="figure">Figure 4</ref>, the epipolar transformer is most effective around viewing angle difference 24 • , which is the setting we use by default for other InterHand experiments.</p><p>Which stage to insert the epipolar transformer: We perform experiments to test the ideal location for inserting the epipolar transformer into a network. We tested two settings:</p><p>"late" means the epipolar transformer is inserted before the final prediction layer, and "early" means we insert the module before the Hourglass unit <ref type="bibr" target="#b22">[23]</ref>. The exact locations are detailed in the supplementary materials. As shown in <ref type="table" target="#tab_3">Table 2</ref>, there is no significant difference where we add the epipolar transformer. In the rest of this paper, we fuse at the late stage by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the number of views used during test time:</head><p>We explore how the number of views used during test time affects the final performance. Since there are many different combinations to sample reference and source views, we randomly sample views multiple times (up to 100 times when there are few cameras) and ensure that for each camera there is at least one neighboring camera with viewing angle difference near 24 • . The baseline compared is a vanilla Hourglass network without using the epipolar transformer. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, the network trained with the epipolar transformer consistently outperforms the baseline. When very few views are used (e.g., two views), the relative improvement using the epipolar transformer is around 15%. This supports our argument: epipolar transformers enable the network to obtain better 2D keypoints using information from neighboring views, and the information is crucial when there are fewer views. Even with more views, the epipolar transformer is still able to improve upon the baseline by around 10%.</p><p>Inference with multiple source views: The epipolar transformer we introduced is not limited to fusing features from    <ref type="table">Table 4</ref>: Comparison with cross-view fusion <ref type="bibr" target="#b27">[28]</ref>. The baseline is using Hourglass networks <ref type="bibr" target="#b22">[23]</ref> for InterHand and Resnet-50 <ref type="bibr" target="#b11">[12]</ref> for Human3.6M <ref type="bibr" target="#b12">[13]</ref> without view fusion. two views. During testing, we can choose different neighboring views as the source view and select the prediction with the highest confidence (i.e., highest peak on the heatmap). As shown in <ref type="table" target="#tab_4">Table 3</ref>, we run prediction with ten different neighboring views and select the prediction with the highest confidence for each individual joint. Testing with multi-views reduces the MPJPE error only by 0.1 mm, which is insignificant. The performance might be further improved when training with more than two views, like in MVSNet <ref type="bibr" target="#b39">[40]</ref>.</p><p>Comparison with cross-view fusion <ref type="bibr" target="#b27">[28]</ref>: Qiu et al. <ref type="bibr" target="#b27">[28]</ref> learns fixed global attention weights for each pair of views. A limitation of this method is it requires more images per view to learn the attention weights. On InterHand, crossview fusion <ref type="bibr" target="#b27">[28]</ref> performs even worse than the baseline detector as shown in <ref type="table">Table 4</ref>. One likely reason is because there are only about 3K images per view on InterHand, instead of 312K images per view on Human3.6M <ref type="bibr" target="#b12">[13]</ref>. Besides, Human3.6M <ref type="bibr" target="#b12">[13]</ref> only has four views, which makes it easier to learn the pairwise attention weights, but learning the weights for 34 views for InterHand is significantly harder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Human3.6M Dataset</head><p>We conducted experiments on the publicly available Hu-man3.6M <ref type="bibr" target="#b12">[13]</ref> dataset. We adopt the same training and test-ing sets as in <ref type="bibr" target="#b27">[28]</ref>, where subjects 1, 5, 6, 7, 8 are used for training, and 9, 11 are for testing.</p><p>As there are only four views in Human3.6M <ref type="bibr" target="#b12">[13]</ref>, we choose a closest view as the source view. We adopt ResNet-50 with image resolution 256×256 proposed in simple baselines for human pose estimation <ref type="bibr" target="#b36">[37]</ref> as our backbone network. We use the ImageNet <ref type="bibr" target="#b5">[6]</ref> pre-trained model <ref type="bibr" target="#b23">[24]</ref> for initialization. The networks are trained for 20 epochs with batch size 16 and Adam optimizer <ref type="bibr" target="#b17">[18]</ref>. Learning rate decays were set at 10 and 15 epochs. Unless specified, we follow Qiu et al. <ref type="bibr" target="#b27">[28]</ref>'s setting and do not do additional data augmentation for a fair comparison. We also follow Qiu et al. <ref type="bibr" target="#b27">[28]</ref> for other hyper-parameters. Following <ref type="bibr" target="#b27">[28]</ref>, as there are only four cameras in this dataset, direct linear transformation (DLT) is used for triangulation <ref type="bibr">(Hartley &amp; Zisserman [11]</ref>, p.312), instead of RANSAC.</p><p>2D Pose Estimation: Again following Qiu et al. <ref type="bibr" target="#b27">[28]</ref>, the 2D pose estimation accuracy is measured by Joint Detection Rate (JDR), which measures the percentage of the successfully detected joints. A joint is detected if the distance between the estimated location and the ground truth is smaller than half of the head size <ref type="bibr" target="#b0">[1]</ref>.</p><p>2D pose estimation results are shown in <ref type="table">Table 5</ref>. As shown in Qiu et al. <ref type="bibr" target="#b27">[28]</ref>, one way to compute a cross-view score for a specific reference view location is to do a sum or max of the heatmap prediction scores along its corresponding epipolar line in the source view, but this does not lead to good performance. So cross-view fusion <ref type="bibr" target="#b27">[28]</ref> improved performance by fusing with learned global attention. In contrast, the epipolar transformer neither operates on heatmap prediction scores nor does a global fusion. It attends to the intermediate features locally along the epipolar line. Using the same backbone ResNet-50 with input image size 256×256, the model with epipolar transformer achieves 97.01% JDR, which outperforms 95.9% JDR from Qiu et al. <ref type="bibr" target="#b27">[28]</ref> by 1%. The improvement suggests that fusing along the epipolar line is better than fusing globally. We further apply data augmentation which consists of random scales drawn from a truncated normal distribution TN(1, 0.25 2 , 0.75, 1.25) and random rotations from <ref type="bibr" target="#b36">[37]</ref>. JDR is further improved to 98.25% JDR.</p><formula xml:id="formula_3">TN(0 • , (30 • ) 2 , −60 • , 60 • )</formula><p>Visualization of feature-matching: The main advantage of the epipolar transformer is that it is easily interpretable through visualizing the feature-matching similarity score along the epipolar line. We visualize the results of featurematching along the epipolar line for color features, deep features learned without the epipolar transformer, and the features learned through the epipolar transformer. For the color features, we first convert the RGB image to the LAB color space. Then we discard the L-channel and only use the AB channel to be more invariant to light intensity. <ref type="figure" target="#fig_4">Figure 6</ref> Net scale shlder elb wri hip knee ankle root belly neck nose head Avg -R152 320 88.50 88.94 85.72 90.37 94.04 90.11 -----sum over epipolar line <ref type="bibr" target="#b27">[28]</ref>   <ref type="table">Table 5</ref>: 2D pose estimation accuracy comparison on Human3.6M <ref type="bibr" target="#b12">[13]</ref> where no external training data is used unless specified. The metric is joint detection rate, JDR (%). +: indicates using data augmentation. "-": We cite numbers from <ref type="bibr" target="#b27">[28]</ref> and these entries were absent. : We trained the models using released code <ref type="bibr" target="#b27">[28]</ref>. R50 and R152 are ResNet-50 and ResNet-152 <ref type="bibr" target="#b11">[12]</ref> respectively. Scale is the input resolution of the network. shows a challenging example where the joint-of-interest is totally occluded in both views. However, given that the features learned with the epipolar transformer have access to multi-view information in the 2D detector itself, the matching along the epipolar line finds the "semantically correct" position, i.e., still finds the occluded right wrist, which is the desired behavior for a pose detector. However, the features without the awareness of the multi-view information have the highest matching score at the "physically correct" location, which is still correct in terms of finding correspondences, but not as useful to reason about occlusions for occluded joints. More examples are shown in the supplementary material.  Effect of the number of views used during test time: As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, compared with cross-view <ref type="bibr" target="#b27">[28]</ref>, the models with epipolar transformer still have better performance when there are fewer views. This shows that the epipolar transformer efficiently fuses features from other views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference view</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source view</head><p>Comparison with state-of-the-art, no external datasets setting: <ref type="table">Table 6</ref> shows the performance of several state-ofthe-art methods when no external datasets are used. Our epipolar transformer outperforms the state-of-the-art by a large margin. Specifically, when using triangulation for estimating 3D human poses, epipolar transformer achieves 33.1 mm, which is ∼ 12 mm better than cross-view <ref type="bibr" target="#b27">[28]</ref> while using the same backbone network (ResNet-50) and input size (256×256). Using the recursive pictorial structural model (RPSM <ref type="bibr" target="#b27">[28]</ref>) for estimating 3D poses, our epipolar transformer achieves 26.9 mm, which is ∼ 14 mm better than the cross-view <ref type="bibr" target="#b27">[28]</ref> equivalent. Furthermore, adding epipolar transformer on ResNet-50 input size 256×256 even surpasses the state-of-the-art result from MPJPE (mm)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dir Disc Eat Greet Phone Photo Pose Purch Sit SitD Smoke Wait WalkD Walk WalkT Avg</head><p>Multi-View Martinez <ref type="bibr" target="#b30">[31]</ref> 46  <ref type="table">Table 6</ref>: Comparison with state-of-the-art methods on Human3.6M <ref type="bibr" target="#b12">[13]</ref>, where no additional training data is used unless specified. The metric is MPJPE (mm). " + ": rotation and scaling augmentation. "-": models trained using released code <ref type="bibr" target="#b27">[28]</ref>, where the per action MPJPE evaluation were not provided.</p><p>cross-view <ref type="bibr" target="#b27">[28]</ref> on ResNet-152 input size 320×320 by ∼ 4 mm, which is a 13% relative improvement. Our model with data augmentation achieves MPJPE 30.4 mm with triangulation, which is better than the state-of-the-art without even requiring RPSM. We believe one source of improvement comes from the fact that the epipolar transformer finds correspondences and fuses features dynamically based on feature similarity. This is more accurate than cross-view <ref type="bibr" target="#b27">[28]</ref>, which uses a static attention map for all input images from a pair of views.</p><p>Comparison with state-of-the-art, with external datasets setting: <ref type="table">Table 7</ref> shows the performance of several state-ofthe-art methods when external datasets are used. Iskakov et al. <ref type="bibr" target="#b14">[15]</ref> established a 22.8 mm MPJPE RANSAC baseline with extra data from MS-COCO <ref type="bibr" target="#b19">[20]</ref> and MPII <ref type="bibr" target="#b0">[1]</ref>. They further proposed the learnable weighted triangulation (algebraic w/ conf) and volumetric triangulation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref>, which achieve 19.2 mm and 17.7 mm respectively. We finetune a MS-COCO+MPII pre-trained ResNet-152 384×384 released in <ref type="bibr" target="#b14">[15]</ref> on Human3.6M <ref type="bibr" target="#b12">[13]</ref> and achieve 19.0 mm, which is the best among methods using vanilla triangulation. Besides, the epipolar transformer contributes very little to the number of parameters and computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Limitations</head><p>The biggest limitation of our method is the reliance on precise geometric camera calibration. Poor calibration will lead to inaccurate epipolar lines thus incorrect feature matching. Another limitation of our method is that the viewing angle of neighboring camera views should not be too large, otherwise there is a high likelihood a 3D point might be occluded in one of the views, which will make feature matching more difficult. <ref type="bibr" target="#b1">2</ref>   <ref type="table">Table 7</ref>: Comparison with state-of-the-art methods using external datasets on Human3.6M <ref type="bibr" target="#b12">[13]</ref>. +: data augmentation (i.e., cube rotation <ref type="bibr" target="#b14">[15]</ref>). "err.": the error metric is MPJPE (mm). "tri." stands for triangulation. Number of Parameters and MAC (multiply-add operations) are calculated using THOP 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed the epipolar transformer, which enables 2D pose detectors to leverage 3D-aware features through fusing features along the epipolar lines of neighboring views. Experiments not only show improvement over the baseline on Human3.6M <ref type="bibr" target="#b12">[13]</ref> and InterHand, but also demonstrate that our method can improve multi-view pose estimation especially when there are few cameras. Qualitative analysis of feature matching along the epipolar line also show that the epipolar transformer can provide more accurate matches in difficult scenarios with occlusions. Finally, the epipolar transformer has very few learnable parameters and outputs features with the same dimension as the input, thus enabling it to be easily augmented to existing 2D pose estimation networks. For future work, we believe that the epipolar transformer can also benefit 3D vision tasks such as deep multiview stereo <ref type="bibr" target="#b39">[40]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dealing with Image Transformations</head><p>As the epipolar transformer relies on camera calibration, any spatial transformation applied on the image needs to be reflected in the calibration parameters.</p><p>Data augmentation: Data augmentation like rotation, scaling and cropping can still be performed with the epipolar transformer. The projection matrix needs to be updated accordingly when the image is transformed with an affine transformation parameterized by A ∈ R 2×2 and b ∈ R 2 :</p><formula xml:id="formula_4">M := A b 0 T 1 M<label>(4)</label></formula><p>Different scaling and cropping parameters can be applied separately to the reference view and source view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scaling of projection matrices:</head><p>We should pay special attention to scale the projection matrices for resizing or pooling images. Suppose the input image is spatially down-sampled s x and s y times along the x-axis and y-axis, the projection matrix is updated as follows:</p><formula xml:id="formula_5">M :=   1/s x 0 (1 − s x )/2s x 0 1/s y (1 − s y )/2s y 0 0 1   M<label>(5)</label></formula><p>The coordinates are aligned with the center of pixels rather than the top-left corners, which is important for extracting features at precise locations in the epipolar transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 2D Prediction Visualization in Video</head><p>In skeletons_1min.mp4, we visualize 2D predicted skeletons on Human3.6M <ref type="bibr" target="#b12">[13]</ref> testing set. All methods are with ResNet-50 <ref type="bibr" target="#b11">[12]</ref> and image size 256×256, corresponding to the following three entries in <ref type="table">Table 6</ref>   <ref type="table" target="#tab_4">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stages to Add Epipolar Transformer</head><p>In our experiments, we compared the performance between adding epipolar transformer in the early stage and adding in the late stage (see <ref type="table" target="#tab_3">Table 2</ref>, paragraph Which stage to insert the epipolar transformer). <ref type="figure">Figure 8</ref> illustrates the precise places of inserting the epipolar transformer for the "early" and "late" settings in an one-stage Hourglass network <ref type="bibr" target="#b22">[23]</ref> on InterHand and ResNet-50 <ref type="bibr" target="#b11">[12]</ref> simple baseline <ref type="bibr" target="#b36">[37]</ref> on Human3.6M <ref type="bibr" target="#b12">[13]</ref> respectively. D. Epipolar Transformer Visualization on Hu-man3.6M <ref type="bibr" target="#b12">[13]</ref> We show the visualizations of feature matching similarity for images from Human3.6M <ref type="bibr" target="#b12">[13]</ref> in <ref type="figure">Figure 9</ref>, <ref type="figure" target="#fig_7">Figure 10</ref>, and <ref type="figure" target="#fig_8">Figure 11</ref>. In <ref type="figure">Figure 9</ref>, we show a visualization of the matching results of an easy case in Human3.6M <ref type="bibr" target="#b12">[13]</ref>. Note that in this case, our prediction aligns well with the ground truth. Using the features from the baseline, that is, deep features extracted without using the epipolar transformer, the matched point denoted by the yellow dot is also accurate. However, for more difficult cases in <ref type="figure" target="#fig_7">Figure 10</ref> and <ref type="figure" target="#fig_8">Figure 11</ref>, the deep features learned with the epipolar transformer can perform more accurate matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Epipolar Transformer Visualization on In-terHand</head><p>We show visualization of feature matching similarity for images from InterHand. As shown in <ref type="figure" target="#fig_0">Figure 12</ref>, the color features have multiple peaks in similarity due to the different fingers having similar colors. On the other hand, for most cases deep features trained through the epipolar transformer are able to discriminate the correct finger from all the other similar looking fingers.</p><p>Right elbow selected, denoted in green. <ref type="figure">Figure 9</ref>: Visualizations of the matching results along the epipolar line in an easy case in Human3.6M <ref type="bibr" target="#b12">[13]</ref>. We here use E.T. as a shorthand for epipolar transformer. The compared features are (a) deep features learned through the epipolar transformer (deep features with E.T., denoted in red), (b) deep feature learned by ResNet-50 <ref type="bibr" target="#b11">[12]</ref> without epipolar transformer (deep features w/o E.T., denoted in yellow), and (c) RGB features (denoted in blue). Green dot on the reference view is the selected joint, and the green dot on the source view is the corresponding point offered by the groundtruth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference view</head><p>Source view (i) Right knee selected, denoted in green.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Different feature fusion module architectures. The feature maps are shown along with the shape of their tensors, e.g., H × W × 256 for 256 channels. "⊕" denotes the element-wise sum and "⊗" denotes the batch matrix multiplication where the batch size is H × W .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of predictions for InterHand. The images are cropped for ease of visualization. Our model still fails on challenging hand poses with occlusions (bottom right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>MPJPE by varying the number of views used for prediction on InterHand. The black lines indicate the standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of the matching results along the epipolar line for various features on Human3.6M<ref type="bibr" target="#b12">[13]</ref>. The (occluded) right wrist is selected and denoted by a green dot in the reference view. Features used for matching are (a) deep features learned through the Epipolar Transformer (deep features with E.T.), (b) deep feature learned by ResNet-50<ref type="bibr" target="#b11">[12]</ref> without epipolar transformer (deep features w/o E.T.), and (c) color features (specifically RGB converted to LAB and then excluding the L channel). Green dot on the source view is the corresponding point of the ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>MPJPE by varying the number of views on Hu-man3.6M<ref type="bibr" target="#b12">[13]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Visualizations of the matching results along the epipolar line in a more difficult case in Human3.6M<ref type="bibr" target="#b12">[13]</ref>. We here use E.T. as a shorthand for Epipolar Transformer. The compared features are (a) deep features learned through the epipolar transformer (deep features with E.T., denoted in red), (b) deep feature learned by ResNet-50<ref type="bibr" target="#b11">[12]</ref> without epipolar transformer (deep features w/o E.T., denoted in yellow), and (c) RGB features (denoted in blue). Green dot on the reference view is the selected joint, and the green dot on the source view is the corresponding point offered by the groundtruth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>(Cont')Visualizations of the matching results along the epipolar line in more difficult cases in Human3.6M<ref type="bibr" target="#b12">[13]</ref>. We here use E.T. as a shorthand for Epipolar Transformer. The compared features are (a) deep features learned through the epipolar transformer (deep features with E.T., denoted in red), (b) deep feature learned by ResNet-50<ref type="bibr" target="#b11">[12]</ref> without epipolar transformer (deep features w/o E.T., denoted in yellow), and (c) RGB features (denoted in blue). Green dot on the reference view is the selected joint, and the green dot on the source view is the corresponding point offered by the groundtruth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Comparison of feature-matching results along the epipolar line. The compared features are color features and the deep features learned through the epipolar transformer (deep features with E.T.). The best matches of the epipolar transformer (red) and RGB (blue) are shown on the epipolar lines. The similarity distributions along the epipolar lines are also shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>github.com/yihui-he/epipolar-transformers</figDesc><table><row><cell>fused features</cell><cell></cell></row><row><cell>(H×W×256)</cell><cell></cell></row><row><cell></cell><cell>matches</cell></row><row><cell></cell><cell>(H×W×256)</cell></row><row><cell></cell><cell>similarity</cell></row><row><cell>query</cell><cell></cell></row><row><cell>(256)</cell><cell>candidates</cell></row><row><cell></cell><cell>(256×K)</cell></row><row><cell>deep features</cell><cell>deep features</cell></row><row><cell>(HxWx256)</cell><cell>(H×W×256)</cell></row><row><cell>Reference view</cell><cell>Source view</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2 (a). Before the epipolar transformer, the features from</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">× ×256</cell><cell>× ×256</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1x1, BN</cell><cell>1x1, BN</cell></row><row><cell></cell><cell cols="2">softmax</cell><cell cols="2">× ×128</cell><cell>softmax</cell><cell>× ×256</cell></row><row><cell cols="2">× ×</cell><cell></cell><cell></cell><cell>× ×</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">× ×128×</cell></row><row><cell></cell><cell cols="3">Epipolar</cell><cell>Epipolar</cell><cell>× ×256×</cell></row><row><cell>1x1</cell><cell>× ×128</cell><cell cols="3">sampler × ×128 sampler 1x1 1x1</cell><cell>Epipolar sampler</cell></row><row><cell></cell><cell>× ×256</cell><cell></cell><cell cols="2">× ×256</cell><cell>× ×256</cell><cell>× ×256</cell></row><row><cell cols="2">Reference view</cell><cell></cell><cell cols="2">Source view</cell><cell>Reference view</cell><cell>Source view</cell></row><row><cell cols="5">(a) Bottleneck embedded Gaussian</cell><cell>(b) Identity Gaussian</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Epipolar</figDesc><table><row><cell>Inference</cell><cell>MPJPE (mm)</cell></row><row><cell>baseline</cell><cell>5.46</cell></row><row><cell>single source view</cell><cell>4.91</cell></row><row><cell>multi source views</cell><cell>4.83</cell></row><row><cell>transformer</cell><cell></cell></row><row><cell>plugged into different stages of</cell><cell></cell></row><row><cell>Hourglass networks [23] on Inter-</cell><cell></cell></row><row><cell>Hand dataset.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Different number of neighboring source views for inference on InterHand.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.2</cell><cell></cell><cell cols="2">Epipolar Transformer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MPJPE (mm)</cell><cell>5.0 5.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Reference</cell><cell>6°( Top 2 )</cell><cell>12º ( Top 7 )</cell><cell>24°( Top 15 )</cell><cell>42°( Top 25 )</cell><cell>4.8</cell><cell>6</cell><cell>12 Rotation angle (degree) 24</cell><cell>42</cell></row></table><note>Figure 4: Illustration of different viewing angles between the reference and source view. From the left to the right are the reference view image, and the images with viewing angles difference 6 • , 12 • , 24 • and 42 • respectively. On the right is the performance measured in MPJPE under different viewing angles for InterHand.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>R152 320 91.36 91.23 89.63 96.19 94.14 90.38 -----max over epipolar line[28] R152 320 92.67 92.45 91.57 97.69 95.01 91..44 94.16 92.16 98.95 97.26 96.62 99.89 99.86 99.68 99.78 99.63 97.01 epipolar transformer + R50 256 97.71 97.34 94.85 99.77 98.32 97.55 99.99 99.99 99.76 99.74 99.54 98.25</figDesc><table><row><cell></cell><cell>88 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>cross-view fusion [28]</cell><cell>R152 320 95.58 95.83 95.01 99.36 97.96 94.75 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>cross-view fusion [28]</cell><cell cols="6">R50 320 95.6 95.0 93.7 96.6 95.5 92.8 96.7 96.4 96.5 96.4 96.2 95.9</cell></row><row><cell>cross-view fusion [28]</cell><cell cols="6">R50 256 86.1 86.5 82.4 96.7 91.5 79.0 100.0 94.1 93.7 95.4 95.5 95.1</cell></row><row><cell>epipolar transformer</cell><cell>R50 256 96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>.5 48.6 54.0 51.5 67.5 70.7 48.5 49.1 69.8 79.4 57.8 53.1 56.7 42.2 45.4 57.0 Pavlakos et al. [25] 41.2 49.2 42.8 43.4 55.6 46.9 40.3 63.7 97.6 119.0 52.1 42.7 51.9 41.8 39.4 56.9 Tome et al. [31] 43.3 49.6 42.0 48.8 51.1 64.3 40.3 43.3 66.0 95.2 50.2 52.2 51.1 43.9 45.3 52.8 Kadkhodamohammadi &amp; Padoy [17] 39.4 46.9 41.0 42.7 53.6 54.8 41.4 50.0 59.9 78.8 49.8 46.2 51.1 40.5 41.0 49.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>R50 256×256+triangulate</cell><cell cols="16">38.9 46.1 36.2 59.7 46.4 44.7 44.9 37.7 51.2 72.0 48.2 61.0 46.2 45.7 52.0 48.7</cell></row><row><cell cols="2">R50 256×256+crossview+triangulate[28] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.5</cell></row><row><cell>R50 256×256+ours+triangulate</cell><cell cols="16">30.6 33.2 26.7 28.2 32.8 38.4 29.3 28.9 36.6 45.2 34.3 31.7 33.1 34.8 31.2 33.1</cell></row><row><cell>R50 256×256+ours+triangulate +</cell><cell cols="16">29.0 30.6 27.4 26.4 31.0 31.8 26.4 28.7 34.2 42.6 32.4 29.3 27.0 29.3 25.9 30.4</cell></row><row><cell>R50 256×256+crossview+RPSM [28]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>41.2</cell></row><row><cell>R50 256×256+ours+RPSM</cell><cell cols="16">25.7 27.7 23.7 24.8 26.9 31.4 24.9 26.5 28.8 31.7 28.2 26.4 23.6 28.3 23.5 26.9</cell></row><row><cell cols="17">R152 320×320+crossview+triangulate[28] 34.8 35.8 32.7 33.5 34.4 38.2 29.7 60.7 53.1 35.2 41.0 41.6 31.9 31.4 34.6 38.3</cell></row><row><cell>R152 320×320+crossview+RPSM</cell><cell cols="16">28.9 32.5 26.6 28.1 28.3 29.3 28.0 36.8 42.0 30.5 35.6 30.0 29.3 30.0 30.5 31.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Supplementary Materials: Epipolar Transformer for Multi-view Pose Estimation</figDesc><table><row><cell>input (3×512×336)</cell><cell></cell><cell>input (3×256×256)</cell><cell></cell></row><row><cell>conv1</cell><cell></cell><cell>conv1</cell><cell></cell></row><row><cell>64×256×168</cell><cell></cell><cell>64×128×128</cell><cell></cell></row><row><cell>conv2</cell><cell></cell><cell>conv2</cell><cell></cell></row><row><cell>256×128×84</cell><cell>early stage</cell><cell>256×64×64</cell><cell>early stage</cell></row><row><cell>hourglass 256×128×84</cell><cell>epipolar transformer</cell><cell>conv3 512×32×32</cell><cell>epipolar transformer</cell></row><row><cell></cell><cell></cell><cell>conv4</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1024×16×16</cell><cell></cell></row><row><cell></cell><cell></cell><cell>conv5</cell><cell></cell></row><row><cell></cell><cell></cell><cell>2048×8×8</cell><cell></cell></row><row><cell></cell><cell></cell><cell>deconv</cell><cell></cell></row><row><cell></cell><cell>late stage</cell><cell>256×64×64</cell><cell>late stage</cell></row><row><cell>final conv</cell><cell>epipolar transformer</cell><cell>final conv</cell><cell>epipolar transformer</cell></row><row><cell>42×128×84</cell><cell></cell><cell>17×64×64</cell><cell></cell></row><row><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell></row><row><cell cols="4">Figure 8: Early stage or late stage where we can add epipolar</cell></row><row><cell cols="4">transformer to the backbone model. (a) Hourglass networks [23]</cell></row><row><cell cols="4">on InterHand. (b) ResNet-50 detector [37] on Human3.6M [13].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The video consists of the visualizations of the ground truth, the 5.46 mm baseline and 4.91 mm model with epipolar transformer in</figDesc><table><row><cell>respecitively:</cell></row><row><cell>1. R50 256×256 + triangulate</cell></row><row><cell>2. R50 256×256 + crossview[28] + triangulate</cell></row><row><cell>3. R50 256×256 + ours + triangulate</cell></row><row><cell>In twohand_30.mp4, we visualize 2D predicted hands on</cell></row><row><cell>InterHand testing set.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d hand shape and pose from images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adnane</forename><surname>Boukhayma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>De Bem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly-supervised 3d hand pose estimation from monocular rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu Catalin Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometry-aware recurrent neural networks for active visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricson</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular rgb hand pose inference from unsupervised refinable nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Endri</forename><surname>Dibra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvan</forename><surname>Melchior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Balkis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">First-person hand action benchmark with rgb-d videos and 3d hand pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryul</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d hand shape and pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Point-to-point regression pointnet for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hand pose estimation via latent 2.5d heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05754</idno>
		<title level="m">Learnable triangulation of human pose</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Monet: Multiview semi-supervised keypoint via epipolar divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasamin</forename><surname>Jafarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00104</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A generalizable approach for multi-view 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdolrahim</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10462</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Point-to-pose voting based hand pose estimation using residual permutation equivariant layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shile</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongheui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Epipolar geometry based learning of multi-view depth and ego-motion from monocular sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brojeshwar</forename><surname>Bhowmick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11922</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Spörri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lourdes Agapito, and Chris Russell. Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Toso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatial common sense with geometry-aware recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu Fish</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricson</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Handmap: robust hand pose estimation via intermediate dense guidance map supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Finnegan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Eamonn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Liang</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning data-adaptive interest points through epipolar adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Farhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungsoo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuewei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanfei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Disentangling latent hands for image synthesis and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Occlusion-aware hand pose estimation using hierarchical mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Rgbbased 3d hand pose estimation via privileged learning with depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07376</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bighand2. 2m benchmark: Hand pose dataset and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end hand mesh recovery from a monocular rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hbe: Hand branch ensemble network for real-time 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d hand pose from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
