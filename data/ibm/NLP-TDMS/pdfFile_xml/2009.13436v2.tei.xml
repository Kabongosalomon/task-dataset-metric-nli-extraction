<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Detect Objects with a 1 Megapixel Event Camera</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>PROPHESEE</roleName><forename type="first">Etienne</forename><surname>Perot</surname></persName>
							<email>eperot@prophesee.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>PROPHESEE</roleName><forename type="first">Pierre</forename><surname>De Tournemire</surname></persName>
							<email>pdetournemire@prophesee.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><forename type="middle">Nitti</forename><surname>Prophesee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Masci</forename><surname>Nnaisense</surname></persName>
							<email>jonathan@nnaisense.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lugano</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>PROPHESEE</roleName><forename type="first">Amos</forename><surname>Sironi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><forename type="middle">Asironi@prophesee</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Learning to Detect Objects with a 1 Megapixel Event Camera</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Event cameras encode visual information with high temporal precision, low datarate, and high-dynamic range. Thanks to these characteristics, event cameras are particularly suited for scenarios with high motion, challenging lighting conditions and requiring low latency. However, due to the novelty of the field, the performance of event-based systems on many vision tasks is still lower compared to conventional frame-based solutions. The main reasons for this performance gap are: the lower spatial resolution of event sensors, compared to frame cameras; the lack of largescale training datasets; the absence of well established deep learning architectures for event-based processing. In this paper, we address all these problems in the context of an event-based object detection task. First, we publicly release the first high-resolution large-scale dataset for object detection. The dataset contains more than 14 hours recordings of a 1 megapixel event camera, in automotive scenarios, together with 25M bounding boxes of cars, pedestrians, and two-wheelers, labeled at high frequency. Second, we introduce a novel recurrent architecture for eventbased detection and a temporal consistency loss for better-behaved training. The ability to compactly represent the sequence of events into the internal memory of the model is essential to achieve high accuracy. Our model outperforms by a large margin feed-forward event-based architectures. Moreover, our method does not require any reconstruction of intensity images from events, showing that training directly from raw events is possible, more efficient, and more accurate than passing through an intermediate intensity image. Experiments on the dataset introduced in this work, for which events and gray level images are available, show performance on par with that of highly tuned and studied frame-based detectors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event cameras <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> promise a paradigm shift in computer vision by representing visual information in a fundamentally different way. Rather than encoding dynamic visual scenes with a sequence of still images, acquired at a fixed frame rate, event cameras generate data in the form of a sparse and asynchronous events stream. Each event is represented by a tuple (x, y, p, t) corresponding to an illuminance change by a fixed relative amount, at pixel location (x, y) and time t, with the polarity p ∈ {0, 1} indicating whether the illuminance was increasing or decreasing. <ref type="figure">Fig. 1</ref> shows examples of data from an event camera in a driving scenario.</p><p>Since the camera does not rely on a global clock, but each pixel independently emits an event as soon as it detects an illuminance change, the events stream has a very high temporal resolution, typically of the order of microseconds <ref type="bibr" target="#b0">[1]</ref>. Moreover, due to a logarithmic pixel response characteristic, event cameras have a large dynamic range (often exceeding 120dB) <ref type="bibr" target="#b3">[4]</ref>. Thanks to these properties, event <ref type="figure">Figure 1</ref>: Results of our event-camera detector on examples of the released 1Mpx Automotive Detection Dataset. Our method is able to accurately detect objects for a large variety of appearances, scenarios, and speeds. This makes it the first reliable event-based system on a large-scale vision task. Detected cars, pedestrians and two-wheelers are shown in yellow, blue and cyan boxes respectively. All figures in this work are best seen in electronic form.</p><p>cameras are well suited for applications in which standard frame cameras are affected by motion blur, pixel saturation, and high latency.</p><p>Despite the remarkable properties of event cameras, we are still at the dawn of event-based vision and their adoption in real systems is currently limited. This implies scarce availability of algorithms, datasets, and tools to manipulate and process events. Additionally, most of the available datasets have limited spatial resolution or they are not labeled, reducing the range of possible applications <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>To overcome these limitations, several works have focused on the reconstruction of gray-level information from an event stream <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. This approach is appealing since the reconstructed images can be fed to standard computer vision pipelines, leveraging more than 40 years of computer vision research. In particular, it was shown <ref type="bibr" target="#b9">[10]</ref> that all information required to reconstruct highquality images is present in the event data. However, passing through an intermediate intensity image comes at the price of adding considerable computational cost. In this work, we show how to build an accurate event-based vision pipeline without the need of gray-level supervision.</p><p>We target the problem of object detection in automotive scenarios, which is characterized by important objects dynamics and extreme lighting conditions. We make the following contributions to the field: First, we acquire and release the first large scale dataset for event-based object detection, with a high resolution (1280×720) event camera <ref type="bibr" target="#b3">[4]</ref>. We also define a fully automated labeling protocol, enabling fast and cheap dataset generation for event cameras. The dataset we release contains more than 14 hours of driving recording, acquired in a large variety of scenarios. We also provide more than 25 million bounding boxes of cars, pedestrians and two-wheelers, labeled at 60Hz.</p><p>Our second contribution is the introduction of a novel architecture for event-based object detection together with a new temporal consistency loss. Recurrent layers are the core building block of our architecture, they introduce a fundamental memory mechanism needed to reach high accuracy with event data. At the same time, the temporal consistency loss helps to obtain more precise localization over time. <ref type="figure">Fig. 1</ref> shows some detections returned by our method on the released dataset. We show that directly predicting the object locations is more efficient and more accurate than applying a detector on the gray-level images reconstructed with a state-of-the-art method <ref type="bibr" target="#b9">[10]</ref>. In particular, since we do not impose any bias coming from intensity image supervision, we let the system learn the relevant features for the given task, which do not necessarily correspond to gray-level values.</p><p>Finally, we run extensive experiments on ours and another dataset for which gray-level images are also available, showing comparable accuracy to standard frame-based detectors and improved state-of-the-art results for event-based detection. To the best of our knowledge, this is the first work showing an event-based system with on par performance to a frame-based one on a large vision task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several machine learning architectures have been proposed for event cameras <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Some of these methods, such as Spiking Neural Networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, exploit the sparsity of the data and can be applied event by event, to preserve the temporal resolution of the events stream <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20</ref>].</p><p>2 However, efficiently applying these methods to inputs with large event rate remains difficult. For these reasons, their efficacy has mainly been demonstrated on low-resolution classification tasks.</p><p>Alternative approaches map the events stream to a dense representation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10]</ref>. Once this representation is computed, it can be used as input to standard architectures. Even if these methods lose some of the event's temporal resolution, they gain in terms of accuracy and scalability <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Recently, the authors of <ref type="bibr" target="#b9">[10]</ref> showed how to use a recurrent UNet <ref type="bibr" target="#b24">[25]</ref> to reconstruct high-quality gray-level images from event data. The results obtained with this method show the richness of the information contained in the events. However, reconstructing a gray-level image before applying a detection algorithm adds a further computational step, which is less efficient and less accurate than directly using the events, as we will show in our experiments.</p><p>Very few other works have focused directly on the task of event-based object detection. In <ref type="bibr" target="#b17">[18]</ref>, the authors propose a sparse convolutional network inspired by the YOLO network <ref type="bibr" target="#b25">[26]</ref>. While in <ref type="bibr" target="#b26">[27]</ref>, temporally pooled binary images from the event camera are fed to a faster-RCNN <ref type="bibr" target="#b27">[28]</ref>. However, these methods have only been tested on simple sequences, with a few moving objects on a static background. As we will see, feed-forward architectures are less accurate in more general scenarios.</p><p>The lack of works on event-based object detection is also related to the scarce availability of large benchmarked datasets. Despite the increasing effort of the community <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, very few datasets provide ground-truth for object detection. The authors of <ref type="bibr" target="#b32">[33]</ref> provide a pedestrian detection dataset. However, it is composed of only 12 sequences of 30 seconds. Simulation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> is an alternative way to obtain large datasets. Unfortunately, existing simulators use too simplified hardware models to accurately reproduce all the characteristics of event cameras. Recently <ref type="bibr" target="#b4">[5]</ref> released an automotive dataset for detection. However, it is acquired with a low-resolution QVGA event camera and it contains low frequency labels (≤ 4Hz). We believe instead that high-spatial resolution and high-labeling frequency are crucial to properly evaluate an automotive detection pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Event-based Object Detection</head><p>In this section, we first formalize the problem of object detection with an event camera, then we introduce our method and the architecture used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Let E = {e i = (x i , y i , p i , t i )} i∈N be an input sequence of events, with x i ∈ [0, M ] and y i ∈ [0, N ] the spatial coordinates of the event, p i ∈ {0, 1} the event's polarity and t i ∈ [0, ∞) its timestamp. We characterize objects by a set of bounding boxes B = {b * j = (x j , y j , w j , h j , l j , t j )} j∈N , where, (x j , y j ) are the coordinates of the top left corner of the bounding box, w j , h j its width and height, l j ∈ {0, . . . , L} the label object class, and t j the time at which the object is present in the scene.</p><p>A general event-based detector is given by a function D, mapping E to B = D(E). Since we want our system to work in real time, we will assume that the output of a detector at time t will only depend on the past, i.e. on events generated before t:</p><formula xml:id="formula_0">D(E) = {D({e i } ti&lt;t )} t&gt;=0 , where D({e i } ti&lt;t )</formula><p>outputs bounding boxes at time t. In this work, we want to learn D.</p><p>Applying the detector D at every incoming event is too expensive and often not required by the final applications, since the apparent motion of objects in the scene is typically much slower than the pixels response time. For this reason, we only apply the detector at fixed time intervals of size ∆t:</p><formula xml:id="formula_1">D(E) ≈ {D({e i } ti&lt;t k )} k∈N ,<label>(1)</label></formula><p>with t k = k∆t 1 . However, a function D working an all past events {e i } ti&lt;t k for every k, would be computationally intractable, since the number of input events would indefinitely increase over time.</p><p>A solution would be to consider at each step k, only the events in the interval [t k−1 , t k ), as it is done for example in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22]</ref> for other event-based tasks. However, as we will see in Sec. 5, this approach leads to poor results for object detection. This is mainly due to two reasons: first, it is hard to choose a single ∆t (or a fixed number of events) working for objects having very different speeds and sizes, such as cars and pedestrians. Secondly, since events contain only relative change information, an event-based object detector must keep a memory of the past. In fact, when the apparent motion of an object is zero, it does not generate events anymore. Tracking objects using hard-coded rules is generally not accurate for edge cases such as reflections, moving shadows or object deformations.</p><p>For these reasons, we decide to learn a memory mechanism end-to-end, directly from the input events.</p><p>To use past events information while keeping computational cost tractable, we choose D such that</p><formula xml:id="formula_2">D(E) ≈ {D({e i } ti∈[t k−1 ,t k ) , h k−1 )} k∈N ,<label>(2)</label></formula><p>where h k−1 is an internal state of our model encoding past information at time t k−1 . For each k, we define h k by a recursive formula</p><formula xml:id="formula_3">h k = F ({e i } ti∈[t k−1 ,t k ) , h k−1 ), with h 0 = 0.</formula><p>In the next sections, we describe the recurrent neural network architecture we propose to learn D and F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method</head><p>In this section, we describe the recurrent architecture we use to learn the detector D. In order to apply our model, we first preprocess the events to build a dense representation. More precisely, given input</p><formula xml:id="formula_4">events {e i } ti∈[t k−1 ,t k )</formula><p>we compute a tensor map H k ∈ R C×M×N , with C the number of channels. We denote H k = H in the following. Our method is not constrained to a particular H (cfr. Sec. 5.1).</p><p>To extract relevant features from the spatial component of the events, H is fed as input to a convolutional neural network <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. In particular, we use Squeeze-and-Excitation layers <ref type="bibr" target="#b37">[38]</ref>, as they performed better in our experiments. In addition, we want our architecture to contain a memory state to accumulate meaningful features over time and to remember the presence of objects, even when they stop generating events. For this, we use ConvLSTM layers <ref type="bibr" target="#b38">[39]</ref>, which have been successfully used to extract spatio-temporal information from data <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Our model first uses K f feed-forward convolutional layers to extract high-level semantic features that are then fed to the remaining K r ConvLSTM layers (cfr. <ref type="figure" target="#fig_0">Fig. 2</ref>). This is to reduce the computational complexity and memory footprint of the method due to recurrent layers operating on large feature maps, and more importantly to avoid the recurrent layers to model the dynamics of low-level features that is not necessary for the given task. We denote this first part of the network feature extractor.</p><p>The output of the feature extractor is fed to a bounding box regression head. In this work, we use Single Shot Detectors (SSD) <ref type="bibr" target="#b36">[37]</ref>, since they are a good compromise between accuracy and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ConvLSTM Block Box regression</head><p>(a) (b) <ref type="figure">Figure 3</ref>: (a) Detail of the box regression heads. In order to regularize temporally our network, we introduce a secondary regression head, predicting, at time t k , the boxes B k+1 for time t k+1 . We impose predictions corresponding to the same time step to be consistent. (b) IoU between ground truth tracks and predicted boxes over time. The consistency loss helps obtaining more precise boxes. computational time. However, our feature extractor could be used in combination with other detector families, such as two-stage detectors. Since we want to extract objects for a large range of scales, we feed features at different resolutions to the regression head. In practice, we use the feature map from each of the recurrent layers. A schematic representation of our architecture is provided in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>As typically done for object detection, to train the parameters of our network, we optimize a loss function composed of a regression term L r for the box coordinates and a classification term L c for the class. We use smooth l1 loss <ref type="bibr" target="#b36">[37]</ref> L s for regression and the softmax focal loss <ref type="bibr" target="#b41">[42]</ref> for classification. More precisely, for a set of J ground-truth bounding boxes at time t k , we encode their coordinates in a tensor B * of size (J · R, 4), as done in <ref type="bibr" target="#b36">[37]</ref>, where R is the number of default boxes of the regression head matching a ground-truth box. Let (B, p) be the output of the regression head, with B the tensor encoding the prediction for the above R default boxes and p the class probability distribution for all default boxes. Then, the regression and classification terms of the loss are:</p><formula xml:id="formula_5">L r = L s (B, B * ), L c = −(1 − p l ) γ log p l ,<label>(3)</label></formula><p>where p l is the probability of the correct class l. We set the constant γ to 2 and also adapt the unbalanced biases for softmax logits in the spirit of <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Dual Regression Head and Temporal Consistency Loss</head><p>To have temporally consistent detections, we would like the internal states of the recurrent layers to learn high-level features that are stable over long periods of time. Even if ConvLSTM can, to some extent, learn slow-changing representations, we further improve detections consistency by introducing an auxiliary loss and an additional regression head trained to predict bounding boxes one time-step into the future. The idea is inspired by unsupervised learning methods such as word2vec <ref type="bibr" target="#b42">[43]</ref> and CPC <ref type="bibr" target="#b43">[44]</ref>, which constraint the latent representation to preserve some domain structure. In our case, given that features are shared for both heads, we argue that this has the additional effect of inducing representations that account for object motion, something which is currently used as regularization, but would require further analysis that goes beyond the scope of the current work.</p><p>Given the input tensor H k computed in the time interval [t k−1 , t k ), the two regression heads will output bounding boxes B k and B k+1 , trying to match ground truth B * k and B * k+1 respectively. This dual regression mechanism is shown in <ref type="figure">Fig. 3(a)</ref>.</p><p>To train the two regression heads, we add to our loss an auxiliary regression term between B k+1 and B * k+1 . This term, when applied at every time step k, indirectly constraints the output B k 's of the second head to be close to the predictions B k 's of the first head at the next time step, cfr. <ref type="figure">Fig. 3(a)</ref>. However, since the two heads are independent, they could converge to different solutions. Therefore, we further regularize training by adding another loss term explicitly imposing B k to be close to B k . In summary, the auxiliary loss is:</p><formula xml:id="formula_6">L t = L s (B k+1 , B * k+1 ) + L s (B k , B k ).<label>(4)</label></formula><p>Then, the final loss we use during training is given by L = L c + L r + L t . We minimize it during training using truncated backpropagation through time <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The 1 Megapixel Automotive Detection Dataset</head><p>In this section, we describe an automated protocol to generate datasets for event cameras. We apply this protocol to generate the detection dataset used in our experiments. However, our approach can be easily adapted to other computer vision tasks, such as face detection and 3D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup and Fully Automated Labeling Protocol</head><p>The key component to obtaining automated labels is to do recordings with an event camera and a standard RGB camera side by side. Labels are first extracted from the RGB camera and then transferred to the event camera pixel coordinates by using a geometric transformation. In our work, we used the 1 megapixel event camera of <ref type="bibr" target="#b3">[4]</ref> and a GoPro Hero6. The two cameras were fixed on a rigid mount side by side, as close as possible to minimize parallax errors. For both cameras, we used a large field of view: 110 degrees for the event camera and 120 degrees for the RGB camera. The video stream of the RGB camera is recorded at 4 megapixels and 60fps. Once data are acquired from the setup, we perform the following label transfer: 1. Synchronize the time of the event and frame cameras; 2. Extract bounding boxes from the frame camera images; 3. Map the bounding box coordinates from the frame camera to the event camera. The bounding boxes from the RGB video stream are obtained using a commercial automotive detector, outperforming freely available ones. The software returns labels corresponding to pedestrians, twowheelers, and cars. The time synchronization can be done using a physical connection between the cameras. However, since this is not always possible, we also propose in the supplementary material an algorithmic way of synchronizing them. Once the 2 signals are synchronized temporally, we need to find a geometric transformation mapping pixels from the RGB camera to the event camera. Since the distance between the two cameras is small, the spatial registration can be approximated by a homography. Both time synchronization and homography estimation can introduce some noise in the labels. Nonetheless, we observed time synchronization errors smaller than the discretization step ∆t we use, and that the homography assumption is good enough for our case, since objects encountered in automotive scenarios are relatively far compared to the cameras baseline. We discuss more in depth failure cases of the labeling protocol in Sec. 5.3. More details can be also be found in the supplementary material.</p><p>Recordings and Dataset Statistics Once the labeling protocol is defined, we can easily collect and label a large amount of data. To this end, we mounted the event and frame cameras behind the windshield of a car. We asked a driver to drive in a variety of scenarios, including city, highway, countryside, small villages, and suburbs. The data collection was conducted over several months, with a large variety of lighting and weather conditions during daytime. At the end of the recording campaign, a total of 14.65 hours was obtained. We split them in 11.19 hours for training, 2.21 hours for validation, and 2.25 hours for testing. The total number of bounding boxes is 25M. More statistics can be found in the supplementary material, together with examples from the dataset. To the best of our knowledge, the presented event-based dataset is the largest in terms of labels and classes. Moreover, it is the only available high-resolution detection dataset for event cameras 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we first evaluate the importance of the main components of our method in an ablation study. Then, we compare it against state-of-the-art detectors. We consider the COCO metrics <ref type="bibr" target="#b45">[46]</ref> and we report COCO mAP, as is it widely used for evaluating detection algorithms. Even if this metric is designed for frame-based data, we explain in the supplementary material how we extend it to event data. Since labeling was done with a 4 Mpx camera, but the input events have lower resolution, in all our experiments, we filter boxes with diagonal smaller than 60 pixels. All networks are trained for 20 epochs using ADAM <ref type="bibr" target="#b46">[47]</ref> and learning rate 0.0002 with exponential decay of 0.98 every epoch. We then select the best model on the validation set and apply it to the test set to report the final mAP. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Study</head><p>As explained in Sec. 3.2, our network can take different representations as input. Here we compare the commonly used Histograms of events <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b21">22]</ref>, Time Surfaces <ref type="bibr" target="#b48">[49]</ref>, and Event Volumes <ref type="bibr" target="#b22">[23]</ref>. The results are given in Tab. 1. We see that Event Volume performs the best. Time Surface is 2% points less accurate than Event Volume, but more accurate than simple Histograms. We notice that we could also learn the input representation H together with the network. For example by combining it time <ref type="figure">Figure 4</ref>: Detections on a 1 Mpx Dataset sequence. From top to bottom: Events-RetinaNet, E2Vid-RetinaNet (with the input reconstructed images), and our method RED. Thanks to the memory representation learned by our network, RED can detect objects even when they stop generating events, as for example the stopped car on the right, even when occluded by the motorbike.</p><p>with <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>. However, for efficiency reasons, we decided to use a predefined representation and introduce the memory mechanism in the deeper layers of the network, rather than at the pixel level.</p><p>In a second set of experiments, we show the importance of the internal memory of our network. To do so, we train while constraining the internal state of the recurrent layers to zero. As we can see in Tab. 1, the performance drops by 12%, showing that the memory of the network is fundamental to reach good accuracy. Finally, we show the advantage of using the loss L t of Sec. 3.2.1. When training with this term, mAP increases by 2%, and COCO mAP 75 increases by 4%, Tab. 1. This shows the advantage of using L t , especially with regard to box precision. In order to better understand the impact of the loss L t on the results, we compute the Intersection over Union (IoU) between 1000 ground truth tracks from the validation set and the predicted boxes. We normalize the duration of the tracks to obtain the average IoU. As shown in <ref type="figure">Fig. 3(b)</ref>, with L t , IoU is higher for all tracks duration. We now compare our method with the state-of-the-art on the 1 Mpx Detection Dataset and the Gen1 Detection Dataset <ref type="bibr" target="#b4">[5]</ref>, which is another automotive dataset acquired with a QVGA event camera <ref type="bibr" target="#b52">[53]</ref>.</p><p>We denote our approach RED for Recurrent Event-camera Detector. For these experiments, we consider Event Volumes of 50ms. Since there are not many available algorithms for event-based detection, we use as a baseline a feed-forward architecture applied to the same input representation as ours, thus emulating the approach of <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b26">[27]</ref>. We considered several architectures, leading to similar results. We report here those of RetinaNet <ref type="bibr" target="#b41">[42]</ref> with ResNet50 <ref type="bibr" target="#b53">[54]</ref> backbone and a feature pyramid scheme, since it gave the best results. We refer to this approach as Events-RetinaNet. Then, we consider the method of <ref type="bibr" target="#b9">[10]</ref>, which is currently the best method to reconstruct graylevel images from events and uses a recurrent Unet. For this, we use code and network publicly released by the authors. Then, we train the RetinaNet detector on these images. We refer to this approach as E2Vid-RetinaNet. For all methods, before passing the input to the first convolutional layer of the detector, input height and width are downsampled by a factor 2. For the Gen1 Detection Dataset, we report also results available from the literature <ref type="bibr" target="#b12">[13]</ref>.</p><p>Finally, since the 1 Mpx Dataset was recorded together with a RGB camera, we can train a framebased detector on these images. Since events do not contain color information, we first convert the RGB images to grayscale. Moreover, to have the same level of noise in the labels due to the automated labeling, we map frame camera pixels to the same resolution and FOV as the event camera.</p><p>In this way, we can have an estimation of how a grayscale detector would perform on our dataset. Similarly, since the Gen1 Dataset was acquired using an event camera providing gray levels, we could run the RetinaNet detector on them. We refer to this approach as Gray-RetinaNet.</p><p>The results we obtain are given in Tab. 2. We also report the number of parameters of the networks and the methods runtime, including both events preprocessing and detector inference, on a i7 CPU at 2.70GHz and a GTX980 GPU. Qualitative results are provided in <ref type="figure">Fig. 4</ref> and in the supplementary material. From <ref type="figure">Fig. 4</ref>, we see in particular that our model continues detecting the car even when it does not generate events. While Events-RetinaNet becomes unstable and E2Vid-RetinaNet begins oversmoothing the image, and thus loses the detection. As we can see from Tab. 2, our method outperforms all the other event-based ones by a large margin. On the 1Mpx dataset the images reconstructed by <ref type="bibr" target="#b9">[10]</ref> are of good quality and therefore E2Vid-RetinaNet is the second best method, even if 18% points behind ours. Instead, on the Gen1 Dataset, the model of <ref type="bibr" target="#b9">[10]</ref> does not generalize well and images are of lower quality. Therefore, on this dataset, Events-RetinaNet scores better. Our method reaches the same mAP as Gray-RetinaNet on the 1Mpx Dataset, making it the first event-camera detector with comparable precision to that of commonly used frame-camera detectors. If we also consider color, the RetinaNet mAP increases to 0.56, confirming that color information is usefull to increase the accuracy. Our method could benefit from color if acquired by the sensor, such as in <ref type="bibr" target="#b54">[55]</ref>. On the Gen1 Dataset our method performs slightly worse, this is due to the higher level of noise of the QVGA sensor and also because the labels lower frequency makes training a recurrent model more difficult. Finally, we observe that our method has less parameters than the others, it runs realtime and, on the 1Mpx Dataset, it is 21x faster than E2Vid-RetinaNet, which reconstructs intensity images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Failure Cases</head><p>In this section, we discuss some of the failure cases of our network and of the automated labeling protocol of Sec. 4. In <ref type="figure" target="#fig_1">Fig. 5</ref>, we show the results of our detector together with the ground truth on some example sequences. In the ground truth we observe two types of errors: geometric errors and semantic errors. Geometric errors, such as misalignment between objects and bounding boxes are due to an imprecise temporal and spatial registration between the event and the frame cameras. Semantic errors, such as label swaps or erroneous boxes, are due to wrong detections of the frame based software used for labeling. Our detector can correct some of the geometric errors, if the errors in the box position are uniformly distribuited around the objects. Semantic labels are harder to correct, and an outlier robust loss might be beneficial during training. In addition, we observe that our detector can produce double detections and is less accurate on small objects.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Generalization to Night Recordings and Other Event Cameras</head><p>We now study the generalization capabilities of our detector. First, we focus on applying our detector, trained with daylight data only, on night recordings. Since event cameras are invariant to absolute illuminance levels, an event-based detector should generalize better than a frame-based one. To test this, we apply RED and Gray-RetinaNet detectors on new recorded night sequences, captured using the event camera of Sec. 4 and a HDR automotive camera. We stress the fact that these networks have been trained exclusively on daylight data. Since the frame-based labeling software of Sec. 4 is not accurate enough for night data, we report qualitative results in <ref type="figure" target="#fig_2">Fig. 6</ref> and in the appendix. It can be observed that the accuracy of Gray-RetinaNet drops considerably. This is due to the very different lighting and the higher level of motion blur inherently present in night sequences. On the contrary, our method performs well also in these conditions.</p><p>In a second experiment, we test the generalization capability of our network when using different camera types as input. Since there is no available dataset with object detection labels, we report qualitative results on the MVSEC dataset <ref type="bibr" target="#b31">[32]</ref>, which is an automotive dataset acquired with a DAVIS-346 camera. For this purpose we use the model trained on the Gen1 Dataset, since it was acquired with an ATIS camera with similar resolution as the DAVIS. From <ref type="figure" target="#fig_3">Fig. 7</ref>, we see that even if the model was trained on a different camera, it generalizes well also on the DAVIS sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a high-resolution event-based detection dataset and a real-time recurrent neural network architecture which can detect objects from event cameras with the same accuracy as mainstream gray-level detectors. We showed it is possible to consistently detect objects over time without the need for an intermediate gray-level image reconstruction. However, our method still needs to pass through a dense event representation. This means that our method does not take advantage of the input data sparsity. In the future, we plan to exploit the sparsity of the events to further reduce computational cost and latency. This could be done for example by adapting our method to run on neuromorphic hardware <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>The integration of an event-based object detection pipeline in real-world applications could positively impact several aspects of existing systems. First, the camera's high temporal resolution would allow faster reaction time and be more robust in situations where standard cameras suffer from motion blur or high latency. Secondly, they could also improve performance in HDR or low light scenes. Both these aspects are essential to increase the safety of driving assistance solutions or autonomous vehicles <ref type="bibr" target="#b57">[58]</ref>. Similarly, these characteristics could be useful in applications where there is an interaction between humans and robots (e.g., in a production line or in a warehouse). Finally, the adoption of similar pipelines in other contexts, like the Internet of Things, could reduce the power consumption and the data storage of existing systems <ref type="bibr" target="#b58">[59]</ref>.</p><p>Although, as demonstrated in <ref type="bibr" target="#b9">[10]</ref>, the possibility to reconstruct intensity images from events stream could create privacy issues, the proposed method allows better privacy management. Encoding events in not human-readable structures and not requiring to have an image-like representation prevents the easy use of the recorded data for purposes different than those defined by the original algorithm, and it limits the possibility to identify people, vehicles or places.</p><p>Further advances in event-based processing and neuromorphic architectures might also open the future to a new class of extremely low-power and low-latency artificial intelligence systems <ref type="bibr" target="#b59">[60]</ref>. In a world where power-hungry deep learning techniques are becoming a commodity, and at the same time, environmental concerns are increasingly pressuring our way of life, neuromorphic systems could be an essential component of a sustainable society <ref type="bibr" target="#b60">[61]</ref>.</p><p>Concerning possible negative outcomes, since our method relies on training data, it will leverage the bias and the limitations contained in it. Similarly, since it relies on deep learning architectures, it might be deceived by adversarial attacks. To mitigate these consequences, several methods have been recently proposed to de-bias deep learning models and make them more robust to adversarial examples <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref>. A failure of the system might cause dangerous incidents and have severe consequences on people and facilities <ref type="bibr" target="#b65">[66]</ref>. Similarly, its integration in a fully autonomous vehicle, poses the ethical question of replacing the human morale in the so called Trolley Problem <ref type="bibr" target="#b66">[67]</ref>. Moreover, autonomous vehicles may impact the careers of millions of people <ref type="bibr" target="#b67">[68]</ref>.</p><p>Finally, we think it is essential to be aware that the event-based perception and similar detection systems could be exploited to harm people and threaten human rights. For example, developing modified versions of this algorithm for mass surveillance <ref type="bibr" target="#b68">[69]</ref> or military applications <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b70">71]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material, we report additional details which did not fit in our submission due to space limitations. In Sec. A, we provide more statistics about the released 1 Mpx Automotive Detection Dataset and details about the automated labeling protocol, introduced in Sec. 4 of the main submission. Then, in Sec. B, we explain how to adapt the COCO metric for an event-based object detection task. In Sec. B.1 and Sec. C, we formally define the losses and the input representations used in our experiments. Finally, in Sec. D we provide the architecture used in our experiments, Attached to this appendix, we also provide a video with the results of our method on sample sequences from the two detection datasets used in our experiments and a comparison with the frame-based detector on night recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The 1 Megapixel Automotive Detection Dataset</head><p>As described in our submission, we build our dataset thanks to a fully automated labeling protocol for event cameras. In this section, we provide more statistics about our dataset and more details about the automated labeling protocol.  Time Synchronization and Spatial Registration The labeling protocol consists in transferring labels from RGB camera data to event camera data, recorded side by side. In the following, we describe an algorithmic way to synchronize the two cameras and to obtain the homography mapping RGB pixels coordinates to event camera pixel coordinates.</p><p>Time synchronization is obtained by using zero-normalized cross-correlation (ZNCC) applied on onedimensional statistics extracted from the two signals. The 1D signals extracted from the event-based camera are the sum and the standard deviation of the number of events per pixel in a time slice of 1/60s. The 1D signals extracted from the RGB camera are the sum and standard deviation of the intensity value of the absolute difference of consecutive gray-level frames. For each pair of signals (e.g., sum of events and sum of intensity of frame difference) the ZNCC is computed and the highest value is the estimate of temporal difference between the event-based camera and the RGB camera. All estimates are finally averaged using the median.</p><p>To estimate the homography, we extract feature points from the two data streams. For the RGB stream, we simply extract FAST <ref type="bibr" target="#b71">[72]</ref> points. For the event camera, feature points are obtained by extracting Harris points from histogram of events in a time slice of length 1/60s. Once feature points are obtained, we can fit an homography using standard methods, such as RANSAC <ref type="bibr" target="#b72">[73]</ref>. An alternative solution to estimate the homography consists in minimizing the squared difference between two images (pixel by pixel) where one image is the histogram of events and the other image is the difference of gray level frames. Such loss is then minimized using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Evaluation Methodology</head><p>In this section, we describe how we adapt the COCO metric protocol to work with event data.</p><p>We first notice that for the datasets we consider, ground-truth labels are given by a set of bounding boxes at fixed frequency. As a consequence, if the detections returned by an algorithm have the same frequency as the ground truth, accuracy computation is equivalent to evaluating a frame-based detection algorithm.</p><p>For the more general case in which detections have a different rate than the ground truth, or when detections are returned in an asynchronous fashion, evaluation can be reduced to the previous case. This is achieved by restricting evaluation to only timestamps for which both detections and groundtruth information are available. If necessary one can add a small time tolerance on the detections timestamps.</p><p>For both datasets we consider, the ground-truth is annotated starting from gray-level images. For this reason, there might be some ground-truth bounding boxes at the beginning of a sequence for which no events have been generated yet (such as a static object in front of the sensor when the recording car is stopped). Since it would be impossible for an event-based algorithm to predict the presence of an object in this particular condition, we decide to ignore such boxes during training and evaluation in the first 0.5 seconds of each sequence. More precisely, no loss is computed during training for 0.5s and the validation is skipped for the same amount of time.</p><p>Similarly, since the frame-based camera of Sec. A, used for annotation has larger resolution, very far objects are not clearly distinguishable in the event camera. For this reason, we ignore bounding boxes with diagonal size smaller then 60 pixels or smaller then 20 pixel in width or height, during training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Losses</head><p>In this section, we formally define the losses used in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Softmax Focal Loss</head><p>For the classification term L c of our loss, we consider the Focal Loss <ref type="bibr" target="#b41">[42]</ref>. In the original paper, the authors of <ref type="bibr" target="#b41">[42]</ref> used a one-versus-all setting with sigmoid. Instead, in our experiments, we obtain better results using a softmax setting.</p><p>The focal loss allows to concentrate on hard examples more efficiently in highly unbalanced cases such as semantic segmentation or object detection. To do so, the individual terms of the cross-entropy loss are weighted with (1 − p l ) γ where p l is the probability of the correct class. It has the effect that a big error term will count exponentially more, compensating for the rare classes. One problem arises at the beginning of training, where most probabilities are random. Therefore, most of negative examples will still overwhelm the loss for a lot of iterations. As a solution, the authors of <ref type="bibr" target="#b41">[42]</ref> provide an efficient biased initialization to increase probability of classifying everything as negative. In their one-versus-all setting, each logit predicts a class versus background (the probability is computed with sigmoid), meaning there is no competition among logits for a single object. They initialize biases of all logits s l with − log px 1−px , where p x is set to 0.01. In this way, after applying the sigmoid, the initial probability to predict the background class is around 1 − p x = 0.99 (it would be exactly 0.99 if the weights were zero).</p><p>We adapt the same principle to initialize the probability in the case of a softmax classification. We set the logit biases of non-background classes s x = 0 and the logit bias for background s ∅ = log(C p ∅ 1−p ∅ ) with p ∅ = 0.99 and C the number of object classes (excluding background). Applying the softmax leads to a probability for background class close to p ∅ = 0.99. To understand this let us assume that the probability of a (non-background) class p x is equal for each class p x = 1−p ∅ C and from the softmax formula we know that p x = e sx e s ∅ +Ce sx . Developing the following gives rise to difference of logits: s ∅ − s x = log(C p ∅ 1−p ∅ ). Therefore we use this formula for bias of background and set the biases of other classes s x to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Smooth l1 loss</head><p>For the regression loss L r and the auxiliary loss L t , we consider the smooth l1 loss <ref type="bibr" target="#b36">[37]</ref>, denoted as L s . The loss L s (B, B * ) applied to a tensor B and ground truth B * is given by the following:</p><formula xml:id="formula_7">L s (B, B * ) = 1 N j L s (B j , B * j ) (5) L s (B j , B * j ) = |B j − B * j | − β 2 if |B j − B * j | ≥ β 1 2β (B j − B * j ) 2 otherwise<label>(6)</label></formula><p>where B j and B * j are the elements of B and B * respectively. In the experiments, we set β = 0.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Input Representation</head><p>In this section, we formally define the input tensor representations we consider in the Ablation Study of the main submission, namely Histograms of events <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b21">22]</ref>, Time Surfaces <ref type="bibr" target="#b48">[49]</ref>, and Event Volumes <ref type="bibr" target="#b22">[23]</ref>. In the following we assume we are given an input sequence of events {e i = (x i , y i , p i , t i )} I i=1 in a time interval of size ∆t. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Histogram</head><p>A Histogram of events is simply given by the sum of events per pixels. We also sum the events independently per polarity, by using one channel per polarity. We clamp at maximum value m of events per pixels and then we divide by this maximum value. This leads to a tensor input of size (2, M, N ), where a generic element is the following: </p><p>In our experiments, we use m = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Timesurface</head><p>A Timesurface is a 2D snapshot of the latest timestamps of events for a given receptive field. It can be seen as a proxy to the normal optical flow <ref type="bibr" target="#b73">[74]</ref>. If we accumulate event's absolute timestamps on this array, we obtain a buffer containing all latest timestamps per pixel since the beginning of the record. To give less weight to old events, we apply an exponential decay with parameter τ j to the timestamps. Thus, assuming for simplicity t 0 = 0, the timesurface T is given by:</p><p>T p,j,x,y = exp ts p,x,y − max x,y (ts p,x,y ) τ j ,</p><p>ts p,x,y = max i,xi=x,yi=y,pi=p t i .</p><p>In order to account for slow and fast motion, we consider two decays τ j of 10 and 100 ms. This way we can see both motion gradients in very recent and also moderately far in the past without need of any rolling buffer. The formulation being differentiable, we could also learn the set of decay constants τ j , but we let this for future work. The timesurface input shape used in the experiments is (4, M, N ) where the first dimension refers to the 2 decays for each polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Event Volume</head><p>Event Volumes have been introduced in <ref type="bibr" target="#b22">[23]</ref>. In the original work, the contribution from the two polarities is subtracted, here instead we consider each polarity independently. Given the input events {e i = (x i , y i , p i , t i )} I i=1 , the corresponding Event Volume V is given by:</p><p>V t,p,x,y = i,xi=x,yi=y,pi=p</p><formula xml:id="formula_11">max(0, 1 − |t − t * i |)<label>(10)</label></formula><formula xml:id="formula_12">t * i = (B − 1) t i − t 0 t I − t 1<label>(11)</label></formula><p>We omit the bilinear kernel for x, y as we restrict to build the volume at maximum resolution. We simply downsample after with a dense operator. In the experiments, we generate Event Volumes of B = 5 bins and 2 polarities (for ON and OFF events). The input tensor shape used in the experiment is (10, M, N ), where bins and polarity are combined in the first dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Neural Network Architecture</head><p>The proposed neural network RED consists of a feature extractor that is fed to bounding box regression heads. The feedforward part of the proposed feature extractor uses Squeeze-Excite Layers, denoted SE and described in Tab. 5. The ConvLSTM uses a BatchNorm with Conv Layer for input-tohidden connection, and a plain Conv Layer for hidden-to-hidden connection. We run input-to-hidden connections (including the BatchNorm layer) in parallel for all timesteps of a batch. Detailed number of layers and parameters for each layer are given in Tab. 4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed architecture. Input events are used to build a tensor map H k at every time step t k . Feed-forward convolutional layers extract low-level features from H k . Then, ConvLSTM layers extract high-level spatio-temporal patterns. Finally, multiscale features from the recurrent layers are passed to the output layers, to predict bounding box locations and class. Thanks to the memory of the ConvLSTM layers, temporal information is accumulated and preserved over time, allowing robust detections even when objects stop generating events in input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Labeling and detector failure cases. White boxes correspond to labels, colored boxes to detections. From left to right: Labels outlier, labels misalignment, double detection, label swap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Top: Gray-Retinanet applied to night recordings of a HDR automotive camera. Bottom: Our detector RED applied to recordings of a 1 Mpx event camera of the same scene. The detectors were trained on day light data. Gray-Retinanet does not generalize well on night images. In contrast, RED generalizes on night sequences because event data is invariant to absolute illuminance levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>RED detector trained on ATIS data and applied to DAVIS sequences. Even if our model was trained on a different camera, it generalizes to other sensors, points of view and light conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Left: RGB frames used for labelling, with overlaid bounding boxes returned by the automotive labeling software. Right: sample snapshots from the 1Mpx Detection Dataset, with transferred bounding boxes. The bounding boxes are transferred from the RGB camera using the automoated labeling protocol described in Sec. A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of a timesurface with a constant decay of 100 ms. Large values are represented with warm colors, small values with cool colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell cols="3">Histogram Time Surface Event Volume</cell></row><row><cell>0.37</cell><cell>0.39</cell><cell>0.41</cell></row></table><note>Ablation study on the 1Mpx Dataset. Left: mAP for different input representations (without consistency loss). Right: mAP and mAP 75 without some components of our method, with Event Volume as input. "w/o memory" means forcing the internal state h k to be zero, for all recurrent layers.w/o memory w/o L t loss memory + L t loss 0.29 | 0.26 0.41 | 0.40 0.43 | 0.44 5.2 Comparison with the State-of-the-art</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on the two automotive detection datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">1Mpx Detection Dataset</cell><cell cols="2">Gen1 Detection Dataset</cell></row><row><cell></cell><cell>mAP</cell><cell>runtime (ms)</cell><cell cols="3">params (M) mAP runtime (ms)</cell></row><row><cell cols="2">MatrixLSTM [50] -</cell><cell>-</cell><cell>-</cell><cell>0.31  *</cell><cell>-</cell></row><row><cell>SparseConv [13]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.15</cell><cell>-</cell></row><row><cell cols="2">Events-RetinaNet 0.18</cell><cell>44.05</cell><cell>32.8</cell><cell>0.34</cell><cell>18.29</cell></row><row><cell cols="2">E2Vid-RetinaNet 0.25</cell><cell>840.66</cell><cell>43.5</cell><cell>0.27</cell><cell>263.28</cell></row><row><cell>RED (ours)</cell><cell>0.43</cell><cell>39.33</cell><cell>24.1</cell><cell>0.40</cell><cell>16.70</cell></row><row><cell>Gray-RetinaNet</cell><cell>0.43</cell><cell>41.43</cell><cell>32.8</cell><cell>0.44</cell><cell>17.35</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Provided by the authors, using a pretrained YOLOv3.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Dataset StatisticsThe 1Mpx Automotive Detection Dataset is composed of 14.65 hours or recordings of a 1280x720 event camera<ref type="bibr" target="#b3">[4]</ref>. Data are recorded in different scenarios, including city, with different level of traffic, highway, countryside, small villages and suburbs. The data collection campaign was conducted over several months, with a large variety of lighting and weather conditions during daytime. Recordings are labeled with the automated protocol described in the main submission, yielding 25 million bounding boxes of cars, pedestrians and two-wheelers (i.e. bikes and motorbikes). We split the recordings in 11.19 hours for train, 2.21 hours for validation and 2.25 hours for test. Moreover, to ease training and evaluation each recording is split in 60 seconds chunks, leaving at least one minute between chunks belonging to the different splits. Precise statistics are given in Tab. 3. Sample images form the dataset are given inFig 8.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Number of labels per class on the 1 Mpx Automotive Detection Dataset.</figDesc><table><row><cell></cell><cell>Car</cell><cell cols="2">Pedestrian Two-wheeler</cell></row><row><cell>Train</cell><cell cols="2">11,567,763 6,022,634</cell><cell>802,707</cell></row><row><cell cols="2">Validation 2,339,095</cell><cell>919,596</cell><cell>138,934</cell></row><row><cell>Test</cell><cell cols="2">2,396,966 1,487,702</cell><cell>171,428</cell></row><row><cell>Total</cell><cell cols="2">16,303,824 8,429,932</cell><cell>1,113,069</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our method can also be applied on a fixed number of events. For clarity, we only describe the fixed ∆t case.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Dataset available at: prophesee.ai/category/dataset/ 3 Evaluation code at github.com/prophesee-ai/prophesee-automotive-dataset-toolbox</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We would like to thank Davide Migliore for the organization and the support inside the company during the whole duration of paper preparation. We would also like to thank Cécile Pignon for acquiring and reviewing most of the sequences of 1 Megapixel dataset. Finally, thank to Matteo Matteucci and Marco Cannici from Politecnico di Milano for fruitful discussions and for providing the MatrixLSTM results of <ref type="table">Table 2</ref>.</p><p>This publication and the related work was performed in the scope of the ES3CAP research project, under the Bpifrance Invest for the Future Program (Programme d'Investissements d'Avenir -PIA). This work was also funded in part by the EU NEOTERIC H2020-ICT-2019-2 project 871330.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A 128× 128 120 db 15 µs latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSSC.2007.914337</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Retinomorphic event-based vision sensors: bioinspired cameras with spiking output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernabe</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1470" to="1484" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">1 a 640× 480 dynamic vision sensor with a 9µm pixel and 300meps address-event representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bongki</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjae</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heejae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Seok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunju</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoobin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinman</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooyeon</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Solid-State Circuits Conference (ISSCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="67" />
		</imprint>
	</monogr>
	<note>et al. 4.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">10 a 1280× 720 backilluminated stacked temporal contrast event-based vision sensor with 4.86 µm pixels, 1.066 geps readout, programmable event-rate controller and compressive data-formatting pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Finateu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsumi</forename><surname>Niwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Matolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koya</forename><surname>Tsuchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Mascheroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Reynaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooria</forename><surname>Mostafalu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Chotard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Legoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Solid-State Circuits Conference-(ISSCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="112" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Pierre De Tournemire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Nitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Migliore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sironi</surname></persName>
		</author>
		<title level="m">A large scale event-based detection dataset for automotive. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2001</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Event-based vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiara</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joerg</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08405</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simultaneous mosaicing and tracking with an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanme</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryad</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sio-Hoi</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Solid State Circ</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simultaneous optical flow and intensity estimation from an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bardow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="884" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time intensity-image reconstruction for event cameras using manifold regularisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gottfried</forename><surname>Munda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1381" to="1393" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High speed and high dynamic range video with an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Space-time event clouds for gesture recognition: From rgb cameras to event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yexin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1826" to="1835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Eventnet: Asynchronous recursive event processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sekikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosuke</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideo</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Event-based asynchronous sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Messikommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision. (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic evolving spiking neural networks for on-line spatio-and spectro-temporal pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Kasabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Dhoble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuttapod</forename><surname>Nuntalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Indiveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="188" to="201" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training deep spiking neural networks using backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Jun Haeng Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">508</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Slayer: Spike layer error reassignment in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bam</forename><surname>Sumit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning in spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Tavanaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothee</forename><surname>Kheradpisheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Masquelier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="47" to="63" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Asynchronous convolutional networks for object detection in neuromorphic cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Romanoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Matteucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hats: Histograms of averaged time surfaces for robust event-based object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuele</forename><surname>Brambilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bourdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryad</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1731" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phased lstm: Accelerating recurrent network training for long or event-based sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3882" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ev-segnet: semantic segmentation for event-based cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Eventbased vision meets deep learning on steering prediction for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Ana I Maqueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narciso</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5419" to="5427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised event-based learning of optical flow, depth, and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="989" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ev-flownet: Self-supervised optical flow estimation for event-based cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="DOI">10.15607/RSS.2018.XIV.062</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems</title>
		<meeting>Robotics: Science and Systems<address><addrLine>Pittsburgh, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive temporal pooling for object detection using dynamic vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Heng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsurk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Converting static image datasets to spiking neuromorphic datasets using saccades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajinkya</forename><surname>Jayawant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thakor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">437</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A low power, fully eventbased gesture recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnon</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Melano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmelo</forename><forename type="middle">Di</forename><surname>Nolfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapan</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Andreopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Garreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcela</forename><surname>Mendoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7243" to="7252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01458</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">End-to-end davis driving dataset. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The multivehicle stereo event camera dataset: An event camera dataset for 3d perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Özaslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Pfrommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2032" to="2039" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neuromorphic benchmark datasets for pedestrian detection, action recognition, and fall detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejia</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenshan</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois C</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neurorobotics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Esim: an open event camera simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="969" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Video to events: Bringing modern computer vision closer to event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Hidalgo-Carrió</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03095</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mobile video object detection with temporally-aware feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5686" to="5695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781" />
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations, ICLR 2013</title>
		<meeting><address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1807.03748" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Representation learning with contrastive predictive coding. CoRR, abs/1807.03748</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Steering a predator robot using a mixed frame/event-driven convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Paul Moeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Corradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmett</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Vance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautham</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dermot</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbrück</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Second International Conference on Event-based Control, Communication, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hots: a hierarchy of event-based time-surfaces for pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryad B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1346" to="1359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A differentiable recurrent surface for asynchronous event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Romanoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Matteucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning an event sequence embedding for dense event-based deep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stepan</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1527" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-to-end learning of representations for asynchronous event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5633" to="5643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A qvga 143 db dynamic range frame-free pwm image sensor with lossless pixel-level video compression and time-domain cds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Matolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Wohlgenannt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Design of an rgbw color vga rolling and global shutter dynamic and active-pixel vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Brandli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="718" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipp</forename><surname>Akopyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sawada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Alvarez-Icaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Merolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabil</forename><surname>Imam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallab</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gi-Joon</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on computer-aided design of integrated circuits and systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1537" to="1557" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Loihi: A neuromorphic manycore processor with on-chip learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayan</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Han</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautham</forename><surname>Chinya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Sri Harsha Choday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Dimou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabil</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shweta</forename><surname>Imam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="99" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Automated vehicles and pedestrian safety: exploring the promise and limits of pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tabitha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">S</forename><surname>Combs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noreen</forename><forename type="middle">C</forename><surname>Michael P Clamann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of preventive medicine</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A literature review of iot energy platforms aimed at end users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Miguel M Martín-Lopo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Álvaro</forename><surname>Boal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sánchez-Miralles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="page">107101</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Low-power neuromorphic hardware for signal processing applications: A review of architectural and system-level design approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Bipin Rajendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayan</forename><surname>Schmuker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eleftheriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="97" to="110" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Scaling, low power needed for a neuromorphic future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Stewart</surname></persName>
		</author>
		<ptr target="https://www.eetimes.com/scaling-low-power-needed-for-a-neuromorphic-future/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mitigating unwanted biases with adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hu Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2018 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="335" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Certified robustness to adversarial examples with differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lecuyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaggelis</forename><surname>Atlidakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Geambasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Jana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="656" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Attribution-driven causal analysis for detection of adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susmit</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunny</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">Lawrence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Kumar Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunjan</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Jalaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05821</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Peernets: Exploiting peer wisdom against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00088</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Trends and issues in safe driver assistance systems: Driver acceptance and assistance for elderly drivers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadayuki</forename><surname>Tsugawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IATSS research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6" to="18" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Self-driving car dilemmas reveal that moral choices are not universal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Maxmen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">562</biblScope>
			<biblScope unit="issue">7728</biblScope>
			<biblScope unit="page" from="469" to="469" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Goldman sachs analysis of autonomous vehicle job loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anita</forename><surname>Balakrishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">The global expansion of ai surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Feldstein</surname></persName>
		</author>
		<imprint>
			<publisher>Carnegie Endowment</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Artificial intelligence and the future of warfare. Chatham House for the Royal Institute of International Affairs London</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Missy</forename><surname>Cummings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Military artificial intelligence can be easily and dangerously fooled. MIT Techonology Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Knight</surname></persName>
		</author>
		<ptr target="https://www.technologyreview.com/2019/10/21/132277/military-artificial-intelligence-can-be-easily-and-dangerously-fooled/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Machine learning for high-speed corner detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Asynchronous frameless event-based optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryad</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sio-Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiara</forename><surname>Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandyam</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="32" to="37" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
