<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Restoration Using Deep Regulated Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyiyang</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">El</forename><surname>Basha</surname></persName>
							<email>mdelbasha@ufl.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">D</forename></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruogu</forename><surname>Fang</surname></persName>
							<email>ruogu.fang@bme.ufl.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Restoration Using Deep Regulated Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While the depth of convolutional neural networks has attracted substantial attention in the deep learning research, the width of these networks has recently received greater interest <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16]</ref>. The width of networks, defined as the size of the receptive fields and the density of the channels, has demonstrated crucial importance in lowlevel vision tasks such as image denoising and restoration <ref type="bibr" target="#b15">[16]</ref>. However, the limited generalization ability, due to increased width of networks, creates a bottleneck in designing wider networks. In this paper we propose Deep Regulated Convolutional Network (RC-Net), a deep network composed of regulated sub-network blocks cascaded by skip-connections, to overcome this bottleneck. Specifically, the Regulated Convolution block (RC-block), featured by a combination of large and small convolution filters, balances the effectiveness of prominent feature extraction and the generalization ability of the network. RC-Nets have several compelling advantages: they embrace diversified features through large-small filter combinations, alleviate the hazy boundary and blurred details in image denoising and super-resolution problems, and stabilize the learning process. Our proposed RC-Nets outperform state-ofthe-art approaches with large performance gains in various image restoration tasks while demonstrating promising generalization ability. The code is available at https: //github.com/cswin/RC-Nets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image restoration aims at recovering a high-quality image from a corrupted one. In particular, image denoising and single image super-resolution (SR) are two of the most important tasks. Recent works in both image denoising and SR consist mainly of learning-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref>, which learn mapping between <ref type="bibr">Figure 1</ref>. A regulated convolution block with 4 composite units (dotted boxes). The first and third group-squares present 1 × 1 convolution; the second and last ones indicate large and small convolution, respectively. The large convolution is regulated by the small one. spaces of the corrupted images and the high-quality images. Among them, Wider Inference Networks (WIN) <ref type="bibr" target="#b15">[16]</ref> have demonstrated substantial performance gain in additive white Gaussian noise denoising. By adopting large reception fields (filters) and dense channels in convolutional neural networks (CNNs), WIN can accurately exploit prominent image features to infer high-quality images.</p><p>However, wider networks such as WIN suffer from low generalization ability. For example, WIN only achieves superior performance in additive Gaussian noise removal with mediocre performance on image SR. To overcome this critical constraint, we investigated the network architecture and found three inherent limitations that constrain its general- ization capability. First, large convolution filters introduce bias. While large convolution filters (e.g. 128 × 7 × 7) improve the performance of extracting prominent local features (corners and edge/color conjunctions) <ref type="bibr" target="#b24">[25]</ref>, they simultaneously introduce bias in the network to learn specific features of similar pixel distributions <ref type="bibr" target="#b15">[16]</ref>. The introduced "bias" can boost performance significantly in one single task such as Gaussian denoising, but will degrade the performance in other tasks, such as image SR, deblurring and inpainting. This is one of the reasons that most learning-based image restoration methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18]</ref> use smaller convolution filters (e.g. 3 × 3).</p><p>Second, large convolution filters lead to significant performance fluctuations in learning (see <ref type="figure">Fig. 1</ref>). The larger the convolution filters, the greater the feature variance will be, especially when noisy level is high in the training images. Small convolution filters (e.g. 3 × 3) tend to capture subtle features favoring shared weights <ref type="bibr" target="#b20">[21]</ref>. This instability issue limits the filter size in learning-based image restoration methods <ref type="bibr" target="#b17">[18]</ref>.</p><p>Third, large convolution filters require expensive computation. As convolution operations dominate the computation times in CNNs <ref type="bibr" target="#b6">[7]</ref>, small filter sizes allow networks go deeper without significantly increasing the computational cost. Most learning-based image restoration methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref> are designed to be very deep (20 to 30 layers) by embedding 3 × 3 filters only, as the generalization capability relies on the network's depth.</p><p>From the three limitations above, our focus is concentrating on how to balance between the generalizability and performance.</p><p>In this work, we investigate a scalable network structure for image restoration with both generalizability and high performance. Inspired by WIN <ref type="bibr" target="#b15">[16]</ref>, we propose a novel cascaded scalable architecture, called Deep Regulated Con-volution Network (RC-Net) ( <ref type="figure">Fig. 3</ref>), which comprises of regulated sub-network blocks ( <ref type="figure">Fig. )</ref> cascaded by shortcut connections <ref type="bibr" target="#b9">[10]</ref>, to handle both image denoising and SR.</p><p>Our proposed RC-Net addresses the aforementioned three limitations. The key contribution is to take advantages of fusing both large and small filters, by regulating the large convolution filters by small filters. This combination of varying filter sizes can both extract the prominent image features and improve the network's generalizability at reasonable computational cost.</p><p>We present comprehensive experiments on both image denoising and SR to evaluate our networks. We show that 1) Our deep regulated convolutional networks can increase accuracy via regulated convolution, producing results substantially better than previous networks (DnCNN <ref type="bibr" target="#b26">[27]</ref>, RED-Net <ref type="bibr" target="#b17">[18]</ref>, and WIN <ref type="bibr" target="#b15">[16]</ref>) on image denoising; 2) Our deep regulated convolutional networks can be generalized to deal with single image SR problems, producing impressive results of high-resolution images that are more appealing for human vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Learning for Image Restoration. Recently, deep learning based methods have shown impressive performance on both high-level <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10]</ref> and low-level <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4]</ref> vision research fields, compared to the non-CNN based models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12]</ref>. "Deeper is better" has been considered as a design criteria of building convolutional neural networks (CNN). The preference of deep and thin CNNs stem comes from the success of deep networks in high-level vision <ref type="bibr" target="#b14">[15]</ref>. Deep learning based approaches boost the performance with the cost of increasing complexity and network depth.</p><p>Despite of having high capability of nonlinear representation, deep neural networks usually encounter potential problems including gradient vanishing, over-fitting, degradation, and high inference cost. Diverse techniques have been utilized to cope with these obstacles; RED-NET <ref type="bibr" target="#b17">[18]</ref> employs a number of skip-connections (shortcuts) to link convolution layers with mirrored deconvolution layers; VDSR <ref type="bibr" target="#b13">[14]</ref> adopts very small (3 × 3) convolution filters to restrict the amount of parameters and exploits a skip-connection to carry the input to the end layer and reconstruct residuals, both of which contribute to speeding up training and reducing the degradation issue; DnCNN accelerates training by assembling batch normalization layers (BN) <ref type="bibr" target="#b12">[13]</ref> and 3 × 3 filters in every convolution layer, produces the residual between high-quality and corrupted images.</p><p>Effective Feature Extraction and Learning. WIN <ref type="bibr" target="#b15">[16]</ref> is built with a competitive shallow network. which is naturally superior in terms of training efficiency. WIN is   <ref type="figure">Figure 3</ref>. A deep RC-Net with four RC-blocks. In RC-blocks, the largest, medium and smallest size of circles denote a composite unit using large, small, and 1 × 1 convolution filters, respectively. The represents a summation computing and indicates residual learning.</p><p>stacked with 4 composite layers of three consecutive operations: convolution (Conv) layer with size 128 × 7 × 7, batch normalization (BN) Layer <ref type="bibr" target="#b12">[13]</ref>, and rectified linear unit (ReLU) Layer <ref type="bibr" target="#b19">[20]</ref>. In addition, a skip-connection linking the input with the end layer allows the network to predict a residual image. With the following three key contributions, WIN offers a performance breakthrough in the Gaussian image denoising task: 1) Utilizing highly dense convolution layers composed of large filters to extract prominent local features; 2) Embedding BN layers to learn the mean and variance of feature maps associated with the pixel-distribution of training images. BN is especially helpful when pixel distributions are similar due to the same type of corruption. BN also performs as a memory storage to keep the shareable mean and variance; 3) Linking input to the end layer with a skipconnection can lead to residual learning and carrying shareable information from corrupted images to the end layers, both of which contribute to improving the accuracy of the loss-error calculation via comparing with the ground-truth images.</p><p>However, WIN also suffers from a number of limitations despite of the substantial performance margin. WIN is not able to be extended to other sub-problems of image restoration, such as SR. This is because the difference between low-resolution (LR) images and high-resolution (HR) images is subtle and not merely Gaussian distribution. Furthermore, there are fewer similarities shared by the pixellevel distribution features among the LR images. In this case, in addition to the disadvantages mentioned in Section 1 on large filters, BN layers cannot help but restrain the network's generalizability due to data normalization. This is especially important to SR because the mapping between a subtle residual from the normalized data is no longer easier than mapping from the original data.</p><p>Convolution is the primary operation in CNNs. The number and size of filters determine the type of features extracted and the computational cost. Herein, design of the convolution filter structures is a key component of developing deep CNN models to optimize the performance.</p><p>However, most deep models primarily adopt 3 × 3 convolution filters and interpolate using 1 × 1 convolution filters to reduce the number of feature-maps, such as bottleneck blocks used in ResNet <ref type="bibr" target="#b9">[10]</ref> and DenseNet <ref type="bibr" target="#b10">[11]</ref>. To challenge the current trend, we propose to regulate the large filters with smaller ones to achieve a balance between performance generalizability, which remarkably outperform the state-of-the-art using only small filters and bottleneck connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Regulating Convolution Nets</head><p>Consider a corrupted image x 0 is passed through a convolutional network. The network intends to learn a mapping function F between the corrupted image x 0 and a noise-free image y. The convolutional network contains L convolution layers (Conv), each of which implements a feature extraction transition:</p><formula xml:id="formula_0">y l = Conv(x l , f l , n l , c l )<label>(1)</label></formula><p>where l indexes the layer, x l denotes the l's input, and f l , n l , and c l represent the filter size, filter number, and channel number, respectively. y l are the feature-maps extracted from x l by Conv(·).</p><p>As the top and bottom layers have different functional attentions <ref type="bibr" target="#b24">[25]</ref>, the network can be decomposed into three parts (see <ref type="figure">Fig. 3</ref>): densely convolved feature extraction, feature regulation and mapping, and image reconstruction.</p><p>Densely convolved feature extraction. We use a considerable amount of large filters in the first two <ref type="bibr" target="#b24">[25]</ref> convolutional layers densely convolutional feature-extraction layers to extract diverse and representative features for feature mapping and spatial transformation. This part is similar to WIN, but WIN adopts dense convolutions in all layers and does not distinguish between the hierarchical characteristics of the bottom-up layers. Based on the empirical results of WIN, we define densely convolved features extracted from the l th layer as:</p><formula xml:id="formula_1">y l = Conv(x l , f l , n l , c l ) f 7×7,n 128<label>(2)</label></formula><p>Composite unit. Motivated by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5]</ref>, we combine a convolution (Conv) layer, a batch normalization (BN) layer <ref type="bibr" target="#b12">[13]</ref>, and a Parametric Rectified Linear Unit (PReLU) layer <ref type="bibr" target="#b7">[8]</ref> as a composite unit in our proposed RC-Net, except for the last layer, which is a single deconvolutional layer. Following <ref type="bibr" target="#b10">[11]</ref>, we define the mapping of the composite unit as a composite function T (·):</p><formula xml:id="formula_2">T (·) = P ReLU (BN (Conv(·))) (3) T l (x l ) = P ReLU (BN (y l−1 ))<label>(4)</label></formula><p>Feature regulation and mapping. We divide the network into multiple regulated convolution RC-blocks, which are cascaded to perform feature extraction, mapping, and transformation. We assume a RC-Net contains m RC-blocks, each of which comprises of four composite units; large and small convolution filters and two other 1 × 1 convolution filters for reducing the feature map dimension and regulating features extracted from the previous composite or RCblocks. Note, the 1×1 convolution filters cannot change but only exert combing effect to the features generated from the preceding composite.</p><p>The key to a RC-block is to use smaller convolution filters to regulate the feature outputs from the proceeding larger filters. This regulation processing aims to balance the feature extraction so that the features generated from the larger filters can fuse information from both the larger receptive fields and the smaller ones for finer details.</p><p>To ensure a RC-block having sufficient representative capability for feature mapping, we also suggest the composite unit embedding larger filters in each RC-block to adopt n n d 2 filters with size of f f d × f d , where n d and f d denote the filter number and the filter size in densely convolutional feature-extraction layers, respectively.</p><p>Residual learning. Without residual learning, traditional convolutional networks pass the output of the l th layer as the input to the (l + 1) th layer <ref type="bibr" target="#b14">[15]</ref>. If l represents a composite unit, there is a transition: y l = T l (y l−1 ). Residual learning <ref type="bibr" target="#b9">[10]</ref> can adopt a skip-connection (shortcut) to perform an identity mapping:</p><formula xml:id="formula_3">y l = T l (y l−1 ) + y l−1<label>(5)</label></formula><p>RC-Net adopts residual learning in three different locations.</p><p>Within each RC-block, there is a skip-connection linking the output of larger filters to the end of the block to fuse the information generated from the smaller filters and forms residual learning as:</p><formula xml:id="formula_4">y l fs = T l fs y l f l + y l f l<label>(6)</label></formula><p>where l fs and l f l denote the layer using small and large filters, respectively. The second residual learning is performed by another skip-connection, which connects the outputs of two adjacent RC-blocks or connects the first RCblock and the composite unit that precedes it. Motivated by <ref type="bibr" target="#b13">[14]</ref>, the third residual learning is achieved by adding the corrupted image to the output of the last layer, which is a deconvolutional layer that converts the preceding featuremaps to an image.</p><p>Scale controlling. To make RC-Nets more compact, motivated by <ref type="bibr" target="#b4">[5]</ref>, we introduce two 1 × 1 composite units as transition layers, refereed as "Shrinking" and "Expanding", which are shown on <ref type="figure">Figure .</ref> After densely convolutional feature-extraction layers, we reduce the number of featuremaps by "Shrinking", in which, inspired by <ref type="bibr" target="#b16">[17]</ref>, we suggest to adopt n n d 2 filters to remain sufficient features for following RC-blocks, where n d denotes the filter number in densely convolutional feature-extraction layers.</p><p>After feature regulation and mapping, we use another transition layers to expand feature-maps so that there are sufficient various features that can be provided for image reconstruction.</p><p>Image reconstruction. RC-Nets reconstruct a noise-free image y by adding the input (corrupted) image x 0 to the one generated from the RC-Net's last layer: a deconvolutional layer, which can be denoted as: Deconv(·). Thus we can define the image reconstruction as:</p><formula xml:id="formula_5">y = x 0 + Deconv(x, f, n, c) s,n=1<label>(7)</label></formula><p>where s, n = 1 represents one small filters used to inverse the preceding feature-maps finely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>We perform experiments on both image denoising and super-resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Image Denoising. The training data for image denoising consists of 91 images from Yang et. al. <ref type="bibr" target="#b22">[23]</ref>.and an additional 200 images from the Berkeley Segmentation Dataset BSD200. We add Gaussian noise at four different levels (σ = 10, 30, 50, 70) to train the RC model separately for each noise level. Noise is added using the randn function in MATLAB. We train other comparing methods using the same dataset of 291 images. To keep consistent with the original paper <ref type="bibr" target="#b26">[27]</ref>, DnCNN follows <ref type="bibr" target="#b0">[1]</ref> to use 400 images of size 180 × 180 for training.</p><p>To evaluate the denoising performance of each method, we use the BSD200 test dataset and Set12. Due to various versions of Set12, we resized images of the 12 standard image to be 481 × 321, as the same size as the majority of images in the BSD200 test dataset. Both datasets are used in the image denoising experiment in <ref type="table" target="#tab_2">Table 2 and Table 3</ref>.</p><p>Single Image Super-Resolution. We use the same dataset of 291 images at three different scale factors (×2, ×3, ×4 ) to train the RC-Net model separately. WIN and VDSR also share the same training dataset "291". SR-CNN <ref type="bibr" target="#b3">[4]</ref> uses a large ImageNet dataset <ref type="bibr" target="#b2">[3]</ref> as in its original paper <ref type="bibr" target="#b3">[4]</ref>. For evaluation, two datasets are used: BSD200 <ref type="bibr" target="#b18">[19]</ref> from the Berkeley Segmentation Dataset <ref type="bibr" target="#b18">[19]</ref> and Set14 <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>Our RC-Net has four RC-blocks for all datasets. RC-Net has a depth of 21 layers, where 20 of them are composite units (see Section 3). The remaining layer is a deconvolutional layer (Deconv). To avoid the "dead features" <ref type="bibr" target="#b24">[25]</ref> issue in ReLU <ref type="bibr" target="#b8">[9]</ref>, we adopt the Parametric Rectified Linear Unit (PReLU) <ref type="bibr" target="#b8">[9]</ref> for the activation function after each bach normalization layer (BN) <ref type="bibr" target="#b12">[13]</ref>.</p><p>All models adopt Stochastic Gradient Descent (SGD) with mini-batch size of 64 for image denoising and 128 for SR. For model optimization, we introduced the BN layer to reduce the internal covariate shift leading to an accelerated learning speed. For weight initialization, we follow the method described in <ref type="bibr" target="#b8">[9]</ref>. This is especially suitable for the networks adopting ReLU or PReLU.</p><p>All experiments are trained over 50 epochs (5,000 iterations per epoch) totaling 250,000 iterations. Learning rate is initially set to 0.1 and is divided by 10 after every 15 × 10 4 iterations. Weight decay is set to 0.0001 and momentum to 0.9. We follow the practice in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14]</ref> for both image denoising and SR. Data-augmentation is performed to increase the size and variance of training samples. A 41 × 41 crop with stride 14 is randomly sampled from an image or a horizontally flipped copy to generate image patches for additional training samples. All training is processed on 8 GeForce GTX TITAN Xp GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Image Denoising</head><p>Both quantitative and qualitative comparisons of various denoising and SR methods with our proposed RC-Net are provided. In <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table">Table 3</ref>, we provide quantitative evaluations of five different methods: BM3D <ref type="bibr" target="#b1">[2]</ref>, RED-Net <ref type="bibr" target="#b17">[18]</ref>, DnCNN <ref type="bibr" target="#b26">[27]</ref>, WIN <ref type="bibr" target="#b15">[16]</ref>, and RC-Net. PSNR, SSIM, and run time are used as evaluation criteria.</p><p>From the results, our proposed RC-Net method significantly outperforms BM3D, RED-Net, and DnCNN on both datasets. When compared with WIN, our RC-Net method has relatively similar performance when noise level is 30. For example, on the dataset BSD200, our RC-Net method generates a PSNR of 33.57 dB while WIN generates 33.62 dB. While RC-Net and WIN have extremely close PSNR, RC-Net still outperforms WIN in terms of SSIM. With the   We illustrate visual comparisons of four different methods: RED-Net, DnCNN, WIN, and RC-Net in <ref type="figure">Figure 4</ref> and <ref type="figure">Figure 5</ref> with sample images from BSD200 dataset. In <ref type="figure">Figure 4</ref>, our RC-Net method excels in maintaining the detailed particle structure on the buildings surface while other methods fail to keep this important texture information. <ref type="figure">Figure 5</ref> is a challenging image due to a great quantity of plants with the same color and the high noise level. In this challenging case, our method still excels in restoring the pillars texture features, as well as clear contours and details of the chair and branches.</p><p>The visual results show a well match with quantitative results. Our method has improved performance compared to the state-of-the-art image denoising approaches, especially when the noise level is high. Our RC-Net gains the best SSIM term among all test datasets as well as the best human visual image result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Super-Resolution</head><p>To demonstrate the generalizability of RC-Net, we show illustrations of single image super-resolution using our RC-Net and other state-of-the-art approaches (A+ <ref type="bibr" target="#b21">[22]</ref>, SR-CNN <ref type="bibr" target="#b3">[4]</ref>, VDSR <ref type="bibr" target="#b13">[14]</ref>, and DnCNN <ref type="bibr" target="#b26">[27]</ref>) in <ref type="figure">Figure 6</ref> and <ref type="figure">Figure 7</ref>. When we scale an image with 2× factor as shown in <ref type="figure">Figure 6</ref>, it is obvious that our method obtains superior eyes texture and boundary of baboon compared to the stateof-the-art methods. In <ref type="figure">Figure 7</ref>, our method has demonstrated notable capability to maintain the image details in SR by capturing the wrinkle and eyebrow texture while the comparing state-of-the-art methods, which tend to lose the texture information and result in a blurred image.</p><p>Furthermore, most existing methods use multiple networks to handle different scale factors in image SR, while one general network that can perform SR at various scale factors is preferred. <ref type="figure" target="#fig_5">Figure 8</ref> exhibits the ability of a single RC network (RC-blind) to perform SR at different scale factors. We compare the performance of recovered images at multiple scales using RC-blind (top) with VDSR (bottom). From <ref type="figure" target="#fig_5">Figure 8</ref>, we can observe that while both methods can handle different scale factors, our proposed RC-Net outperforms VDSR in terms of clarity and sharpness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Ostensibly, the specific structure design on RC-Nets (Table 3) that differs from other deep convolutional networks merely in the cascaded RC-Blocks, which include largesmall filter combination for regulation <ref type="figure">(Figure )</ref>. However, this small modification leads to a remarkable image restoration performance of the networks.</p><p>Diversified Feature Representation. The most direct consequence of introducing RC-Blocks is efficiency in diversified feature extraction . <ref type="table">Table 3</ref> shows the comparison between RC-Nets and WIN, which is a simplified structure of RC-Net without RC-blocks but more parameters. It is apparent that RC-Nets outperforms WIN even with with a much smaller parameter number (1.8 M vs. 2.4 M). Even though the performance degrades a little after we remove one RC-block, RC-3-Blocks Nets with only 1.6 M parameters still outperforms state-of-the-art methods and similar to WIN. We also further remove one Composite Unit (Table 3 and shows that with a minor degradation of performance, the parameters number decrease from 1.8 M to 1.0 M. These variations and comparisons demonstrate the effectiveness of our RC-Block in feature utilization and the substantial potential for a more compact network.</p><p>Handling Training Fluctuation. There is connection between filter sizes and loss-error curve as shown in <ref type="figure">Figure 1</ref>. WIN shows strong fluctuating even in platform period. However, RC-Nets substantially improve the stability in the loss error during the training process and outperforms WIN after around 15×10 4 iterations when the entire dataset has been traversed.</p><p>Enhanced Generalizability. Cascaded RC-Blocks overcomes the limitation in generalizability of merely using dense convolutional layers in WIN. RC-Nets show superb visual results in single image super-resolution ( <ref type="figure">Figure 6</ref> and <ref type="figure">Figure 7</ref>). The collaborative effect large-small filter combination in RC-Blocks encourages feature extraction with detail preservation as both dominant and detailed features are collected by large and small filters. The overall performance of RC-Nets in both image restoration indicates the enhanced generalization ability with cascaded and specially RC-Blocks. It also implies the great potential of our proposed RC-Nets in solving broader image restoration tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a new convolutional network architecture, referred as Deep Regulated Convolutional Network (RC-Net), for image restoration. It introduces large-small filter combinations (RC-Block) where small convolution filters regulate the features extracted by the large ones. In our experiments, we demonstrated that RC-Nets achieved state-of-the-art performance across several highly competitive datasets for multiple image restoration tasks. Moreover, RC-Nets need substantially fewer parameters and less computational cost to outperform state-of-the-art approaches with large margins, with appealing performance in structural information preservation and generating results appealing for the human visual systems. Owing to the regu-   The advantages of RC-Net lie in the balance between large and small convolutional filters, a simple combination that naturally integrates diversified feature extraction, sta-bility in the training process, and enhanced generalizability. This design allows regularized diversified feature representation throughout the networks, which consequently yielding more efficient and compact models with highly competitive image restoration performance. Due to their efficient and compact model for feature representation, we plan to investigate the generalization ability of RC-Nets to various computer vision tasks that build on convolutional neutral networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shortcomings of using Batch Normalization</head><p>A RC-Net using batch normalization (BN) does not perform as well in Super-resolution (SR) as it does in the Gaussian denoising task. On one hand, following the feature extraction in the regulated convolution layers, BN is able to enhance the high-frequency features (lines, edges, corners, etc.) by summarizing mean and variance of featuremaps during training into two learned parameters. The enhanced features benefit both image denoising and single image SR. On the other hand, for Gaussian denoising, because the noisy images fed into networks follow Gaussian pixeldistribution, BN normalizes the information flow and keeps the distribution of each layer's inputs the same as the original input (noisy images) during training. Nevertheless, for SR, the information processed by BN is shifted and largely different from the original input (low-resolution images). This gives rise to negative effects on SR when the predicated high-resolution image is generated from the summation of the shifted signals and the original one through a skip-connection linking the input to the end of the last layer. The high-frequency features still can be maintained but the low-frequency features are latently lost during shifting and normalization. This is the reason why the visual results from RC-Nets for SR are better to the human vision system but PSNR and SSIM are less than the most competitive state-of-the-art methods.</p><p>We may overcome this drawback in three potential ways: (1) Follow the work of DnCNN by directly predicting the residual between the corrupted image and the noisefree one. In this case, we remove the input-to-end skipconnection and BN layers in the first and last layers. <ref type="bibr" target="#b1">(2)</ref> Simply remove all BN layers from RC-Nets; (3) Dynamically route the training and testing to a specific part of a RC-Net based on the property of an input. <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref> are simper and may impact the denoising performance. (3) needs an additional learning process to allow the network knows how and where to route the information. We consider (3) as a future work and implemented <ref type="bibr" target="#b1">(2)</ref>. The results for SR (see B) demonstrate that BN indeed has the negative effects on SR, and that RC-Nets without BN achieve competitive PSNR and SSIM with state-of-the-art methods. Code and models are available at https://github. com/cswin/RC-Nets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Results</head><p>More visual results are shown below to demonstrate the competitive performance of RC-Nets for image restoration. We illustrate image results on datasets BSD200-test for image denoising with noise level σ = 30 ( <ref type="figure">Figure 9</ref>) and 50 ( <ref type="figure">Figure 10</ref>). While RC-Nets outperform all competing methods, they perform exceptionally well in complex images and at high noise level. Our method reconstructs well the shape and edges of left eye in <ref type="figure">Figure 9</ref> as presented in the ROI. Similarly, in <ref type="figure">Figure 10</ref>, contours are clean and visually discernible. <ref type="figure">Figure 11</ref> and Figure12 are visual results of single image SR with scale factor ×3. We removed all BN layers of a RC-Net and call this network as "RC-Net without BN", while the original RC-Net keeping all BN layers is called "RC-Net with BN". <ref type="figure">Figure 11</ref> shows an image from dataset Set5. The appealing human visual results showing sharp and bright hair recovered by RC-Net with BN. However, removing BN layers leads a better quantitative evaluation in terms of PSNR and SSIM, and comparable performance to the state-of-the-art methods VDSR and DnCNN. We illustrate one more SR result from the dataset BSD200-test shown in <ref type="figure" target="#fig_0">Figure 12</ref>. Simiarly, RC-Net with BN restores sharp edges, while RC-Net without BN network performs better in terms of PSNR and SSIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Discussions</head><p>The effect of Batch Normalization layer. Normally, BN layers are introduced into the network for reducing internal covariate shift and then accelerating training speed. However, BN layers are preseted as a regulator in our RC-Nets by potentially storing and enhancing high-frequency features. This property helps with restoring images to appear visually close to ground-truth. RC-Nets with BN layer performs remarkably well in the denoising task. In the SR task, RC-Nets with BN provides nice human visual results while RC-Nets without BN achieve excellent quantitative numbers.</p><p>The properties of large-small filter combination. Indeed, the large-small filter combination introduced in our RC-Nets fundamentally strengthens our model in image restoration. High-frequency features collected by large filters provide major features while details collected by small filters help to regulate the learned feature maps. After removing BN layers, the competitive PSNR and SSIM values demonstrate that our large-small filter combination can enhance RC-Nets' generalization ability both visually and quantitatively.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of validation error of RC-Nets with different number of blocks, a RC-Net having the 2 nd layer removed, and WIN during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Visual results of one image from BSD200-test with σ = 10 along with PSNR(dB) / SSIM. Visual results of one image from BSD200-test with σ = 70 along with PSNR(dB) / SSIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Visual results of one image from Set14 with scale factor x2 along with PSNR(dB) / SSIM. Visual results of one image from B100 with scale factor x3 along with PSNR(dB) / SSIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Visual results of single network for multiple scale factors. ROIs are selected and enlarged on the right. Top: Restoration results within RC-blind network, RC-blind is trained on scale factor = ×2, ×3, and ×4. Bottom: SR results generated by VDSR lated convolutions, RC-Nets can balance the feature extraction and generalizability by fusing information from both the representative features extracted from the larger receptive fields and the smaller textures obtained from finer de-tails.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Image Denoising: Visual results of one image from BSD200-test with σ = 30 along with PSNR(dB) / SSIM. Compares to state-of-the-art method, our RC-Net well recovers facial features of kids, especially the eyes are almost identical to the ground truth while state-of-the-art methods lose contours and details because of the high noise level. Image Denoising: Visual results of one image from BSD200-test with noise level of σ = 50 along with PSNR(dB) / SSIM. Horn and ears of rhinoceros are accurately reproduced using our RC-Net method. Bushes an woods in the background are discernible instead of the blurred restoration from other state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of structure detail of RC-Net; RC-3-Blocks which removes one RC-Block from RC-Net; RC-2nd-Layer-Removed which removes Composite unit(2) from RC-Net; WIN consists of three parts as shown in table. number of parameters calculate the total parameter number of network.Comparison is based on image denoising performance, average PSNR (dB) / SSIM / Run Time (s), are evaluated on the BSD200-test with noise level σ = 50.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The average results of PSNR (dB) / SSIM / Run Time (seconds) of different methods on the BSD200-test [19] (200 images).All methods are applied on several noise level(σ = 10, 30, 50, 70).The best results are highlighted in bold. .9182/ 1.01 32.96/ 0.8963/ 60.73 34.60/ 0.9283/ 13.49 35.83/ 0.9494/ 28.72 36.36/ 0.9541/ 32.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>PSNR (dB) / SSIM / Time (s)</cell><cell></cell><cell></cell></row><row><cell>σ</cell><cell>BM3D [2]</cell><cell>RED-Net [18]</cell><cell>DnCNN [27]</cell><cell>WIN [16]</cell><cell>RC-Net</cell></row><row><cell cols="6">10 34.02/ 055</cell></row><row><cell cols="6">30 28.57/ 0.7823/ 1.23 29.05/ 0.8049/ 60.27 29.13/ 0.8060/ 13.48 33.62/ 0.9193/ 31.94 33.57/ 0.9271/ 33.52</cell></row><row><cell cols="6">50 26.44/ 0.7028/ 2.02 26.88/ 0.7230/ 66.62 26.99/ 0.7289/ 12.55 31.79/ 0.8831/ 23.77 32.48/ 0.9112/ 33.52</cell></row><row><cell cols="6">70 25.23/ 0.6522/ 1.98 26.66/ 0.7108/ 66.99 25.65/ 0.6709/ 13.42 30.34/ 0.8362/ 23.90 31.17/ 0.8795/ 32.81</cell></row><row><cell cols="6">Table 3. The average PSNR(dB) / SSIM / Run Time (seconds) of different methods on the resized 12 standard testset (12 images). All</cell></row><row><cell cols="5">methods are applied on several noise level(σ = 10, 30, 50, 70). The best results are highlighted in bold.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>PSNR (dB) / SSIM / Time (s)</cell><cell></cell><cell></cell></row><row><cell>σ</cell><cell>BM3D [2]</cell><cell>RED-Net [18]</cell><cell>DnCNN [27]</cell><cell>WIN [16]</cell><cell>RC-Net</cell></row><row><cell cols="6">10 34.28/ 0.9208/ 0.91 30.48/ 0.8610/ 61.89 33.30/ 0.9096/ 12.79 37.24/ 0.9546/ 12.03 38.54/ 0.9627/ 29.56</cell></row><row><cell cols="6">30 29.09/ 0.8287/ 0.99 30.48/ 0.8610/ 62.71 28.19/ 0.8151/ 12.52 34.89/ 0.9162/ 12.77 35.18/ 0.9409/ 29.50</cell></row><row><cell cols="6">50 25.23/ 0.6522/ 1.98 28.03/ 0.7988/ 63.66 25.85/ 0.7522/ 12.48 32.99/ 0.8825/ 12.37 34.55/ 0.9270/ 30.44</cell></row><row><cell cols="6">70 25.20/ 0.7149/ 1.75 27.95/ 0.7950/ 63.20 23.75/ 0.6890/ 12.41 30.91/ 0.8263/ 11.75 32.87/ 0.8961/ 30.43</cell></row><row><cell></cell><cell>(a) Ground-truth</cell><cell></cell><cell>(b) Noise = 10 / 26.94 dB / 0.8718</cell><cell></cell><cell>(c) RED-Net / 31.19 dB / 0.8958</cell></row><row><cell></cell><cell>(d) DnCNN / 32.32 dB / 0.9161</cell><cell></cell><cell>(e) WIN / 33.90 dB / 0.9455</cell><cell></cell><cell>(f) RC-Net / 34.34 dB / 0.9505</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>increased noise level, the denoising results of RC-Net surpasses WIN remarkably. For example, on Set12, RC-Net generates an average PSNR of 32.87 dB, while WIN produces a PSNR of 30.34 dB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Super-resolution: Average PSNR / SSIM / Run Time (seconds on GPU) for scale factor ×2, ×3 and ×4 on datasets Set5, Set14 and B100. Red color indicates the best performance and blue color indicates the second best performance. .9542 / 3.37 37.60 / 0.9591 / 0.13 35.48 / 0.9515 / 0.68 37.42 / 0.9586 / 0.46 ×3 32.67 / 0.9086 / 3.35 33.54 / 0.9207 / 0.14 27.47 / 0.7851 / 0.68 33.43 / 0.9191 / 0.44 ×4 30.37 / 0.8620 / 3.30 30.99 / 0.8800 / 0.15 26.06 / 0.7117 / 0.73 31.01 / 0.8775 / 0.47 B100 ×2 31.24 / 0.8881 / 3.81 31.89 / 0.8956 / 0.17 30.61 / 0.8880 / 1.09 31.86 / 0.8959 / 0.52 ×3 28.40 / 0.7869 / 3.74 28.76 / 0.7973 / 0.17 27.47 / 0.7851 / 1.14 28.76 / 0.7966 / 0.49 ×4 26.88 / 0.7114 / 3.32 27.18 / 0.7245 / 0.17 26.06 / 0.7117 / 1.07 27.21 / 0.7242 / 0.53</figDesc><table><row><cell></cell><cell></cell><cell cols="2">PSNR (dB) / SSIM / Time (s)-GPU</cell></row><row><cell cols="2">Dataset scale SRCNN</cell><cell>VDSR</cell><cell>RC-Net with BN</cell><cell>RC-Net without BN</cell></row><row><cell>×2</cell><cell>36.55 / 0</cell><cell></cell><cell></cell></row><row><cell>Set5</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 11</ref><p>. Super-resolution: Visual results of one image from Set 5 with a scale factor of ×3 along with PSNR(dB) / SSIM. ROIs show the improved human visual results using RC-Net with BN by restoring sharp and clean contour of the hair. When comparing PSNR and SSIM, RC-Net without BN outperforms VDSR. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BM3D image denoising with shape-adaptive principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPARS&apos;09-Signal Processing with Adaptive Sparse Structured Representations</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks at constrained time cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5353" to="5360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Densely connected convolutional networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Wide inference network for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05414</idno>
		<idno>abs/1707.05414</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02540</idno>
		<title level="m">The expressive power of neural networks: A view from the width</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Image restoration using convolutional auto-encoders with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08921</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int&apos;l Conf. Computer Vision</title>
		<meeting>8th Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Wide residual networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03981</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
