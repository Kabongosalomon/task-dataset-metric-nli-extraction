<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TSM: Temporal Shift Module for Efficient Video Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
							<email>jilin@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mit</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
							<email>ganchuang@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><forename type="middle">Han</forename><surname>Mit</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TSM: Temporal Shift Module for Efficient Video Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive, making it expensive to deploy. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. Specifically, it can achieve the performance of 3D CNN but maintain 2D CNN's complexity. TSM shifts part of the channels along the temporal dimension; thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. We also extended TSM to online setting, which enables real-time low-latency online video recognition and video object detection. TSM is accurate and efficient: it ranks the first place on the Something-Something leaderboard upon publication; on Jetson Nano and Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video recognition. The code is available at: https://github. com/mit-han-lab/temporal-shift-module.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Hardware-efficient video understanding is an important step towards real-world deployment, both on the cloud and on the edge. For example, there are over 10 5 hours of videos uploaded to YouTube every day to be processed for recommendation and ads ranking; tera-bytes of sensitive videos in hospitals need to be processed locally on edge devices to protect privacy. All these industry applications require both accurate and efficient video understanding.</p><p>Deep learning has become the standard for video understanding over the years <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">58]</ref>. One key difference between video recognition and image recognition is the need for temporal modeling. For example, to distinguish between opening and closing a box, reversing the order will give opposite results, so temporal modeling is critical.   It is computationally free on top of a 2D convolution, but achieves strong temporal modeling ability. TSM efficiently supports both offline and online video recognition. Bi-directional TSM mingles both past and future frames with the current frame, which is suitable for high-throughput offline video recognition. Uni-directional TSM mingles only the past frame with the current frame, which is suitable for low-latency online video recognition.</p><p>Existing efficient video understanding approaches directly use 2D CNN <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b58">58]</ref>. However, 2D CNN on individual frames cannot well model the temporal information. 3D CNNs <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b3">4]</ref> can jointly learn spatial and temporal features but the computation cost is large, making the deployment on edge devices difficult; it cannot be applied to real-time online video recognition. There are works to trade off between temporal modeling and computation, such as post-hoc fusion <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b6">7]</ref> and mid-level temporal fusion <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b45">46]</ref>. Such methods sacrifice the low-level temporal modeling for efficiency, but much of the useful information is lost during the feature extraction before the temporal fusion happens.</p><p>In this paper, we propose a new perspective for efficient temporal modeling in video understanding by proposing a novel Temporal Shift Module (TSM). Concretely, an activation in a video model can be represented as A ∈ R N ×C×T ×H×W , where N is the batch size, C is the number of channels, T is the temporal dimension, H and W are the spatial resolutions. Traditional 2D CNNs operate independently over the dimension T ; thus no temporal modeling takes effects <ref type="figure" target="#fig_2">(Figure 1a</ref>). In contrast, our Temporal Shift Module (TSM) shifts the channels along the temporal dimension, both forward and backward. As shown in <ref type="figure" target="#fig_2">Figure 1b</ref>, the information from neighboring frames is mingled with the current frame after shifting. Our intuition is: the convo-lution operation consists of shift and multiply-accumulate. We shift in the time dimension by ±1 and fold the multiplyaccumulate from time dimension to channel dimension. For real-time online video understanding, future frames can't get shifted to the present, so we use a uni-directional TSM <ref type="figure" target="#fig_2">(Figure 1c</ref>) to perform online video understanding.</p><p>Despite the zero-computation nature of the shift operation, we empirically find that simply adopting the spatial shift strategy <ref type="bibr" target="#b50">[51]</ref> used in image classifications introduces two major issues for video understanding: (1) it is not efficient: shift operation is conceptually zero FLOP but incurs data movement. The additional cost of data movement is non-negligible and will result in latency increase. This phenomenon has been exacerbated in the video networks since they usually have a large memory consumption (5D activation). (2) It is not accurate: shifting too many channels in a network will significantly hurt the spatial modeling ability and result in performance degradation. To tackle the problems, we make two technical contributions. (1) We use a temporal partial shift strategy: instead of shifting all the channels, we shift only a small portion of the channels for efficient temporal fusion. Such strategy significantly cuts down the data movement cost <ref type="figure" target="#fig_3">(Figure 2a</ref>). (2) We insert TSM inside residual branch rather than outside so that the activation of the current frame is preserved, which does not harm the spatial feature learning capability of the 2D CNN backbone.</p><p>The contributions of our paper are summarized as follows:</p><p>• We provide a new perspective for efficient video model design by temporal shift, which is computationally free but has strong spatio-temporal modeling ability.</p><p>• We observed that naive shift cannot achieve high efficiency or high performance. We then proposed two technical modifications partial shift and residual shift to realize a high efficiency model design.</p><p>• We propose bi-directional TSM for offline video understanding that achieves state-of-the-art performance. It ranks the first on Something-Something leaderboard upon publication.</p><p>• We propose uni-directional TSM for online real-time video recognition with strong temporal modeling capacity at low latency on edge devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Video Recognition</head><p>2D CNN. Using the 2D CNN is a straightforward way to conduct video recognition <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref>. For example, Simonyan et al. <ref type="bibr" target="#b38">[39]</ref> designed a two-stream CNN for RGB input (spatial stream) and optical flow <ref type="bibr" target="#b55">[55]</ref> input (temporal stream) respectively. Temporal Segment Networks (TSN) <ref type="bibr" target="#b47">[48]</ref> extracted averaged features from strided sampled frames. Such methods are more efficient compared to 3D counterparts but cannot infer the temporal order or more complicated temporal relationships.</p><p>3D CNN. 3D convolutional neural networks can jointly learn spatio-temporal features. Tran et al. <ref type="bibr" target="#b44">[45]</ref> proposed a 3D CNN based on VGG models, named C3D, to learn spatio-temporal features from a frame sequence. Carreira and Zisserman <ref type="bibr" target="#b3">[4]</ref> proposed to inflate all the 2D convolution filters in an Inception V1 model <ref type="bibr" target="#b42">[43]</ref> into 3D convolutions. However, 3D CNNs are computationally heavy, making the deployment difficult. They also have more parameters than 2D counterparts, thus are more prone to over-fitting. On the other hand, our TSM has the same spatial-temporal modeling ability as 3D CNN while enjoying the same computation and parameters as the 2D CNNs.</p><p>Trade-offs. There have been attempts to trade off expressiveness and computation costs. Lee et al. <ref type="bibr" target="#b26">[27]</ref> proposed a motion filter to generate spatio-temporal features from 2D CNN. Tran et al. <ref type="bibr" target="#b45">[46]</ref> and Xie et al. <ref type="bibr" target="#b52">[53]</ref> proposed to study mixed 2D and 3D networks, either first using 3D and later 2D (bottom-heavy) or first 2D and later 3D (top-heavy) architecture. ECO <ref type="bibr" target="#b61">[61]</ref> also uses a similar top-heavy architecture to achieve a very efficient framework. Another way to save computation is to decompose the 3D convolution into a 2D spatial convolution and a 1D temporal convolution <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>. For mixed 2D-3D CNNs, they still need to remove low-level temporal modeling or high-level temporal modeling. Compared to decomposed convolutions, our method completely removes the computation cost of temporal modeling has enjoys better hardware efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Temporal Modeling</head><p>A direct way for temporal modeling is to use 3D CNN based methods as discussed above. Wang et al. <ref type="bibr" target="#b48">[49]</ref> proposed a spatial-temporal non-local module to capture longrange dependencies. Wang et al. <ref type="bibr" target="#b49">[50]</ref> proposed to represent videos as space-time region graphs. An alternative way to model the temporal relationships is to use 2D CNN + posthoc fusion <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b6">7]</ref>. Some works use LSTM <ref type="bibr" target="#b18">[19]</ref> to aggregate the 2D CNN features <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>. Attention mechanism also proves to be effective for temporal modeling <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref>. Zhou et al. <ref type="bibr" target="#b58">[58]</ref> proposed Temporal Relation Network to learn and reason about temporal dependencies. The former category is computational heavy, while the latter cannot capture the useful low-level information that is lost during feature extraction. Our method offers an efficient solution at the cost of 2D CNNs, while enabling both low-level and high-level temporal modeling, just like 3D-CNN based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Efficient Neural Networks</head><p>The efficiency of 2D CNN has been extensively studied. Some works focused on designing an efficient model <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b56">56]</ref>. Recently neural architecture search <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b30">31]</ref> has been introduced to find an efficient architecture automatically <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b2">3]</ref>. Another way is to prune, quantize and compress an existing model for efficient deployment <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b46">47]</ref>. Address shift, which is a hardware-friendly primitive, has also been exploited for compact 2D CNN design on image recognition tasks <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b57">57]</ref>. Nevertheless, we observe that directly adopting the shift operation on video recognition task neither maintains efficiency nor accuracy, due to the complexity of the video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporal Shift Module (TSM)</head><p>We first explain the intuition behind TSM: data movement and computation can be separated in a convolution. However, we observe that such naive shift operation neither achieves high efficiency nor high performance. To tackle the problem, we propose two techniques minimizing the data movement and increasing the model capacity, which leads to the efficient TSM module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Intuition</head><p>Let us first consider a normal convolution operation. For brevity, we used a 1-D convolution with the kernel size of 3 as an example. Suppose the weight of the convolution is W = (w 1 , w 2 , w 3 ), and the input X is a 1-D vector with infinite length. The convolution operator Y = Conv(W, X) can be written as:</p><formula xml:id="formula_0">Y i = w 1 X i−1 + w 2 X i + w 3 X i+1 .</formula><p>We can decouple the operation of convolution into two steps: shift and multiply-accumulate: we shift the input X by −1, 0, +1 and multiply by w 1 , w 2 , w 3 respectively, which sum up to be Y . Formally, the shift operation is:</p><formula xml:id="formula_1">X −1 i = X i−1 , X 0 i = X i , X +1 i = x i+1<label>(1)</label></formula><p>and the multiply-accumulate operation is:</p><formula xml:id="formula_2">Y = w 1 X −1 + w 2 X 0 + w 3 X +1<label>(2)</label></formula><p>The first step shift can be conducted without any multiplication. While the second step is more computationally expensive, our Temporal Shift module merges the multiplyaccumulate into the following 2D convolution, so it introduces no extra cost compared to 2D CNN based models. The proposed Temporal Shift module is described in Figure 1. In <ref type="figure" target="#fig_2">Figure 1a</ref>, we describe a tensor with C channels and T frames. The features at different time stamps are denoted as different colors in each row. Along the temporal dimension, we shift part of the channels by −1, another part by +1, leaving the rest un-shifted ( <ref type="figure" target="#fig_2">Figure 1b</ref>). For online video recognition setting, we also provide an online version of TSM <ref type="figure" target="#fig_2">(Figure 1c</ref>). In the online setting, we cannot access  future frames, therefore, we only shift from past frames to future frames in a uni-directional fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Naive Shift Does Not Work</head><p>Despite the simple philosophy behind the proposed module, we find that directly applying the spatial shift strategy <ref type="bibr" target="#b50">[51]</ref> to the temporal dimension cannot provide high performance nor efficiency. To be specific, if we shift all or most of the channels, it brings two disasters: (1) Worse efficiency due to large data movement. The shift operation enjoys no computation, but it involves data movement. Data movement increases the memory footprint and inference latency on hardware. Worse still, such effect is exacerbated in the video understanding networks due to large activation size (5D tensor). When using the naive shift strategy shifting every map, we observe a 13.7% increase in CPU latency and 12.4% increase in GPU latency, making the overall inference slow. (2) Performance degradation due to worse spatial modeling ability. By shifting part of the channels to neighboring frames, the information contained in the channels is no longer accessible for the current frame, which may harm the spatial modeling ability of the 2D CNN backbone. We observe a 2.6% accuracy drop when using the naive shift implementation compared to the 2D CNN baseline (TSN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Module Design</head><p>To tackle the two problem from naive shift implementation, we discuss two technical contributions. Reducing Data Movement. To study the effect of data movement, we first measured the inference latency of TSM models and 2D baseline on different hardware devices. We shifted different proportion of the channels and measured the latency. We measured models with ResNet-50 backbone and 8-frame input using no shift (2D baseline), partial shift (1/8, 1/4, 1/2) and all shift (shift all the channels). The timing was measure on server GPU (NVIDIA Tesla P100), mobile GPU (NVIDIA Jetson TX2) and CPU (Intel Xeon E5-2690). We report the average latency from 1000 runs after</p><formula xml:id="formula_3">conv Y Y + X Y + X Y + conv1 shift conv2 v shift conv (a) In-place TSM. shift conv X Y X Y + X Y + X Y + conv1 shift conv2 shift conv shift conv (b) Residual TSM. Figure 3.</formula><p>Residual shift is better than in-place shift. In-place shift happens before a convolution layer (or a residual block). Residual shift fuses temporal information inside a residual branch.</p><p>200 warm-up runs. We show the overhead of the shift operation as the percentage of the original 2D CNN inference time in 2a. We observe the same overhead trend for different devices. If we shift all the channels, the latency overhead takes up to 13.7% of the inference time on CPU, which is definitely non-negligible during inference. On the other hand, if we only shift a small proportion of the channels, e.g., 1/8, we can limit the latency overhead to only 3%. Therefore, we use partial shift strategy in our TSM implementation to significantly bring down the memory movement cost.</p><p>Keeping Spatial Feature Learning Capacity. We need to balance the model capacity for spatial feature learning and temporal feature learning. A straight-forward way to apply TSM is to insert it before each convolutional layer or residual block, as illustrated in <ref type="figure">Figure 3a</ref>. We call such implementation in-place shift. It harms the spatial feature learning capability of the backbone model, especially when we shift a large amount of channels, since the information stored in the shifted channels is lost for the current frame.</p><p>To address such issue, we propose a variant of the shift module. Instead of inserting it in-place, we put the TSM inside the residual branch in a residual block. We denote such version of shift as residual shift as shown in 3b. Residual shift can address the degraded spatial feature learning problem, as all the information in the original activation is still accessible after temporal shift through identity mapping.</p><p>To verify our assumption, we compared the performance of in-place shift and residual shift on Kinetics <ref type="bibr" target="#b24">[25]</ref> dataset. We studied the experiments under different shift proportion setting. The results are shown in 2b. We can see that residual shift achieves better performance than in-place shift for all shift proportion. Even we shift all the channels to neighboring frames, due to the shortcut connection, residual shift still achieves better performance than the 2D baseline. Another finding is that the performance is related to the proportion of shifted channels: if the proportion is too small, the ability of temporal reasoning may not be enough to handle complicated temporal relationships; if too large, the spatial feature learning ability may be hurt. For residual shift, we found that the performance reaches the peak when 1/4 (1/8 for each direction) of the channels are shifted. Therefore, we use this setting for the rest of the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TSM Video Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Offline Models with Bi-directional TSM</head><p>We insert bi-directional TSM to build offline video recognition models. Given a video V , we first sample T frames F i , F 1 , ..., F T from the video. After frame sampling, 2D CNN baselines process each of the frames individually, and the output logits are averaged to give the final prediction. Our proposed TSM model has exactly the same parameters and computation cost as 2D model. During the inference of convolution layers, the frames are still running independently just like the 2D CNNs. The difference is that TSM is inserted for each residual block, which enables temporal information fusion at no computation. For each inserted temporal shift module, the temporal receptive field will be enlarged by 2, as if running a convolution with the kernel size of 3 along the temporal dimension. Therefore, our TSM model has a very large temporal receptive field to conduct highly complicated temporal modeling. In this paper, we used ResNet-50 <ref type="bibr" target="#b16">[17]</ref> as the backbone unless otherwise specified.</p><p>A unique advantage of TSM is that it can easily convert any off-the-shelf 2D CNN model into a pseudo-3D model that can handle both spatial and temporal information, without adding additional computation. Thus the deployment of our framework is hardware friendly: we only need to support the operations in 2D CNNs, which are already welloptimized at both framework level (CuDNN <ref type="bibr" target="#b5">[6]</ref>, MKL-DNN, TVM <ref type="bibr" target="#b4">[5]</ref>) and hardware level (CPU/GPU/TPU/FPGA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Online Models with Uni-directional TSM</head><p>Video understanding from online video streams is important in real-life scenarios. Many real-time applications requires online video recognition with low latency, such as AR/VR and self-driving. In this section, we show that we can adapt TSM to achieve online video recognition while with multi-level temporal fusion.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 1</ref>, offline TSM shifts part of the channels bi-directionally, which requires features from future frames to replace the features in the current frame. If we only shift the feature from previous frames to current frames, we can achieve online recognition with uni-directional TSM.</p><p>The inference graph of uni-directional TSM for online video recognition is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. During inference, for each frame, we save the first 1/8 feature maps of each residual block and cache it in the memory. For the next frame, we replace the first 1/8 of the current feature maps with the cached feature maps. We use the combination of 7/8 current feature maps and 1/8 old feature maps to generate the next layer, and repeat. Using the uni-directional TSM for online video recognition shares several unique advantages:</p><p>1. Low latency inference. For each frame, we only need to replace and cache 1/8 of the features, without incurring any extra computations. Therefore, the latency of giving perframe prediction is almost the same as the 2D CNN baseline. Existing methods like <ref type="bibr" target="#b61">[61]</ref> use multiple frames to give one prediction, which may leads to large latency.</p><p>2. Low memory consumption. Since we only cache a small portion of the features in the memory, the memory consumption is low. For ResNet-50, we only need 0.9MB memory cache to store the intermediate feature.</p><p>3. Multi-level temporal fusion. Most of the online method only enables late temporal fusion after feature extraction like <ref type="bibr" target="#b58">[58]</ref> or mid level temporal fusion <ref type="bibr" target="#b61">[61]</ref>, while our TSM enables all levels of temporal fusion. Through experiments ( <ref type="table" target="#tab_2">Table 2</ref>) we find that multi-level temporal fusion is very important for complex temporal modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We first show that TSM can significantly improve the performance of 2D CNN on video recognition while being computationally free and hardware efficient. It further demonstrated state-of-the-art performance on temporal-related datasets, arriving at a much better accuracy-computation pareto curve. TSM models achieve an order of magnitude speed up in measured GPU throughput compared to conventional I3D model from <ref type="bibr" target="#b49">[50]</ref>. Finally, we leverage unidirectional TSM to conduct low-latency and real-time online prediction on both video recognition and object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Setups</head><p>Training &amp; Testing. We conducted experiments on video action recognition tasks. The training parameters for the Kinetics dataset are: 100 training epochs, initial learning rate 0.01 (decays by 0.1 at epoch 40&amp;80), weight decay 1e-4, batch size 64, and dropout 0.5. For other datasets, we scale the training epochs by half. For most of the datasets, the model is fine-tuned from ImageNet pre-trained weights; while HMDB-51 <ref type="bibr" target="#b25">[26]</ref> and UCF-101 <ref type="bibr" target="#b39">[40]</ref> are too small and prone to over-fitting <ref type="bibr" target="#b47">[48]</ref>, we followed the common practice <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref> to fine-tune from Kinetics <ref type="bibr" target="#b24">[25]</ref> pre-trained weights and freeze the Batch Normalization <ref type="bibr" target="#b21">[22]</ref> layers. For testing, when pursue high accuracy, we followed the common setting in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> to sample multiple clips per video (10 for Kinetics, 2 for others) and use the full resolution image <ref type="table">Table 1</ref>. Our method consistently outperforms 2D counterparts on multiple datasets at zero extra computation (protocol: ResNet-50 8f input, 10 clips for Kinetics, 2 for others, full-resolution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Model Acc1 Acc5 ∆ Acc1 with shorter side 256 for evaluation, so that we can give a direct comparison; when we consider the efficiency (e.g., as in <ref type="table" target="#tab_2">Table 2</ref>), we used just 1 clip per video and the center 224×224 crop for evaluation. We keep the same protocol for the methods compared in the same table.</p><p>Model. To have an apple-to-apple comparison with the state-of-the-art method <ref type="bibr" target="#b49">[50]</ref>, we used the same backbone (ResNet-50) on the dataset ( Something-Something-V1 <ref type="bibr" target="#b13">[14]</ref>).This dataset focuses on temporal modeling. The difference is that <ref type="bibr" target="#b49">[50]</ref> used 3D ResNet-50, while we used 2D ResNet-50 as the backbone to demonstrate efficiency.</p><p>Datasets. Kinetics dataset <ref type="bibr" target="#b24">[25]</ref> is a large-scale action recognition dataset with 400 classes. As pointed in <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b52">53]</ref>, datasets like Something-Something (V1&amp;V2) <ref type="bibr" target="#b13">[14]</ref>, Charades <ref type="bibr" target="#b37">[38]</ref>, and Jester <ref type="bibr" target="#b0">[1]</ref> are more focused on modeling the temporal relationships , while UCF101 <ref type="bibr" target="#b39">[40]</ref>, HMDB51 <ref type="bibr" target="#b25">[26]</ref>, and Kinetics <ref type="bibr" target="#b24">[25]</ref> are less sensitive to temporal relationships. Since TSM focuses on temporal modeling, we mainly focus on datasets with stronger temporal relationships like Something-Something. Nevertheless, we also observed strong results on the other datasets and reported it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Improving 2D CNN Baselines</head><p>We can seamlessly inject TSM into a normal 2D CNN and improve its performance on video recognition. In this section, we demonstrate a 2D CNN baseline can significantly benefit from TSM with double-digits accuracy improvement. We chose TSN <ref type="bibr" target="#b47">[48]</ref> as the 2D CNN baseline. We used the same training and testing protocol for TSN and our TSM. The only difference is with or without TSM.</p><p>Comparing Different Datasets. We compare the results on several action recognition datasets in <ref type="table">Table 1</ref>. The chart is split into two parts. The upper part contains datasets Kinetics <ref type="bibr" target="#b24">[25]</ref>, UCF101 <ref type="bibr" target="#b39">[40]</ref>, HMDB51 <ref type="bibr" target="#b25">[26]</ref>, where temporal relationships are less important, while our TSM still consistently outperforms the 2D TSN baseline at no extra computation. For the lower part, we present the results on Something-Something V1 and V2 <ref type="bibr" target="#b13">[14]</ref> and Jester <ref type="bibr" target="#b0">[1]</ref>, which depend heavily on temporal relationships. 2D CNN baseline cannot achieve a good accuracy, but once equipped with TSM, the performance improved by double digits.</p><p>Scaling over Backbones. TSM scales well to backbones of different sizes. We show the Kinetics top-1 accuracy with MobileNet-V2 <ref type="bibr" target="#b35">[36]</ref>, ResNet-50 <ref type="bibr" target="#b16">[17]</ref>, ResNext-101 <ref type="bibr" target="#b51">[52]</ref> and ResNet-50 + Non-local module <ref type="bibr" target="#b48">[49]</ref> backbones in <ref type="table">Table 3</ref>. TSM consistently improves the accuracy over different backbones, even for NL R-50, which already has temporal modeling ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State-of-the-Arts</head><p>TSM not only significantly improves the 2D baseline but also outperforms state-of-the-art methods, which heavily rely on 3D convolutions. We compared the performance of our TSM model with state-of-the-art methods on both Something-Something V1&amp;V2 because these two datasets focus on temporal modeling. <ref type="bibr" target="#b0">1</ref> We reported the performance of NL I3D described in <ref type="bibr" target="#b49">[50]</ref>, which is a variant of the original NL I3D <ref type="bibr" target="#b48">[49]</ref>. It uses fewer temporal dimension pooling to achieve good performance, but also incur larger computation. <ref type="bibr" target="#b1">2</ref> Includes parameters and FLOPs of the Region Proposal Network.</p><p>Something-Something-V1. Something-Something-V1 is a challenging dataset, as activity cannot be inferred merely from individual frames (e.g., pushing something from right to left). We compared TSM with current state-of-the-art methods in <ref type="table" target="#tab_2">Table 2</ref>. We only applied center crop during testing to ensure the efficiency unless otherwise specified. TSM achieves the first place on the leaderboard upon publication. We first show the results of the 2D based methods TSN <ref type="bibr" target="#b47">[48]</ref> and TRN <ref type="bibr" target="#b58">[58]</ref>. TSN with different backbones fails to achieve decent performance (&lt;20% Top-1) due to the lack of temporal modeling. For TRN, although late temporal fusion is added after feature extraction, the performance is still significantly lower than state-of-the-art methods, showing the importance of temporal fusion across all levels.</p><p>The second section shows the state-of-the-art efficient video understanding framework ECO <ref type="bibr" target="#b61">[61]</ref>. ECO uses an early 2D + late 3D architecture which enables medium-level temporal fusion. Compared to ECO, our method achieves better performance at a smaller FLOPs. For example, when using 8 frames as input, our TSM achieves 45.6% top-1 accuracy with 33G FLOPs, which is 4.2% higher accuracy than ECO with 1.9× less computation. The ensemble versions of ECO (ECO En Lite and ECO En Lite RGB+Flow , using an ensemble of {16, 20, 24, 32} frames as input) did achieve competitive results, but the computation and parameters are too large for deployment. While our model is much more efficient: we only used {8, 16} frames model for ensemble (TSM En ), and the model achieves better performance using 2.7× less computation and 3.1× fewer parameters.</p><p>The third section contains previous state-of-the-art methods: Non-local I3D + GCN <ref type="bibr" target="#b49">[50]</ref>, that enables all-level temporal fusion. The GCN needs a Region Proposal Network <ref type="bibr" target="#b33">[34]</ref> trained on MSCOCO object detection dataset <ref type="bibr" target="#b29">[30]</ref>   to generate the bounding boxes, which is unfair to compare since external data (MSCOCO) and extra training cost is introduced. Thus we compared TSM to its CNN part: Nonlocal I3D. Our TSM (8f) achieves 1.2% better accuracy with 10× fewer FLOPs on the validation set compared to the Non-local I3D network. Note that techniques like Non-local module <ref type="bibr" target="#b48">[49]</ref> are orthogonal to our work, which could also be added to our framework to boost the performance further. Generalize to Other Modalities. We also show that our proposed method can generalize to other modalities like optical flow. To extract the optical flow information between frames, we followed <ref type="bibr" target="#b47">[48]</ref> to use the TVL1 optical flow algorithm <ref type="bibr" target="#b55">[55]</ref> implemented in OpenCV with CUDA. We conducted two-stream experiments on both Something-Something V1 and V2 datasets, and it consistently improves over the RGB performance: introducing optical flow branch brings 5.4% and 2.6% top-1 improvement on V1 and V2. Something-Something-V2. We also show the result on Something-Something-V2 dataset, which is a newer release to its previous version. The results compared to other stateof-the-art methods are shown in <ref type="table" target="#tab_3">Table 4</ref>. On Something-Something-V2 dataset, we achieved state-of-the-art performance while only using RGB input.</p><p>Cost vs. Accuracy. Our TSM model achieves very competitive performance while enjoying high efficiency and low computation cost for fast inference. We show the FLOPs for each model in <ref type="table" target="#tab_2">Table 2</ref>. Although GCN itself is light, the method used a ResNet-50 based Region Proposal Net- work <ref type="bibr" target="#b33">[34]</ref> to extract bounding boxes, whose cost is also considered in the chart. Note that the computation cost of optical flow extraction is usually larger than the video recognition model itself. Therefore, we do not report the FLOPs of two-stream based methods. We show the accuracy, FLOPs, and number of parameters trade-off in <ref type="figure" target="#fig_5">Figure 5</ref>. The accuracy is tested on the validation set of Something-Something-V1 dataset, and the number of parameters is indicated by the area of the circles. We can see that our TSM based methods have a better Pareto curve than both previous state-of-the-art efficient models (ECO based models) and high-performance models (non-local I3D based models). TSM models are both efficient and accurate. It can achieve state-of-the-art accuracy at high efficiency: it achieves better performance while consuming 3× less computation than the ECO family . Considering that ECO is already an efficiency-oriented design, our method enjoys highly competitive hardware efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Latency and Throughput Speedup</head><p>The measured inference latency and throughput are important for the large-scale video understanding. TSM has low latency and high throughput. We performed measurement on a single NVIDIA Tesla P100 GPU. We used batch size of 1 for latency measurement; batch size of16 for throughput measurement. We made two comparisons:</p><p>(1) Compared with the I3D model from <ref type="bibr" target="#b49">[50]</ref>, our method is faster by an order of magnitude at 1.8% higher accuracy ( <ref type="table" target="#tab_4">Table 5</ref>). We also compared our method to the state-of-theart efficient model ECO <ref type="bibr" target="#b61">[61]</ref>: Our TSM model has 1.75× lower latency (17.4ms vs. 30.6ms), 1.7× higher throughput, and achieves 2% better accuracy. ECO has a twobranch (2D+3D) architecture, while TSM only needs the in-expensive 2D backbone.</p><p>(2) We then compared TSM to efficient 3D model designs. One way is to only inflate the first 1 × 1 convolution in each of the block as in <ref type="bibr" target="#b48">[49]</ref>, denoted as "I3D from <ref type="bibr" target="#b48">[49]</ref>" in the table. Although the FLOPs are similiar due to pooling, it suffers from 1.5× higher latency and only 55% the throughput compared with TSM, with worse accuracy. We speculate the reason is that TSM model only uses 2D convolution which is highly optimized for hardware. To excliude the factors  <ref type="figure">Figure 6</ref>. Early recognition on UCF101. TSM gives high prediction accuracy after only observing a small portion of the video.</p><p>of backbone design, we replace every TSM primitive with 3 × 1 × 1 convolution and denote this model as I3D replace . It is still much slower than TSM and performs worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Online Recognition with TSM</head><p>Online vs. Offline Online TSM models shift the feature maps uni-directionally so that it can give predictions in real time. We compare the performance of offline and online TSM models to show that online TSM can still achieve comparable performance. Follow <ref type="bibr" target="#b61">[61]</ref>, we use the prediction averaged from all the frames to compare with offline models, i.e., we compare the performance after observing the whole videos. The performance is provided in <ref type="table" target="#tab_5">Table 6</ref>. We can see that for less temporal related datasets like Kinetics, UCF101 and HMDB51, the online models achieve comparable and sometimes even better performance compared to the offline models. While for more temporal related datasets Something-Something, online model performs worse than offline model by 1.0%. Nevertheless, the performance of online model is still significantly better than the 2D baseline. We also compare the per-frame prediction latency of pure 2D backbone (TSN) and our online TSM model. We compile both models with TVM [5] on GPU. Our online TSM model only adds to less than 0.1ms latency overhead per frame while bringing up to 25% accuracy improvement. It demonstrates online TSM is hardware-efficient for latency-critical real-time applications. Early Recognition Early recognition aims to classify the video while only observing a small portion of the frames. It gives fast response to the input video stream. Here we compare the early video recognition performance on UCF101 dataset ( <ref type="figure">Figure 6</ref>). Compared to ECO, TSM gives much higher accuracy, especially when only observing a small portion of the frames. For example, when only observing the first 10% of video frames, TSM model can achieve 90% accuracy, which is 6.6% higher than the best ECO model.  <ref type="table" target="#tab_6">Table 7</ref>. Compared to 2D baseline R-FCN <ref type="bibr" target="#b22">[23]</ref>, our online TSM model significantly improves the performance, especially on the fast moving objects, where TSM increases mAP by 4.6%.</p><p>We also compare to a strong baseline FGFA <ref type="bibr" target="#b60">[60]</ref> that uses optical flow to aggregate the temporal information from 21 frames (past 10 frames and future 10 frames) for offline video detection. Compared to FGFA, TSM can achieve similar or higher performance while enabling online recognition at much smaller latency. We visualize some video clips in the supplementary material to show that online TSM can leverage the temporal consistency to correct mis-predictions. Edge Deployment TSM is mobile device friendly. We build an online TSM model with MobileNet-V2 backbone, which achieves 69.5% accuracy on Kinetics. The latency and energy on NVIDIA Jetson Nano &amp; TX2, Raspberry Pi 4B, Samsung Galaxy Note8, Google Pixel-1 is shown in <ref type="table">Table 8</ref>. The models are compiled using TVM <ref type="bibr" target="#b4">[5]</ref>. Power is measured with a power meter, subtracting the static power. TSM achieves low latency and low power on edge devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose Temporal Shift Module for hardware-efficient video recognition. It can be inserted into 2D CNN backbone to enable joint spatial-temporal modeling at no additional cost. The module shifts part of the channels along temporal dimension to exchange information with neighboring frames. Our framework is both efficient and accurate, enabling lowlatency video recognition on edge devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Uni-directional TSM for Online Video Detection</head><p>In this section, we show more details about the online video object detection with uni-directional TSM.</p><p>Object detection suffers from poor object appearance due to motion blur, occlusion, defocus, etc. Video based object detection gives chances to correct such errors by aggregating and inferring temporal information.</p><p>Existing methods on video object detection <ref type="bibr" target="#b60">[60]</ref> fuses information along temporal dimension after the feature is extracted by the backbone. Here we show that we can enable temporal fusion in online video object detection by injecting our uni-directional TSM into the backbone. We show that we can significantly improve the performance of video detection by simply modifying the backbone with online TSM, without changing the detection module design or using optical flow features.</p><p>We conducted experiments with R-FCN <ref type="bibr" target="#b22">[23]</ref> detector on ImageNet-VID <ref type="bibr" target="#b34">[35]</ref> dataset. Following the setting in <ref type="bibr" target="#b60">[60]</ref>, we used ResNet-101 <ref type="bibr" target="#b16">[17]</ref> as the backbone for R-FCN detector. For TSM experiments, we inserted uni-directional TSM to the backbone, while keeping other settings the same. We used the official training code of <ref type="bibr" target="#b60">[60]</ref> to conduct the experiments, and the results are shown in <ref type="table" target="#tab_6">Table 7</ref>. Compared to 2D baseline R-FCN <ref type="bibr" target="#b22">[23]</ref>, our online TSM model significantly improves the performance, especially on the fast moving objects, where TSM increases mAP by 4.6%. FGFA <ref type="bibr" target="#b60">[60]</ref> is a strong baseline that uses optical flow to aggregate the temporal information from 21 frames (past 10 frames and future 10 frames) for offline video detection. Compared to FGFA, TSM can achieve similar or higher performance while enabling online recognition (using information from only past frames) at much smaller latency per frame. The latency overhead of TSM module itself is less than 1ms per frame, making it a practical tool for real deployment.</p><p>We visualize two video clips in <ref type="figure">Figure 7</ref> and 8. In <ref type="figure">Figure 7</ref>, 2D baseline R-FCN generates false positive due to the glare of car headlight on frame 2/3/4, while TSM suppresses false positive. In <ref type="figure">Figure 8</ref>, R-FCN generates false positive surrounding the bus due to occlusion by the traffic sign on frame 2/3/4. Also, it fails to detect motorcycle on frame 4 due to occlusion. TSM model addresses such issues with the help of temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Video Demo</head><p>We provide more video demos of our TSM model in the following project page: https://hanlab.mit.edu/ projects/tsm/. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-FCN</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Offline temporal shift (bi-direction).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Online temporal shift (uni-direction).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Temporal Shift Module (TSM) performs efficient temporal modeling by moving the feature map along the temporal dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>(a) Latency overhead of TSM due to data movement. (b) Residual TSM achieve better performance than in-place shift. We choose 1/4 proportion residual shift as our default setting. It achieves higher accuracy with a negligible overhead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Uni-directional TSM for online video recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>TSM enjoys better accuracy-cost trade-off than I3D family and ECO family on Something-Something-V1<ref type="bibr" target="#b13">[14]</ref> dataset. (GCN includes the cost of ResNet-50 RPN to generate region proposals.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Comparing the result of R-FCN baseline and TSM model. 2D baseline R-FCN generates false positive due to the glare of car headlight on frame 2/3/4, while TSM does not have such issue by considering the temporal information. Comparing the result of R-FCN baseline and TSM model. 2D baseline R-FCN generates false positive surrounding the bus due to occlusion by the traffic sign on frame 2/3/4. Also, it fails to detect motorcycle on frame 4 due to occlusion. TSM model addresses such issues with the help of temporal information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Overhead vs. proportion.</figDesc><table><row><cell cols="2">15% 15%</cell><cell>Naive shift: Naive shift:</cell><cell></cell><cell></cell><cell cols="2">75% 75%</cell><cell cols="3">Our Choice Our Choice</cell></row><row><cell cols="2">Latency Overhead 12% 3% 6% 9% 3% 12% Latency Overhead 9% 6%</cell><cell>P100 TX2 CPU large overhead P100 TX2 CPU large overhead</cell><cell></cell><cell></cell><cell cols="2">Accuracy 69% 71% 73% 69% 73% 71% Accuracy</cell><cell>In-place TSM Naive shift: low acc. 2D baseline In-place TSM Naive shift: low acc. 2D baseline</cell><cell></cell><cell></cell></row><row><cell>0% 0%</cell><cell></cell><cell>Our Choice Our Choice</cell><cell></cell><cell></cell><cell cols="2">67% 67%</cell><cell>Residual TSM Residual TSM</cell><cell></cell><cell></cell></row><row><cell>(a) 0</cell><cell>0</cell><cell>1/8 Shift Proportion 1/4 1/2 1/8 1/4 1/2 Shift Proportion</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1/8 Shift Proportion 1/4 1/2 1/8 1/4 1/2 Shift Proportion</cell><cell>1</cell><cell>1</cell></row></table><note>(b) Residual vs. in-place.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparing TSM against other methods on Something-Something dataset (center crop, 1 clip/video unless otherwise specified).</figDesc><table><row><cell cols="2">Model</cell><cell></cell><cell>Backbone</cell><cell></cell><cell cols="4">#Frame FLOPs/Video #Param. Val Top-1 Val Top-5 Test Top-1</cell></row><row><cell cols="2">TSN [58]</cell><cell></cell><cell cols="2">BNInception</cell><cell>8</cell><cell>16G</cell><cell>10.7M</cell><cell>19.5</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">TSN (our impl.)</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>8</cell><cell>33G</cell><cell>24.3M</cell><cell>19.7</cell><cell>46.6</cell><cell>-</cell></row><row><cell cols="2">TRN-Multiscale [58]</cell><cell></cell><cell cols="2">BNInception</cell><cell>8</cell><cell>16G</cell><cell>18.3M</cell><cell>34.4</cell><cell>-</cell><cell>33.6</cell></row><row><cell cols="3">TRN-Multiscale (our impl.)</cell><cell>ResNet-50</cell><cell></cell><cell>8</cell><cell>33G</cell><cell>31.8M</cell><cell>38.9</cell><cell>68.1</cell><cell>-</cell></row><row><cell cols="3">Two-stream TRNRGB+Flow [58]</cell><cell cols="2">BNInception</cell><cell>8+8</cell><cell>-</cell><cell>36.6M</cell><cell>42.0</cell><cell>-</cell><cell>40.7</cell></row><row><cell cols="2">ECO [61]</cell><cell></cell><cell cols="2">BNIncep+3D Res18</cell><cell>8</cell><cell>32G</cell><cell>47.5M</cell><cell>39.6</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ECO [61]</cell><cell></cell><cell cols="2">BNIncep+3D Res18</cell><cell>16</cell><cell>64G</cell><cell>47.5M</cell><cell>41.4</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ECOEnLite [61]</cell><cell></cell><cell cols="2">BNIncep+3D Res18</cell><cell>92</cell><cell>267G</cell><cell>150M</cell><cell>46.4</cell><cell>-</cell><cell>42.3</cell></row><row><cell cols="2">ECOEnLiteRGB+Flow [61]</cell><cell></cell><cell cols="2">BNIncep+3D Res18</cell><cell>92+92</cell><cell>-</cell><cell>300M</cell><cell>49.5</cell><cell>-</cell><cell>43.9</cell></row><row><cell cols="2">I3D from [50]</cell><cell></cell><cell cols="2">3D ResNet-50</cell><cell>32×2clip</cell><cell>153G 1 ×2</cell><cell>28.0M</cell><cell>41.6</cell><cell>72.2</cell><cell>-</cell></row><row><cell cols="2">Non-local I3D from [50]</cell><cell></cell><cell cols="2">3D ResNet-50</cell><cell>32×2clip</cell><cell>168G 1 ×2</cell><cell>35.3M</cell><cell>44.4</cell><cell>76.0</cell><cell>-</cell></row><row><cell cols="3">Non-local I3D + GCN [50]</cell><cell cols="3">3D ResNet-50+GCN 32×2clip</cell><cell>303G 2 ×2</cell><cell>62.2M 2</cell><cell>46.1</cell><cell>76.8</cell><cell>45.0</cell></row><row><cell>TSM</cell><cell></cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>8</cell><cell>33G</cell><cell>24.3M</cell><cell>45.6</cell><cell>74.2</cell><cell>-</cell></row><row><cell>TSM</cell><cell></cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>16</cell><cell>65G</cell><cell>24.3M</cell><cell>47.2</cell><cell>77.1</cell><cell>46.0</cell></row><row><cell cols="2">TSMEn</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>24</cell><cell>98G</cell><cell>48.6M</cell><cell>49.7</cell><cell>78.5</cell><cell>-</cell></row><row><cell cols="2">TSMRGB+Flow</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>16+16</cell><cell>-</cell><cell>48.6M</cell><cell>52.6</cell><cell>81.9</cell><cell>50.7</cell></row><row><cell cols="6">Table 3. TSM can consistently improve the performance over dif-</cell><cell></cell><cell></cell></row><row><cell cols="4">ferent backbones on Kinetics dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Mb-V2 R-50 RX-101 NL R-50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TSN</cell><cell>66.5</cell><cell>70.7</cell><cell>72.4</cell><cell>74.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TSM</cell><cell>69.5</cell><cell>74.1</cell><cell>76.3</cell><cell>75.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>∆Acc.</cell><cell>+3.0</cell><cell>+3.4</cell><cell>+3.9</cell><cell>+1.1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results on Something-Something-V2. Our TSM achieves state-of-the-art performance.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell>Val</cell><cell></cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Top-1</cell><cell>Top-5</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell cols="3">TSN (our impl.)</cell><cell>30.0</cell><cell>60.5</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">MultiScale TRN [58]</cell><cell>48.8</cell><cell>77.6</cell><cell>50.9</cell><cell>79.3</cell></row><row><cell cols="3">2-Stream TRN [58]</cell><cell>55.5</cell><cell>83.1</cell><cell>56.2</cell><cell>83.2</cell></row><row><cell></cell><cell cols="2">TSM8F</cell><cell>59.1</cell><cell>85.6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">TSM16F</cell><cell>63.4</cell><cell>88.5</cell><cell>64.3</cell><cell>89.6</cell></row><row><cell cols="3">TSMRGB+Flow</cell><cell>66.0</cell><cell>90.5</cell><cell>66.6</cell><cell>91.3</cell></row><row><cell></cell><cell>51</cell><cell>TSMEn Ours</cell><cell>ECO [ ] 61</cell><cell cols="2">I3D from [ ] 50</cell><cell>Storage</cell></row><row><cell>Accuracy (%)</cell><cell>43 46 48</cell><cell>ECO16F TSM16F TSM8F</cell><cell cols="3">ECOEnLite NL I3D I3D # Parameters NL I3D+GCN</cell></row><row><cell></cell><cell>41</cell><cell>ECO8F</cell><cell cols="2">30M 100M</cell><cell>150M</cell></row><row><cell></cell><cell>38</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell cols="4">100 200 300 400 500 600 700</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">FLOPs/Video (G)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>TSM enjoys low GPU inference latency and high throughput. V/s means videos per second, higher the better (Measured on NVIDIA Tesla P100 GPU).</figDesc><table><row><cell>Model</cell><cell></cell><cell>Efficiency Statistics</cell><cell cols="2">Accuracy</cell></row><row><cell></cell><cell cols="4">FLOPs Param. Latency Thrput. Sth. Kinetics</cell></row><row><cell cols="4">I3D from [50] 306G 35.3M 165.3ms 6.1V/s 41.6%</cell><cell>-</cell></row><row><cell>ECO 16F [61]</cell><cell>64G</cell><cell cols="2">47.5M 30.6ms 45.6V/s 41.4%</cell><cell>-</cell></row><row><cell cols="2">I3D from [49] 33G</cell><cell>29.3M 25.8ms 42.4V/s</cell><cell>-</cell><cell>73.3%</cell></row><row><cell>I3D replace</cell><cell>48G</cell><cell cols="2">33.0M 28.0ms 37.9V/s 44.9%</cell><cell>-</cell></row><row><cell>TSM 8F</cell><cell cols="4">33G 24.3M 17.4ms 77.4V/s 45.6% 74.1%</cell></row><row><cell>TSM 16F</cell><cell>65G</cell><cell cols="3">24.3M 29.0ms 39.5V/s 47.2% 74.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparing the accuracy of offline TSM and online TSM on different datasets. Online TSM brings negligible latency overhead.</figDesc><table><row><cell cols="7">Model Latency Kinetics UCF101 HMDB51 Something</cell></row><row><cell>TSN</cell><cell>4.7ms</cell><cell>70.6%</cell><cell>91.7%</cell><cell cols="2">64.7%</cell><cell>20.5%</cell></row><row><cell>+Offline</cell><cell>-</cell><cell>74.1%</cell><cell>95.9%</cell><cell cols="2">73.5%</cell><cell>47.3%</cell></row><row><cell cols="2">+Online 4.8ms</cell><cell>74.3%</cell><cell>95.5%</cell><cell cols="2">73.6%</cell><cell>46.3%</cell></row><row><cell></cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy %</cell><cell>88 92 84</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ECO (s=8) ECO (s=12)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ECO (s=20)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TSM</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell cols="3">Video Observation %</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Video detection results on ImageNet-VID.Table 8. TSM efficiently runs on edge devices with low latency.</figDesc><table><row><cell>Model</cell><cell>Online</cell><cell cols="2">Need Flow Latency</cell><cell cols="3">mAP Overall Slow Medium Fast</cell></row><row><cell>R-FCN [23]</cell><cell></cell><cell></cell><cell>1×</cell><cell>74.7</cell><cell>83.6</cell><cell>72.5</cell><cell>51.4</cell></row><row><cell>FGFA [60]</cell><cell></cell><cell></cell><cell>2.5×</cell><cell>75.9</cell><cell>84.0</cell><cell>74.4</cell><cell>55.6</cell></row><row><cell>Online TSM</cell><cell></cell><cell></cell><cell>1×</cell><cell>76.3</cell><cell>83.4</cell><cell>74.8</cell><cell>56.0</cell></row><row><cell>Devices</cell><cell cols="6">Jetson Nano Jetson TX2 Rasp. Note8 Pixel1</cell></row><row><cell></cell><cell cols="4">CPU GPU CPU GPU</cell><cell></cell></row><row><cell cols="7">Latency (ms) 47.8 13.4 36.4 8.5 69.6 34.5 47.4</cell></row><row><cell cols="2">Power (watt) 4.8</cell><cell>4.5</cell><cell cols="2">5.6 5.8</cell><cell>3.8</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">Online Object Detection Real-time online video object</cell></row><row><cell cols="7">detection is an important application in self-driving vehicles,</cell></row><row><cell cols="7">robotics, etc. By injecting our online TSM into the backbone,</cell></row><row><cell cols="7">we can easily take the temporal cues into consideration at</cell></row><row><cell cols="7">negligible overhead, so that the model can handle poor object</cell></row><row><cell cols="7">appearance like motion blur, occlusion, defocus, etc. We</cell></row><row><cell cols="7">conducted experiments on R-FCN [23] detector with ResNet-</cell></row><row><cell cols="7">101 backbone on ImageNet-VID [35] dataset. We inserted</cell></row><row><cell cols="7">the uni-directional TSM to the backbone, while keeping</cell></row><row><cell cols="7">other settings the same. The results are shown in</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We thank MIT Quest for Intelligence, MIT-IBM Watson AI Lab, MIT-SenseTime Alliance, Samsung, SONY, AWS, Google for supporting this research. We thank Oak Ridge National Lab for Summit supercomputer.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://20bn.com/datasets/jester.5" />
		<title level="m">The 20bn-jester dataset v1</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic image networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3034" to="3042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">{TVM}: An automated end-toend optimizing compiler for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cudnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Webly-supervised video recognition by mutually voting for relevant web images and web video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="849" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2568" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">You lead, we exceed: Labor-free video concept learning by jointly exploiting web videos and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The something something video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<editor>Kaiming He Jian Sun Jifeng Dai, Yi Li</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunggi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyutae</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="387" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Videolstm convolves, attends and flows for action recognition. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Runtime neural pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2181" to="2191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00559</idno>
		<title level="m">Progressive neural architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7834" to="7843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4597" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<title level="m">Platform-aware neural architecture search for mobile</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08886</idno>
		<title level="m">Haq: Hardware-aware automated quantization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07971</idno>
		<title level="m">Non-local neural networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01810</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Shift: A zero flop, zero parameter alternative to spatial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Golmant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholaminejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08141</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Rethinking spatiotemporal feature learning</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Speed-accuracy trade-offs in video classification</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1707.01083</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Shift-based primitives for efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huasong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianggen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08458</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08496</idno>
		<title level="m">Temporal relational reasoning in videos</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenzhuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<title level="m">Trained ternary quantization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09066</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<idno>2017. 3</idno>
		<title level="m">Learning transferable architectures for scalable image recognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
