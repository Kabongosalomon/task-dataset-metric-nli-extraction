<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Viewpoint-Aware Loss with Angular Regularization for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent YouTu Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent YouTu Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent YouTu Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent YouTu Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weishi</forename><surname>Zheng</surname></persName>
							<email>wszheng@ieee.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Viewpoint-Aware Loss with Angular Regularization for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although great progress in supervised person reidentification (Re-ID) has been made recently, due to the viewpoint variation of a person, Re-ID remains a massive visual challenge. Most existing viewpoint-based person Re-ID methods project images from each viewpoint into separated and unrelated sub-feature spaces. They only model the identity-level distribution inside an individual viewpoint but ignore the underlying relationship between different viewpoints. To address this problem, we propose a novel approach, called Viewpoint-Aware Loss with Angular Regularization (VA-reID). Instead of one subspace for each viewpoint, our method projects the feature from different viewpoints into a unified hypersphere and effectively models the feature distribution on both the identity-level and the viewpoint-level. In addition, rather than modeling different viewpoints as hard labels used for conventional viewpoint classification, we introduce viewpoint-aware adaptive label smoothing regularization (VALSR) that assigns the adaptive soft label to feature representation. VALSR can effectively solve the ambiguity of the viewpoint cluster label assignment. Extensive experiments on the Market1501 and DukeMTMC-reID datasets demonstrated that our method outperforms the state-of-the-art supervised Re-ID methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person re-identification(Re-ID) which targets at recognizing pedestrians across non-overlapping camera views, is an important and challenging problem in visual surveillance analysis and has drawn increasing research attention <ref type="bibr" target="#b14">(Zheng et al. 2015;</ref><ref type="bibr" target="#b14">Zhao et al. 2017;</ref><ref type="bibr" target="#b13">Sun et al. 2018)</ref>. While Re-ID has gained considerable development in recent years, existing supervised person re-identification still faces some major visual appearance challenges, such as changes in viewpoint or poses, low resolution, illumination and etc.</p><p>Among these challenges, in this work, we focus on the problem of viewpoint variations, which is one of the most important and difficult challenges in Re-ID research and  practical application. In practice, due to the effect of viewpoint variation, images from different viewpoints of the same identity usually have massive visual appearance differences, and it may even be possible that some images of different identities from the same viewpoints are more similar in visual appearance than images of the same identity from different viewpoints. Some examples are shown in <ref type="figure" target="#fig_1">Figure 1</ref> (a)(b). This problem greatly limits the practical application of Re-ID.</p><p>A key problem for tackling viewpoint variations is to learn discriminative feature representations for body images with different viewpoints. However, there are some inadequacies of the existing viewpoint-based feature learning method <ref type="bibr" target="#b2">(Chen, Xu, and Deng 2018;</ref><ref type="bibr" target="#b9">Qian et al. 2018;</ref><ref type="bibr" target="#b10">Saquib Sarfraz et al. 2018)</ref>. 1) They treat viewpoint learning and identity discrimination as two separate progresses, In such a case, it is not a principle way to learn optimal identity classification under various viewpoint variations. 2) They cast viewpoints of persons as hard labels, while in reality the viewpoint of person is ambiguous.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 1</ref>(c), these methods learn separate features for different viewpoints. For example, DVAML <ref type="bibr" target="#b2">(Chen, Xu, and Deng 2018)</ref> learns two different feature subspaces for image pairs with similar and dissimilar viewpoints, OS-CNN  and <ref type="bibr">PSE (Saquib Sarfraz et al. 2018</ref>) learn a linear combination of different viewpoints' features. Actually, projecting the features into separated and unrelated subspaces only models the identity-level distribution within each viewpoint but may ignore the underlying relationship among different viewpoints. Thus, the relationship between the features from separate viewpoint subspaces cannot be directly learned, compromising the model's ability to match images of a person from different viewpoints.</p><p>To solve this problem, we propose a novel angular-based feature learning method that projects all features into a unified subspace and directly models the distribution of the features from different viewpoints. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>(d)(f), the feature distribution is modeled at both the identity-level and viewpoint-level. At the identity-level, different identities are pushed away from each other to form identity-level clusters. At the viewpoint-level, the features in each identity cluster will further produce three viewpoint-level clusters (front, side, back), and a novel center regularization is used to pull the centers of these clusters closer to each other due to their visual similarity to the same identity.</p><p>In addition, we further consider the problem of the viewpoint cluster label assignment. While conventionally, each image will be assigned a hard viewpoint cluster label, we find that the viewpoint of some image samples are indeed ambiguous, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(e), and that a hard viewpoint cluster label assignment may mislead the learning. Therefore, we propose to relax the hard assignment problem and instead perform a soft label assignment, as shown in <ref type="bibr">Figure 1 (f)</ref>. We propose a novel viewpoint-aware regularization method, called view aware label smoothing regularization (VALSR). VALSR replaces the common one-hot hard label with the adaptive soft label that changes adaptively according to similarity to the classification centers. Notice that we use the viewpoint label only for training.</p><p>In summary, this work develop a joint learning model for the identity and viewpoint discrimination learning, where in particular we introduce the soft multilabel to model viewpoint aware feature distribution for overcoming the ambiguity problem on viewpoint labelling and greatly solve the viewpoint variations. The main contributions include:</p><p>• Weput forward the idea of modeling the viewpoint distribution and identity distribution jointly rather than separately. For this purpose, we propose a novel solution called Viewpoint-Aware Loss with Angular Regularization, which effectively models the distribution on both identity-level and viewpoint-level, and specially we impose the center regularization to connect identity and viewpoint discrimination. The experimental results also demonstrate our advanced performance against related methods a lot.</p><p>• To overcome the ambiguity on viewpoint labelling, we develop viewpoint-aware adaptive label smoothing, which allows smooth transition between features of different dis-crete veiwpoints by assigning adaptive soft viewpoint label. The soft label will get self-adaption dynamically according to the prediction probability and have better performance against noise data and over-fitting  <ref type="bibr" target="#b2">(Chen, Xu, and Deng 2018)</ref>. Pose-based methods and segmentation-based methods are common practices to address this problem. Pose-based methods usually take advantage of pose information to pay attention to human body parts while segmentation-based methods utilize human parsing information to obtain position information of human body parts. Then they can make align the body parts or extract the local feature of body parts. Generation-based methods usually use the generative model to generate images <ref type="bibr" target="#b9">(Qian et al. 2018)</ref> or generate feature of other viewpoints (Zhou and Shao 2018). Recently PersonX (Sun and Zheng 2019) utilizes a large-scale synthetic data engine to generate pedestrian images with arbitrary rotation angle, and analyses the important impact of viewpoints on Re-ID in detail. Viewpoint-based methods use hard viewpoint label of image directly and utilize them to help features learning. DVAML <ref type="bibr" target="#b2">(Chen, Xu, and Deng 2018)</ref> tries to learn two different feature sub-spaces for image pairs with similar and dissimilar viewpoints but gets little improvement. Loss Function for Identification. Metric learning targets at learning a metric space in which the samples from different classes are far away while the samples from the same classes are compact. The popular loss functions include the softmax-based loss, the contrastive loss and the triplet loss. In face recognition field, amount of representative softmax-based methods (Salimans and Kingma 2016; <ref type="bibr" target="#b7">Liu et al. 2017;</ref><ref type="bibr" target="#b9">Ranjan et al. 2018;</ref><ref type="bibr" target="#b3">Deng et al. 2019)</ref> have been proposed.These improvements are mainly concentrated on two aspects: normalization and margin. The former is applied for features and weight of fully connected layer. As early as in (Salimans and Kingma 2016), the advantages of weight normalization, such as reducing computational complexity and making the network converge faster, have been demonstrated. Crystal loss <ref type="bibr" target="#b9">(Ranjan et al. 2018)</ref> proposes feature normalization and verified the effectiveness. Feature normalization is beneficial to increase angular discrimination in feature space direction. The combination of feature normalization and weight normalization can achieve better results. Recent softmax-based work mainly focused on the latter. The concept of angular margin is firstly introduced to metric space in L-Softmax <ref type="bibr" target="#b6">(Liu et al. 2016)</ref>. Later works such as SphereFace <ref type="bibr" target="#b7">(Liu et al. 2017</ref>), ArcFace <ref type="bibr" target="#b3">(Deng et al. 2019)</ref> continue to make some improvements to obtain better performance. 3 Methodology</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation and Overview</head><p>Given a query image, the target of Re-ID is to get a ranking list of images from gallery set across non-overlapping camera views. Define an image</p><formula xml:id="formula_0">I i = (x i , y i , v i ) where x i denotes the feature extracted by a deep model of the i-th image, y i is the identity label and v i is the viewpoint label. Notice that each image x i has only one viewpoint label v i ∈ {f ront, side, back}. Given a training set D = {(x i , y i , v i )}</formula><p>with N images, the deep feature x i is a global ReID feature extracted by a CNN backbone (e.g., ResNet, DenseNet) denoted as F (Θ; I i ) where Θ denotes the parameters of CNN. Now we briefly introduce the pipeline of the proposed Viewpoint-Aware ReID method (VA-reID). As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, a CNN backbone extracts a global feature x i for each image I i . We introduce two types of losses: the identity angular loss L y and the viewpoint-aware angular loss L v . Integrating two angular-related losses into a unified framework of learning can build a two-level distribution for feature x i on a unified hyper-sphere, including the identitylevel distribution and the viewpoint-level distribution. For the identity-level distribution, features with the same identity can be assembled to form identity clusters by L y (i.e., the large light green and the light blue circles in <ref type="figure" target="#fig_2">Figure 2</ref>). For the viewpoint-level, in each identity cluster, we pull the features of the same viewpoint close to form viewpoint clusters (i.e, the small-dark green and the brown circle inside the large circle) by angular loss L v and center regularization term L R . As a result, the overall loss of our proposed method is:</p><formula xml:id="formula_1">L va = L y + L v + βL R .<label>(1)</label></formula><p>Furthermore, as shown in the bottom-right rectangle in <ref type="figure">Figure</ref> 2, at the viewpoint-level, a novel viewpoint-aware adaptive LSR regularization item is used in L v to eliminate the hard margin between viewpoint clusters. At the identitylevel, a small adaptive label smoothing regularization is explored to effectively increase the generalization ability of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VA-reID for Person Re-Identification</head><p>Softmax loss is widely used for Re-ID feature learning. Let K denote the number of identities, x denote the deep learning ReID feature of a image, and W y denote the y-th column of the weights W ∈ R d * K . As a result, the prediction probability of I belongs the identity y is:</p><formula xml:id="formula_2">q(y) = e W T y x K j=1 e W T j x .</formula><p>(2)</p><p>We remove the bias and normalize the feature x and the weight W as advised in arcface loss <ref type="bibr" target="#b3">(Deng et al. 2019</ref>). Generally, weight normalization helps to improve class imbalance, and feature normalization is instrumental in generalization of metric space. Therefore, all the features can be projected onto a hypershere with the same length, and the probability of deep feature x belonging to identity y is equivalent to the cosine distance between x i and W y : </p><p>where θ y denotes the angle between the feature vector x and the y-th column of W y : cos(θ y ) = W T y ·x ||Wy||·||x|| . m is a margin that is used to improve the discriminative ability of classification, and s is the scale factor used to promote convergence.</p><p>Based on previous experience <ref type="bibr" target="#b7">(Liu et al. 2017;</ref><ref type="bibr" target="#b3">Deng et al. 2019)</ref>, the angular loss helps to model a better discriminative distribution of identities. Analogously, we extend the identity-level angular loss to the proposed viewpoint-aware loss (VA-reID). From Eq.(4), we observe that the y-th column of the weight W y can be viewed as the center of identity y. To get a higher probability of image I belonging to y, we need to pull the feature vector x i closer to center W y . To model the distribution of different viewpoints, each identity class is further classified into V subclass, corresponding to V subclasses corresponding to V viewpoints (i.e., front viewpoint, side viewpoint and back viewpoint. V denotes the number of viewpoints and is assigned 3 in this paper). We denote the viewpoint centers as U ∈ R d * K * V . As a result, given a deep feature x, we model the probability that x belongs to the identity y and viewpoint v as follows:</p><formula xml:id="formula_4">r(y, v) = e s cos(φ(y,v)−m) e s cos(φ(y,v)−m) + K l=1,l =y V o=1,o =v e l cos(φ(l,o))<label>(4)</label></formula><p>where φ yv denotes the angle between feature x and the center of the y-th identity and the v-th viewpoint U yv :</p><formula xml:id="formula_5">cos(φ(y, v)) = U T yv ·x ||Uyv||·||x||</formula><p>. Given a training sample I i with identity label y i and viewpoint label v i , the identity classification loss L y and viewpoints-aware loss L v are:</p><formula xml:id="formula_6">L y = 1 K K j p j log(q(j))<label>(5)</label></formula><formula xml:id="formula_7">L v = 1 K K j V k t j,k log(r(j, k))<label>(6)</label></formula><p>where p and t is the classification label. The traditional methods use the hard label for features learning, e.g.,</p><formula xml:id="formula_8">p j = 1 if i = y i 0 otherwise<label>(7)</label></formula><p>t jk = 1 if j = y i and j = v i 0 otherwise <ref type="formula">(8)</ref> which ignore the ambiguity problem on viewpoint labelling. To overcome this problem, we introduce the soft label learning methods. Furthermore, to maintain the visual similarity between features from the same person but with different viewpoints, we propose to the center regularization term to connect identity and viewpoint discrimination. It helps to pull the V viewpoint centers W ij closer to the corresponding identity center and the formula is:</p><formula xml:id="formula_9">L R = K k=1 V j=1 W T j · U jk ||W j || · ||U jk || .<label>(9)</label></formula><p>-Adaptive Identity Label Learning. In this section, we introduce the soft identity label to replace the conventional hard identity label. The soft label will get self-adaption dynamically according to the prediction probability and have better performance against noise data and over-fitting.</p><p>Assumption 1. The network tends to prioritize learning simple patterns of real data firstly and then the noise <ref type="bibr" target="#b0">(Arpit et al. 2017)</ref>.</p><p>As aforementioned, in cross entropy loss, the one-hot encoding is used to be as ground-truth probability distribution and thus the model tends to maximize the expected loglikelihood of one label, which may result in over-fitting and harm the generalization ability of model. Label smoothing regularization(LSR) <ref type="bibr" target="#b13">(Szegedy et al. 2016</ref>) is proposed to further address the problems. The probability distribution formula of LSR is</p><formula xml:id="formula_10">p j = 1 − ε if j = y ε K − 1 otherwise<label>(10)</label></formula><p>where ε is a manual value. It replaces the one-hot hard label with the soft label by introduce a small manual parameter ε to adjust the probability distribution. This encourages the model to lower some confidence on other categories except the ground-truth. However, notice that ε is a fixed value, which results in the same expected probability of groundtruth category for every input sample and so do other categories. Actually input samples have different logits and applying the same expected probability for them is unsuitable. Considering a case that there exist noises in training set, an image I with the true identity label y 1 is annotated as y 2 wrongly. According the assumption <ref type="bibr" target="#b0">(Arpit et al. 2017)</ref>, the network tends to learn positive samples firstly and then noise samples. And usually noise samples have smaller logits of label category than that of positive samples.</p><formula xml:id="formula_11">p j = 1 − α(1 − q(j)) if j = y α(1 − q(j)) K − 1 otherwise<label>(11)</label></formula><p>We develop a new smoothing parameter ε = α(1 − q(j) , which is related to the prediction probability of network. α is a multiplicative scaling coefficient. When the output probability q(j) is large, we will get a small ε and get a large confidence on this category. In contrast, when the output probability q(j) is small, we will get large ε and a large confidence on other categories. The adaptive label smoothing regularization have better performance against noise data and over-fitting. An illustration of the adaptive soft identity label is in <ref type="figure" target="#fig_4">Figure 3(a)</ref>. We apply the adaptive-LSR (ALSR) into the identity-level by subsituting Eq.(11) into L y .</p><p>-Viewpoint-Aware Adaptive Label Learning.</p><p>Assumption 2. The viewpoint of person in reality is a continuous value rather than the hard one.</p><p>Based on this assumption, we extend the ALSR to a viewpoint-aware angular loss, i.e., Viewpoint-Aware ALSR (VALSR). As mentioned earlier, our proposed viewpointaware angular loss models two levels of distribution, i.e., the identity-level distribution (identity clusters) and the viewpoint-level distribution (viewpoint clusters). We split every identity label into three sub-categories according to the viewpoints (front, side, back) and thus each image is classified into 3K viewpoint-aware categories. We argue that, when assigning the soft label for viewpoint-aware angular loss, the degree of regularization will vary according to the level of cluster that the label belongs to. If current unassigned viewpoint-aware soft label is in the same identity cluster with the ground-truth label's cluster, a stronger relaxation (higher probability) should be assigned as shown in <ref type="figure" target="#fig_4">Figure 3</ref>(b), because images in this cluster will have the strong correlation and the visual similarity with the groundtruth. On the opposite, as shown in <ref type="figure" target="#fig_4">Figure 3</ref>(b), if current soft label is in the different identity cluster with the groundtruth, a weaker relaxation should be assigned. As a result, given a training sample I i with identity label y i and viewpoint label v i , the soft label for viewpoint-aware ALSR is</p><formula xml:id="formula_12">t jk =        1 − ε 1 − ε 2 if j = y i , k = v i ε 2 2 if j = y i , k = v i ε 1 K − 3 otherwise<label>(12)</label></formula><p>where ε 1 = α(1− 3 v=1 r(y i , v)) and ε 2 = α(1−r(y i , v i )). We apply the viewpoint adaptive-LSR into angular loss by subsituting Eq.(12) into L v .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Joint Global and Local Features</head><p>Furthermore, we expect to build a model for both global and local features extraction. VA-reID method have excellent performance of feature extraction, especially for images with various viewpoints. Interestingly, we observe that the viewpoint label are more suitable for the whole body rather than body parts due to huge similarity of body parts with different viewpoint (e.g., lower body, leg). Thus, we mainly apply our VA-reID method to global feature.</p><p>In order to extract local features effectively, we choose the classical multistripe pyramid structure, which is similar to <ref type="bibr" target="#b13">(Sun et al. 2018;</ref><ref type="bibr" target="#b15">Zheng et al. 2019a</ref>) for local branches, as shown in <ref type="figure" target="#fig_5">Figure 4</ref>. Jointing the global and local features could effectively boost the performance of the Re-ID model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics.</head><p>We annotate the viewpoint label 1 of two widely used benchmarks including Market-1501 and DukeMTMC-reID. Viewpoints are divided into three categories: front, side, back. We 1 Available at https://github.com/zzhsysu/VA-ReID evaluate our model in the two datasets. Notice that we use the viewpoint label only for training. During the test stage, we don't use any viewpoint label. Market-1501 dataset contains 32,668 person images of 1,501 identities captured by six cameras. Training set is composed of 12,936 images of 751 identities while testing data is composed of the other images of 750 identities. In addition, 2,793 distractors also exist in testing data. DukeMTMC-reID dataset contains 36,411 person images of 1,404 identities captured by eight cameras. They are randomly divided, with 702 identities as the training set and the remaining 702 identities as the testing set. In the testing set, For each identity in each camera, one image is picked for the query set while the rest remain for the gallery set. Evaluation Metrics. Two widely used evaluation metrics including mean average precision (mAP) and matching accuracy (Rank-1/Rank-5) are adopted in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details.</head><p>We resize images to 384 × 128 as in many re-ID systems. In training stage, we set batch size to be 64 by sampling 16 identities and 4 images per identity. The SeResnext model with the pretrained parameters on ImageNet is considered as the backbone network. Some common data augmentation strategies include horizontal flipping, random cropping, padding, random erasing (with a probability of 0.5) are used. We adopt Adam optimizer to train our model and set weight decay 5 × 10 −4 . The total number of epoch is 200 and the epoch milestones are 50, 100, 160. The learning rate is initialized to 3.5 × 10 −4 and is decayed by a factor of 0.1 when the epoch get the milestones. At the beginning, we warm up the models for 10 epochs and the learning rate grows linearly from 3.5 × 10 −5 to 3.5 × 10 −4 . The parameters in the loss function are set as follows: β = 1, α = 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to the State-of-the-art.</head><p>We evaluate our proposed VA-reID model with the state-ofthe-art methods based on deep learning. These methods include: 1) the stripe based methods PCB, MGN; 2) the metric learning related methods SRB,Pyramid,HPM; 3) the human semantic parsing based methods SPReID,DSA-reID; 4) the attention mechanisms based methods HA-CNN,ABD-Net; 5) the pose/view related methods OSCNN, PDC, PN-GAN, PIE, PGR. We show the results in <ref type="table" target="#tab_2">Table 1</ref> and we can observe that our model achieves state-of-the-art.</p><p>-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study.</head><p>We perform comprehensive ablation study to demonstrate: 1) the effectiveness of the adaptive label smoothing; 2) the effectiveness of the viewpoint-aware adaptive label smoothing; 3) the effectiveness of center regularization; 4) the effectiveness of jointing global and local features. Notice that the viewpoint-aware loss function L va is for global branch. Loss of VA-reID is L av = L y +L v +βL R , where L y is adaptive label smoothing loss item, L v is viewpoint-aware adaptive label smoothing loss item and L R is center regularization. We use the model trained only with L y as the baseline, and set parameter β=0.1 and α=0.2. The performance (%) comparisons of different modules on Market-1501 and DukeMTMC-reID datasets are shown in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>-Effectiveness of Adaptive Label Smoothing. Comparing results of cross entropy loss (Xent) and label smoothing loss (LSR), we can observe that using label smoothing gets better performance. We use adaptive label smoothing (ALSR) as basic loss and it achieves an improvement over label smoothing by 0.25%/0.35% on mAP/Rank-1 metrics in Market-1501 and by 0.52%/0.45% on mAP/Rank-1 metrics in DukeMTMC-reID. This is because ALSR replaces the one-hot hard label with the adaptive soft label for identity classification. The adaptive soft label actually helps to learn discriminative features while ignore the negative impact of noises. This comparison demonstrates the effectiveness of adaptive label smoothing.</p><p>-Effectiveness of Viewpoint-Aware Adaptive Label Smoothing. Comparing results to the baseline model, we observe that combining the viewpoint-aware adaptive la- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Further Evaluations.</head><p>Following evaluations and analysis are made to further verify the effectiveness of our methods including: (1) the influence of the noisy viewpoint label;</p><p>(2) visualization of the influence of viewpoints variance to the retrival; (3) the effect of hyperparameters; (4) viewpoint based retrieval and complexity analysis (see supplementary material 2 ).</p><p>-Influence of the Noisy Viewpoint Label. We exam the influence of the noises in the viewpoint labels on the Re-ID performance. As show in <ref type="table">Table 3</ref> three different types of viewpoints labels and the performance comparison using different viewpoint labels: 1) VA-reID with P uses viewpoints labels predicted by a simple Resnet-50 classifier trained on 600 images of extra viewpoints datasets. 2) VA-reID with PC uses viewpoints labels predicted by clustering pose information generated by Open-Pose. 3) VA-reID with GT directly uses ground-truth labels annotated by human. The accuracy of viewpoints classification is 78.7% and the accuracy of pose clustering is 44.57%.</p><p>From <ref type="table">Table 3</ref>, we observe that the amount of noises in the viewpoints labels have very little influence on our method. VA-reID with P achieves very similar performance to VA-reID with GT. What's more, even with the viewpoints prediction accuracy of only 44.57%, the performance of VA-reID with PC does not drop much, our method still achieves state-of-art performance on DukeMTMC-reID. This experiments verify that our proposed VA-reID method is robust to viewpoints label noises and a simple classifier or clustering method is good enough to get the viewpoints information.</p><p>-Visualization of Results. <ref type="figure">Figure 5</ref> shows examples of retrieval results by baseline method and VA-reID. We observe that, in the case of high viewpoints variance between query and gallery, our method has a much higher retrieval precision compared to baseline. Take the result in the first row as an example, for a query image in front viewpoint, baseline method only correctly retrieves two images with the same front viewpoint, while VA-reID is able to successfully retrieve images from all three viewpoints. Similar to the first row, the results in other rows further demonstrate excellent performance on cross-viewpoints images retrieval.</p><p>2 Available at https://github.com/zzhsysu/VA-ReID <ref type="table">Table 3</ref>: Performance (%) comparisons for different generation methods of the viewpoint label. P: using predictive viewpoint labels. PC: using pose-based clustering viewpoint labels. GT: using ground-truth viewpoint labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>DukeMTMC-reID mAP Rank-1 Rank-5 PN-GAN <ref type="bibr" target="#b9">(Qian et al. 2018)</ref>   <ref type="figure">Figure 5</ref>: Visual results of the baseline method and the VA-reID method. The red box represents the wrong result while the green box represents the correct result.</p><p>-Hyperparameters Analysis. <ref type="figure">Figure 6</ref> shows how the parameter α and β affect the performance in Market-1501 dataset. The performace of our method is stable within a wide range for both parameters. It also presents the excellence of the adaptive label smoothing method.  <ref type="figure">Figure 6</ref>: Performance of VA-reID method on Market-1501 with different hyperparameters. In <ref type="figure">figure (a)</ref>, we fix β = 0.1.</p><p>In <ref type="figure">figure (b)</ref>, we fix α = 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This study proposes a novel method to address the viewpoint variation problem in Re-ID. Overall, we make two contributions to the community. Firstly, we propose the viewpointaware angular loss to learn the embedding of viewpointaware feature in a unified hyper-sphere, which effectively model the feature distribution on both identity-level and viewpoints-level. Secondly, we propose a novel viewpoint aware adaptive label smoothing method to relax the hard margin caused with adaptive soft labels. Experiments show the effectiveness our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A person with different viewpoints (b) Different persons with the same viewpoint</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Comparisons of different feature learning methods. Our VA-reID method learns the unified space using the soft label instead of the hard label. Images with the thick purple border in the figure are the ambiguous viewpoint categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed VA-reID method. Feature x i is extracted by backbone network. The proposed viewpointaware angular loss projects features onto a hyper-sphere to form the identity-level cluster (light green and blue circle) and the viewpoint-level cluster (dark green and brown circle). Furthermore, our Adaptive Label Smoothing Regularization eliminates the hard margin between clusters by introducing adaptive soft labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>q(y) = e s cos(θy−m) e s cos(θy−m) + K j=1,j =y e s cos(θj )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>(a) Illustration of adaptive soft label for identitylevel learning. (b) Illustration of viewpoint-aware adaptive soft label for viewpoint-level learning .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Jointing global and local features. the VA-reID method is used to extract global features while multi-stripes structure is for local features. Xent: cross entropy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance (%) comparisons to the state-of-the-art results on Market-1501 and DukeMTMC-reID. Our proposed VA-reID model outperforms the state-of-the-art methods.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell>mAP</cell><cell cols="5">Market-1501 Rank-1 Rank-5 mAP Rank-1 Rank-5 DukeMTMC-reID</cell></row><row><cell></cell><cell>PCB (Sun et al. 2018)</cell><cell>77.4</cell><cell>92.3</cell><cell>97.2</cell><cell>66.1</cell><cell>81.7</cell><cell>89.7</cell></row><row><cell>stripe based</cell><cell>PCB+RPP (Sun et al. 2018)</cell><cell>81.6</cell><cell>93.8</cell><cell>97.5</cell><cell>69.2</cell><cell>83.3</cell><cell>90.5</cell></row><row><cell></cell><cell>MGN (Wang et al. 2018)</cell><cell>86.9</cell><cell>95.7</cell><cell>-</cell><cell>78.4</cell><cell>88.7</cell><cell>-</cell></row><row><cell>attention based</cell><cell>HA-CNN (Li, Zhu, and Gong 2018) ABD-Net (Chen et al. 2019)</cell><cell>75.7 88.28</cell><cell>91.2 95.60</cell><cell>--</cell><cell>63.8 78.59</cell><cell>80.5 89.00</cell><cell>--</cell></row><row><cell>human parsing</cell><cell>SPReID (Kalayeh et al. 2018) DSA-reID (Zhang et al. 2019)</cell><cell>83.36 87.6</cell><cell>93.68 95.7</cell><cell>97.57 98.4</cell><cell>73.34 74.3</cell><cell>85.95 86.2</cell><cell>92.95</cell></row><row><cell></cell><cell>Pyramid (Zheng et al. 2019a)</cell><cell>88.2</cell><cell>95.7</cell><cell>98.4</cell><cell>79.0</cell><cell>89.0</cell><cell>-</cell></row><row><cell>metric learning</cell><cell cols="2">SRB(ResNet50) (Luo et al. 2019) SRB(SeResNext101) (Luo et al. 2019) 88.0 85.9</cell><cell>94.5 95.0</cell><cell>--</cell><cell>76.4 79.0</cell><cell>86.4 88.4</cell><cell>--</cell></row><row><cell></cell><cell>HPM (Fu et al. 2019)</cell><cell>82.7</cell><cell>94.2</cell><cell>97.5</cell><cell>74.3</cell><cell>86.6</cell><cell>-</cell></row><row><cell></cell><cell>OSCNN (Chen et al. 2018)</cell><cell>73.5</cell><cell>83.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PDC (Su et al. 2017)</cell><cell>63.41</cell><cell>84.14</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>pose/view related</cell><cell>PN-GAN (Qian et al. 2018)</cell><cell>72.58</cell><cell>89.43</cell><cell>-</cell><cell>53.20</cell><cell>73.58</cell><cell>-</cell></row><row><cell></cell><cell>PIE (Zheng et al. 2019b)</cell><cell>69.25</cell><cell>87.33</cell><cell>95.56</cell><cell>64.09</cell><cell>80.84</cell><cell>88.30</cell></row><row><cell></cell><cell>PGR (Li et al. 2019)</cell><cell>77.21</cell><cell>93.87</cell><cell>97.74</cell><cell>65.98</cell><cell>83.63</cell><cell>91.66</cell></row><row><cell>This work</cell><cell>Ours Ours+reranking</cell><cell>91.70 95.43</cell><cell>96.23 96.79</cell><cell>98.69 98.31</cell><cell>84.51 91.82</cell><cell>91.61 93.85</cell><cell>96.23 96.50</cell></row><row><cell cols="2">attention mechanisms based methods. Comparison to the re-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">cent state-of-the-art method ABD-net, our model achieves</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">an improvement by 3.42%/0.63% on mAP/Rank-1 metrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">in Market-1501 and by 5.92%/2.61% on mAP/Rank-1 met-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">rics in DukeMTMC-reID without reranking.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance (%) comparisons of different modules on Market-1501 and DukeMTMC-reID datasets. RR: using reranking. Xent: cross entropy loss. This is because the center regularization and the viewpoint-aware adaptive soft label can complement each other. For each identity, viewpoint-aware adaptive soft label generates feature clusters of viewpoints while the center regularization makes centers of these clusters closer. This comparison demonstrates the effectiveness of center regularization.-Effectiveness of Jointing Global and Local Features.Comparing results of the VA-reID model, without reranking, jointing VA-reID and local feature can get a significant</figDesc><table><row><cell>Method</cell><cell cols="4">Market-1501 mAP Rank-1 mAP DukeMTMC-reID Rank-1</cell></row><row><cell>Xent</cell><cell cols="2">86.30 94.31</cell><cell>76.70</cell><cell>86.94</cell></row><row><cell>LSR</cell><cell cols="2">86.72 94.35</cell><cell>77.47</cell><cell>87.43</cell></row><row><cell>Ly(baseline)</cell><cell>86.97</cell><cell>94.7</cell><cell>77.99</cell><cell>87.39</cell></row><row><cell>Ly+Lv</cell><cell cols="2">88.67 95.37</cell><cell>78.86</cell><cell>88.38</cell></row><row><cell>Ly+LR</cell><cell cols="2">88.25 95.25</cell><cell>78.25</cell><cell>87.84</cell></row><row><cell>VA-reID</cell><cell cols="2">89.97 95.87</cell><cell>81.48</cell><cell>91.11</cell></row><row><cell>VA-reID+RR</cell><cell cols="2">95.09 96.32</cell><cell>90.66</cell><cell>92.46</cell></row><row><cell>VA-reID+local</cell><cell cols="2">91.70 96.23</cell><cell>84.51</cell><cell>91.61</cell></row><row><cell cols="3">VA-reID+local+RR 95.43 96.79</cell><cell>91.82</cell><cell>93.85</cell></row><row><cell cols="5">bel smoothing(VALSR) and the adaptive label smooth-</cell></row><row><cell cols="5">ing(ALSR) can achieve an improvement by 1.70%/0.67% on</cell></row><row><cell cols="5">mAP/Rank-1 metrics in Market-1501 and by 0.87%/0.99%</cell></row><row><cell cols="5">on mAP/Rank-1 metrics in DukeMTMC-reID. This is be-</cell></row><row><cell cols="5">cause VALSR uses the viewpoint-aware adaptive soft label</cell></row><row><cell cols="5">for identity-viewpoint classification. For each identity, the</cell></row><row><cell cols="5">viewpoint-aware adaptive soft label helps to learn the em-</cell></row><row><cell cols="5">bedding of viewpoint-related compact features. This com-</cell></row><row><cell cols="5">parison demonstrates the effectiveness of viewpoint-aware</cell></row><row><cell cols="2">adaptive label smoothing.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">-Effectiveness of Center Regularization. Comparing re-</cell></row><row><cell cols="5">sults of the baseline model and baseline with L R , we can</cell></row><row><cell cols="5">observe that adding an center regularization to the base-</cell></row><row><cell cols="5">line model can get a slight improvement. Comparing re-</cell></row><row><cell cols="5">sults of L y +L R and VA-reID, we can observe an signifi-</cell></row><row><cell cols="5">cant improvement by 3.00%/1.17% on mAP/Rank-1 metrics</cell></row><row><cell cols="5">in Market-1501 and by 3.45%/3.27% on mAP/Rank-1 met-</cell></row><row><cell cols="2">rics in DukeMTMC-reID.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv abs/1706.05394</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person re-identification with a body orientation-specific convolutional neural network</title>
		<idno type="arXiv">arXiv:1908.01114</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="26" to="37" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Abd-net: Attentive but diverse person re-identification</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep view-aware metric learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Deng ; Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="620" to="626" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8295" to="8302" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pose-guided representation learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>A survey of open-world person re-identification</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Harmonious attention network for person reidentification. In ICML</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6738" to="6746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crystal loss and quality pooling for unconstrained face verification and recognition</title>
		<idno>ArXiv abs/1804.01159</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Pose-normalized image generation for person re-identification. Salimans and Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In NIPS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarfraz</forename><surname>[saquib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dissecting person re-identification from the viewpoint of viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>The IEEE International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pose invariant embedding for deep person reidentification</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Viewpoint-aware attentive multi-view inference for vehicle re-identification</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
