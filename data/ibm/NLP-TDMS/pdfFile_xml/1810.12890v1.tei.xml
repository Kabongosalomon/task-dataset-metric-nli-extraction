<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DropBlock: A regularization method for convolutional networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghiasi</forename><surname>Golnaz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Tsung-Yi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le Google Brain</surname></persName>
						</author>
						<title level="a" type="main">DropBlock: A regularization method for convolutional networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves 78.13% accuracy, which is more than 1.6% improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from 36.8% to 38.4%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural nets work well when they have a large number of parameters and are trained with a massive amount of regularization and noise, such as weight decay and dropout <ref type="bibr" target="#b0">[1]</ref>. Though the first biggest success of dropout was associated with convolutional networks <ref type="bibr" target="#b1">[2]</ref>, recent convolutional architectures rarely use dropout <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. In most cases, dropout was mainly used at the fully connected layers of the convolutional networks <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>.</p><p>We argue that the main drawback of dropout is that it drops out features randomly. While this can be effective for fully connected layers, it is less effective for convolutional layers, where features are correlated spatially. When the features are correlated, even with dropout, information about the input can still be sent to the next layer, which causes the networks to overfit. This intuition suggests that a more structured form of dropout is needed to better regularize convolutional networks.</p><p>In this paper, we introduce DropBlock, a structured form of dropout, that is particularly effective to regularize convolutional networks. In DropBlock, features in a block, i.e., a contiguous region of a feature map, are dropped together. As DropBlock discards features in a correlated area, the networks must look elsewhere for evidence to fit the data (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>In our experiments, DropBlock is much better than dropout in a range of models and datasets. Adding DropBlock to ResNet-50 architecture improves image classification accuracy on ImageNet from 76.51% to 78.13%. On COCO detection, DropBlock improves AP of RetinaNet from 36.8% to 38.4%. Dropping out activations at random is not effective in removing semantic information because nearby activations contain closely related information. Instead, dropping continuous regions can remove certain semantic information (e.g., head or feet) and consequently enforcing remaining units to learn features for classifying input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Since its introduction, dropout <ref type="bibr" target="#b0">[1]</ref> has inspired a number of regularization methods for neural networks such as DropConnect <ref type="bibr" target="#b13">[14]</ref>, maxout <ref type="bibr" target="#b14">[15]</ref>, StochasticDepth <ref type="bibr" target="#b15">[16]</ref>, DropPath <ref type="bibr" target="#b16">[17]</ref>, Sched-uledDropPath <ref type="bibr" target="#b7">[8]</ref>, shake-shake regularization <ref type="bibr" target="#b17">[18]</ref>, and ShakeDrop regularization <ref type="bibr" target="#b18">[19]</ref>. The basic principle behind these methods is to inject noise into neural networks so that they do not overfit the training data. When it comes to convolutional neural networks, most successful methods require the noise to be structured <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. For example, in DropPath, an entire layer in the neural network is zeroed out of training, not just a particular unit. Although these strategies of dropping out layers may work well for layers with many input or output branches, they cannot be used for layers without any branches. Our method, DropBlock, is more general in that it can be applied anywhere in a convolutional network. Our method is closely related to SpatialDropout <ref type="bibr" target="#b19">[20]</ref>, where an entire channel is dropped from a feature map. Our experiments show that DropBlock is more effective than SpatialDropout.</p><p>The developments of these noise injection techniques specific to the architectures are not unique to convolutional networks. In fact, similar to convolutional networks, recurrent networks require their own noise injection methods. Currently, Variational Dropout <ref type="bibr" target="#b20">[21]</ref> and ZoneOut <ref type="bibr" target="#b21">[22]</ref> are two of the most commonly used methods to inject noise to recurrent connections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DropBlock</head><p>DropBlock is a simple method similar to dropout. Its main difference from dropout is that it drops contiguous regions from a feature map of a layer instead of dropping out independent random units. Pseudocode of DropBlock is shown in Algorithm 1. DropBlock has two main parameters which are block_size and γ. block_size is the size of the block to be dropped, and γ, controls how many activation units to drop.</p><p>We experimented with a shared DropBlock mask across different feature channels or each feature channel has its DropBlock mask. Algorithm 1 corresponds to the latter, which tends to work better in our experiments. Similar to dropout we do not apply DropBlock during inference. This is interpreted as evaluating an averaged prediction across the exponentially-sized ensemble of sub-networks. These sub-networks include a special subset of sub-networks covered by dropout where each network does not see contiguous parts of feature maps.</p><p>Setting the value of block_size. In our implementation, we set a constant block_size for all feature maps, regardless the resolution of feature map. DropBlock resembles dropout <ref type="bibr" target="#b0">[1]</ref> when block_size = 1 and resembles SpatialDropout <ref type="bibr" target="#b19">[20]</ref> when block_size covers the full feature map.</p><p>Setting the value of γ. In practice, we do not explicitly set γ. As stated earlier, γ controls the number of features to drop. Suppose that we want to keep every activation unit with the probability of keep_prob, in dropout <ref type="bibr" target="#b0">[1]</ref> the binary mask will be sampled with the Bernoulli distribution with mean 1 − keep_prob. However, to account for the fact that every zero entry in the mask will be expanded by block_size 2 and the blocks will be fully contained in feature map, we need to adjust γ accordingly when we sample the initial binary mask. In our implementation, γ can be computed as</p><formula xml:id="formula_0">γ = 1 − keep_prob block_size 2 f eat_size 2 (f eat_size − block_size + 1) 2<label>(1)</label></formula><p>where keep_prob can be interpreted as the probability of keeping a unit in traditional dropout. The size of valid seed region is (f eat_size − block_size + 1) 2 where f eat_size is the size of feature map. The main nuance of DropBlock is that there will be some overlapped in the dropped blocks, so the above equation is only an approximation. In our experiments, we first estimate the keep_prob to use (between 0.75 and 0.95), and then compute γ according to the above equation.</p><p>Scheduled DropBlock. We found that DropBlock with a fixed keep_prob during training does not work well. Applying small value of keep_prob hurts learning at the beginning. Instead, gradually decreasing keep_prob over time from 1 to the target value is more robust and adds improvement for the most values of keep_prob. In our experiments, we use a linear scheme of decreasing the value of keep_prob, which tends to work well across many hyperparameter settings. This linear scheme is similar to ScheduledDropPath <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the following sections, we empirically investigate the effectiveness of DropBlock for image classification, object detection, and semantic segmentation. We apply DropBlock to ResNet-50 <ref type="bibr" target="#b3">[4]</ref> with extensive experiments for image classification. To verify the results can be transferred to a different architecture, we perform DropBlock on a state-of-the-art model architecture, AmoebaNet <ref type="bibr" target="#b9">[10]</ref>, and show improvements. In addition to image classification, We show DropBlock is helpful in training RetinaNet <ref type="bibr" target="#b23">[24]</ref> for object detection and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ImageNet Classification</head><p>The ILSVRC 2012 classification dataset [25] contains 1.2 million training images, 50,000 validation images, and 150,000 testing images. Images are labeled with 1,000 categories. We used horizontal flip, scale, and aspect ratio augmentation for training images as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. During evaluation, we applied a single-crop rather than averaging results over multiple crops. Following the common practice, we report classification accuracy on the validation set.  <ref type="bibr" target="#b0">[1]</ref> 76.80 ± 0.04 93.41 ± 0.04 ResNet-50 + DropPath (kp=0.9) <ref type="bibr" target="#b16">[17]</ref> 77.10 ± 0.08 93.50 ± 0.05 ResNet-50 + SpatialDropout (kp=0.9) <ref type="bibr" target="#b19">[20]</ref> 77.41 ± 0.04 93.74 ± 0.02 ResNet-50 + Cutout <ref type="bibr" target="#b22">[23]</ref> 76.52 ± 0.07 93.21 ± 0.04 ResNet-50 + AutoAugment <ref type="bibr" target="#b26">[27]</ref> 77.63 93.82 ResNet-50 + label smoothing (0.1) <ref type="bibr" target="#b27">[28]</ref> 77 Where to apply DropBlock. In residual networks, a building block consists of a few convolution layers and a separate skip connection that performs identity mapping. Every convolution layer is followed by batch normalization layer and ReLU activation. The output of a building block is the sum of outputs from the convolution branch and skip connection.</p><p>A residual network can be represented by building groups based on the spatial resolution of feature activation. A building group consists of multiple building blocks. We use group 4 to represent the last group in residual network (i.e., all layers in conv5_x) and so on.</p><p>In the following experiments, we study where to apply DropBlock in residual networks. We experimented with applying DropBlock only after convolution layers or applying DropBlock after both convolution layers and skip connections. To study the performance of DropBlock applying to different feature groups, we experimented with applying DropBlock to Group 4 or to both Groups 3 and 4.</p><p>DropBlock vs. dropout. The original ResNet architecture does not apply any dropout in the model. For the ease of discussion, we define the dropout baseline for ResNet as applying dropout on convolution branches only. We applied DropBlock to both groups 3 and 4 with block_size = 7 by default. We decreased γ by factor 4 for group 3 in all the experiments. In <ref type="figure">Figure 3</ref>-(a), we show that DropBlock outperforms dropout with 1.3% for top-1 accuracy. The scheduled keep_prob makes DropBlock more robust to the change of keep_prob and adds improvement for the most values of keep_prob (3-(b)).</p><p>With the best keep_prob found in <ref type="figure">Figure 3</ref>, we swept over block_size from 1 to size covering full feature map. <ref type="figure">Figure 4</ref> shows applying larger block_size is generally better than applying block_size of 1. The best DropBlock configuration is to apply block_size = 7 to both groups 3 and 4.</p><p>In all configurations, DropBlock and dropout share the similar trend and DropBlock has a large gain compared to the best dropout result. This shows evidence that the DropBlock is a more effective regularizer compared to dropout.</p><p>DropBlock vs. SpatialDropout. Similar as dropout baseline, we define the SpatialDropout <ref type="bibr" target="#b19">[20]</ref> baseline as applying it on convolution branches only. SpatialDropout is better than dropout but inferior to DropBlock. In <ref type="figure">Figure 4</ref>, we found SpatialDropout can be too harsh when applying to high resolution feature map on group 3. DropBlock achieves the best result by dropping block with constant size on both groups 3 and 4.</p><p>Comparison with DropPath. Following ScheduledDropPath <ref type="bibr" target="#b7">[8]</ref> we applied scheduled DropPath on all connections except the skip connections. We trained models with different values for keep_prob parameter. Also, we trained models where we applied DropPath in all groups and similar to our other experiments only at group 4 or at group 3 and 4. We achieved best validation accuracy of 77.10% when we only apply it to group 4 with keep_prob = 0.9.</p><p>Comparison with Cutout. We also compared with Cutout <ref type="bibr" target="#b22">[23]</ref> which is a data augmentation method and randomly drops a fixed size block from the input images. Although Cutout improves accuracy on the CIFAR-10 dataset as suggested by <ref type="bibr" target="#b22">[23]</ref>, it does not improve the accuracy on the ImageNet dataset in our experiments.</p><p>Comparison with other regularization techniques. We compare DropBlock to data augmentation and label smoothing, which are two commonly used regularization techniques. In <ref type="table" target="#tab_2">Table 1</ref>, DropBlock has better performance compared to strong data augmentation <ref type="bibr" target="#b26">[27]</ref> and label smoothing <ref type="bibr" target="#b27">[28]</ref>. The performance improves when combining DropBlock and label smoothing and train for 290 epochs, showing the regularization techniques can be complimentary when we train for longer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">DropBlock in AmoebaNet</head><p>We also show the effectiveness of DropBlock on a recent AmoebaNet-B architecture which is a state-of-art architecture, found using evolutionary architecture search <ref type="bibr" target="#b9">[10]</ref>. This model has dropout with keep probability of 0.5 but only on the the final softmax layer.</p><p>We apply DropBlock after all batch normalization layers and also in the skip connections of the last 50% of the cells. The resolution of the feature maps in these cells are 21x21 or 11x11 for input image with the size of 331x331. Based on the experiments in the last section, we used keep_prob of 0.9 and set block_size = 11 which is the width of the last feature map. DropBlock improves top-1 accuracy of AmoebaNet-B from 82.25% to 82.52% ( <ref type="table" target="#tab_3">Table 2)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Analysis</head><p>DropBlock demonstrates strong empirical results on improving ImageNet classification accuracy compared to dropout. We hypothesize dropout is insufficient because the contiguous regions in convolution layers are strongly correlated. Randomly dropping a unit still allows information to flow through neighboring units. In this section, we conduct an analysis to show DropBlock is more effective in dropping semantic information. Subsequently  more robust compared to model regularized by dropout. We study the problem by applying DropBlock with block_size of 1 and 7 during inference and observing the differences in performance.</p><p>DropBlock drops more semantic information. We first took the model trained without any regularization and tested it with DropBlock with block_size = 1 and block_size = 7. The green curves in <ref type="figure">Figure 5</ref> show the validation accuracy reduced quickly with decreasing keep_prob during inference. This suggests DropBlock removes semantic information and makes classification more difficult. The accuracy drops more quickly with decreasing keep_prob, for block_size = 1 in comparison with block_size = 7 which suggests DropBlock is more effective to remove semantic information than dropout.</p><p>Model trained with DropBlock is more robust. Next we show that model trained with large block size, which removes more semantic information, results in stronger regularization. We demonstrate the fact by taking model trained with block_size = 7 and applied block_size = 1 during inference and vice versa. In <ref type="figure">Figure 5</ref>, models trained with block_size = 1 and block_size = 7 are both robust with block_size = 1 applied during inference. However, the performance of model trained with block_size = 1 reduced more quickly with decreasing keep_prob when applying block_size = 7 during inference. The results suggest that block_size = 7 is more robust and has the benefit of block_size = 1 but not vice versa.</p><p>DropBlock learns spatially distributed representations. We hypothesize model trained with DropBlock needs to learn spatially distributed representations because DropBlock is effective in removing semantic information in a contiguous region. The model regularized by DropBlock should learn multiple discriminative regions instead of only focusing on one discriminative region. We use class activation maps (CAM) introduced in <ref type="bibr" target="#b28">[29]</ref> to visualize conv5_3 class activations of ResNet-50 on ImageNet validation set. <ref type="figure">Figure 6</ref> shows the class activations of original model and models trained with DropBlock with block_size = 1 and block_size = 7. In general, models trained with DropBlock learn spatially distributed representations that induce high class activations on multiple regions, whereas model without regularization tends to focus on one or few regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Object Detection in COCO</head><p>DropBlock is a generic regularization module for CNNs. In this section, we show DropBlock can also be applied for training object detector in COCO dataset <ref type="bibr" target="#b29">[30]</ref>. We use RetinaNet <ref type="bibr" target="#b23">[24]</ref> framework for the experiments. Unlike an image classifier that predicts single label for an image, RetinaNet runs convolutionally on multiscale Feature Pyramid Networks (FPNs) <ref type="bibr" target="#b30">[31]</ref> to localize and classify objects in different scales and locations. We followed the model architecture and anchor definition in <ref type="bibr" target="#b23">[24]</ref> to build FPNs and classifier/regressor branches.</p><p>Where to apply DropBlock to RetinaNet model. RetinaNet model uses ResNet-FPN as its backbone model. For simplicity, we apply DropBlock to ResNet in ResNet-FPN and use the best keep_prob we found for ImageNet classification training. DropBlock is different from recent work <ref type="bibr" target="#b31">[32]</ref> which learns to drop a structured pattern on features of region proposals.</p><p>Training object detector from random initialization. Training object detector from random initialization has been considered as a challenging task. Recently, a few papers tried to address the issue using novel model architecture <ref type="bibr" target="#b32">[33]</ref>, large minibatch size <ref type="bibr" target="#b33">[34]</ref>, and better normalization layer <ref type="bibr" target="#b34">[35]</ref>. In our experiment, we look at the problem from the model regularization perspective. We tried DropBlock with keep_prob = 0.9, which is the identical hyperparamters as training image classification model, and experimented with different block_size. In <ref type="table" target="#tab_5">Table 3</ref>  The model was trained using 150 epochs (280k training steps). The initial learning rate 0.08 was applied for first 120 epochs and decayed 0.1 at 120 and 140 epochs. The model with ImageNet initialization was trained for 28 epochs with learning decay at 16 and 22 epochs. We used α = 0.25 and γ = 1.5 for focal loss. We used a weight decay of 0.0001 and a momentum of 0.9. The model was trained on COCO train2017 and evaluated on COCO val2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Semantic Segmentation in PASCAL VOC</head><p>We show DropBlock also improves semantic segmentation model. We use PASCAL VOC 2012 dataset for experiments and follow the common practice to train with augmented 10,582 training images <ref type="bibr" target="#b35">[36]</ref> and report mIOU on 1,449 test set images. We adopt open-source RetinaNet implementation for semantic segmentation. The implementation uses the ResNet-FPN backbone model to extract multiscale features and attaches fully convolution networks on top to predict segmentation. We use default hyperparameters in open-source code for training.</p><p>Following the experiments for object detection, we study the effect of DropBlock for training model from random initialization. We trained model started with pre-trained ImageNet model for 45 epochs and model with random initialization for 500 epochs. We experimented with applying DropBlock to ResNet-FPN backbone model and fully convolution networks and found apply DropBlock to fully convolution networks is more effective. Applying DropBlock greatly improves mIOU for training model from scratch and shrinks performance gap between training from ImageNet pre-trained model and randomly initialized model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this work, we introduce DropBlock to regularize training CNNs. DropBlock is a form of structured dropout that drops spatially correlated information. We demonstrate DropBlock is a more effective regularizer compared to dropout in ImageNet classification and COCO detection. DropBlock consistently outperforms dropout in an extensive experiment setup. We conduct an analysis to show that model trained with DropBlock is more robust and has the benefits of model trained with dropout. The class activation mapping suggests the model can learn more spatially distributed representations regularized by DropBlock.</p><p>Our experiments show that applying DropBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) input image to a convolutional neural network. The green regions in (b) and (c) include the activation units which contain semantic information in the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 1 : 5 :Figure 2 :</head><label>1152</label><figDesc>DropBlock Input:output activations of a layer (A), block_size, γ, mode 2: if mode == Inference then Randomly sample mask M : M i,j ∼ Bernoulli(γ) 6: For each zero position M i,j , create a spatial square mask with the center being M i,j , the width, height being block_size and set all the values of M in the square to be zero (see Figure 2). 7: Apply the mask: A = A × M 8: Normalize the features: A = A × count(M )/count_ones(M ) Mask sampling in DropBlock. (a) On every feature map, similar to dropout, we first sample a mask M . We only sample mask from shaded green region in which each sampled entry can expanded to a mask fully contained inside the feature map. (b) Every zero entry on M is expanded to block_size × block_size zero block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>ImageNet validation accuracy against keep_prob with ResNet-50 model. All methods drop activation units in group 3 and 4. DropBlock wo/ scheduling and not on the skip connections DropBlock wo/ scheduling DropBlock Group 4 (resolution: 7x7) Comparison of ResNet-50 trained on ImageNet when DropBlock is applied to group 4 or groups 3 and 4. From left to right, we show performance of applying DropBlock on convolution branches only and progressively improve it by applying DropBlock on skip connections and adding scheduling keep_prob. The best accuracy is achieved by using block_size = 7 (bottom right figure).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, the model regularized by DropBlock is Inference with block_size = 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 : 7 Figure 6 :</head><label>576</label><figDesc>ResNet-50 model trained with block_size = 7 and keep_prob = 0.9 has higher accuracy compared to the ResNet-50 model trained with block_size = 1 and keep_prob = 0.9: (a) when we apply DropBlock with block_size = 1 at inference with different keep_prob or (b) when we apply DropBlock with block_size = 7 at inference with different keep_prob. The models are trained and evaluated with DropBlock at groups 3 and 4. Class activation mapping (CAM)<ref type="bibr" target="#b28">[29]</ref> for ResNet-50 model trained without DropBlock and trained with DropBlock with the block_size of 1 or 7. The model trained with DropBlock tends to focus on several spatially distributed regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Implementation DetailsWe trained models on Tensor Processing Units (TPUs) and used the official Tensorflow implementations for ResNet-50 1 and AmoebaNet 2 . We used the default image size (224 × 224 for ResNet-50 and 331 × 331 for AmoebaNet), batch size (1024 for ResNet-50 and 2048 for AmoebaNet) and hyperparameters setting for all the models. We only increased number of training epochs from 90 to 270 for ResNet-50 architecture. The learning rate was decayed by the factor of 0.1 at 125, 200 and 250 epochs. AmoebaNet models were trained for 340 epochs and exponential decay scheme was used for scheduling learning rate. Since baselines are usually overfitted for the longer training scheme and have lower validation accuracy at the end of training, we report the highest validation accuracy over the full training course for fair comparison.4.1.1 DropBlock in ResNet-50ResNet-50<ref type="bibr" target="#b3">[4]</ref> is a widely used Convolutional Neural Network (CNN) architecture for image recognition. In the following experiments, we apply different regularization techniques on ResNet-50 and compare the results with DropBlock. The results are summarized inTable 1.</figDesc><table><row><cell>Model</cell><cell>top-1(%)</cell><cell>top-5(%)</cell></row><row><cell>ResNet-50</cell><cell>76.51 ± 0.07</cell><cell>93.20 ± 0.05</cell></row><row><cell>ResNet-50 + dropout (kp=0.7)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Summary of validation accuracy on ImageNet dataset for ResNet-50 architecture. For dropout, DropPath, and SpatialDropout, we trained models with different keep_prob values and reported the best result. DropBlock is applied with block_size = 7. We report average over 3 runs.</figDesc><table><row><cell>.17 ±0.05</cell><cell>93.45 ±0.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Top-1 and top-5 validation accuracy of AmoebaNet-B architecture trained on ImageNet.</figDesc><table><row><cell>Model</cell><cell cols="2">top-1(%) top-5(%)</cell></row><row><cell>AmoebaNet-B (6, 256)</cell><cell>82.25</cell><cell>95.88</cell></row><row><cell>AmoebaNet-B (6, 256) + DropBlock</cell><cell>82.52</cell><cell>96.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, we show that the model trained from random initialization surpasses ImageNet pre-trained model. Adding DropBlock gives additional 1.6% AP. The results suggest model regularization is an important ingredient to train object detector from scratch and DropBlock is an effective regularization approach for object detection.</figDesc><table><row><cell>Model</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell></row><row><cell>RetinaNet, fine-tuning from ImageNet</cell><cell>36.5</cell><cell>55.0</cell><cell>39.1</cell></row><row><cell>RetinaNet, no DropBlock</cell><cell>36.8</cell><cell>54.6</cell><cell>39.4</cell></row><row><cell>RetinaNet, keep_prob = 0.9, block_size = 1</cell><cell>37.9</cell><cell>56.1</cell><cell>40.6</cell></row><row><cell>RetinaNet, keep_prob = 0.9, block_size = 3</cell><cell>38.3</cell><cell>56.4</cell><cell>41.2</cell></row><row><cell>RetinaNet, keep_prob = 0.9, block_size = 5</cell><cell>38.4</cell><cell>56.4</cell><cell>41.2</cell></row><row><cell>RetinaNet, keep_prob = 0.9, block_size = 7</cell><cell>38.2</cell><cell>56.0</cell><cell>40.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Object detection results trained from random initialization in COCO using RetinaNet and ResNet-50 FPN backbone model.Implementation details.We use open-source implementation of RetinaNet 3 for experiments. The models were trained on TPU with 64 images in a batch. During training, multiscale jitter was applied to resize images between scales [512, 768] and then the images were padded or cropped to max dimension 640. Only single scale image with max dimension 640 was used during testing. The batch normalization layers were applied after all convolution layers, including classifier/regressor branches.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Semantic segmentation results trained from random initialization in PASCAL VOC 2012 using ResNet-101 FPN backbone model.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tensorflow/tpu/tree/master/models/official/resnet 2 https://github.com/tensorflow/tpu/tree/master/models/experimental/amoeba_net</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/tensorflow/tpu/tree/master/models/official/retinanet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inception-v4, inceptionresnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6307" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fractalnet</surname></persName>
		</author>
		<title level="m">Ultra-deep neural networks without residuals. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Shake-shake regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno>abs/1705.07485</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Kise</surname></persName>
		</author>
		<idno>abs/1802.02375</idno>
		<title level="m">Shakedrop regularization. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Zoneout: Regularizing rnns by randomly preserving hidden activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">János</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<idno>abs/1606.01305</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1805.09501</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A-Fast-RCNN: Hard positive generation via adversary for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DSOD: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MegDet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
