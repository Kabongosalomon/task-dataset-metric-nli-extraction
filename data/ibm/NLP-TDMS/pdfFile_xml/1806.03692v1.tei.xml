<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deconvolution-Based Global Decoding for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-06-10">10 Jun 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Foreign Languages</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Foreign Languages</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deconvolution-Based Global Decoding for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-06-10">10 Jun 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A great proportion of sequence-to-sequence (Seq2Seq) models for Neural Machine Translation (NMT) adopt Recurrent Neural Network (RNN) to generate translation word by word following a sequential order. As the studies of linguistics have proved that language is not linear word sequence but sequence of complex structure, translation at each step should be conditioned on the whole target-side context. To tackle the problem, we propose a new NMT model that decodes the sequence with the guidance of its structural prediction of the context of the target sequence. Our model generates translation based on the structural prediction of the target-side context so that the translation can be freed from the bind of sequential order. Experimental results demonstrate that our model is more competitive compared with the state-of-the-art methods, and the analysis reflects that our model is also robust to translating sentences of different lengths and it also reduces repetition with the instruction from the target-side context for decoding. This work is licensed under a Creative Commons Attribution 4.0 International License.</p><p>License details:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has achieved tremendous success in machine translation, outperforming the traditional linguistic-rule-based and statistical methods. In recent studies of Neural Machine Translation (NMT), most models are based on the sequence-to-sequence (Seq2Seq) model based on the encoder-decoder framework <ref type="bibr" target="#b10">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b32">Sutskever et al., 2014;</ref> with the attention mechanism . While traditional linguistic-rule-based and statistical methods of machine translation require much work of feature engineering, NMT can be trained in the end-to-end fashion. Besides, the attention mechanism can model the alignment relationship between the source text and translation , and some recent improved versions of attention have proved successful in this task <ref type="bibr" target="#b35">(Tu et al., 2016;</ref><ref type="bibr" target="#b23">Mi et al., 2016;</ref><ref type="bibr" target="#b22">Meng et al., 2016;</ref><ref type="bibr" target="#b40">Xiong et al., 2017;</ref><ref type="bibr" target="#b37">Vaswani et al., 2017)</ref>.</p><p>However, the decoding pattern of the recent Seq2Seq models is inconsistent with the linguistic analysis. As the conventional decoder translates words in a sequential order, the current generation is highly dependent on the previous generation and it is short of the knowledge about future generation. Nida <ref type="bibr">(1969)</ref> pointed out that translation goes through a process of analysis, transfer and reconstruction, involving the deep syntactic and semantic structure of the source and target languages. Language generation involves complex syntactic analysis and semantic integration, instead of a step-by step word generation <ref type="bibr" target="#b5">(Frazier, 1987)</ref>. Moreover, from the perspective of semantics and pragmatics, the syntactic analysis of utterance can be guided by the global lexical-semantic and discourse information <ref type="bibr" target="#b0">(Altmann and Steedman, 1988;</ref><ref type="bibr" target="#b33">Trueswell et al., 1994</ref><ref type="bibr" target="#b34">Trueswell et al., , 1993</ref><ref type="bibr" target="#b36">Tyler and Marslen-Wilson, 1977)</ref>. In brief, the process of translation is in need of the global information from the target-side context, but the decoding pattern of the conventional Seq2Seq model in NMT does not meet the requirement.</p><p>Recent researches in NMT have taken this issue into consideration by the implementation of bidirectional decoding. Some methods of bidirectional decoding <ref type="bibr" target="#b4">Cong et al., 2017)</ref> rerank the candidate translations with the scores from the bidirectional decoding. However, these bidirectional decoding methods cannot provide effective complementary information due to the limited search space of beam search.</p><p>In this article, we extend the conventional attention-based Seq2Seq model by introducing the deconvolution-based decoder, which is a Convolutional Neural Network (CNN) to perform deconvolution. Recently, deconvolution has been applied to the studies of natural language , which can be regarded as the transposition of the convolution <ref type="bibr" target="#b17">(Long et al., 2015;</ref><ref type="bibr" target="#b26">Noh et al., 2015)</ref>.  applied this method to natural language by modeling sentences with a convolution-deconvolution autoencoder. The study of  showed that the deconvolution solves the problems by reconstructing a representation of high quality irrespective to the order or length. It can be found that deconvolution owns the potential to provide global information for guidance of decoding. Therefore, we follow this idea and propose a new model with deconvolution for NMT.</p><p>To be specific, the conventional RNN encoder encodes the source sentences to new representations and sends the final state to the decoder, and the conventional RNN decoder decodes it to the target sentences with the attention to the encoder outputs. In our model, our designed deconvolution-based decoder decodes the final state of the encoder to a matrix representing the global information of the target-side contexts. Each column of the matrix is learned to be close to the word embedding of the target words. The conventional RNN decoder can attend to the columns for the information of the target-side context to perform global decoding in the translation.</p><p>Our contributions in this study are illustrated in the following:</p><p>• We propose a new model for NMT, which contains a deconvolution-based decoder to provide global information of the target-side contexts to the RNN decoder, so that the model is able to perform global decoding 1 .</p><p>• Experimental results demonstrate that our model outperforms the baseline models in both the Chinese-to-English translation and the English-to-Vietnamese translation, outperforming the Seq2Seq model in the BLEU score evaluation with the advantages of BLEU score 2.82 and 1.54 respectively.</p><p>• The analysis shows that our model that performs global decoding is more capable of reducing repetition and more robust to the translation of sentences of different lengths, and the case study reflects that it is able to capture the syntactic structure for the translation and has a better reflection of the semantic meaning of the source text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>In the following, we introduce the details of our model, including the encoder, the deconvolution-based decoder and the conventional RNN-based decoder. The functions of each decoder are illustrated below to show how they collaborate to improve the quality of the translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoder</head><p>In our model, the encoder reads the embeddings of the input text sequence x = {x 1 , ..., x n } and encodes a sequence of encoder outputs h = {h 1 , ..., h n }. The final hidden state h n is sent to the decoder as the initial state for it to decode a sequence of output text. The encoder outputs provide the information of the source-side contexts to our RNN-Based decoder through the attention mechanism.</p><p>The encoder in our model is a bidirectional LSTM <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997)</ref>, which reads the input in two directions to generate two sequences of hidden states</p><formula xml:id="formula_0">− → h = { − → h 1 , − → h 2 , − → h 3 , ..., − → h n } and ← − h = { ← − h 1 , ← − h 2 , ← − h 3 , ..., ← − h n }, where: − → h i = LST M(x i , − − → h i−1 , C i−1 ) (1) ← − h i = LST M(x i , ← − − h i−1 , C i−1 )<label>(2)</label></formula><p>The encoder outputs corresponding to each time step are concatenated as There are three components in the proposed model, i.e., the LSTM encoder, the deconvolution-based decoder, and the conventional LSTM decoder. The encoder distills the input sentence into a state h n , which is then used in the deconvolution-based decoder to obtain the global information of the target-side contexts. Based on the target-side contexts and the input-side contexts, the conventional LSTM decoder generates the output from the state h n .</p><formula xml:id="formula_1">h i = [ − → h i ; ← − h i ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deconvolution-Based Decoder</head><p>In our model, there are two decoders, which perform different tasks for the whole decoding process. While the RNN-based decoder is similar to the conventional decoder, which decodes the output text sequence in a sequential order and attends to the annotations of the encoder via the attention mechanism, our proposed deconvolution-based decoder does not decode the text but provide global information of the target-side contexts to the RNN-based decoder so that it can decode structurally instead of sequentially.</p><p>To be specific, the deconvolution-based decoder learns to generate the word embedding matrix of the target text sequence.</p><p>In order to provide global information of the target-side contexts to the RNN decoder, we implement a multilayer CNN as the deconvolution-based decoder to perform deconvolution. With deconvolution, it is available for a vector or a small matrix to be transformed to a large matrix. In our model, the deconvolution is implemented on the final states in both directions from the encoder. As words in our model are represented with word embedding vectors, sentences can be formed as word embedding matrices. In our model, the deconvolution-based decoder is designed to learn word embedding matrices of the target sequences with the representation matrix from the encoder. As the conjugate operation of convolution, deconvolution expands the dimension of the input representation to a matrix of our designed size. There are L layers in the deconvolution, each of which has f l filters of kernel size k l . The i th filter W i l ∈ R k×dim (dim refers to the size of the input representation vector) with stride s i l and padding p i l performs deconvolution on the input representation matrix I ∈ R m×dim (m × dim refers to the size of the input representation matrix), the final hidden state of the encoder. The computation of convolution is illustrated as below:</p><formula xml:id="formula_2">c i l = g(W i l * X + b)<label>(3)</label></formula><p>where X refers to the convolved matrix and g refers to non-linear activation function, which is ReLU <ref type="bibr" target="#b24">(Nair and Hinton, 2010)</ref> following , and deconvolution is its transposed operation.</p><p>With the input I, our objective of the deconvolution operation is to generate a word embedding matrix E ∈ R T ×dim where T refers to the sentence length designed for the output text sequence, which is a hyper-parameter. At the l th layer, deconvolution generates a matrix</p><formula xml:id="formula_3">E l ∈ R T l ×dim where T l = T l−1 × s l + k l − 2 × p l .</formula><p>With the control of stride and kernel size, the height of the matrix can be assigned, and with the control of the number of filters, the width of the matrix can also be assigned, which are the length of the output sequence and the dimension of the word embedding respectively. The deconvolution-based decoder can generate meaningful representation with information different from that in the conventional RNN decoder. The conventional RNN decoder generates sequence in a way similar to Markov Decision Process, which is highly dependent on the previous generation and follows <ref type="bibr">(a)</ref> Deconvolution-based decoder.</p><p>(b) Deconvolution. <ref type="figure" target="#fig_1">Figure 2</ref>: Deconvolution-based decoder. On the right shows an example of a 1d deconvolution on a input of size 2 with a kernel of size 4, a padding of 1 and, a stride of 2. The depth means the dimension of the channel, which is dim in our case. a strict sequential order, so it contains high sequential dependency. On the contrary, the deconvolutionbased decoder generates a word embedding matrix depending on the representation from the encoder without considering the order, which does not have the problem of long-term dependency. Moreover, although it is not capable as the RNN encoder to generate coherent text, it reveals the information of the text from a global perspective, including syntactic and semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">RNN-Based Decoder</head><p>Different from the deconvolution-based decoder, the RNN-based decoder is responsible for decoding the representation h n to generate the translation y = {y 1 , ..., y m }. With the final encoder state as the initial state, the decoder is initialized to decode in sequential order, until it generates the token representing the end of sentence. During decoding, the attention mechanism is applied for the decoder to extract the information from the source-side contexts, which are the annotations of the encoder, as well as the information from the target-side contexts, which are the outputs of the deconvolution-based decoder.</p><p>For the RNN-based decoder, we implement a unidirectional LSTM. The output of the RNN-based decoder at each time step is sent into a feed-forward neural network to be projected into the space of the target vocabulary Y ∈ R |Y |×dim for the prediction of the translated word. At each time step, the decoder generates a word y t by sampling from a conditional probability distribution of the target vocabulary P vocab , where:</p><formula xml:id="formula_4">P vocab = sof tmax(W o v t ) (4) v t = g(s t , c t ,c t ) (5) s t = LST M (y t−1 , s t−1 , C t−1 )<label>(6)</label></formula><p>where g(·) refers to non-linear activation function, and c t andc t are the outputs of the attention mechanism, which are illustrated in the following. The attention mechanism in our model is the global attention mechanism . Different from the conventional attention mechanism, which only computes the attention scores on the source-side contexts, the attention mechanism in our model consists of two parts. The first one is similar to the conventional one, attending to the source-side contexts from the encoder, but the second one is original, which attends to the target-side contexts, which is the word embedding matrix generated by the deconvolution-based decoder. By attending to the encoder annotations, the model computes the attention α t,i of the RNN-based decoder output s t on the annotations of the encoder h i and generates the context vector c t . Similarly, by attending to the outputs of the deconvolution-based decoder, the RNN-based decoder computes the attentions of s t on each column E i of its matrix E and generates the context vector c t :</p><formula xml:id="formula_5">c t = n i=1 α t,i h i (7) c t = n i=1α t,i E i (8)</formula><p>where α t,i andα t,i are defined as below (as they are computed in the same way, they are both represented by α t,i and the annotations are represented by x i ):</p><formula xml:id="formula_6">α t,i = exp(e t,i ) n j=1 exp(e t,j ) (9) e t,i = s ⊤ t−1 W a x i<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>The training for the Seq2Seq model is usually based on maximum likelihood estimation. Given the parameters θ and source text x, the model generates a sequenceỹ. The learning process is to minimize the negative log-likelihood between the generated textỹ and reference y, which in our context is the sequence in target language for machine translation:</p><formula xml:id="formula_7">L = − 1 N N i=1 T t=1 log P (y (i) t |ỹ (i) &lt;t , x (i) , θ)<label>(11)</label></formula><p>where the loss function is equivalent to maximizing the conditional probability of sequence y given parameters θ and source sequence x. However, as there are two decoders in our model, the loss function should also be designed for the deconvolution-based decoder. In our model, we compute the smooth L1 loss between the generated matrix of the deconvolution-based decoder E and the word embedding matrixẼ (which both contain M elements), which is more robust to outliers <ref type="bibr" target="#b7">(Girshick, 2015)</ref>, as well as the cross-entropy loss between the prediction of the deconvolution-based decoderŷ and reference y given the parameters of the encoder and the deconvolution-based decoder θ ′ . Therefore, the generated matrix E can be closer to the word embedding matrixẼ, and it contains information beneficial to the prediction of the target words. Moreover, for the cross entropy loss of the deconvolution-based decoder, we apply the method of  as it increases no parameter for the prediction by computing the cosine similarity between the output and the word embeddings. To sum up, the loss function is defined as below:</p><formula xml:id="formula_8">L = − 1 N N i=1 ( T t=1 log P (y (i) t |ỹ (i) &lt;t , x (i) , θ) + M m=1 smooth L1 (E m −Ẽ m ) + T t=1 log P (y (i) t |x (i) , θ ′ )) (12)</formula><p>where smooth L1 loss is defined below:</p><formula xml:id="formula_9">smooth L1 (x, y) = 0.5 || x − y || 2 2 if || x − y ||&lt; 1 || x − y || 1 −0.5 if || x − y ||≥ 1<label>(13)</label></formula><p>We have tested L1 loss, L2 loss as well as smooth L1 loss in our experiments and found that smooth L1 loss encourages the model to reach the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluate our proposed model on the NIST translation task for the Chinese-to-English translation and provide the analysis on the same task. Moreover, in order to evaluate the performance of our model on the low-resource translation, we also evaluate our model on the IWLST 2015 <ref type="bibr" target="#b2">(Cettolo et al., 2015)</ref> for the English-to-Vietnamese translation task.</p><p>Chinese-to-English Translation For the NIST translation task, we train our model on 1.25M sentence pairs extracted from LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06, with 27.9M Chinese words and 34.5M English words. Following , we validate our model on the dataset for the NIST 2002 translation task and tested our model on that for the <ref type="bibr">NIST 2003</ref><ref type="bibr">NIST , 2004</ref><ref type="bibr">NIST , 2005</ref><ref type="bibr">NIST , 2006</ref> translation tasks. We use the most frequent 30K words for both the Chinese vocabulary and the English vocabulary, which includes around 97.4% and 99.5% of the Chinese and English words in the training data. The sentence pairs longer than 50 words are filtered. The evaluation metric is BLEU <ref type="bibr" target="#b27">(Papineni et al., 2002)</ref>.</p><p>English-to-Vietnamese Translation The data is from the translated TED talks, containing around 133K training sentence pairs provided by the IWSLT 2015 Evaluation Campaign <ref type="bibr" target="#b2">(Cettolo et al., 2015)</ref>. We follow the studies of <ref type="bibr" target="#b9">Huang et al. (2017)</ref>, and use the same preprocessing methods as well as the same validation and the test set. The validation set is the TED tst2012 with 1553 sentences and the test set is the TED tst2013 with 1268 sentences. The English vocabulary is 17.7K words and the Vietnamese vocabulary is 7K words. The evaluation metric is also BLEU score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Setting</head><p>We implement the models on PyTorch 2 , and the experiments are conducted on an NVIDIA 1080Ti GPU. Both the size of word embedding and the number of units in the hidden layers are 512, and the batch size is 64. We use Adam optimizer (Kingma and Ba, 2014) to train the model with the setting β 1 = 0.9, β 2 = 0.98 and ǫ = 1 × 10 −9 following <ref type="bibr" target="#b37">Vaswani et al. (2017)</ref>, and we initialize the learning rate to 0.0003.</p><p>Gradient clipping is applied so that the norm of the gradients cannot be larger than a constant, which is 10 in our experiments. Dropout is used with the dropout rate set to 0.3 for the Chinese-to-English translation and 0.4 for the English-to-Vietnamese translation, based on the performance on the validation set.</p><p>Based on the performance on the validation set, we use beam search with a beam width of 10 to generate translation for the evaluation and test, and we normalize the log-likelihood scores by sentence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>For the Chinese-to-English translation, we compare our model with the state-of-the-art NMT systems for the task.</p><p>• Moses An open source phrase-based translation system with default configurations and a 4-gram language model trained on the training data for the target language;</p><p>• RNNsearch An attention-based Seq2Seq with fine-tuned hyperparameters ;</p><p>• Coverage The method extends RNNSearch with a coverage model for the attention mechanism that tackles the problem of over-translation and under-translation <ref type="bibr" target="#b35">(Tu et al., 2016)</ref>;</p><p>• Lattice The Seq2Seq model with a word-lattice-based RNN encoder that tackles the problem of tokenization in NMT <ref type="bibr" target="#b30">(Su et al., 2016)</ref>;</p><p>• InterAtten The Seq2Seq model that records the interactive history of decoding <ref type="bibr" target="#b22">(Meng et al., 2016)</ref>;</p><p>• MemDec Based on the RNNSearch, it is equipped with external memory that the model reads and writes during decoding .</p><p>For the English-to-Vietnamese translation, we compare our model with the recent NMT models for this task, and we present the results of the baselines reported in their articles.  <ref type="table">Table 2</ref>: Results of our model and the baselines (directly reported in the referred articles) on the Englishto-Vietnamese translation, tested on the TED tst2013 with the BLEU score evaluation.</p><p>• RNNsearch-1 The attention-based Seq2Seq model by ;  <ref type="table">Table 1</ref> shows the overall results of the models on the Chinese-to-English translation task. Beside our reimplementation of the attention-based Seq2Seq model, we report the results of the recent NMT models, which are results in their original articles or improved results of the reimplementation. To facilitate fair comparison, we compare with the baselines that are trained on the same training data. The results have shown that for the <ref type="bibr">NIST 2003</ref><ref type="bibr">NIST , 2004</ref><ref type="bibr">NIST , 2005</ref><ref type="bibr">NIST and 2006</ref> translation tasks, our model with the deconvolutionbased decoder outperforms the baselines, and the advantage of BLEU score over the attention-based Seq2Seq model is 2.82 on average compared with our reimplementation of the attention-based Seq2Seq model. From the results mentioned above, it can be inferred that the global information of the target-side contexts retrieved from the deconvolution-based decoder is contributive to the translation. Our analysis and case study in the following can further demonstrate how the deconvolution-based decoder improves the attention-based Seq2Seq model. <ref type="table">Table 2</ref> presents the results of the models on the English-to-Vietnamese translation. Compared with the attention-based Seq2Seq model, including the implementation with the strongest performance, our model with the deconvolution-based decoder can outperform it with the advantage of BLEU score 1.54. We also display the most recent model NPMT <ref type="bibr" target="#b9">(Huang et al., 2017)</ref> trained and tested on the dataset. Compared with NPMT, our model has an advantage of BLEU score 0.78. It can be indicated that for the low-resource translation, the information from the deconvolution-based decoder is important, which brings significant improvement to the conventional attention-based Seq2Seq model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>As our model generates translation with global information from the deconvolution-based decoder, it should learn to reduce repetition as it can learn to avoid generating same contents according to the conjecture by the deconvolution-based decoder about the target-side contexts. In order to test whether our model can mitigate the problem of repetition in translation, we test the repetition on the NIST 2003 dataset, following <ref type="bibr" target="#b28">See et al. (2017)</ref>. The proportions of the duplicates of 1-gram, 2-gram, 3-gram and 4-gram in each sentence are calculated. Results on <ref type="figure" target="#fig_2">Figure 3</ref>(a) show that our model generates less repetitive translation. In particular, the proportion of duplicates of our model is less than half of that of the conventional Seq2Seq model.</p><p>Moreover, to validate its robustness on different sentence-length levels, we test the BLEU scores on sentences of length no shorter than 10 to 60 of the NIST 2003 dataset. According to the results on <ref type="figure" target="#fig_2">Figure 3(b)</ref>, though with the increase in length, the performance of our model is always stronger than the Seq2Seq model. However, with the increase of length, the advantage of our model becomes smaller. This is consistent with our hypothesis. Since the length of generation of the deconvolution-based decoder is assigned a particular value (30 words in Chinese-to-English translation) due to the limited computation resources, there is not enough global target-side information for translating long sentences (say, longer than 30 words). In our future work, we will delve into this problem and conduct further research to reduce computation cost. <ref type="figure" target="#fig_3">Figure 4</ref> presents the attention heatmaps of the RNN-based decoder on the generated matrix of the deconvolution-based decoder in the English-to-Vietnamese translation. They reflect that the RNN-based decoder has diverse local focuses on the self-contained target-side contexts at different time steps. Contrary to the conventional attention on the source-side contexts which captures the corresponding annotations, it focuses on groups of the columns of the generated matrix from the deconvolution-based decoder. With the guidance of the information of global decoding, the model generates translation of higher accuracy and higher coherence. However, as the deconvolution-based decoder is not responsible for generating translation, it is hard to interpret what each column of the generated matrix represents. Moreover, as it does not capture alignment relationship as the conventional attention mechanism does, it is our future work to improve the attention on the outputs of the deconvolution-based decoder and explain the group focuses as shown in the heatmaps.  <ref type="table">Table 3</ref> demonstrates three examples of the translation of our model in comparison with the Seq2Seq model and the golden translation. In <ref type="table">Table 3</ref>(a), while the Seq2Seq model cannot recognize the objects of the main clause and the infinitive, causing inaccuracy and repetition, our model better captures the syntactic structure of the sentence and translates the main idea of the source text, though leaving the information "that causes disease". In <ref type="table">Table 3</ref>(b), the source sentence is more complicated. With a temporal adverbial clause, its syntactic structure is more complex than simple sentence. The translation of the conventional Seq2Seq model cannot capture the syntactic information in the source text and regards the "parliament members" as the argument of the predicate "talk". Moreover, it is confused by the word "中期", meaning "middle", and translates "mid -autumn festival". In comparison, our model can recognize the adverbial clause and the main clause as well as their syntactic structures. In <ref type="table">Table 3</ref>(c), it can be shown that when translating the long and relatively complex text, the baseline model makes a series of mistake of repetition. In contrast, the translation generated by our model though repeats the word "disaster", it is much more coherent and more semantically consistent with the source text as it successfully presents "sent 250,000 yuan" corresponding to the source text "调拨25万元人民币", while the baseline cannot translate the content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In the following, we review the studies in NMT and the application of deconvolution in NLP. <ref type="bibr" target="#b10">Kalchbrenner and Blunsom (2013)</ref>; ; <ref type="bibr" target="#b32">Sutskever et al. (2014)</ref> studied the application of the encoder-decoder framework on the machine translation task, which launched the development of NMT. Another significant innovation in this field is the attention mechanism, which builds connection between the translated contents and the source text . To improve the quality of NMT, researchers have focused on improving the attention mechanism. <ref type="bibr" target="#b35">Tu et al. (2016)</ref> and <ref type="bibr" target="#b23">Mi et al. (2016)</ref> modeled coverage in the NMT, <ref type="bibr" target="#b22">Meng et al. (2016)</ref> and <ref type="bibr" target="#b40">Xiong et al. (2017)</ref> incorporated the external memory to the attention, and <ref type="bibr" target="#b39">Xia et al. (2017)</ref> as well as <ref type="bibr" target="#b14">Lin et al. (2018a)</ref> utilized the information from the previous generation by target-side attention and memory for attention history respectively. For more target-side information,  incorporated bag of words as target. A breakthrough of NMT in recent years is that <ref type="bibr" target="#b37">Vaswani et al. (2017)</ref> invented a model only with the attention mechanism that reached the state-of-the-art performance.</p><p>Although many researches in NLP focused on the application of RNN, CNN is also an important type of network for the study of language <ref type="bibr" target="#b12">(Kim, 2014;</ref><ref type="bibr" target="#b11">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b41">Zhang et al., 2015;</ref>. Also, its application in NMT has been successful <ref type="bibr" target="#b6">(Gehring et al., 2017)</ref>. Recently, deconvolution was applied to modeling text , which is able to construct a representation of high quality with the self-contained information.</p><p>Text: 基因 科学家 的 目标 是 , 提供 诊断 工具 以 发现 致病 的 缺陷 基因 Gold: the goal of geneticists is to provide diagnostic tools to identify defective genes that cause diseases Seq2Seq: the objective of genetic scientists is to provide genes to detect genetic genetic genes DeconvDec: the objective of the gene scientist is to provide diagnostic tools to detect the defective genes <ref type="bibr">(a)</ref> Text: 叛军 暗杀 两位 菲 国 国会 议员 后, 菲律宾 总统 雅罗育 在 二零零一 年 中期 停止 与 共 党 谈判 。 Gold: after the rebels assassinated two philippine legislators , philippine president arroyo ceased negotiations with the communist party in mid 2001 . Seq2Seq: philippine president gloria arroyo stopped the two philippine parliament members in the mid -autumn festival . DeconvDec: philippine president gloria arroyo stopped holding talks with the communist party after the rebels assassinated two philippine parliament members . <ref type="bibr">(b)</ref> Text: 中国 红十字会 已 在 24日 地震 发生 后 紧急 向 新疆 灾区 调拨 25万 元 人民币 , 又 于 25日 向 灾区 派出 救灾 工作组 。 Gold: china red cross has released 250 thousand renminbi for the xinjiang disaster area immediately after the earthquake on the 24th . a disaster relief team was dispatched to the area on the 25th . Seq2Seq:</p><p>the red cross society of china ( red cross ) , the china red cross society ( red cross ) , emergency relief team sent an emergency team to xinjiang for disaster relief in the disaster areas after the earthquake on 24 june . DeconvDec: the china red cross society has sent 250,000 yuan to the disaster areas in xinjiang after the earthquake occurred on the 24 th, and sent a relief team to disaster disaster areas on the 25 th.</p><p>(c) <ref type="table">Table 3</ref>: Two examples of the translation of our model in comparison with the conventional attentionbased Seq2Seq model on the NIST 2003 Chinese-to-English translation task. The errors in the translation are colored in red and the successful translation of some particular contents are colored in yellow (e.g., the contents that the model successfully translates but the other does not).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we propose a new model with the global decoding mechanism. With our deconvolutionbased decoder, which provides global information of the target-side contexts, the model can effectively exploit the information for the inference of syntactic structure and semantic meaning in the translation. Experimental results on the Chinese-to-English translation and English-to-Vietnamese translation demonstrate that our model outperforms the baseline models, and the analysis shows that our model generates less repetitive translation and demonstrates higher robustness to the sentences of different lengths. Furthermore, the case study shows that the translation of our model better observes the syntactic and semantic requirements for the translation and generates coherent and accurate translation with fewer irrelevant contents.</p><p>In the future, we will further develop analysis of the mechanism of deconvolution in NMT and try to figure out its generalized patterns for the construction of the target-side contexts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>• RNNsearch- 2</head><label>2</label><figDesc>The implementation of the attention-based Seq2Seq by Huang et al. (2017);• LabelEmb Extending RNNSearch with soft target representation<ref type="bibr" target="#b31">(Sun et al., 2017)</ref>;• NPMT The Neural Phrased-based Machine Translation model by<ref type="bibr" target="#b9">Huang et al. (2017)</ref>;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Percentage of the duplicates at sentence level and the BLEU scores on sentences of different lengths Tested on the NIST 2003 dataset. The red bar and line indicate the performance of our model, and the blue bar and line indicate that of the attention-based SeqSeq model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Attention heatmaps of the RNN-based decoder on the deconvolution-based decoder Words on the left refer to the translation of the RNN-based decoder. The heatmaps show that the RNNbased decoder can focus on certain parts of the outputs from the deconvolution-based decoder.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is released in https://github.com/lancopku/DeconvDec</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://pytorch.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by National Natural Science Foundation of China (No. 61673028) and the National Thousand Young Talents Program. Xu Sun is the corresponding author of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Interaction with context during human sentence processing. Cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Altmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">191</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Roldano Cattoni, and Marcello Federico</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWSLT</title>
		<meeting>of IWSLT<address><addrLine>Da Nang, Vietnam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>The iwslt 2015 evaluation campaign</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Towards decoding as continuous optimisation in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy</forename><surname>Vu Hoang Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyn</forename><surname>Frazier</surname></persName>
		</author>
		<title level="m">Sentence Processing: A Tutorial Review</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1706.05565</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Decoding-history-based adaptive control of attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1802.01812</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Global encoding for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
		<idno>abs/1805.03989</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Agreement on targetbidirectional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="411" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation</title>
		<meeting>the International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Query and output: Generating words by querying distributed word representations for paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="196" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bag-of-words as target for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1805.04871</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interactive attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2174" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coverage embedding models for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Baskaran Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="955" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eugene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nida</surname></persName>
		</author>
		<title level="m">Science of translation. Language</title>
		<imprint>
			<date type="published" when="1969" />
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deconvolutional latent-variable model for text sequence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno>abs/1709.07109</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Lattice-based recurrent neural network encoders for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1609.07730</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Label embedding network: Learning label representation for soft training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<idno>abs/1710.10393</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic influences on parsing: Use of thematic role information in syntactic ambiguity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Trueswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Garnsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="285" to="318" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Verb-specific constraints in sentence processing: separating effects of lexical preference from garden-paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Trueswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology Learning Memory and Cognition</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="528" to="553" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The on-line effects of semantic context on syntactic processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorraine</forename><forename type="middle">K</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">D</forename><surname>Marslen-Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Verbal Learning and Verbal Behavior</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="683" to="692" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. CoRR, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Memory-enhanced decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="278" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence generation with target attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases -European Conference, ECML PKDD 2017</title>
		<meeting><address><addrLine>Skopje, Macedonia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-18" />
			<biblScope unit="page" from="816" to="831" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multi-channel encoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1712.02109</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><forename type="middle">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deconvolutional paragraph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4172" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
