<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compact Trilinear Interaction for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuong</forename><surname>Do</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AIOZ Pte Ltd</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huy</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AIOZ Pte Ltd</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erman</forename><surname>Tjiputra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AIOZ Pte Ltd</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AIOZ Pte Ltd</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compact Trilinear Interaction for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Visual Question Answering (VQA), answers have a great correlation with question meaning and visual contents. Thus, to selectively utilize image, question and answer information, we propose a novel trilinear interaction model which simultaneously learns high level associations between these three inputs. In addition, to overcome the interaction complexity, we introduce a multimodal tensor-based PARALIND decomposition which efficiently parameterizes trilinear interaction between the three inputs. Moreover, knowledge distillation is first time applied in Free-form Opened-ended VQA. It is not only for reducing the computational cost and required memory but also for transferring knowledge from trilinear interaction model to bilinear interaction model. The extensive experiments on benchmarking datasets TDIUC, VQA-2.0, and Visual7W show that the proposed compact trilinear interaction model achieves state-of-the-art results when using a single model on all three datasets. The source code is available at https://github.com/aioz-ai/ICCV19_</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The aim of VQA is to find out a correct answer for a given question which is consistent with visual content of a given image <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10]</ref>. There are two main variants of VQA which are Free-Form Opened-Ended (FFOE) VQA and Multiple Choice (MC) VQA. In FFOE VQA, an answer is a free-form response to a given image-question pair input, while in MC VQA, an answer is chosen from an answer list for a given image-question pair input.</p><p>Traditional approaches to both VQA tasks mainly aim to learn joint representations between images and questions, while the answers are treated in a "passive" form, i.e., the answers are only considered as classification targets. However, an answer is expected to have high correlation with its corresponding question-image input, hence a jointly and explicitly information extraction from these three inputs will give a highly meaningful joint representation. In this paper, we propose a novel trilinear interaction model which simul- † indicates equal contribution. taneously learns high level associations between all three inputs, i.e., image, question, and answer.</p><p>The main difficulty in trilinear interaction is the dimensionality issue which causes expensive computational cost and huge memory requirement. To tackle this challenge, we propose to use PARALIND decomposition <ref type="bibr" target="#b5">[6]</ref> which factorizes a large tensor into smaller tensors which reduces the computational cost and the usage memory.</p><p>The proposed trilinear interaction takes images, questions and answers as inputs. However, answer information in FFOE VQA <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref> is only available in the training phase but not in the testing phase. To apply the trilinear interaction for FFOE VQA, we propose to use knowledge distillation to transfer knowledge from trilinear model to bilinear model. The distilled bilinear model only requires pairs of image and question as inputs, hence it can be used for the testing phase. For MC VQA <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44</ref>], the answer information can be easily extracted, thanks to the given answer list that contains few candidate answers for each image-question pair and is available in both training and testing phases. Thus, the proposed trilinear interaction can be directly applied to MC VQA.</p><p>To evaluate the effectiveness of the proposed model, the extensive experiments are conducted on the benchmarking datasets TDIUC, VQA-2.0, and Visual7W. The results show that the proposed model achieves state-of-the-art results on all three datasets.</p><p>The main contributions of the paper are as follows. (i) We propose a novel trilinear interaction model which simultaneously learns high level joint presentation between image, question, and answer information in VQA task. (ii) We utilize PARALIND decomposition to deal with the dimensionality issue in trilinear interaction. (iii) To make the proposed trilinear interaction applicable for FFOE VQA, we propose to use knowledge distillation for transferring knowledge from trilinear interaction model to bilinear interaction model. The remaining of this paper is organized as follows. Section 2 presents the related work. Section 3 presents the proposed compact trilinear interaction (CTI). Section 4 presents the proposed models when applying CTI to FFOE VQA and MC VQA. Section 5 presents ablation studies, experimental results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Joint embedding in Visual Question Answering. There are different approaches have been proposed for VQA <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref>. Most of the successful methods focus on learning joint representation between the input question and image <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45]</ref>. In the state-of-the-art VQA, the features of the input image and question are usually represented under matrix forms. E.g., each image is described by a number of interested regions, and each region is represented by a feature vector. Similar idea is applied for question, e.g., an question contains a number of words and each word is represented by a feature vector. A fully expressive interaction between an image region and a word should be the outer product between their two corresponding vectors <ref type="bibr" target="#b7">[8]</ref>. The outer product allows a multiplicative interaction between all elements of both vectors. However, a fully bilinear interaction using outer product between every possible pairs of regions and words will dramatically increase the output space. Hence instead of directly computing the fully bilinear with outer product, most of works try to compress or decompose the fully bilinear interaction.</p><p>In <ref type="bibr" target="#b7">[8]</ref>, the authors proposed the Multimodal Compact Bilinear pooling which is an efficient method to compress the bilinear interaction. The method works by projecting the visual and linguistic features to a higher dimensional space and then convolving both vectors efficiently by using element-wise product in Fast Fourier Transform space. In <ref type="bibr" target="#b4">[5]</ref>, the authors proposed Multimodal Tucker Fusion which is a tensor-based Tucker decomposition to efficiently parameterize bilinear interaction between visual and linguistic representations. In <ref type="bibr" target="#b44">[45]</ref>, the author proposed Factorized Bilinear Pooling that uses two low rank matrices to approximate the fully bilinear interaction. Recently, in <ref type="bibr" target="#b17">[18]</ref> the authors proposed Bilinear Attention Networks (BAN) that finds bilinear attention distributions to utilize given visual-linguistic information seamlessly. BAN also uses low rank approximation to approximate the bilinear interaction for each pair of vectors from image and question.</p><p>There are other works that consider answer information, besides image and question information, to improve VQA performance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b33">34]</ref>. Typically, in <ref type="bibr" target="#b13">[14]</ref>, the authors learned two embedding functions to transform an image-question pair and an answer into a joint embedding space. The distance between the joint embedded imagequestion and the embedded answer is then measured to determine the output answer. In <ref type="bibr" target="#b40">[41]</ref>, the authors computed joint representations between image and question, and between image and answer. They then learned a joint embedding between the two computed representations.</p><p>In <ref type="bibr" target="#b33">[34]</ref>, the authors computed "ternary potentials" which capture the dependencies between three inputs, i.e., image, question, and answer. For every triplet of vectors, each from each different input, to compute the interaction be-tween three vectors, instead of calculating the outer products, the author computed the sum of element-wise product of the three vectors. This greatly reduces the computational cost but it might not be expressive enough to fully capture the complex associations between the three vectors.</p><p>Different from previous works that mainly aim to learn the joint representations from pairs of modalities <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b40">41]</ref> or greatly simplify the interaction between the three modalities by using the element-wise operator <ref type="bibr" target="#b33">[34]</ref>, in this paper, we propose a principle and direct approach -a trilinear interaction model, which simultaneously learns a joint representation between three modalities. In particular, we firstly derive a fully trilinear interaction between three modalities. We then rely on a decomposition approach to develop a compact model for the interaction.</p><p>Knowledge Distillation. Knowledge Distillation is a general approach for transferring knowledge from a cumbersome model (teacher model) to a lighten model (student model) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4]</ref>. In FFOE VQA, the trilinear interaction model, which takes image, question, and answer as inputs, can only be applied for training phase but not for testing phase due to the omission of answer in testing. To overcome this challenge and also to reduce computational cost, inspired from the Hinton's seminar work <ref type="bibr" target="#b12">[13]</ref>, we propose to use knowledge distillation to transfer knowledge from trilinear model to bilinear model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Compact Trilinear Interaction (CTI)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully parameterized trilinear interaction</head><p>Let M = {M 1 , M 2 , M 3 } be the representations of three inputs. M t ∈ R nt×dt , where n t is the number of channels of the input M t and d t is the dimension of each channel. For example, if M 1 is the region-based representation for an image, then n 1 is the number of regions and d 1 is the dimension of the feature representation for each region. Let m te ∈ R 1×dt be the e th row of M t , i.e., the feature representation of e th channel in M t , where t ∈ {1, 2, 3}.</p><p>The joint representation resulted from a fully parameterized trilinear interaction over the three inputs is presented by z ∈ R dz which is computed as follows</p><formula xml:id="formula_0">z T = ((T × 1 vec(M 1 )) × 2 vec(M 2 )) × 3 vec(M 3 ) (1) where T ∈ R d M 1 ×d M 2 ×d M 3 ×dz is a learning tensor; d Mt = n t × d t ; vec(M t )</formula><p>is a vectorization of M t which outputs a row vector; operator × i denotes the i-mode tensor product.</p><p>The tensor T helps to learn the interaction between the three input through i-mode product. However, learning such a large tensor T is infeasible when the dimension d Mt of each input modality is high, which is the usual case in VQA. Thus, it is necessary to reduce the size T to make the learning feasible.</p><p>Inspired by <ref type="bibr" target="#b42">[43]</ref>, we rely on the idea of unitary attention mechanism. Specifically, let z p ∈ R dz be the joint representation of p th triplet of channels where each channel in the triplet is from a different input. The represen-</p><formula xml:id="formula_1">tation of each channel in a triplet is m 1i , m 2j , m 3 k , where i ∈ [1, n 1 ], j ∈ [1, n 2 ], k ∈ [1, n 3 ]</formula><p>, respectively. There are n 1 × n 2 × n 3 possible triplets over the three inputs. The joint representation z p resulted from a fully parameterized trilinear interaction over three channel representations m 1i , m 2j , m 3 k of p th triplet is computed as</p><formula xml:id="formula_2">z T p = (T sc × 1 m 1i ) × 2 m 2j × 3 m 3 k<label>(2)</label></formula><p>where T sc ∈ R d1×d2×d3×dz is the learning tensor between channels in the triplet. Follow the idea of unitary attention <ref type="bibr" target="#b42">[43]</ref>, the joint representation z is approximated by using joint representations of all triplets described in (2) instead of using fully parameterized interaction over three inputs as in <ref type="bibr" target="#b0">(1)</ref>. Hence, we</p><formula xml:id="formula_3">compute z = p M p z p<label>(3)</label></formula><p>Note that in <ref type="formula" target="#formula_3">(3)</ref>, we compute a weighted sum over all possible triplets. The p th triplet is associated with a scalar weight</p><formula xml:id="formula_4">M p . The set of M p is called as the attention map M, where M ∈ R n1×n2×n3 .</formula><p>The attention map M resulted from a reduced parameterized trilinear interaction over three inputs M 1 , M 2 and M 3 is computed as follows</p><formula xml:id="formula_5">M = ((T M × 1 M 1 ) × 2 M 2 ) × 3 M 3<label>(4)</label></formula><p>where T M ∈ R d1×d2×d3 is the learning tensor of attention map M. Note that the learning tensor T M in (4) has a reduced size compared to the learning tensor T in (1). By integrating <ref type="formula" target="#formula_2">(2)</ref> into <ref type="formula" target="#formula_3">(3)</ref>, the joint representation z in (3) can be rewritten as <ref type="formula" target="#formula_18">(5)</ref> is actually a scalar attention weight M p of the attention map M in <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_6">z T = n1 i=1 n2 j=1 n3 k=1 M ijk (T sc × 1 m 1i ) × 2 m 2j × 3 m 3 k (5) where M ijk in</formula><p>It is also worth noting from (5) that to compute z, instead of learning the large tensor <ref type="formula" target="#formula_18">(1)</ref>, we now only need to learn two smaller tensors T sc ∈ R d1×d2×d3×dz in (2) and T M ∈ R d1×d2×d3 in (4).</p><formula xml:id="formula_7">T ∈ R d M 1 ×d M 2 ×d M 3 ×dz in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Parameter factorization</head><p>Although the large tensor T of trilinear interaction model is replaced by two smaller tensors T M and T sc , the dimension of these two tensors still large which makes the learning difficult. To further reduce the computational complexity, the PARALIND decomposition <ref type="bibr" target="#b5">[6]</ref> is applied for T M and T sc . The PARALIND decomposition for the learning tensor T M ∈ R d1×d2×d3 can be calculated as <ref type="figure">Figure 1</ref>. PARALIND decomposition for a tensor TM.</p><formula xml:id="formula_8">   1 ≈ W 3 1 W 2 1   W 3 R W 2 R + . . . + W 1 1 W 1 R</formula><formula xml:id="formula_9">T M ≈ R r=1 ((G r × 1 W 1r ) × 2 W 2r ) × 3 W 3r<label>(6)</label></formula><p>where R is a slicing parameter, establishing a trade-off between the decomposition rate (which is directly related to the usage memory and the computational cost) and the performance. Each G r ∈ R d1 r ×d2 r ×d3 r is a smaller learnable tensor called Tucker tensor. The number of these Tucker tensors equals to R. The maximum value for R is usually set to the greatest common divisor of d 1 , d 2 and d 3 . In our experiments, we found that R = 32 gives a good trade-off between the decomposition rate and the performance.</p><p>Here, we have dimension <ref type="figure">Figure 1</ref> shows the illustration of PARALIND decomposition for a tensor T M .</p><formula xml:id="formula_10">d 1r = d 1 /R, d 2r = d 2 /R and d 3r = d 3 /R; W 1r ∈ R d1×d1 r , W 2r ∈ R d2×d2 r and W 3r ∈ R d3×d3 r are learnable factor matrices.</formula><p>The shorten form of T M in (6) can be rewritten as</p><formula xml:id="formula_11">T M ≈ R r=1 G r ; W 1r , W 2r , W 3r<label>(7)</label></formula><p>Integrating the learning tensor T M from <ref type="formula" target="#formula_11">(7)</ref> into <ref type="formula" target="#formula_5">(4)</ref>, the attention map M can be rewritten as</p><formula xml:id="formula_12">M = R r=1 G r ; M 1 W 1r , M 2 W 2r , M 3 W 3r<label>(8)</label></formula><p>Similar to T M , PARALIND decomposition is also applied to the tensor T sc in (5) to reduce the complexity. It is worth noting that the size of T sc directly effects to the dimension of the joint representation z ∈ R dz . Hence, to minimize the loss of information, we set the slicing parameter R = 1 and the projection dimension of factor matrices at d z , i.e., the same dimension of the joint representation z.</p><p>Therefore, T sc ∈ R d1×d2×d3×dz in (5) can be calculated as</p><formula xml:id="formula_13">T sc ≈ ((G sc × 1 W z1 ) × 2 W z2 ) × 3 W z3 (9) where W z1 ∈ R d1×dz , W z2 ∈ R d2×dz , W z3 ∈ R d3×dz</formula><p>are learnable factor matrices and G sc ∈ R dz×dz×dz×dz is a smaller tensor (compared to T sc ).</p><p>Up to now, we already have M by <ref type="bibr" target="#b7">(8)</ref> and T sc by <ref type="formula" target="#formula_18">(9)</ref>, hence, we can compute z using <ref type="bibr" target="#b4">(5)</ref>. z from <ref type="formula" target="#formula_18">(5)</ref> can be rewrit-ten as</p><formula xml:id="formula_14">z T = n1 i=1 n2 j=1 n3 k=1 M ijk (G sc × 1 m 1i W z1 ) × 2 m 2j W z2 × 3 m 3 k W z3</formula><p>(10) Here, it is interesting to note that G sc ∈ R dz×dz×dz×dz in (10) has rank 1. Thus, the result got from i-mode tensor products in (10) can be approximated by the Hadamard products without the presence of rank-1 tensor G sc <ref type="bibr" target="#b20">[21]</ref>. In particular, z in (10) can be computed without using G sc as</p><formula xml:id="formula_15">z T = n1 i=1 n2 j=1 n3 k=1 M ijk m 1i W z1 • m 2j W z2 • m 3 k W z3 (11)</formula><p>Note that d z , which is the joint embedding dimension, is a user-defined parameter which makes a trade-off between the capability of the representation and the computational cost. In our experiments, we found that d z = 1, 024 gives a good trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Compact Trilinear Interaction for VQA</head><p>The input for training VQA is set of (</p><formula xml:id="formula_16">V, Q, A) in which V is an image representation; V ∈ R v×dv</formula><p>where v is the number of interested regions (or bounding boxes) in the image and d v is the dimension of the representation for a region; Q is a question representation; Q ∈ R q×dq where q is the number of hidden states and d q is the dimension for each hidden state.</p><p>A is an answer representation; A ∈ R a×da where a is the number of hidden states and d a is the dimension for each hidden state.</p><p>By applying the Compact Trilinear Interaction (CTI) to each (V, Q, A), we achieve the joint representation z ∈ R dz . Specifically, we firstly compute the attention map M by (8) as follows</p><formula xml:id="formula_17">M = R r=1 G r ; V W vr , QW qr , AW ar<label>(12)</label></formula><p>Then the joint representation z is computed by <ref type="bibr" target="#b10">(11)</ref> as follows </p><formula xml:id="formula_18">z T = v i=1 q j=1 a k=1 M ijk V i W zv • Q j W zq • A k W za<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multiple Choice Visual Question Answering</head><p>To make a fair comparison to the state of the art in MC VQA <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">41]</ref>, we follow the representations used in those works. Specifically, each input question and each answer are trimmed to a maximum of 12 words which will then be zero-padded if shorter than 12 words. Each word is then represented by a 300-D GloVe word embedding <ref type="bibr" target="#b31">[32]</ref>. Each image is represented by a 14 × 14 × 2048 grid feature (i.e., 196 cells; each cell is with a 2, 048-D feature), extracted from the second last layer of ResNet-152 which is pre-trained on ImageNet <ref type="bibr" target="#b11">[12]</ref>.</p><p>Follow <ref type="bibr" target="#b40">[41]</ref>, input samples are divided into positive samples and negative samples. A positive sample, which is labelled as 1 in binary classification, contains image, question and the right answer. A negative sample, which is labelled as 0 in binary classification, contains image, question, and the wrong answer. These samples are then passed through our proposed CTI to get the joint representation z. The joint representation is passed through a binary classifier to get the prediction. The Binary Cross Entropy loss is used for training the proposed model. <ref type="figure" target="#fig_0">Figure 2</ref> visualizes the proposed model when applying CTI to MC VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Free-Form Opened-Ended Visual Question Answering</head><p>Unlike MC VQA, FFOE VQA treats the answering as a classification problem over the set of predefined answers. Hence the set possible answers for each question-image pair is much more than the case of MC VQA. Therefore the model design proposed in Section 4.1, i.e. for each question-image input, the model takes every possible answers from its answer list to computed the joint representation, causes high computational cost. In addition, the proposed CTI requires all three V, Q, A inputs to compute the joint representation. However, during the testing, there are no available answer information in FFOE VQA. To overcome these challenges, we propose to use Knowledge Distillation <ref type="bibr" target="#b12">[13]</ref> to transfer the learned knowledge from a teacher model to a student model. <ref type="figure" target="#fig_1">Figure 3</ref> visualizes the proposed design for FFOE VQA.</p><p>Our teacher model takes triplets of image-question-right answer as inputs. Each triplet is passed through the proposed CTI to get the joint representation z. The joint representation z is then passed through a multiclass classifier (over the set of predefined answers) to get the prediction which is similar to <ref type="bibr" target="#b36">[37]</ref>. The Cross Entropy loss is used for training the teacher model. Regarding the student models, any state-of-the-art VQA can be used. In our experiments, we use BAN2 <ref type="bibr" target="#b17">[18]</ref> or SAN <ref type="bibr" target="#b42">[43]</ref> as student models. The student models take pairs of image-question as inputs and treat the prediction as a mutilclass classification problem. The loss function for the student model is defined as</p><formula xml:id="formula_19">L KD = αT 2 L CE (Q τ S , Q τ T ) + (1 − α)L CE (Q S , y true )<label>(14)</label></formula><p>where L CE stands for Cross Entropy loss; Q S is the standard softmax output of the student; y true is the ground-truth answer labels; α is a hyper-parameter for controlling the im- portance of each loss component; Q τ S , Q τ T are the softened outputs of the student and the teacher using the same temperature parameter T <ref type="bibr" target="#b12">[13]</ref>, which are computed as follows</p><formula xml:id="formula_20">Q τ i = exp(l i /T ) i exp(l i /T )<label>(15)</label></formula><p>where for both teacher and the student models, the logit l is the predictions outputted by the corresponding classifiers.</p><p>Following by the current state of the art in FFOE VQA <ref type="bibr" target="#b17">[18]</ref>, for image representation, we use object detection-based features with FPN detector (ResNet152 backbone) <ref type="bibr" target="#b21">[22]</ref>, in which the number of maximum detected bounding boxes is set to 50. For question and answer representations, we trim question and answer to a maximum of 12 words which will then be zero-padded if shorter than 12 words. Each word is then represented by a 600-D vector that is a concatenation of the 300-D GloVe word embedding <ref type="bibr" target="#b31">[32]</ref> and the augmenting embedding from training data as <ref type="bibr" target="#b17">[18]</ref>. In the other words, a question is with a representation with size 12 × 600. It is similar for answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset and evaluation protocol</head><p>Dataset. We conduct the experiments on three benchmarking VQA datasets that are Visual7W <ref type="bibr" target="#b46">[47]</ref> for the MC VQA, VQA-2.0 <ref type="bibr" target="#b9">[10]</ref> and TDIUC <ref type="bibr" target="#b16">[17]</ref> for the FFOE VQA. We use training set to train and validation set to evaluate in all mentioned datasets when conducting ablation study.</p><p>Implementation details. Our CTI is implemented using PyTorch <ref type="bibr" target="#b30">[31]</ref>. The experiments are conducted on a NVIDIA Titan V GPUs with 12GB RAM. In all experiments, the learning rate is set to 10 Evaluation Metrics. We follow the literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47]</ref> in which the evaluation metrics for each VQA task are different. For FFOE VQA, the single accuracy, which is a standard VQA accuracy (Acc) <ref type="bibr" target="#b2">[3]</ref>, is applied for both TDIUC and VQA-2.0 datasets. In addition, due to the imbalance in the question types of TDIUC dataset, follow <ref type="bibr" target="#b16">[17]</ref>, we also report four other metrics that compensate for the skewed question-type distribution. They are Arithmetic MPT (Ari), Arithmetic Norm-MPT (Ari-N), Harmonic MPT (Har), and Harmonic Norm-MPT (Har-N). For MC VQA, we follow the evaluation metric (Acc-MC) proposed by <ref type="bibr" target="#b46">[47]</ref> in which the performance is measured by the portion of correct answers selected by the VQA model from the candidate answer set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study</head><p>The effectiveness of CTI on FFOE VQA. We compare our distilled BAN2 (BAN2-CTI) and distilled SAN (SAN-CTI) student models to the state-of-the-art baselines BAN2 <ref type="bibr" target="#b17">[18]</ref> and SAN <ref type="bibr" target="#b42">[43]</ref>. <ref type="table" target="#tab_1">Table 1</ref> presents a comprehensive evaluation on five different metrics on TDIUC. Among all metrics, on overall, our BAN2-CTI and SAN-CTI outperform corresponding baselines by a noticeable margin.  These results confirm the effectiveness of our proposed CTI for learning the joint representation. In addition, the proposed teacher model <ref type="figure" target="#fig_1">(Figure 3)</ref> is also effective. It successfully transfers useful learned knowledge to the student models. Note that in <ref type="table" target="#tab_1">Table 1</ref>, the "Absurd" question category indicates the cases in which input questions are irrelevant to the image contents. Thus, the answers are always "does not apply", i.e., "no answer". Using these meaningless answers when training the teacher causes negative effect when learning the joint representation, hence, reducing the model capacity. If the "Absurd" category is not taken into account, the proposed model achieves more improvements over baselines.  <ref type="table" target="#tab_2">Table 2</ref> presents detail performances with Acc metric over each question category of TDIUC when all categories, including "Absurd", are used for training. The results show that we achieve the best results on all question categories but "Absurd". We note that in the real applications, the "Absurd" question problem may be mitigated in some cases by using a simple trick, i.e., asking a "presence question" before asking the main question, e.g., we have an image with no human but the main question is "Is the people wearing hat?", i.e., a "Absurd" question. By asking a "presence question" as "Are there any people in the picture?", we can have a confirmation about the presence of human in the considered image, before asking the main question. <ref type="table" target="#tab_3">Table 3</ref> presents comparative results between our distilled student models and two baselines BAN2, SAN on Acc metric on VQA-2.0. Although our proposal outperforms the baselines, the improvement gap is not much. This is understandable because the VQA-2.0 dataset has a large number of questions of which answers are "yes/no" or contain only one word (i.e., answers for "number" question types). These answers have little semantic meanings which prevent proposed trilinear interaction from promoting its efficiency.</p><p>The effectiveness of CTI on MC VQA. We still use the state-of-the-art BAN2 <ref type="bibr" target="#b17">[18]</ref> and SAN <ref type="bibr" target="#b42">[43]</ref> as baselines and conduct experiments on Visual7W dataset. In MC VQA, in both training and testing, each image-question pair has a corresponding answer list that contains four answers. To make a fair comparison, we try different pair combinations over three modalities (image, question, and answer) for the baselines BAN2 and SAN. Similar to <ref type="bibr" target="#b40">[41]</ref>, we find the following combination gives best results for the baselines. Using BAN2 (or SAN), we first compute the joint representation between image and question; and the joint representation between image and answer. Then, we concatenate the two computed representations to get the joint "imagequestion-answer" representation, and pass it through VQA classifier with cross entropy loss for training the baseline. <ref type="table">Table 4</ref> presents comparative results on Visual7W with Acc-MC metric. The results show that our proposed model outperforms the baselines by a noticeable margin. These results confirm that the joint representation learned by the proposed trilinear interaction achieves better performance than the combination of joint representations computed by BAN (or SAN) of pairs of modalities. In addition, in <ref type="table">Table 4</ref> we also provide the number of total parameters of our proposed  The higher weight of a triplet is, the more contribution it makes to the joint representation. We can see that three triplets (V=4, Q="tube", A="white"), (V=4, Q="tube", A="red"), (V=4, Q="tube", A="blue") have high weight values. That means that these triplets give high contribution to the joint representation. The input sample (a) is from Vi-sual7W validation set. Best view in color.</p><p>MC VQA model with CTI ( <ref type="figure" target="#fig_0">Figure 2)</ref> and BAN2, SAN. The results show that our model requires less memory than those baselines. That means that the proposed MC VQA model with CTI not only outperforms the baselines in term of accuracy, but also more efficient than those baselines in term of the usage memory. <ref type="figure" target="#fig_4">Figure 4</ref> visualizes the attention map resulted by CTI for an example of image-question-answer. The attention map is computed by (12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with the state of the art</head><p>To further evaluate the effectiveness of CTI, we conduct a detailed comparison with the current state of the art. For FFOE VQA, we compare our proposal with the recent stateof-the-art methods on TDIUC and VQA-2.0 datasets, including SAN <ref type="bibr" target="#b42">[43]</ref>, QTA <ref type="bibr" target="#b34">[35]</ref>, BAN2 <ref type="bibr" target="#b17">[18]</ref>, Bottom-up <ref type="bibr" target="#b36">[37]</ref>, MCB <ref type="bibr" target="#b7">[8]</ref>, and RAU <ref type="bibr" target="#b28">[29]</ref>. For MC VQA, we compare with the state-of-the-art methods on Visual7W dataset, including BAN2 <ref type="bibr" target="#b17">[18]</ref>, SAN <ref type="bibr" target="#b42">[43]</ref>, MLP <ref type="bibr" target="#b15">[16]</ref>, MCB <ref type="bibr" target="#b7">[8]</ref>, STL <ref type="bibr" target="#b40">[41]</ref>, and fPMC <ref type="bibr" target="#b13">[14]</ref>). It is worth noting that depending on tasks FFOE VQA or MC VQA, we use different representations for images and questions as clearly mentioned in Section 4. This ensures a fair comparison with compared methods.</p><p>Regarding FFOE VQA, <ref type="table" target="#tab_3">Tables 3 and 5</ref> show comparative results on VQA-2.0 and TDIUC respectively. Specifcaly, <ref type="table" target="#tab_4">Table 5</ref> shows that our distilled student BAN2-CTI outperforms all compared methods over all metrics by a large margin, i.e., our model outperforms the current stateof-the-art QTA <ref type="bibr" target="#b34">[35]</ref> on TDIUC by 3.4% and 5.4% on Ari and Har metrics, respectively. The results confirm that the proposed trilinear interaction has learned informative repre-  sentations from the three inputs and the learned information is effectively transferred to student models by distillation. Regarding MC VQA, <ref type="table" target="#tab_5">Table 6</ref> shows that the proposed model (denoted as CTI in <ref type="table" target="#tab_5">Table 6</ref>) outperforms compared methods by a noticeable margin. Our model outperforms the current state-of-the-art STL [41] 1.1%. Again, this validates the effectiveness of the proposed joint presentation learning, which precisely and simultaneously learns interactions between the three inputs. We note that when comparing with other methods on Visual7W, for image representations, we used the grid features extracted from ResNet-512 <ref type="bibr" target="#b11">[12]</ref> for a fair comparison. Our proposed model can achieve further improvements by using the object detection-based features used in FFOE VQA. With new features, our model denoted as CTIwBoxes in <ref type="table" target="#tab_5">Table 6</ref> achieve 72.3% accuracy with Acc-MC metric which improves over the current stateof-the-art STL [41] 4.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Further analysis</head><p>The effectiveness of PARALIND decomposition. In this section, we compute the decomposition rate of PAR-ALIND. For a fully interaction between the three inputs, using (1), we would need to learn 2199.02 billions parameters which is infeasible in practice. By using the PARALIND decomposition presented in Section 3 with the provided set-tings, i.e., the number of slicing R = 32 and the dimension of the joint representation d z = 1024, the number of parameters that need to learn is only 33.69 millions. In the other words, we achieve a decomposition rate ≈ 65, 280.</p><p>Compact Trilinear Interaction as the generalization of BAN <ref type="bibr" target="#b17">[18]</ref>. The proposed compact trilinear interaction model can be seen as a generalization of the state-of-the-art joint embedding BAN <ref type="bibr" target="#b17">[18]</ref>.</p><p>In BAN, each input contains an image representation V ∈ R dv×v and a question representation Q ∈ R dq×q . The trilinear interaction model can be modified to adapt to these two inputs. The joint representation z ∈ R dz in (1) can be adapted for two input as</p><formula xml:id="formula_21">z T = (T vq × 1 vec(V )) × 2 vec(Q)<label>(16)</label></formula><p>where T vq ∈ R d V ×d Q ×dz is a learnable tensor; vec(V ) is the vectorization of V and vec(Q) is the vectorization of Q which output row vectors;</p><formula xml:id="formula_22">d V = d v × v; d Q = d q × q.</formula><p>By applying "Parameter factorization" described in Section 3.2, z in (16) can be approximated based on <ref type="bibr" target="#b12">(13)</ref> as</p><formula xml:id="formula_23">z T = v i=1 q j=1 M ij V T i W zv • Q T j W zq<label>(17)</label></formula><p>where W zv ∈ R dv×dz and W zq ∈ R dq×dz are learnable factor matrices; M ij is an attention weight of attention map M ∈ R v×q which can be computed from <ref type="formula" target="#formula_2">(12)</ref> as</p><formula xml:id="formula_24">M = R r=1 G r ; V T W vr , Q T W qr<label>(18)</label></formula><p>where W vr ∈ R dv×dv r and W qr ∈ R dq×dq r are learnable factor matrices; d vr = d v /R; d qr = d q /R; each G r ∈ R dv r ×dq r is a learnable Tucker tensor. Interestingly, <ref type="bibr" target="#b16">(17)</ref> can be reorganized to have a form of BAN <ref type="bibr" target="#b17">[18]</ref> as</p><formula xml:id="formula_25">z k = v i=1 q j=1 M ij V T i W zv k W T zq k Q j<label>(19)</label></formula><p>where z k is the k th element of the joint representation z; W zv k and W zq k are k th column in factor matrices W zv and W zq . Note that in <ref type="bibr" target="#b18">(19)</ref>, our attention map M is resulted from the PARALIND decomposition, while in BAN <ref type="bibr" target="#b17">[18]</ref>, their attention map is computed by bilinear pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a novel compact trilinear interaction which simultaneously learns high level associations between image, question, and answer in both MC VQA and FFOE VQA. In addition, knowledge distillation is the first time applied to FFOE VQA to overcome the computational complexity and memory issue of the interaction. The extensive experimental results show that the proposed models achieve the state-of-the-art results on three benchmarking datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The proposed model when CTI is applied to MC VQA. The details are described in Section 4.1. Best view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>− 3 .</head><label>3</label><figDesc>Batch size is set to 128 for training MC VQA and 256 for training FFOE VQA. When training both MC VQA model (Section 4.1) and FFOE VQA model (Section 4.2), except the image representation extraction, other components are trained end-toend. The temperature parameter T in (15) is set to 3. The dimension of the joint representation z is set at 1, 024 for both MC VQA and FFOE VQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The proposed model when CTI is applied on FFOE VQA. The details are described in Section 4.2. Best view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>What colors are the toothpaste tube? GT Answer: Blue, red, and white Predicted Answer: Blue, red, and white</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The visualization of an attention map (b) computed from Eq. (12) for an image-question-answer input (a). The attention map indicates attention weights over triplets of "detected bounding box -word in question -word in answer".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b12">13)</ref> where W vr , W qr , W ar in (12) and W zv , W zq , W za in (13) are learnable factor matrices; each G r in (12) is a learnable Tucker tensor.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Overall performance of the proposal and the baselines BAN2, SAN in different evaluation metrics on TDIUC validation set. The performance is shown with and without considering Absurd question category. BAN2-CTI and SAN-CTI are student models trained under our proposed CTI teacher model.</figDesc><table><row><cell>QT</cell><cell>Models</cell><cell>Acc</cell><cell cols="3">Evaluation metrics Ari Har Ari-N Har-N</cell></row><row><cell></cell><cell cols="3">BAN2-CTI 87.0 72.5 65.5</cell><cell>45.8</cell><cell>28.6</cell></row><row><cell>with</cell><cell>BAN2[18]</cell><cell cols="2">85.5 67.4 54.9</cell><cell>37.4</cell><cell>15.7</cell></row><row><cell>Abs</cell><cell>SAN-CTI</cell><cell cols="2">84.5 68.7 59.9</cell><cell>41.3</cell><cell>23.3</cell></row><row><cell></cell><cell>SAN[43]</cell><cell cols="2">82.3 65.0 53.7</cell><cell>35.4</cell><cell>14.7</cell></row><row><cell></cell><cell cols="3">BAN2-CTI 85.0 70.6 63.8</cell><cell>41.5</cell><cell>26.9</cell></row><row><cell>w/o</cell><cell>BAN2[18]</cell><cell cols="2">81.9 64.6 52.8</cell><cell>31.9</cell><cell>14.6</cell></row><row><cell>Abs</cell><cell>SAN-CTI</cell><cell cols="2">82.8 66.7 58.1</cell><cell>36.8</cell><cell>21.8</cell></row><row><cell></cell><cell>SAN[43]</cell><cell cols="2">79.1 62.4 51.7</cell><cell>30.2</cell><cell>13.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance (Acc) of the proposal and the baselines BAN2, SAN for each question-type on TDIUC validation set. BAN2-CTI and SAN-CTI are student models trained under our compact trilinear interaction teacher model.</figDesc><table><row><cell cols="2">Question-types BAN2-CTI</cell><cell>BAN2 [18]</cell><cell>SAN-CTI</cell><cell>SAN [43]</cell></row><row><cell>Scene Rec</cell><cell>94.5</cell><cell>93.1</cell><cell>93.6</cell><cell>92.3</cell></row><row><cell>Sport Rec</cell><cell>96.3</cell><cell>95.7</cell><cell>95.5</cell><cell>95.5</cell></row><row><cell>Color Attr</cell><cell>74.3</cell><cell>67.5</cell><cell>70.9</cell><cell>60.9</cell></row><row><cell>Other Attr</cell><cell>60.5</cell><cell>53.2</cell><cell>56.4</cell><cell>46.2</cell></row><row><cell>Activity Rec</cell><cell>63.2</cell><cell>54.0</cell><cell>54.5</cell><cell>51.4</cell></row><row><cell>Positional Rec</cell><cell>40.5</cell><cell>27.9</cell><cell>34.3</cell><cell>27.9</cell></row><row><cell>Sub-Obj Rec</cell><cell>89.3</cell><cell>87.5</cell><cell>87.6</cell><cell>87.5</cell></row><row><cell>Absurd</cell><cell>93.9</cell><cell>98.2</cell><cell>90.6</cell><cell>93.4</cell></row><row><cell>Util &amp; Aff</cell><cell>36.3</cell><cell>24.0</cell><cell>31.0</cell><cell>26.3</cell></row><row><cell>Obj Pres</cell><cell>96.1</cell><cell>95.1</cell><cell>94.9</cell><cell>92.4</cell></row><row><cell>Count</cell><cell>59.7</cell><cell>53.9</cell><cell>55.6</cell><cell>52.1</cell></row><row><cell>Sentiment</cell><cell>66.1</cell><cell>58.7</cell><cell>59.9</cell><cell>53.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance of the proposal and baselines BAN2, SAN in VQA-2.0 validation set and test-dev set. BAN2-CTI and SAN-CTI are student models trained under proposed teacher model.</figDesc><table><row><cell>Ref</cell><cell></cell><cell>Validation</cell><cell>Test-dev</cell></row><row><cell cols="2">models</cell><cell>Accuracy</cell><cell>Accuracy</cell></row><row><cell cols="2">Bottom-up [37]</cell><cell>63.2</cell><cell>65.4</cell></row><row><cell cols="2">SAN [43]</cell><cell>61.7</cell><cell>63.0</cell></row><row><cell cols="2">SAN-CTI</cell><cell>62.1</cell><cell>63.4</cell></row><row><cell cols="2">BAN2 [18]</cell><cell>65.6</cell><cell>66.5</cell></row><row><cell cols="2">BAN2-CTI</cell><cell>66.0</cell><cell>67.4</cell></row><row><cell>Ref</cell><cell cols="3">Visual7W validation set</cell></row><row><cell>models</cell><cell cols="3">Acc-MC Number of parameters</cell></row><row><cell>BAN2 [18]</cell><cell>65.7</cell><cell></cell><cell>∼ 86.5M</cell></row><row><cell>SAN [43]</cell><cell>59.3</cell><cell></cell><cell>∼ 69.7M</cell></row><row><cell>CTI</cell><cell>67.0</cell><cell></cell><cell>∼ 66.5M</cell></row><row><cell cols="4">Table 4. The performance (Acc-MC) and the number of parameters</cell></row><row><cell cols="4">of the proposed MC VQA model and the baselines BAN2, SAN</cell></row><row><cell cols="2">on Visual7W validation set.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison between different approaches with different evaluation metrics on TDIUC validation set. BAN2-CTI and SAN-CTI are the student models trained under our compact trilinear interaction teacher model.</figDesc><table><row><cell>Models</cell><cell>Acc</cell><cell cols="3">Evaluation metrics Ari Har Ari-N Har-N</cell></row><row><cell cols="3">BAN2 [18] 85.5 67.4 54.9</cell><cell>37.4</cell><cell>15.7</cell></row><row><cell>SAN [43]</cell><cell cols="2">82.3 65.0 53.7</cell><cell>35.4</cell><cell>14.7</cell></row><row><cell>QTA [35]</cell><cell cols="2">85.0 69.1 60.1</cell><cell></cell></row><row><cell>MCB [8]</cell><cell cols="2">79.2 65.8 58.0</cell><cell>39.8</cell><cell>24.8</cell></row><row><cell>RAU [29]</cell><cell cols="2">84.3 67.8 59.0</cell><cell>41.0</cell><cell>24.0</cell></row><row><cell>SAN-CTI</cell><cell cols="2">84.5 68.7 59.9</cell><cell>41.3</cell><cell>23.3</cell></row><row><cell cols="3">BAN2-CTI 87.0 72.5 65.5</cell><cell>45.8</cell><cell>28.6</cell></row><row><cell cols="2">Dataset</cell><cell>Ref models</cell><cell>Acc-MC</cell></row><row><cell></cell><cell></cell><cell>MLP [16]</cell><cell>67.1</cell></row><row><cell></cell><cell></cell><cell>MCB [8]</cell><cell>62.2</cell></row><row><cell></cell><cell></cell><cell>fPMC [14]</cell><cell>66.0</cell></row><row><cell cols="2">Visual7W</cell><cell>STL [41]</cell><cell>68.2</cell></row><row><cell></cell><cell>test set</cell><cell>SAN [43]</cell><cell>61.5</cell></row><row><cell></cell><cell></cell><cell>BAN2 [18]</cell><cell>67.5</cell></row><row><cell></cell><cell></cell><cell>CTI</cell><cell>69.3</cell></row><row><cell></cell><cell></cell><cell>CTIwBoxes</cell><cell>72.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Performance comparison between different approaches on Visual7W test set. Both training set and validation set are used for training. All models but CTIwBoxes are trained with same image and question representations. Both train set and validation set are used for training. Note that CTIwBoxes is the proposed CTI model using Bottom-up features [2] instead of grid features for image representation.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dhruv Batra and Aniruddha Kembhavi. Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Cadène</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling multi-way data with linearly dependent loadings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Bro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lundy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemometrics: A Journal of the Chemometrics Society</title>
		<imprint>
			<biblScope unit="page" from="324" to="340" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning efficient object detection models with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guobin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vqs: Linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning answer embeddings for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A focused dynamic attention model for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01485</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An analysis of visual question answering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bader</surname></persName>
		</author>
		<title level="m">Tensor decompositions and applications. SIAM review</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="455" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual question answering with memoryaugmented networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards a visual turing challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Duy-Kien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Training recurrent answering units with joint loss minimization for vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03647</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Highorder attention models for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Question type guided attention in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Xiaodong He, and Anton van den Hengel. Tips and tricks for visual question answering: Learnings from the 2017 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Damien Teney and Anton van den Hengel. Zero-shot visual question answering</title>
		<idno type="arXiv">arXiv:1611.05546</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual question answering as a meta learning task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structured triplet learning with postag guided attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multilevel attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multimodal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Simple baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual7W: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
