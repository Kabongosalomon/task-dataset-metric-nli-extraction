<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HybridPose: 6D Object Pose Estimation under Hybrid Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Song</surname></persName>
							<email>song@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaru</forename><surname>Song</surname></persName>
							<email>jiarus@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
							<email>huangqx@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HybridPose: 6D Object Pose Estimation under Hybrid Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce HybridPose, a novel 6D object pose estimation approach. HybridPose utilizes a hybrid intermediate representation to express different geometric information in the input image, including keypoints, edge vectors, and symmetry correspondences. Compared to a unitary representation, our hybrid representation allows pose regression to exploit more and diverse features when one type of predicted representation is inaccurate (e.g., because of occlusion). Different intermediate representations used by HybridPose can all be predicted by the same simple neural network, and outliers in predicted intermediate representations are filtered by a robust regression module. Compared to state-of-the-art pose estimation approaches, Hy-bridPose is comparable in running time and accuracy. For example, on Occlusion Linemod [3] dataset, our method achieves a prediction speed of 30 fps with a mean ADD(-S) accuracy of 47.5%, representing a state-of-the-art performance 1 . The implementation of HybridPose is available at joys a multitude of advantages. First, HybridPose integrates more signals in the input image: edge vectors encode spacial relations among object parts, and symmetry correspondences incorporate interior details. Second, HybridPose offers more constraints than using keypoints alone for pose regression, enabling accurate pose prediction even if a significant fraction of predicted elements are outliers (e.g., because of occlusion). Finally, it can be shown that symmetry correspondences stabilize the rotation component of pose prediction, especially along the normal direction of the reflection plane (details are provided in the supp. material).</p><p>Given the intermediate representation predicted by the first module, the second module of HybridPose performs pose regression. In particular, HybridPose employs trainable robust norms to prune outliers in predicted intermediate representation. We show how to combine pose initialization and pose refinement to maximize the quality of the resulting object pose. We also show how to train Hybrid-Pose effectively using a training set for the pose prediction module, and a validation set for the pose regression module.</p><p>We evaluate HybridPose on two popular benchmark datasets, Linemod <ref type="bibr" target="#b12">[12]</ref> and Occlusion Linemod <ref type="bibr" target="#b2">[3]</ref>. In terms of accuracy (under the ADD(-S) metric), Hybrid-Pose leads to improvements from state-of-the-art methods that merely utilize keypoints. On Occlusion Linemod [3], HybridPose achieves an accuracy of 47.5%, which beats DPOD [44], the current state-of-the-art method on this benchmark dataset.</p><p>Despite the gain in accuracy, our approach is efficient and runs at 30 frames per second on a commodity workstation. Compared to approaches which utilize sophisticated network architecture to predict one single intermediate representation (such as Pix2Pose [30]), HybridPose achieves better performance by using a relative simple network to predict hybrid representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the 6D pose of an object from an RGB image is a fundamental problem in 3D vision and has diverse applications in object recognition and robot-object interaction. Advances in deep learning have led to significant breakthroughs in this problem. While early works typically formulate pose estimation as end-to-end pose classification <ref type="bibr" target="#b39">[39]</ref> or pose regression <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b42">42]</ref>, recent pose estimation methods usually leverage keypoints as an intermediate representation <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b34">34]</ref>, and align predicted 2D keypoints with ground-truth 3D keypoints. In addition to ground-truth pose labels, these methods incorporate keypoints as an intermediate supervision, facilitating smooth model training. Keypoint-based methods are built upon two assumptions: * Authors contributed equally <ref type="bibr">1</ref> We are informed by readers that our previous experimental setup is inconsistent with baselines. This problem is fixed in the current version of the paper. Please refer to our GitHub issues for related discussions.  <ref type="figure">Figure 1</ref>. HybridPose predicts keypoints, edge vectors, and symmetry correspondences. In (a), we show the input RGB image, in which the object of interest (driller) is partially occluded. In (b), red markers denote predicted 2D keypoints. In (c), edge vectors are defined by a fully-connected graph among all keypoints. In (d), symmetry correspondences connect each 2D pixel on the object to its symmetric counterpart. For illustrative purposes, we only draw symmetry correspondences of 50 random samples from 5755 detected object pixels in this example. The predicted pose (f) is obtained by jointly aligning all predictions with the 3D template, which involves solving a non-linear optimization problem.</p><p>(1) a machine learning model can accurately predict 2D keypoint locations; and (2) these predictions provide sufficient constraints to regress the underlying 6D pose. Both assumptions easily break in many real-world settings. Due to object occlusions and representational limitations of the prediction network, it is often impossible to accurately predict 2D keypoint coordinates from an RGB image alone. In this paper, we introduce HybridPose, a novel 6D pose estimation approach that leverages multiple intermediate representations to express the geometric information in the input image. In addition to keypoints, HybridPose integrates a prediction network that outputs edge vectors between adjacent keypoints. As most objects possess a (partial) reflection symmetry, HybridPose also utilizes predicted dense pixel-wise correspondences that reflect the underlying symmetric relations between pixels. Compared to a unitary representation, this hybrid representation en-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Intermediate representation for pose. To express the geometric information in an RGB image, a prevalent intermediate representation is keypoints, which achieves stateof-the-art performance <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b36">36]</ref>. The corresponding pose estimation pipeline combines keypoint prediction and pose regression initialized by the PnP algorithm <ref type="bibr" target="#b18">[18]</ref>. Keypoint predictions are usually generated by a neural network, and previous works use different types of tensor descriptors to express 2D keypoint coordinates. A common approach represents keypoints as peaks of heatmaps <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b48">48]</ref>, which becomes sub-optimal when keypoints are occluded, as the input image does not provide explicit visual cues for their locations. Alternative keypoint representations include vector-fields <ref type="bibr" target="#b34">[34]</ref> and patches <ref type="bibr" target="#b14">[14]</ref>. These representations allow better keypoint predictions under occlusion, and eventually lead to improvement in pose estimation accuracy.</p><p>However, keypoints alone are a sparse representation of the object pose, whose potential in improving estimation accuracy is limited.</p><p>Besides keypoints, another common intermediate representation is the coordinate of every image pixel in the 3D physical world, which provides dense 2D-3D correspondences for pose alignment, and is robust under occlusion <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b20">20]</ref>. However, regressing dense object coordinates is much more costly than keypoint prediction. They are also less accurate than keypoints due to the lack of corresponding visual cues. In addition to keypoints and pixel-wise 2D-3D correspondences, depth is another alternative intermediate representation in visual odometry settings, which can be estimated together with pose in an unsupervised manner <ref type="bibr" target="#b47">[47]</ref>. In practice, the accuracy of depth estimation is limited by the representational power of neural networks.</p><p>Unlike previous approaches, HybridPose combines multiple intermediate representations, and exhibits collaborative strength for pose estimation. Multi-modal input. To address the challenges for pose estimation from a single RGB image, several works have considered inputs from multiple sensors. A popular approach is to leverage information from both RGB and depth images <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b42">42]</ref>. In the presence of depth information, pose regression can be reformulated as the 3D point alignment problem, which is then solved by the ICP algorithm <ref type="bibr" target="#b42">[42]</ref>.  <ref type="bibr" target="#b1">[2]</ref>, salient edges <ref type="bibr" target="#b23">[23]</ref>, and straight line segments <ref type="bibr" target="#b45">[45]</ref>. Unlike these low-level image features, HybridPose leverages semantic edge vectors defined between adjacent keypoints. This representation, which captures correlations between keypoints and reveals underlying structure of object, is concise and easy to predict. Such edge vectors offer more constraints than keypoints alone for pose regressions and have clear advantages under occlusion. Our approach is similar to <ref type="bibr" target="#b4">[5]</ref>, which predicts directions between adjacent keypoints to link keypoints into a human skeleton. However, we predict both the direction and the magnitude of edge vectors, and use these vectors to estimate object poses. Symmetry detection from images. Symmetry detection has received significant attention in computer vision. We refer readers to <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b27">27]</ref> for general surveys, and <ref type="bibr">[1,</ref><ref type="bibr" target="#b41">41]</ref> for recent advances. Traditional applications of symmetry detection include face recognition <ref type="bibr" target="#b31">[31]</ref>, depth estimation <ref type="bibr" target="#b21">[21]</ref>, and 3D reconstruction <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b43">43]</ref>. In the context of object pose estimation, people have studied symmetry from the perspective that it introduces ambiguities for pose estimation (c.f. <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b42">42]</ref>), since symmetric objects with differ-  The prediction networks take an image as input, and output predicted keypoints, edge vectors, and symmetry correspondences. The pose regression module consists of a initialization sub-module and a refinement sub-module. The initialization sub-module solves a linear system with predicted intermediate representations to obtain an initial pose. The refinement sub-module utilizes GM robust norm and optimizes <ref type="bibr" target="#b9">(9)</ref> to obtain the final pose prediction. ent poses can have the same appearance in image. Several works <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b30">30]</ref> have explored how to address such ambiguities, e.g., by designing loss functions that are invariant under symmetric transformations. Robust regression. Pose estimation via intermediate representation is sensitive to outliers in predictions, which are introduced by occlusion and cluttered backgrounds <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b40">40]</ref>. To mitigate pose error, several works assign different weights to different predicted elements in the 2D-3D alignment stage <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b32">32]</ref>. In contrast, our approach additionally leverages robust norms to automatically filter outliers in the predicted elements.</p><p>Besides the reweighting strategy, some recent works propose to use deep learning-based refiners to boost the pose estimation performance <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b44">44]</ref>. <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b19">19]</ref> use point matching loss and achieve high accuracy. <ref type="bibr" target="#b26">[26]</ref> predicts pose updates using contour information. Unlike these works, our approach considers the critical points and the loss surface of the robust objective function, and does not involve a fixed pre-determined iteration count used in recurrent network based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The input to HybridPose is an image I containing an object in a known class, taken by a pinhole camera with known intrinsic parameters. Assuming that the class of objects has a canonical coordinate system Σ (i.e. the 3D point cloud), HybridPose outputs the 6D camera pose (R I ∈ SO(3), t I ∈ R 3 ) of the image object under Σ, where R I is the rotation and t I is the translation component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Approach Overview</head><p>As illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, HybridPose consists of a prediction module and a pose regression module. Prediction module (Section 3.2). HybridPose utilizes three prediction networks f K θ , f E φ , and f S γ to estimate a set of keypoints K = {p k }, a set of edges between keypoints E = {v e }, and a set of symmetry correspondences between image pixels S = {(q s,1 , q s,2 )}. K, E, and S are all expressed in 2D. θ, φ, and γ are trainable parameters.</p><p>The keypoint network f K θ employs an off-the-shelf prediction network <ref type="bibr" target="#b34">[34]</ref>. The other two prediction networks, f E φ , and f S γ , are introduced to stabilize pose regression when keypoint predictions are inaccurate. Specifically, f E φ predicts edge vectors along a pre-defined graph of keypoints, which stabilizes pose regression when keypoints are cluttered in the input image. f S γ predicts symmetry correspondences that reflect the underlying (partial) reflection symmetry. A key advantage of this symmetry representation is that the number of symmetry correspondences is large: every image pixel on the object has a symmetry correspondence. As a result, even with a large outlier ratio, symmetry correspondences still provide sufficient constraints for estimating the plane of reflection symmetry for regularizing the underlying pose. Moreover, symmetry correspondences incorporate more features within the interior of the underlying object than keypoints and edge vectors. Pose regression module (Section 3.3). The second module of HybridPose optimizes the object pose (R I , t I ) to fit the output of the three prediction networks. This module combines a trainable initialization sub-module and a train-able refinement sub-module. In particular, the initialization sub-module performs SVD to solve for an initial pose in the global affine pose space. The refinement sub-module utilizes robust norms to filter out outliers in the predicted elements for accurate object pose estimation. Training HybridPose (Section 3.4). We train HybridPose by splitting the dataset into a training set and a validation set. We use the training set to learn the prediction module, and the validation set to learn the hyper-parameters of the pose regression module. We have tried training HybridPose end-to-end using one training set. However, the difference between the prediction distributions on the training set and testing set leads to sub-optimal generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hybrid Representation</head><p>This section describes three intermediate representations used in HybridPose. Keypoints. The first intermediate representation consists of keypoints, which have been widely used for pose estimation. Given the input image I, we train a neural network f K θ (I) ∈ R 2×|K| to predict 2D coordinates of a pre-defined set of |K| keypoints. In our experiments, HybridPose incorporates an off-the-shelf architecture called PVNet <ref type="bibr" target="#b34">[34]</ref>, which is the state-of-the-art keypoint-based pose estimator that employs a voting scheme to predict both visible and invisible keypoints.</p><p>Besides outliers in predicted keypoints, another limitation of keypoint-based techniques is that when the difference (direction and distance) between adjacent keypoints characterizes important information of the object pose, inexact keypoint predictions incur large pose error. Edges. The second intermediate representation, which consists of edge vectors along a pre-defined graph, explicitly models the displacement between every pair of keypoints. As illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, HybridPose utilizes a simple network f E φ (I) ∈ R 2×|E| to predict edge vectors in the 2D image plane, where |E| denotes the number of edges in the predefined graph. In our experiments, E is a fully-connected graph, i.e., |E| = |K|·(|K|−1) 2 . Symmetry correspondences. The third intermediate representation consists of predicted pixel-wise symmetry correspondences that reflect the underlying reflection symmetry. In our experiments, HybridPose extends the network architecture of FlowNet 2.0 <ref type="bibr" target="#b15">[15]</ref> that combines a dense pixelwise flow and the semantic mask predicted by PVNet. The resulting symmetry correspondences are given by predicted pixel-wise flow within the mask region. Compared to the first two representations, the number of symmetry correspondences is significantly larger, which provides rich constraints even for occluded objects. However, symmetry correspondences only constrain two degrees of freedom in the rotation component of the object pose (c.f. <ref type="bibr" target="#b24">[24]</ref>). It is necessary to combine symmetry correspondences with other intermediate representations.</p><p>A 3D model may possess multiple reflection symmetry planes. For these models, we train HybridPose to predict symmetry correspondences with respect to the most salient reflection symmetry plane, i.e., one with the largest number of symmetry correspondences on the original 3D model. Summary of network design. In our experiments, f K θ (I), f E φ (I), and f S γ are all based on ResNet <ref type="bibr" target="#b11">[11]</ref>, and the implementation details are discussed in Section 4.1. Trainable parameters are shared across all except the last convolutional layer. Therefore, the overhead of introducing the edge prediction network f E φ (I) and the symmetry prediction network f S γ is insignificant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pose Regression</head><p>The second module of HybridPose takes predicted intermediate representations {K, E, S} as input and outputs a 6D object pose (R I ∈ SO(3), t I ∈ R 3 ) for the input image I. Similar to state-of-the-art pose regression approaches <ref type="bibr" target="#b35">[35]</ref>, HybridPose combines an initialization sub-module and a refinement sub-module. Both sub-modules leverage all predicted elements. The refinement sub-module additionally leverages a robust function to model outliers in the predicted elements.</p><p>In the following, we denote 3D keypoint coordinates in the canonical coordinate system as p k , 1 ≤ k ≤ |K|. To make notations uncluttered, we denote output of the first module, i.e., predicted keypoints, edge vectors, and symmetry correspondences as p k ∈ R 2 , 1 ≤ k ≤ |K|, v e ∈ R 2 , 1 ≤ e ≤ |E|, and (q s,1 ∈ R 2 , q s,2 ∈ R 2 ), 1 ≤ s ≤ |S|, respectively. Our formulation also uses the homogeneous coordinatesp k ∈ R 3 ,v e ∈ R 3 ,q s,1 ∈ R 3 andq s,2 ∈ R 3 of p k , v e , q s,1 and q s,2 respectively. The homogeneous coordinates are normalized by the camera intrinsic matrix. Initialization sub-module. This sub-module leverages constraints between (R I , t I ) and predicted elements and solves (R i , t I ) in the affine space, which are then projected to SE(3) in an alternating optimization manner. To this end, we introduce the following difference vectors for each type of predicted elements:</p><formula xml:id="formula_0">r K R,t (p k ) :=p k × (Rp k + t),<label>(1)</label></formula><formula xml:id="formula_1">r E R,t (v e , p es ) :=v e × (Rp et + t) +p es × (Rv e ) (2) r S R,t (q s,1 , q s,2 ) := (q s,1 ×q s,2 ) T Rn r .<label>(3)</label></formula><p>where e s and e t are end vertices of edge e, v e = p et −p es ∈ R 3 , and n r ∈ R 3 is the normal of the reflection symmetry plane in the canonical system. HybridPose modifies the framework of EPnP <ref type="bibr" target="#b18">[18]</ref> to generate the initial poses. By combining these three constraints from predicted elements, we generate a linear system of the form Ax = 0, where A is matrix and its dimen-</p><formula xml:id="formula_2">sion is (3|K|+3|E|+|S|)×12. x = [r T 1 , r T 2 , r T 3 , t T ] T 12×1</formula><p>is a vector that contains rotation and translation parameters in affine space. To model the relative importance among keypoints, edge vectors, and symmetry correspondences, we rescale <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_1">(3)</ref> by hyper-parameters α E and α S , respectively, to generate A.</p><p>Following EPnP <ref type="bibr" target="#b18">[18]</ref>, we compute x as</p><formula xml:id="formula_3">x = N i=1 γ i v i (4)</formula><p>where v i is the i th smallest right singular vector of A. Ideally, when predicted elements are noise-free, N = 1 with x = v 1 is an optimal solution. However, this strategy performs poorly given noisy predictions. Same as EPnP <ref type="bibr" target="#b18">[18]</ref>, we choose N = 4. To compute the optimal x, we optimize latent variables γ i and the rotation matrix R in an alternating optimization procedure with following objective function:</p><formula xml:id="formula_4">min R∈R 3×3 ,γi 4 i=1 γ i R i − R 2 F<label>(5)</label></formula><p>where R i ∈ R 3×3 is reshaped from the first 9 elements of v i . After obtaining optimal γ i , we project the resulting affine transformation  <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula">(2)</ref>, which do not minimize the projection errors (i.e., with respect to keypoints and edges), which are known to be effective in keypoint-based pose estimation (c.f. <ref type="bibr" target="#b35">[35]</ref>).</p><formula xml:id="formula_5">4 i=1 γ i R i into</formula><p>Benefited from having an initial object pose (R init , t init ), the refinement sub-module performs local optimization to refine the object pose. We introduce two difference vectors that involve projection errors: ∀k, e, s,</p><formula xml:id="formula_6">r K R,t (p k ) := P R,t (p k ) − p k ,<label>(6)</label></formula><formula xml:id="formula_7">r E R,t (v e ) := P R,t (p et ) − P R,t (p es ) − v e ,<label>(7)</label></formula><p>where P R,t : R 3 → R 2 is the projection operator induced from the current pose (R, t).</p><p>To prune outliers in the predicted elements, we consider a generalized German-Mcclure (or GM) robust function</p><formula xml:id="formula_8">ρ(x, β) := β 2 1 /(β 2 2 + x 2 ).<label>(8)</label></formula><p>With this setup, HybridPose solves the following non-linear optimization problem for pose refinement:</p><formula xml:id="formula_9">min R,t |K| k=1 ρ( r K R,t (p k ) , β K ) r K R,t (p k ) 2 Σ k + |K| |E| |E| e=1 ρ( r E R,t (v e ) , β E ) r E R,t (v e ) 2 Σe + |K| |S| |S| s=1 ρ(r S R,t (q s,1 , q s,2 ), β S )r S R,t (q s,1 , q s,2 ) 2<label>(9)</label></formula><p>where β K , β E , and β S are separate hyper-parameters for keypoints, edges, and symmetry correspondences. Σ k and Σ e denote the covariance information attached to the keypoint and edge predictions.</p><formula xml:id="formula_10">x A = (x T Ax) 1 2 .</formula><p>When covariances of predictions are unavailable, we simply set Σ k = Σ e = I 2 . The above optimization problem is solved by Gauss-Newton method starting from R init and t init .</p><p>In the supp. material, we provide a stability analysis of (9), and show how the optimal solution of (9) changes with respect to noise in predicted representations. We also show collaborative strength among all three intermediate representations. While keypoints significantly contribute to the accuracy of t, edge vectors and symmetry correspondences can stablize the regression of R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">HybridPose Training</head><p>This section describes how to train the prediction networks and hyper-parameters of HybridPose using a labeled dataset T = {I, (K gt I , E gt I , S gt I , (R gt I , t gt I ))}. With I, K gt I , E gt I , S gt I , and (R gt I , t gt I ), we denote the RGB image, labeled keypoints, edges, symmetry correspondences, and groundtruth object pose, respectively. A popular strategy is to train the entire model end-to-end, e.g., using recurrent networks to model the optimization procedure and introducing loss terms on the object pose output as well as the intermediate representations. However, we found this strategy suboptimal. The distribution of predicted elements on the training set differs from that on the testing set. Even by carefully tuning the trade-off between supervisions on predicted elements and the final object pose, the pose regression model, which fits the training data, generalizes poorly on the testing data.</p><p>Our approach randomly divides the labeled set T = T train ∪ T val into a training set and a validation set. T train is used to train the prediction networks, and T val trains the hyper-parameters of the pose regression model. Implementation and training details of the prediction networks are presented in Section 4.1. In the following, we focus on training the hyper-parameters using T val . Initialization sub-module. Let R init I and t init I be the output of the initialization sub-module. We obtain the optimal hyper-parameters α E and α S by solving the following optimization problem:</p><formula xml:id="formula_11">min α E ,α S I∈Tpose R init I − R gt I 2 F + t init I − t gt I 2 . (10)</formula><p>Since the number of hyper-parameters is rather small, and the pose initialization step does not admit an explicit expression, we use the finite-difference method to compute numerical gradient, i.e., by fitting the gradient to samples of the hyper-parameters around the current solution. We then apply back-track line search for optimization. The refinement module solves an unconstrained optimization problem, whose optimal solution is dictated by its critical points and the loss surface around the critical points. We consider two simple objectives. The first objective forces ∂f I ∂c (0, β) ≈ 0, or in other words, the groundtruth is approximately a critical point. The second objective minimizes the condition number κ(</p><formula xml:id="formula_12">∂ 2 f I ∂ 2 c (0, β)) = λ max ∂ 2 f I ∂ 2 c (0, β) /λ min ∂ 2 f I ∂ 2 c (0, β)</formula><p>. This objective regularizes the loss surface around each optimal solution, promoting a large converge radius for f I (c, β). With this setup, we formulate the following objective function to optimize β:</p><formula xml:id="formula_13">min β I∈T val ∂f I ∂c (0, β) 2 + γκ ∂ 2 f I ∂ 2 c (0, β)<label>(11)</label></formula><p>where γ is a constant hyperparemeter. The same strategy used in <ref type="formula" target="#formula_0">(10)</ref> is then applied to optimize (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>This section presents an experimental evaluation of the proposed approach. Section 4.1 describes the experimental setup. Section 4.2 quantitatively and qualitatively compares HybridPose with other 6D pose estimation methods. Section 4.3 presents an ablation study to investigate the effectiveness of symmetry correspondences, edge vectors, and the refinement sub-module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. We consider two popular benchmark datasets that are widely used in the 6D pose estimation problem, Linemod <ref type="bibr" target="#b12">[12]</ref> and Occlusion Linemod <ref type="bibr" target="#b2">[3]</ref>. In comparsion to Linemod, Occlusion Linemod contains more examples where the objects are under occlusion. Our keypoint annotation strategy follows that of <ref type="bibr" target="#b34">[34]</ref>, i.e., we choose |K| = 8 keypoints via the farthest point sampling algorithm. Edge vectors are defined as vectors connecting each pair of keypoints. In total, each object has |E| = |K|·(|K|−1) 2 = 28 edges. We further use the algorithm proposed in <ref type="bibr" target="#b8">[8]</ref> to annotate Linemod and Occlusion Linemod with reflection symmetry labels.</p><p>Following the convention described in <ref type="bibr" target="#b3">[4]</ref>, we select 15% of Linemod examples as the training data, and the rest 85% as well as all of Occlusion Linemod examples for testing. To avoid overfitting, we use the same synthetic data generation scheme introduced in PVNet <ref type="bibr" target="#b34">[34]</ref>. <ref type="bibr" target="#b1">2</ref> Implementation details. We use ResNet <ref type="bibr" target="#b11">[11]</ref> with pretrained weights on ImageNet <ref type="bibr" target="#b7">[7]</ref> to build the prediction net-</p><formula xml:id="formula_14">works f K θ , f E φ , and f S γ .</formula><p>The prediction networks take an RGB image I of size (3, H, W ) as input, and output a tensor of size (C, H, W ), where (H, W ) is the image resolution, and C = 1 + 2|K| + 2|E| + 2 is the number of channels in the output tensor.</p><p>The first channel in the output tensor is a binary segmentation mask M . If M (x, y) = 1, then (x, y) corresponds to a pixel on the object of interest in the input image I. The segmentation mask is trained using the cross-entropy loss.</p><p>The 2|K| channels afterwards in the output tensor give x and y components of all |K| keypoints. A voting-based keypoint localization scheme <ref type="bibr" target="#b34">[34]</ref> is applied to extract the coordinates of 2D keypoints from this 2|K|-channel tensor and the segmentation mask M .</p><p>The next 2|E| channels in the output tensor give the x and y components of all |E| edges, which we denote as Edge. is a set of 2-tuples containing pixel-wise predictions of the i th edge vector in Edge. The mean of Edge i is extracted as the predicted edge.</p><p>The final 2 channels in the output tensor define the x and y components of symmetry correspondences. We denote this 2-channel "map" of symmetry correspondences as Sym. Let (x, y) be a pixel on the object of interest in the input image, i.e. M (x, y) = 1. Assuming ∆x = Sym(0, x, y) and ∆y = Sym(1, x, y), we consider (x, y) and (x + ∆x, y + ∆y) to be symmetric with respect to the reflection symmetry plane.</p><p>We train all three intermediate representations using the smooth 1 loss described in <ref type="bibr" target="#b9">[9]</ref>. Network training employs the Adam <ref type="bibr" target="#b17">[17]</ref> optimizer for 200 epochs. The learning rate is set to 0.001. Training weights of the segmentation mask, keypoints, edge vectors, and symmetry correspondences are 1.0, 10.0, 0.1, and 0.1, respectively.</p><p>The architecture described above achieves good performance in terms of detection accuracy. Nevertheless, it  should be emphasized that the framework of HybridPose can incorporate future improvements in keypoint, edge vector, and symmetry correspondence detection techniques. Besides, Hybridpose can be extended to handling multiple objects within an image. One approach is to predict instance-level rather than semantic-level segmentation masks by methods such as Mask R-CNN <ref type="bibr" target="#b10">[10]</ref>. Intermediate representations are then extracted from each instance, and fed to the pose regression module in 3.3.</p><p>Evaluation protocols. We use two metrics to evaluate the performance of HybridPose:</p><p>1. ADD(-S) <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b42">42]</ref> first calculates the distance between two point sets transformed by predicted pose and groundtruth pose respectively, and then extracts the mean distance. When the object possesses symmetric pose ambiguity, the mean distance is computed from the closest points between two transformed sets. ADD(-S) accuracy is defined as the percentage of examples whose calculated mean distance is less than 10% of the model diameter.</p><p>2. In the ablation study, we compute and report the the an-   <ref type="bibr" target="#b20">[20]</ref>, and DPOD <ref type="bibr" target="#b44">[44]</ref>. Objects annotated with ( †) possess symmetric pose ambiguity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of Results</head><p>As shown in <ref type="table">Table 1</ref>, <ref type="table">Table 2</ref>, and <ref type="figure" target="#fig_6">Figure 3</ref>   <ref type="bibr" target="#b14">[14]</ref>, PVNet <ref type="bibr" target="#b34">[34]</ref>, and DPOD <ref type="bibr" target="#b44">[44]</ref>. Objects annotated with ( †) possess symmetric pose ambiguity.</p><p>Baseline comparison on Linemod. HybridPose outperforms PVNet <ref type="bibr" target="#b34">[34]</ref>, the backbone model we use to predict keypoints. The improvement is consistent across all object classes, which demonstrates clear advantage of using a hybrid as opposed to unitary intermediate representation.</p><p>HybridPose shows competitive results against DPOD <ref type="bibr" target="#b44">[44]</ref>, winning on six object classes. The advantage of DPOD comes from data augmentation and explicit modeling of dense correspondences between input and projected images, both of which cater to situations without object occlusion. A detailed analysis reveals that the classes of objects on which HybridPose exhibits sub-optimal performance are among the smallest objects in Linemod. It suggests that pixel-based descriptors used in our pipeline are limited by image resolution.   <ref type="table" target="#tab_4">Table 3</ref>. Qualitative evaluation with different intermediate representations. We report errors using two metrics: the median of absolute angular error in rotation, and the median of relative error in translation with respect to object diameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>rotation. On the other hand, the translation error remains almost the same. One explanation is that symmetry correspondences only constrain two degrees of freedom in a total of three rotation parameters, and provide no constraint on translation parameters (see <ref type="formula" target="#formula_1">(3)</ref>). Full model. Adding edge vectors to keypoints and symmetry correspondences leads to salient performance gain in both rotation and translation estimations. One explanation is that edge vectors provide more constraints on both translation and rotation (see <ref type="formula">(2)</ref>). Edge vectors provide more constraints on translation than keypoints as they represent adjacent keypoints displacement and provide gradient information for regression. Unlike symmetry correspondences, edge vectors constrain 3 degrees of freedom on rotation parameters which further boosts the performance of rotation estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this paper, we introduce HybridPose, a 6D pose estimation approach that utilizes keypoints, edge vectors, and symmetry correspondences. Experiments show that Hy-bridPose enjoys real-time prediction and outperforms current state-of-the-art pose estimation approaches in accuracy. HybridPose is robust to occlusion. In the future, we plan to extend HybridPose to include more intermediate representations such as shape primitives, normals, and planar faces. Another possible direction is to enforce consistency across different representations in a similar way to <ref type="bibr" target="#b46">[46]</ref> as a selfsupervision loss in network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>We would like to acknowledge the support of this research from NSF DMS-1700234, a Gift from Snap Research, and a hardware donation from NVIDIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material: HybridPose: 6D Object Pose Estimation under Hybrid Representations</head><p>Chen Song * , Jiaru Song * , Qixing Huang</p><p>The University of Texas at Austin song@cs.utexas.edu, jiarus@cs.utexas.edu, huangqx@cs.utexas.edu This is the supplemental material to "HybridPose: 6D Object Pose Estimation under Hybrid Representations". We provide detailed explanations to our the algorithm used in the initialization sub-module. We also conduct a stability analysis of the refinement sub-module, and show how the optimal solution to the the objective function changes with respect to noise in predicted representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Initial Solution for Pose Regression</head><p>Recall that we denote 3D keypoint coordinates in the canonical coordinate system as p k , 1 ≤ k ≤ |K|. To make notations uncluttered, we denote output of the first module, i.e., predicted keypoints, edge vectors, and symmetry correspondences as p k ∈ R 2 , 1 ≤ k ≤ |K|, v e ∈ R 2 , 1 ≤ e ≤ |E|, and (q s,1 ∈ R 2 , q s,2 ∈ R 2 ), 1 ≤ s ≤ |S|, respectively. Our formulation also uses the homogeneous coordinatesp k ∈ R 3 ,v e ∈ R 3 ,q s,1 ∈ R 3 andq s,2 ∈ R 3 of p k , v e , q s,1 and q s,2 respectively. The homogeneous coordinates are normalized by camera intrinsic matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Three constraints for object pose.</head><p>We seek to generalize the EPnP algorithm which only exploits keypoint 2D-3D correspondences for pose estimation by leveraging hybrid representations, keypoint, edge vector and symmetry correspondence. To this end, we introduce the following difference vectors for each type of predicted elements:</p><formula xml:id="formula_15">r K R,t (p k ) :=p k × (Rp k + t),<label>(1)</label></formula><formula xml:id="formula_16">r E R,t (v e , p es ) :=v e × (Rp et + t) +p es × (Rv e ), (2) r S R,t (q s,1 , q s,2 ) := (q s,1 ×q s,2 ) T Rn r .<label>(3)</label></formula><p>where e s and e t are end vertices of edge e, v e = p et −p es ∈ R 3 , and n r ∈ R 3 is the normal of the reflection symmetry plane in the canonical system.</p><p>Proposition 1 If there is a perfect alignment between the predicted elements and the corresponding 3D keypoint tem-* Authors contributed equally plate with respect to the ground-truth pose R , t . Then</p><formula xml:id="formula_17">r K R ,t (p k ) = 0, r E R ,t</formula><p>(v e p es ) = 0, r S R,t (q s,1 , q s,2 ) = 0 Proof:</p><p>1. The proof of the first equality is straight-forward as there exists a "depth" λ k &gt; 0 so that</p><formula xml:id="formula_18">λ kpk = R p k + t</formula><p>It follows that Since 2(n T r (p s − q s,1 )) is a non-zero scalar, we can delete this term and finally get (q s,2 ×q s,1 ) T R n r = 0</p><formula xml:id="formula_19">0 = λ kpk ×p k =p k × (R p k + t )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Pose solution in eigenvector space.</head><p>A nice feature shared by (1), <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula" target="#formula_1">(3)</ref> is that all constraints are linear in the elements of R and t. This allows us to derive a closed-form solution of R and t in the affine transformation space. Specifically, we can define x = (r T 1 , r T 2 , r T 3 , t T ) T 12×1 as a vector that contains rotation and translation parameters in affine space. Expanding constraint (1) and constraint (2) yields three linear equations for each predicted element respectively for x, and expanding constraint (3) yields one linear equation. By concatenating all linear equations of predicted elements together, we can generate a linear system of the form Ax = 0, where A is matrix and its dimension is (3|K| + 3|E| + |S|) × 12.</p><p>To model the relative importance among keypoints, edge vectors, and symmetry correspondences, we rescale (2) and (3) by hyper-parameters α E and α S , respectively, to generate A. As discussed in the body of this paper, we calculate α E and α S by solving an optimization problem using finitedifference and back-track line search.</p><p>Then following EPnP <ref type="bibr">[1]</ref>, we compute x as</p><formula xml:id="formula_20">x = N i=1 γ i v i<label>(6)</label></formula><p>where v i is the i th smallest right singular vector of A. Ideally, when predicted elements are noise-free, N = 1 with x = v 1 is an optimal solution. However, this strategy performs poorly given noisy predictions. Same as EPnP <ref type="bibr">[1]</ref>, we choose N = 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Optimize a good linear combination.</head><p>To compute the optimal x, we optimize latent variables γ i and the rotation matrix R with following objective function:</p><formula xml:id="formula_21">min R∈R 3×3 ,γi 4 i=1 γ i R i − R 2 F (7)</formula><p>where R i ∈ R 3×3 is reshaped from the first 9 elements of v i . We solve this optimization problem with the following alternating procedure:</p><p>1. Fix γ i and solve for R by SVD. i.e.</p><formula xml:id="formula_22">R = U diag(1, 1, 1)V T given 4 i=1 γ i R i = U ΣV T1 ,</formula><p>2. Fix R and solve for γ i 's by optimizing a linear system 4 i=1 γ i R i = R in an element-wise manner.</p><p>To initialize γ i 's for the above optimization problem, we calculate γ i with i = 1...3 by enforcing that</p><formula xml:id="formula_23">3 i=1 γ i R i is an orthogonal matrix 2 : ( 3 i=1 γ i R i ) T 3 i=1 γ i R i = I 3<label>(8)</label></formula><p>Since I 3 is a symmetric matrix, expanding (8) yields 6 nonlinear constraints for γ = (γ 1 , γ 2 , γ 3 ) T , which is however uneasy to solve. We then define a new vector y = (y 1 , y 2 , y 3 , y 4 , y 5 , y 6 ) T = (γ 2 1 , γ 1 γ 2 , γ 1 γ 3 , γ 2 2 , γ 2 γ 3 , γ 2 3 ) T and form a linear system Cy = z which has the unique solution with z generated from I 3 . Afterwards, it is easy to recover γ i from y and optimize from initialized γ i alone with γ 4 = 0.</p><p>After optimization, we again apply SVD to project 4 i=1 γ i R i onto the space of SO(3), i.e., R init = U diag(1, 1, 1)V T and enforce det(R init ) &gt; 0 where R init = U ΣV T . Leveraging Ax = 0 defined in section (1.2), the corresponding translation t init is</p><formula xml:id="formula_24">t init = −(A T 2 A 2 ) −1 A T 2 A 1 r init<label>(9)</label></formula><p>where A 1 = A [:,1:9] , A 2 = A [:, <ref type="bibr">10:12]</ref> , r init 9×1 is reshaped from R init .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Stability Analysis for Pose Refinement</head><p>In this section, we provide a local stability analysis of the pose regression procedure, which amounts to solving <ref type="bibr">1</ref> If det(R) &lt; 0 we enforce det(R) &gt; 0 by defining R = U diag(1, 1, −1)V T . <ref type="bibr" target="#b1">2</ref> The reason of initializing 3 γ i 's is that <ref type="formula" target="#formula_8">(8)</ref> is unable to provide enough linear constraints for 4 γ i 's and this initialization ensures the convergence of optimization. the following optimization problem:</p><formula xml:id="formula_25">min R,t |K| k=1 ρ( r K R,t (p k ) , β K ) r K R,t (p k ) 2 Σ k + |K| |E| |E| e=1 ρ( r E R,t (v e ) , β E ) r E R,t (v e ) 2 Σe + |K| |S| |S| s=1 ρ(r S R,t (q s,1 , q s,2 ), β S )r S R,t (q s,1 , q s,2 ) 2<label>(10)</label></formula><p>When predictions are accurate, then the optimal solution of the objective function described above should recover the underlying ground-truth. However, when the predictions possess noise, then the optimal object pose can drift from the underlying ground-truth. Our focus is local analysis, which seeks to understand the interplay between different objective terms defined by keypoints, edge vectors, and symmetry correspondences. Therefore, we assume the noise level of the input is small, and the perturbation of the output is well captured by low-order Taylor expansion of the output.</p><p>Our goal is to characterize the relation between the variance of the input noise and the variance of the output pose. We show that incorporating edge vectors and symmetry correspondences generally help to reduce the variance of the output.</p><p>The remainder of this section is organized as follows. In Section 2.1, we provide a local stability analysis framework for regression problems. In Section 2.2, we describe the structure of the pose regression and apply this framework to provide a preliminary analysis of the stability of pose regression. In Section 2.3, we provide further analysis on a specific example, which indicates the interactions among keypoints, edge vectors, and symmetry correspondences. Finally, Section 2.4 provide proofs of the propositions in this analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Local Stability Analysis Framework</head><p>We begin with a general result regarding an optimization problem of the following form </p><p>In the context of this paper, y encodes the noise associated with the predictions, i.e., keypoints, edge vectors, and symmetry correspondences. x ∈ R 6 provides a local parameterization of the output, i.e., the object pose. The specific expressions of y and x will be described in Section 2.2. Without losing generality, we further assume that f satisfies the following assumptions (which are valid in the context of this paper):</p><p>• f (x, y) ≥ 0. Moreover, f (x, y) = 0 if and only if x = 0 and y = 0. This means x (0) = 0, and (0, 0) is the strict global optimal solution.</p><p>• f is smooth and at least C 3 continuous.</p><p>• The following Hessian matrix is positive definite in some local neighborhood of (0, 0):</p><formula xml:id="formula_27">∂ 2 f ∂ 2 x ∂ 2 f ∂x∂y ∂ 2 f ∂x∂y ∂ 2 f ∂ 2 y .</formula><p>Our analysis will utilize the following partial derivative of x with respect to y.</p><p>Proposition 2 Under the assumptions described above, x (y) is unique in the local neighborhood of 0, and</p><formula xml:id="formula_28">∂x ∂y (y) := − ∂ 2 f ∂ 2 x (x (y), y) −1 ∂ 2 f ∂x∂y (x (y), y).<label>(12)</label></formula><p>Proof. See Section 2.4.2.</p><p>Since we are interested in local stability analysis, we assume the magnitude of y is small. Thus,</p><formula xml:id="formula_29">x (y) ≈ ∂x ∂y (0) · y.<label>(13)</label></formula><p>If we further assume y follows some random distribution whose variance matrix if Var(y). Then the variance of the output x is given by</p><formula xml:id="formula_30">Var(x (y)) ≈ ∂ 2 f ∂ 2 x −1 · ∂ 2 f ∂x∂y · Var(y)( ∂ 2 f ∂x∂y ) T ∂ 2 f ∂ 2 x −1 . (14)</formula><p>Note that in our problem, f consists of non-linear least squares, i.e., f = β 2 i,1 · r i 2 Σi</p><formula xml:id="formula_31">β 2 i,2 + r i 2 .<label>(15)</label></formula><p>The following proposition characterizes how to compute ∂ 2 f ∂ 2 x and ∂ 2 f ∂x∂y . Proposition 3 Under the expression described in <ref type="bibr" target="#b15">(15)</ref>, the second-order derivatives ∂ 2 f ∂ 2 x and ∂ 2 f ∂x∂y at (0, 0) are given by </p><formula xml:id="formula_32">∂ 2 f ∂ 2 x = i β 2 i,1 β 2 i,2 ∂r i ∂x Σ i ∂r i ∂x T<label>(16)</label></formula><p>Proof. See Section 2.4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Structure of Pose Stability</head><p>We begin by rephrasing the pose-regression problem described in the main paper. Ground-truth setup. We use the same definition of variables as that in full paper. Recall that p k is coordinates of keypoint in canonical system. Let R gt and t gt be the ground-truth pose. Then the ground-truth 3D location of p k in the camera coordinate system is</p><formula xml:id="formula_34">p gt k = R gt p k + t gt , 1 ≤ k ≤ |K|.</formula><p>Let p gt k = (p gt,3D k,x , p gt,3D k,y , p gt,3D k,z ) T . Then the ground-truth image coordinates of the projected keypoint p gt k ∈ R 2 is given by Likewise, recall (q s1 , q s2 ) are symmetry correspondence in the world coordinate system, and let q gt s1 := R gt q s1 + t gt q gt s2 := R gt q s2 + t gt denote the transformed points in the camera coordinate system, where q gt si = (q gt,3D si,x , q gt,3D si,y , q gt,3D si,z ) T . So the image coordinates of each symmetry correspondence are given by Noise model. we proceed to describe the noise model used in the stability analysis. In this analysis, we assume each input keypoint is perturbed from the ground-truth location by y k = (y k,x , y k,y ) T , i.e., p k = p gt k + y k . Likewise, we assume each input edge vector is perturbed from the ground-truth edge vector by y e = (y e,x , y e,y ) T , i.e., v e = p gt es − p gt et + y e . Finally, for symmetry correspondences, we assume that q s1 is not perturbed), and q s2 is perturbed by y s = (y s,x , y s,y ) T , i.e., q s1 = q gt s1 , q s2 = q gt s2 + y s . Local parameterization. We parameterize the 6D object pose locally using exponential map with coefficients (c ∈ R 3 , c ∈ R 3 ), i.e., Moreover, assume that the normal to the reflection plane is <ref type="figure">(1, 0, 0)</ref>. The ground-truth symmetry correspondences are dense, and they are in the form of (x, y) and (−x, y), where 0 ≤ x ≤ 1, −1 ≤ y ≤ 1.</p><p>With this setup and after simple calculations, we have </p><p>a i , 1 ≤ i ≤ 8 are functions of δ, β E , β S and σ 2 K , σ 2 E , σ 2 S . For simplify, we only analyze a 8 , which is</p><formula xml:id="formula_36">a 8 = σ 2</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Approach overview. HybridPose consists of intermediate representation prediction networks and a pose regression module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Refinement sub-module. Let β = {β K , β E , β S } be the hyper-parameters of this sub-module. For each instance (I, (K gt I , E gt I , S gt I , (R gt I , t gt I ))) ∈ T val , denote the objective function in (9) as f I (c, β), where c = (c T , c T ) T ∈ R 6 is a local parameterization of R I and t I , i.e., R I = exp(c×)R gt I , t I = t gt I + c. c encodes the different the current estimated pose and the ground-truth pose in SE(3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Let i (0 ≤ i &lt; |E|) be the index of an edge. Then Edgei = {(Edge(2i, x, y), Edge(2i + 1, x, y))|M (x, y) = 1}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Pose regression results. HybridPose is able to accurately predict 6D poses from RGB images. HybridPose handles situations where the object has no occlusion (c), light occlusion (b, e, f, h), and severe occlusion (a, d, g). For illustrative purposes, we only draw 50 randomly selected symmetry correspondences in each example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>gular rotation error log(R T gt R I ) 2 and the relative translation error t I −tgt d between the predicted pose (R I , t I ) and the ground-truth pose (R gt , t gt ), where d is object diameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>, HybridPose leads to accurate pose estimation. On Linemod and Occlusion Linemod, HybridPose has an average ADD(-S) accuracy of 91.3 and 47.5, respectively. The result on Linemod outperforms all except one state-of-the-art approaches that regress poses from intermediate representations. The result on Occlusion-Linemod outperforms all state-of-the-art approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Baseline comparison on Occlusion Linemod. HybridPose outperforms all baselines. In terms of ADD(-S), our approach improves PVNet [34] from 40.8 to 47.5, representing a 16.4% enhancement, which clearly shows the advantage of HybridPose on occluded objects, where predictions of invisible keypoints can be noisy, and visible keypoints may not provide sufficient constraints for pose regression alone. HybridPose also outperforms DPOD, the state-ofthe-art model on this dataset. Running time. On a desktop with 16-core Intel(R) Xeon(R) E5-2637 CPU and GeForce GTX 1080 GPU, Hy-bridPose takes 0.6 second to predict the intermediate representations, 0.4 second to regress the pose. Assuming a batch size of 30, this gives an an average processing speed of around 30 fps, enabling real-time analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2 . 3 .</head><label>23</label><figDesc>The proof of the second equality follows the first equality. So we havê p es × (R p es + t ) = 0p et × (R p et + t ) = 0 Replacingp et byv e +p es , we havê v e × (R p et + t ) +p es × (R p et + t ) = 0Replacing the second p et by v e + p es in the above equation, we havê v e × (R p et + t ) +p es × R v e = 0 To prove the third equality, define the depths ofq s,1 andq s,2 as λ s,1 and λ s,2 and the corresponding 3D model points in the canonical system as q s,1 and q s,2 . p s is a point on the reflectional symmetry plane, whose normal is n r . Given a symmetry correspondence pair (q s,1 ,q s,2 ), we have q s,2 = (I 3 − 2n r n T r )q s,1 + 2n r n T r p s(4)Let R s = I 3 − 2n r n T r , t s = 2n r n T r p s . Following the camera perspective model, we haveλ s,1qs,1 = R q s,1 + t λ s,2qs,2 = R R s q s,1 + R t s + t Subtracting these two equations, we have λ s,2qs,2 − λ s,1qs,1 = R (R s q s,1 + t s − q s,1 ) Left multiply both sides of the equation byq s,2 × yields − λ s,1qs,2 ×q s,1 =q s,2 × [R (R s q s,1 + t s − q s,1 )](5) Geometrically, (5) reveals thatq s,2 ×q s,1 is perpendicular to the plane with span of {q s,2 , R (R s q s,1 + t s − q s,1 )}, thus we have (q s,2 ×q s,1 ) T R (R s q s,1 + t s − q s,1 ) = 2(n T r (p s − q s,1 ))(q s,2 ×q s,1 ) T R n r = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>x</head><label></label><figDesc>(y) := argmin y f (x, y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>)</head><label></label><figDesc>T = (p gt k,x , p gt k,y ) T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>where c 1 (δ) = 8+12δ 2 +10δ 4 8 , and c 2 ( 1 · (σ 2 K H K + λ 2 σ 2 E H E + µ 2 σ 2 S</head><label>1821222</label><figDesc>We proceed to assume the following noise model for the input:Var(y k ) = σ 2 K I 2 , Var(y e ) = σ 2 E I 2 , Var(y s ) = σ 2 S .In other words, noises in different predictions are independent.Applying Prop. 2.4.3, we have thatVar([c, c]) ≈(H K + λH E + µH S ) −H S )· (H K + λH E + µH S )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Although HybridPose utilizes multiple intermediate representations, all intermediate representations are predicted from an RGB image alone. HybridPose handles situations in which depth information is absent. Edge features. Edges are known to capture important image features such as object contours</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>a rigid transformation. Due to space constraint, we defer details to the supp. material. Refinement sub-module. Although (5) combines hybrid intermediate representations and admits good initialization, it does not directly model outliers in predicted elements. Another limitation comes from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>summarizes the performance of HybridPose using different predicted intermediate representations on the Linemod dataset. With keypoints. As a baseline approach, we estimate object poses by only utilizing keypoint information. This gives a mean absolute rotation error of 1.357°, and a mean relative translation error of 0.061. With keypoints and symmetry. Adding symmetry correspondences to keypoints leads to some performance gain in</figDesc><table><row><cell></cell><cell cols="3">keypoints keypoints + symmetries full model Rot. Trans. Rot. Trans. Rot. Trans.</cell></row><row><cell cols="2">ape benchvise 1.295°0.038 1.295°0.114 cam 1.215°0.080 can 1.305°0.052 cat 1.201°0.052 driller 1.267°0.040 duck 1.738°0.099 eggbox 1.098°0.050 glue 1.440°0.071 holepuncher 1.434°0.062 iron 1.274°0.046 lamp 1.371°0.030 phone 1.678°0.055</cell><cell>1.295°0.114 1.294°0.038 1.215°0.080 1.303°0.052 1.201°0.052 1.267°0.040 1.737°0.099 1.098°0.050 1.440°0.071 1.357°0.060 1.274°0.046 1.371°0.030 1.671°0.055</cell><cell>1.241°0.079 0.858°0.016 1.133°0.043 0.951°0.026 1.050°0.041 0.898°0.029 1.598°0.078 1.008°0.044 1.281°0.051 1.124°0.040 1.058°0.019 0.903°0.022 1.220°0.033</cell></row><row><cell>mean</cell><cell>1.357°0.061</cell><cell>1.350°0.061</cell><cell>1.104°0.040</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A previous version of this paper uses a different dataset split, which is inconsistent with baseline approaches. This problem has been fixed now. Please refer to our GitHub issues page for related discussions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For convenience, we negate both r K k and r E e defined in the body of this paper.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>R = exp(c×) · R gt , t = t gt + c.</p><p>Note this parameterization is quite standard for rigid transformations. Now consider the three terms used in pose regression <ref type="bibr" target="#b2">3</ref> :</p><p>r E e := v e − (P R,t (p es ) − P R,t (p et )), r S s := (q s,1 ×q s,2 ) T Rn r .</p><p>The following proposition characterizes the derivatives between each term and the parameters of the noise model and the parameters of the local parameterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 4 Define</head><p>The derivatives of r K R,t (p k ) = r K k are given by</p><p>The derivatives of r E R,t (v e ) = r E e are given by Moreover, the derivatives of r S s are given by</p><p>where n gt = Rn r = (n gt x , n gt y , n gt z ) T ,q gt si is homogeneous coordinate of q gt si normalized by camera intrinsic matrix.</p><p>Proof. See Section 2.4.1.</p><p>Let y collect all the random variables in a vector. Let J K , J E , and J S collect the Jacobi matrices for the predicted elements under each type in its column. Note that the size of J S is 3 × 3 according to the derivations above. To facilitate the definition below, we reshape J S as a 6 × 6 matrix by placing original elements to the upper-left corner, and zeros to elsewhere. Denote β E and β S as the weight in front of In other words, incorporating edge vectors is helpful for reducing the velocity of the third dimension of the rotational component.</p><p>Similar analysis can be done for other a i . As the rationale is similar, we omit them for brevity. Contributions of keypoints, edge vectors, and symmetry correspondences. It is very interesting to study the structure of <ref type="bibr" target="#b22">(22)</ref>. First of all, all elements are relevant to keypoints. Edge vectors provide full constraints on the underlying rotation. Symmetry correspondences also provide constraints on two dimensions of the underlying rotation. However, by analyzing the structure of H E and H K , one can see that they do not provide constraints on two dimensions of the underlying translation (albeit on this simple model). This explains why only using edge vectors and symmetry correspondences leads to poor results on object translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Proof of Propositions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Proof of Proposition 4</head><p>Derivatives of r K k and r E e . It is straightforward to compute the derivatives of r K k and r E e with respect to y k and y e , respectively. In the following, we focus on the derivatives of r K k with respect to (c, c). The derivatives of r E e can be obtained by subtracting those of r K es and those of r K et . Recall the local parameterization R = exp(c×)R gt and t = t gt + c. We have</p><p>Using chain rule, we have </p><p>Moreover, ∂r S s ∂c = ∂det((q gt s1 ×q gt s2 , c, n)) ∂c = n × (q gt s1 ×q gt s2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Proof of Proposition 2</head><p>Proof: First of all, any optimal solution x (y) is a critical point of f . Therefore, it shall satisfy:</p><p>Consider a neighborhood, where y ≤ 1 , and x ≤ 2 . 2 is chosen so that it contains for each y, the critical point with the smallest norm. Assume that ∂ 2 f ∂ 2 x is positive semidefinite in this neighborhood.</p><p>By contradiction, suppose there exists two distinctive local minimums x 1 (y) and x 1 (y) for a given y, i.e.,</p><p>Through integration, (24) yields</p><p>Since the weighted sum of positive definite matrices is also positive definite. It follows that</p><p>In other words, it cannot have a zero eigenvalue, with nonzero eigenvector x 2 − x 1 . In other words, the critical point is unique. Since the second order derivatives are positive definite, then each critical point is also a local minimum.</p><p>Computing the derivatives of (23) with respect to y, we obtain ∂ 2 f ∂ 2 x · ∂x ∂y + ∂ 2 f ∂x∂y = 0 (25)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Proof of Proposition 3</head><p>The proof is straight-forward as r i (0, 0) = 0, and</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reflection symmetry detection via appearance of structure descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ibragim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungkyu</forename><surname>Atadjanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference, Amsterdam</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deepedge: A multi-scale bifurcated deep network for topdown contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4380" to="4389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pose estimation for objects with rotational symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enric</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7215" to="7222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Seeing behind the scene: Using symmetry to reason about objects in cluttered environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandrs</forename><surname>Ecins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Fermüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7193" to="7200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Asian Conference on Computer Vision -Volume Part I, ACCV&apos;12</title>
		<meeting>the 11th Asian Conference on Computer Vision -Volume Part I, ACCV&apos;12<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On symmetry and multiple-view geometry: Structure, pose, and calibration from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><forename type="middle">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="265" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmentation-driven 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3385" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Epnp: An accurate o (n) solution to the pnp problem. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="683" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06079</idno>
		<title level="m">Duygu Ceylan, and Qixing Huang. Symmetry-aware depth estimation using deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Computational symmetry in computer vision and computer graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagit</forename><surname>Hel-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">S</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="195" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning relaxed deep supervision for better edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sastry. An Invitation to 3-D Vision: From Images to Geometric Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>SpringerVerlag</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Explaining the ambiguity of object detection and 6d pose from visual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><forename type="middle">Martin</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep model-based 6d pose refinement in rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="800" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Symmetry in 3d geometry: Extraction and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ceylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Making deep heatmaps robust to partial occlusions for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using facial symmetry to handle pose variations in real-world 3d face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Perakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theoharis</forename><surname>Theoharis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1938" to="1951" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Konstantinos G Derpanis, and Kostas Daniilidis. 6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation. CoRR, abs/1812.11788</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lambda twist: An accurate fast robust perspective three point (p3p) solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klas</forename><surname>Nordberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="699" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6d object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reflection symmetry detection using locally affine invariant edge correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaozhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zesheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1297" to="1301" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems XIV</title>
		<meeting><address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Symmetric piecewise planar object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2577" to="2584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Dpod: 6d pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Ppgnet: Learning point-pair graph for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03415</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Path-invariant map networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxiao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11084" to="11094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Starmap for category-agnostic keypoint and viewpoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Karpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Epnp: An accurate o (n) solution to the pnp problem. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">155</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
