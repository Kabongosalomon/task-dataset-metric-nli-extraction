<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Seed the Views: Hierarchical Semantic Alignment for Contrastive Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohang</forename><surname>Xu</surname></persName>
							<email>xuhaohang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
							<email>xionghongkai@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Seed the Views: Hierarchical Semantic Alignment for Contrastive Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning based on instance discrimination has shown remarkable progress. In particular, contrastive learning, which regards each image as well as its augmentations as an individual class and tries to distinguish them from all other images, has been verified effective for representation learning. However, pushing away two images that are de facto similar is suboptimal for general representation. In this paper, we propose a hierarchical semantic alignment strategy via expanding the views generated by a single image to Cross-samples and Multi-level representation, and models the invariance to semantically similar images in a hierarchical way. This is achieved by extending the contrastive loss to allow for multiple positives per anchor, and explicitly pulling semantically similar images/patches together at different layers of the network. Our method, termed as CsMl, has the ability to integrate multilevel visual representations across samples in a robust way. CsMl is applicable to current contrastive learning based methods and consistently improves the performance. Notably, using the moco as an instantiation, CsMl achieves a 76.6% top-1 accuracy with linear evaluation using ResNet-50 as backbone, and 66.7% and 75.1% top-1 accuracy with only 1% and 10% labels, respectively. All these numbers set the new state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As a fundamental task in machine learning, representation learning targets at extracting compact features from the raw data, and has been dominated by the fully supervised paradigm over the past decades <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref>. Recent progress on representation learning has witnessed a remarkable success over self-supervised learning <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b31">[31]</ref>, which facilitates feature learning without human annotated labels. In self-supervised learning, a network is trained based on a series of predefined tasks ac-  cording to the intrinsic distribution priors of images, such as image colorization <ref type="bibr" target="#b36">[36]</ref>, rotation prediction <ref type="bibr" target="#b7">[8]</ref>, context completion <ref type="bibr" target="#b20">[21]</ref>, etc. More recently, the contrastive learning method <ref type="bibr" target="#b10">[11]</ref>, which is based on instance discrimination as a pretext task, has taken-off as it has been demonstrated to outperform the supervised counterparts on several downstream tasks like classification and detection. In contrastive learning, each image as well its augmentations is treated as a separate class, and the views generated by a single image are pulled closer together, while all other images are treated as negatives and pushed away. In this setting, the invariance is only encoded from low-level image transformations such as cropping, blurring, and color jittering, etc. While the invariance to semantically similar images is not explicitly modeled but on the contrary, they are treated as negatives and pushed away as in MoCo <ref type="bibr" target="#b10">[11]</ref>. This is contradictory with the alignment principle <ref type="bibr" target="#b29">[29]</ref> of feature representation, which favors the encoders to assign similar features to similar samples. As a result, the optimization is contradictory with the intrinsic distribution of images and is not optimal for feature representation.</p><p>In this paper, we introduce a hierarchical semantic alignment strategy via seed the views that are constrained within a single image and a single level representation to Crosssamples and Multi-levels. The idea behind our strategy is to align semantically similar samples in different latent space, and thus enable better representation throughout the network. Specifically, the cross-sample views are achieved by simply searching the feature representation in the embedding space, and selecting the nearest neighbors that are similar with the anchor for contrastive learning. While the multi-level views are expanded at the intermediate layers of a network, which enables hierarchical representation of the same image/patch. As a result, the views are seeded from a single image, and expanded across different samples at different levels, these views are pulled together for more general and discriminative representation.</p><p>For cross-sample views, it is a dilemma to select appropriate nearest samples as positives, pulling samples that are de facto very similar in the feature space brings about limited performance gain since current representation handles these invariance well, while enlarging the searching space would inevitably introduce noisy samples. To solve this issue, we rely on data mixing to generate extra positive samples, which can be treated as a smoothing regularization of the anchor. In this way, similar samples are extended in a smoother and robust way, and we are able to better model the intra-class similarity for compact representation.</p><p>For multi-level views, we find that although the linear classification accuracy of the last layers is approaching the supervised baseline <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, the middle-level representation of current contrastive based methods suffers much lower discrimination capacity, which is harmful for downstream tasks such as detection that require intermediate discrimination ability. Towards this goal, we extend the views to intermediate layers of a network and propose a hierarchical training strategy that enables the feature representation to be more discriminative in the intermediate layers. However, it suffers from optimization contradiction when directly adding a loss layer on the intermediate representation due to the gradient competition issue. To solve this issue, we add a bottleneck layer for each intermediate loss, which we find is applicable for robust optimization. In this way, the features are deeply supervised throughout the network, which also benefits for transferability.</p><p>The proposed hierarchical semantic alignment strategy via CsMl significantly boosts the feature representation of contrastive learning when evaluated on several selfsupervised learning benchmarks. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, using MoCo as an instantiation <ref type="bibr" target="#b10">[11]</ref>, we achieve 76.6% top-1 accuracy with a standard ResNet-50 on ImageNet linear evaluation, and 66.7% and 75.1% top-1 accuracy with 1% and 10% labels, respectively. We also validate its transferring ability on several downstream tasks covering detec-tion and segmentation, and achieve better results comparing with previous self-supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-supervised representation learning has attracted more and more attentions over the past few years since it is free of labels and is easy to scale up. Self-supervised learning aims at exploring the intrinsic distribution of data samples via constructing a series of pretext tasks, which varies in utilizing different priors of images. Traditional self-supervised learning has sought to learn a compressed code which can effectively reconstruct the input. Among them, a typical strategy is to take advantage of the spatial properties of images, like predicting the relative spatial positions of images patches <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b19">[20]</ref>, or inferring the missing parts of images by inpainting <ref type="bibr" target="#b20">[21]</ref>, colorization <ref type="bibr" target="#b36">[36]</ref>, or rotation prediction <ref type="bibr" target="#b7">[8]</ref> etc.. Recent progress in self-supervised learning is mainly based on instance discrimination <ref type="bibr" target="#b31">[31]</ref>, in which each image as well as its augmentations is treated as a separate class. The motivation behind these works is the InfoMax principle, which aims at maximizing mutual information <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b31">[31]</ref> across different augmentations of the same image <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b27">[27]</ref>. The design choices of the Info-Max principle, such as the number of negatives and how to sample them, hyper-parameter settings, and data augmentations all play a critical role for a good representation.</p><p>Data augmentation plays a key role for contrastive learning based representation. Since the invariance is only encoded by different transformations of an image. According to <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b28">[28]</ref>, the performance of contrastive learning based approaches strongly relies on the types and strength of augmentations, i.e., image transformation priors that do not change object identity. In this way, the network is encouraged to hold invariance in the local vicinities of each sample, and usually more augmentations benefit for feature representation. However, current widely used data augmentation methods are mostly operated within a single sample. One exception is the method in <ref type="bibr" target="#b24">[25]</ref>, which makes use of mixup mixture for flattened contrastive predictions. However, such a mixture strategy is conducted among all the images, which destroys the local similarity when contrasting mixed samples that are semantically dissimilar.</p><p>Our method is reminiscent of the recent proposed Supervised Contrastive Learning <ref type="bibr" target="#b14">[15]</ref>, which pulls multiple positive samples together. The differences are that, first, SCL is designed for fully supervised paradigm, where the positive samples are simply selected from the ground truth labels, while our method does not rely on these labels, and deliberately design a positive sample selection strategy to facilitate semantic alignment in a robust way. Second, we extend the contrastive loss to the intermediate hidden layers to enhance its discriminative power of the earlier layers, which is beneficial for discriminative representation and with better transferability, especially for semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we start by reviewing contrastive loss for self-supervised learning, and investigate its drawbacks for general feature representation. Then we present the proposed hierarchical training strategy that pulls semantically similar images at different layers of a network. As shown in <ref type="figure">Fig. 2</ref>, the core idea includes two modules, first, we deliberately design a positive sample selection strategy to expand the neighborhood of a single image, and adjust the contrastive loss to allow for multiple positives during each forward propagation. Furthermore, we propagate the semantic alignment to the earlier layers to encourage class separability. In this way, the features are trained in a hierarchical way for more compact representation. Each module would be elaborated in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">An Overview of Contrastive Learning</head><p>Contrastive learning targets at learning an encoder that is able to map positive pairs to similar representations while pushing away those negative samples in the embedding space. It can be efficiently addressed via momentum contrast <ref type="bibr" target="#b10">[11]</ref>, which substantially increases the number of negative samples. Given a reference image with two augmented views x and x , MoCo aims to learn feature representation q = f q (x) by a query encoder f q , that can distinguish x from all other images x i , where x and all the negatives x i are encoded by an asynchronously updated key encoder f k , with k + = f k (x ) and k i = f k (x i ), the contrastive loss can be defined as:</p><formula xml:id="formula_0">L q = − log exp(q · k + /τ ) N i=0 exp(q · k i /τ ) ,<label>(1)</label></formula><p>where τ is the temperature parameter scaling the distribution of distances. However, the positive samples are constrained within a single image with different transformations, and only support one positive sample for each query q, which is low efficient and hard for modeling invariance to semantically similar images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contrastive Learning with Cross-Sample Views</head><p>In this section, we extend the views generated from a single image to cross-sample views, and describe the proposed positive sample selection strategy and adjust contrastive loss to allow for multiple positives to explicitly model the invariance among similar images.</p><p>Positive Sample Selection. For positive samples, we simply make use of k nearest neighbors to search semantically similar images in the embedding space. Specially, given unlabeled training set X = {x 1 , x 2 , ..., x n } and a query encoder f q , we obtain the corresponding embedding</p><formula xml:id="formula_1">representation V = {v 1 , v 2 , ..., v n } where v i = f q (x i ).</formula><p>For a typical Res-50 network, the embedding is obtained from the last average pooled features with dimension 2048. Given an anchor sample x a , we compute the cosine similarity with all other images, and select the top k samples with the highest similarity as positives Ω = {x 1 , x 2 , ..., x k }.</p><p>Loss Function. We simply adjust the contrastive loss in Eq. 1 to allow for multiple positives per anchor. Given an anchor sample x a and its nearest neighborhood set Ω, we randomly select a positive sample x p ∈ Ω, and the loss term L qa for x a can be reformulated as:</p><formula xml:id="formula_2">L qa = − 1 2 log exp(q a · k a /τ ) exp(q a · k a /τ ) + N i=1 exp(q a · k i /τ ) + log exp(q a · k p /τ ) exp(q a · k p /τ ) + N i=1 exp(q a · k i /τ ) ,<label>(2)</label></formula><p>where each anchor sample q a encoded with f q , is pulled with two samples k a and k q encoded with f k , and pushed away with all other samples in the key encoder f k . Symmetrically, the loss term L qp for positive sample x p can be obtained accordingly. The overall loss is the combination of the two losses, which is equipped with two positive samples in the query encoder f q , and the corresponding two positive samples in the key encoder f k . Each sample is accompanied with a random data augmentation as described in <ref type="bibr" target="#b3">[4]</ref>, and is pulled together with all positive samples (also undergo a random data augmentation) from the other encoder. Expanding the Neighborhood. It is a dilemma to define an appropriate k for nearest sample selection, setting it too small, the objective pulls samples that are already very similar in the feature space, and brings about limited performance gain since current representation handles these invariance well, while setting it too large, it would inevitably introduce noisy samples, and pulling these samples would destroy the local similarity constraint and is harmful for general representation. To solve this issue, we rely on data mixture to expand the neighborhood space of an anchor based on the selected positive samples. The assumption is that the mixed samples act as an interpolation between two samples, and lies in the local neighborhood of the two samples. In this way, the generated mixed samples expand the neighbors in the embedding space that current model cannot handle well, and pulling these samples is beneficial for better generalization.</p><p>In particular, we apply CutMix <ref type="bibr" target="#b34">[34]</ref> augmentation, which is widely used as a regularization strategy to train neural networks in fully supervised paradigm. Given an anchor sample x a , and its positive neighbor x p ∈ Ω, the  <ref type="figure">Figure 2</ref>. An overview of our proposed hierarchical semantic alignment framework. The baseline is based on MoCo <ref type="bibr" target="#b10">[11]</ref>, which requires a query encoder fq and an asynchronously updated key encoder f k . Given an anchor image xa, we randomly select a positive sample xp from the nearest neighborhood set Ω, and generate the mixed samplex. The hierarchical semantic alignment is enforced by pulling the positive samples xa, xp, andx in the intermediate layers as well as the last embedding space.</p><p>mixed samplex is generated as follows:</p><formula xml:id="formula_3">x = M x a + (1 − M) x p ,<label>(3)</label></formula><p>where M ∈ {0, 1} W ×H is a binary mask that has the same size as x a , and indicates where to drop out the region in x a and replaced with a randomly selected patch from x p , and W, H denotes the width and height of an image, respectively. 1 is a binary mask filled with ones, and is the element-wise multiplication operation. For mask M generation, we simply follow the setting in <ref type="bibr" target="#b34">[34]</ref>, and do not carefully tune the parameters. Note that different from CutMix used in fully supervised learning that randomly selects two images for mixing and changes the corresponding labels accordingly, we only sample those similar samples and ensure that the generated samples lie in the local neighborhood of the anchor. The mixed samplesx is treated as a new positive sample, and pulled together with x a and x p accordingly:</p><formula xml:id="formula_4">Lq = − λ log exp(q · k a /τ ) exp(q · k a /τ ) + N i=1 exp(q · k i /τ ) + (1 − λ) log exp(q · k p /τ ) exp(q · k p /τ ) + N i=1 exp(q · k i /τ ) ,<label>(4)</label></formula><p>where λ is a combination ratio that determines the cropped area for CutMix operation, and is sampled from beta distribution Beta(α, α) with parameter α (α = 1). The final loss function can be formulated as :</p><formula xml:id="formula_5">L = L qa + L qp + Lq.<label>(5)</label></formula><p>We simply set the balancing factors of the three terms as 1 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Contrastive Learning with Multi-level Views</head><p>Following training a customized network, the contrastive loss is only penalized at the last embedding layers, while the optimization of the intermediate hidden layers is implicitly penalized by back propagating the gradients to the earlier layers. However, due to the lack of labels, the optimization objective is more challenging and suffers slow convergence, and the intermediate layers are especially under fitted and with limited discriminative power comparing with fully supervised learning (c.f. <ref type="figure" target="#fig_1">Fig. 3</ref>). Inspired by <ref type="bibr" target="#b15">[16]</ref>, we extend the proposed contrastive loss in Eq. 5 to the intermediate hidden layers, which targets at explicitly modeling the similarities among image/patches for better discrimination. Specifically, we introduce a companied objective at the end of each stage for a typical ResNet network, which acts as an additional constraint during the optimization procedure.</p><p>As demonstrated in <ref type="bibr" target="#b22">[23]</ref>, it is too aggressive to directly add a loss layer as side branch of the intermediate layers due to the gradient competition issue. The optimization objective from one side branch would probably inconsistent with that from other branches since they undergoes extremely different levels of layers during back propagation. To solve this issue, for each companied loss at stage l, we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy(%) BigBiGAN <ref type="bibr" target="#b6">[7]</ref> 56.6 Local aggregation <ref type="bibr" target="#b37">[37]</ref> 58.8 SeLa <ref type="bibr" target="#b33">[33]</ref> 61.5 PIRL <ref type="bibr" target="#b18">[19]</ref> 63.6 CPCv2 <ref type="bibr" target="#b13">[14]</ref> 63.8 PCL <ref type="bibr" target="#b16">[17]</ref> 65.9 SimCLRv2 <ref type="bibr" target="#b2">[3]</ref> 71.7 MoCo v2 <ref type="bibr" target="#b3">[4]</ref> 71.1 BYOL <ref type="bibr" target="#b9">[10]</ref> 74. add another embedding layer g l which consists of bottleneck layers before the contrastive loss as side branch. In practice, we find that a single bottleneck layer is applicable to alleviate the gradient competition and enable efficient optimization. Note that These companied branches are removed after training and hence do not increase complexity of the network after pretraining. Specifically, given encoder f q and f k , we define feature map at stage l as f l q and f l k , which is truncated at stage l. Each encoder is passed through another embedding layer g l , which consists of a bottleneck layer and 2 MLP layers to encode the feature into a 128-dim vector. The loss function of q a specific to stage l is defined as:</p><formula xml:id="formula_6">L l qa = − 1 2 log exp(q l a · k l a /τ ) exp(q l a · k l a /τ ) + N i=1 exp(q l a · k i l /τ ) + log exp(q l a · k l p /τ ) exp(q l a · k l p /τ ) + N i=1 exp(q l a · k i l /τ ) ,<label>(6)</label></formula><p>where q l = g l (f l q ) and k l = g l (f l k ). Similarly, the loss term L l q and L l can be obtained according to Eq. 4 and Eq. 5, respectively. When there are L losses corresponding to L intermediate stages, the final losses of the whole network can be computed as: </p><formula xml:id="formula_7">L total = L + L l=1 L l<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we access our proposed feature representation on several widely used unsupervised benchmarks. We first evaluate the classification performance on Ima-geNet under linear evaluation and semi-supervised protocols <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Then we transfer the representation to several downstream tasks including detection and instance segmentation. We also analyze the performance of our feature representation with detailed ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pre-training Details</head><p>The feature representation is trained based on a standard ResNet-50 <ref type="bibr" target="#b12">[13]</ref> network, using ImageNet 2012 training dataset <ref type="bibr" target="#b4">[5]</ref>. We follow the settings MoCo as in <ref type="bibr" target="#b10">[11]</ref> 1 , which employs an asynchronously updated key encoder to enlarge the capacity of negative samples, and add a 2-layer MLP on top of the last average pooling layer to form a 128-d embedding vector <ref type="bibr" target="#b3">[4]</ref>. The model is trained using SGD optimizer with momentum 0.9 and weight decay 0.0001. The batch size and learning rate are set to 1024 and 0.12 for 32 GPUs, according to the parameters recommended by <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>. The learning rate is decayed to 0 by cosine scheduler <ref type="bibr" target="#b17">[18]</ref> during the whole training process.</p><p>For companied loss in the intermediate stages, each branch is added with a bottleneck with three layers (1 × 1, 3 × 3, and 1 × 1 convolutions, and the number of channels follows the setting of the corresponding stage) and 2-MLP layers. We add the companied loss at both stage 2 and stage 3, and simply set the balance factor of each layer as 1. For positive sample selection, we perform knn every 5 epochs and select top-10 nearest neighbors for each anchor. We find that the update frequency does not affect the performance too much when it ranges from 1 to 20. For efficiency, we only pull one positive sample per anchor as well as another mixed sample during each forward propagation, <ref type="table" target="#tab_10">Table 3</ref>. Semi-supervised learning by fine-tuning 1% and 10% labeled images on ImageNet. The last row reports results of using a simple data mining procedure (averaged over 5 trials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>1% labels 10% labels Top-1 Top- which we find is sufficient (see appendix for pulling more positive samples at once). For data augmentation, except for those used in <ref type="bibr" target="#b10">[11]</ref>, we also report the results of multicrop augmentation <ref type="bibr" target="#b0">[1]</ref>, which has been demonstrated to be effective for further performance gain. The final model is trained for 800 epochs for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on ImageNet</head><p>Classification with Linear Evaluation. We first evaluate our pretrained features by training a linear classifier on top of the frozen representation, following a common protocol in <ref type="bibr" target="#b10">[11]</ref>. The classifier is trained on global average pooled features of ResNet-50 for 100 epochs, and we report the center crop, top-1 classification accuracy on ImageNet validation set. As shown in <ref type="table" target="#tab_2">Table 1</ref>, for fair comparison, all results are based on the same network structure with the same amount of parameters. CsMl achieves 74.4% top-1 accuracy under 800 epochs pretraining, which outperforms the MoCo v2 baseline <ref type="bibr" target="#b3">[4]</ref> by 3.3%, and the performance can be further boosted to 76.6% when adding multi-crop data augmentations, which is better than previous best performed result SwAV by 1.3%.</p><p>Classification with KNN Classifier. We also evaluate our representation with KNN classifier, which is able to evaluate the pre-trained features more directly. Following <ref type="bibr" target="#b0">[1]</ref>, we center crop the images to obtain features from the last average pooled layers, and report the accuracy with 20 and 200 NN (we choose the result of 800 epochs with multi-crop augmentation) in <ref type="table" target="#tab_4">Table 2</ref>. For convenient comparison, we also list the KNN classification results of fully supervised model, which achieves accuracy of 75.0%. CsMl is only 2.6% lower than the supervised baseline, and significantly outperforms previous methods, which validates the effectiveness of explicitly modeling similarities cross samples.</p><p>Semi-supervised Settings. We also evaluate the representations by fine-tuning the whole network with few shot labels. Following the evaluation protocol in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, we fine-tune all layers with only 1% and 10% labeled data. For fair comparison, we use the same splits of training data as in <ref type="bibr" target="#b1">[2]</ref>, using SGD optimizer with momentum 0.9 to finetune all layers for 60 epochs. The initial learning rate is set to 10 −4 for backbone and 10 for randomly initialized fc layer. During fine-tuning, only random cropping and horizontal flipping are applied for fair comparisons. Note that our method does not apply any special design like <ref type="bibr" target="#b2">[3]</ref>, which makes use of more MLP layers and has shown improved results when fine-tuning with few labels. As shown in <ref type="table" target="#tab_10">Table 3</ref>, our method achieves 62.2% top-1 accuracy with only 1% labels and 72.9% top-1 accuracy with 10% labels. In both two settings, our method consistently outperforms other semi-supervised and self-supervised methods, especially when 1% labeled samples are available.</p><p>Following semi-supervised learning setting that the unlabeled samples are used for training via assigning pseudo labels, we further conduct a simple data mining procedure to access how the pre-trained models improve data mining. The data mining procedure is conducted as follows: 1) Using the fine-tuned model to infer on all unlabeled samples, and obtaining a confidence distribution for each image. 2) The information entropy is calculated to measure the confidence degree of each image, and we filter those samples with entropy higher than a threshold. 3) Convert the soft labels into one-hot pseudo label via only retaining the highest scored dimension, and train the model together with the ground truth labeled data. We simply set the entropy threshold as 1 and the model is trained following a standard fully supervised learning procedure <ref type="bibr" target="#b12">[13]</ref>. Specifically, the model is trained for 90 epoch with initial learning rate set as 10 −4 for backbone and 10 −2 for fc layer, and they are both decayed by 0.1 after every 30 epochs. The results are shown in the last row of <ref type="table" target="#tab_10">Table 3</ref>, the performance can be further boosted via a simple data mining procedure, and we achieve 66.7% top-1 accuracy with 1% labeled samples, and 75.1% top-1 accuracy with 10% labeled samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Downstream Tasks</head><p>We also test the generalization of our unsupervised learned representations on more downstream tasks, including object detection and instance segmentation. All experiments follow MoCo <ref type="bibr" target="#b10">[11]</ref> settings for fair comparisons.  PASCAL VOC Following evaluation protocol in <ref type="bibr" target="#b10">[11]</ref>, we use Faster-RCNN <ref type="bibr" target="#b21">[22]</ref> detector with R50-C4 backbone. All layers are fine-tuned end-to-end on the union set of VOC07+12 for 2× schedule, and we evaluate the performance on VOC test07. As shown in <ref type="table" target="#tab_10">Table 5</ref>, CsMl achieves 82.7% and 64.1% mAP under AP50 and AP75 metric, which is slightly better than the results of MoCo v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS COCO</head><p>We also evaluate the representation learned on a large scale COCO dataset. Following <ref type="bibr" target="#b10">[11]</ref>, we use mask R-CNN <ref type="bibr" target="#b11">[12]</ref> detector with FPN, fine-tune all the layers end-to-end over the train2017 set, and evaluate the performance on val2017. As shown in <ref type="table" target="#tab_6">Table 4</ref>, which compare the detection and segmentation results under default 1× learning schedule. Our method consistently outperforms the supervised and MoCo v2 baseline. Notably, we achieve much higher performance on small and medium objects. This is mainly due to the hierarchical alignment strategy that increases the discrimination power of the intermediate layers. Note that our method is comparable with recently proposed DenseCL <ref type="bibr" target="#b30">[30]</ref> which is particularly designed for detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>In this section, we conduct extensive ablation studies to better understand how each component affects the performance. Unless specified, all results are compared with models trained for 200 epochs for efficiency, and we report the top-1 accuracy under linear evaluation protocol.</p><p>Effects of Different Modules. We first diagnose how each component affects the performance, as shown in Table 6, simply add one positive sample q p boost the baseline MoCo v2 by 2.9%, and introducing mixed samples for pulling further improve the performance by another 1.2%. Following <ref type="bibr" target="#b0">[1]</ref>, we also adopt multi-crop augmentations, and the performance can be further improved by 3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of k in KNN Positive Sample Selection</head><p>We then inspect the influence when selecting different number of samples as positive candidates. <ref type="figure" target="#fig_2">Fig. 4(a)</ref> shows the top-1 accuracy with respect to different k in knn. It can be shown that when no mixed sample is included, the performance is relatively sensitive to k, and the results are relatively robust for a range of k (k = 1, 5, 10, 100) when mixed samples are introduced. The reason is that mixed samples act as a strong smoothing regularization, which alleviates the noise introduced by knn selection. We also consider a special case, i.e., always select the same sample, i.e.,, x p = x a , noted as k = 0. In this setting, we always select the correct positive sample for pulling, but sacrifice the diversity of using cross-samples. The performance gain is limited    <ref type="table" target="#tab_9">Table 7</ref> shows the linear classification accuracy of each stage, it can be shown that pulling similar samples in the shallow layers consistently increases its discrimination power, especially for stage 3, e.g., the accuracy increased by 3.3%, from 60.7% to 64.0%. The performance gain is relatively small in stage 2, partially because the representation in this stage is too low-level, and is hard for global representation. We also note that the performance of the last layers is not significantly affected by penalizing the shallow layer. However, We find that better separability in the shallow layers is beneficial for fine-tuning 1% labels, the accuracy increased by 1.4% when introducing shallow pulling. The advantages of increased representation of shallow layers can be also validated by the downstream detection and segmentation tasks, as shown in <ref type="table" target="#tab_6">Table 4</ref>. We also compare the linear classification accuracy of different methods at intermediate layers, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis and Discussions</head><p>Analysis of epoch&amp; batch size In our implementation, the batch size is measured based on the number of anchors, and the actual samples are doubled during each forward propagation when we select one positive sample for pulling. In order to get rid of the effects of batch size, we compare our methods with MoCo v2 that trained for double epochs.</p><p>The results are 68.5% and 70.4% for 100 and 200 epochs, which is better than MoCo v2 with accuracies of 67.5% and 69.5% for 200 and 400 epochs, respectively. Note that, Moco v2 achieves marginal improvement when extending the training epochs from 800 to 1600 (71.1% → 71.9%), while the proposed method achieves much better (74.4% for 800 epochs) performance. In addition, we double the batch size of MoCo v2 and train it for 200 epochs, the actual batch size of MoCo is same as our method in such setting. As shown in <ref type="figure" target="#fig_2">Fig. 4(c)</ref>, the performance gain of enlarging batch size is 1.4% ((67.5% → 68.9%)), which is lower than that of CsMl under same epochs (70.4%). The results verify that the improvement of CsMl is not simply caused by extending the training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Complexity</head><p>We compare the computational complexity with MoCo v2 under 200 epochs pretraining and different variants of CsMl under 100 epochs pretraining with 8 v100 GPUs. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>(b), when mixed samples are not involved, the cost time of MoCo v2 and CsMl is almost the same: 53h v.s. 57h. The increased 4 hours is mainly from the cost of nearest sample selection. When the mixed sample is included in the query, the computation cost increased by roughly 25%. Finally, we find that the computation cost of multi-level is marginal because the additional bottleneck's computation is very small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes a hierarchical training strategy that pulls semantically similar images for contrastive learning. The main contributions are two folds, first, in order to select similar samples without labels, we deliberately design a sample selection strategy relying on data mixing, which generates new samples that current model does not perform well. The highlight is that samples are only mixed from those similar samples and does not destroy the local similarity structure. Second, we extend the semantic alignment to intermediate hidden layers and enforces the feature representation to be discriminative throughout the network. In this way, the network can be optimized in a more robust way and we find it is beneficial for general representation. We conduct extensive experiments on widely used self-supervised benchmarks, and consistently outperforms previous self-supervised learning methods. A. Appendix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Results on BYOL</head><p>Our method is applicable to other contrastive based methods to further improve the performance. As shown in A.1, we applied CsMl to BYOL <ref type="bibr" target="#b9">[10]</ref>, which does not need negative samples during contrastive learning. Following the hyper-parameters used in <ref type="bibr" target="#b9">[10]</ref>, we pretrain the model with CsMl for 300 epochs. CsMl achieves an accuracy of 75.3% linear classification accuracy, which is even better than BYOL under 1000 epochs pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Results of more positive pairs</head><p>Based on Eq. (2), CsMl can also be extended to support pulling multiple positive samples during each forward propagation. As shown in <ref type="table" target="#tab_10">Table A</ref>.2, using 2 positive pairs achieves similar result as using 1 positive pairs. As comparison, introducing mixed samples as positive pairs is more effective. Considering that adding more positive samples would inevitably increasing the computational complexity, we simply choose one positive sample for each anchor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. MixUp v.s. CutMix augmentations</head><p>Besides CutMix, We also consider another cross-sample augmentation strategy Mixup <ref type="bibr">[ref]</ref> to expand the neighborhood of an anchor. As shown in <ref type="table" target="#tab_10">Table A</ref>.3, Mixup is worse than Cutmix, and even slightly worse than baseline method that do not involve any mixed sample in query, i.e. q a + q p setting, the reason may be that Mixup augmentations destroy the naturality of the pixel distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCo v2</head><p>CsMl Supervised </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Visualization of Feature Representation</head><p>We visualize the last embedding feature to better understand the semantic alignment properties of the proposed method. Specifically, we randomly choose 10 classes from the validation set and provide the t-sne visualization of feature representation, supervised training and MoCo v2. As shown in <ref type="figure" target="#fig_0">Fig. A.1</ref>, the same color denotes features with the same label. It can be shown that CsMl presents higher alignment property comparing with MoCo, and the fully supervised learned representation reveals the highest alignment due to the available of image labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Performance comparisons of CsMl and supervised model</head><p>Here we diagnose the performance difference of CsMl and fully supervised baseline to uncover the advantages of each model. The performance evaluation is under the linear classification protocol and we report per-category accuracy. As shown in <ref type="figure" target="#fig_1">Fig. A.3</ref>, we illustrate the most successful categories of each model, and find that the advantage of supervised model lies in discriminating fine-grained subcategories, e.g., on many sub-classes of dogs (Border collie, Siberian husky, bull mastiff etc.). It is intuitive since feature representation among fine-grained sub-categories is very similar, and it is hard to discriminate them without ground truth labels. While for unsupervised model CsMl, the most successful categories roughly around categories that require contour information for discrimination, e.g. shopping basket and plate rack, while supervised model usually focuses on discriminative details such as texture. Following the evaluation in <ref type="table" target="#tab_4">Table 2</ref> in the original paper, we also show some example images that use knn for similar samples selection, i.e., given an image in the validation set, and find its top-k nearest neighbors in the training set. The most successful categories of CsMl results from capturing the global contour information. CsMl Supervised <ref type="figure">Figure A.</ref>3. An illustration of the selected knn samples using CsMl and the supervised model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GXPEEHOO</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An overall performance comparison of our proposed pretrained features on several widely evaluated unsupervised benchmarks. Here, a standard ResNet-50 is used as backbone. CsMl significantly improves the performance on various tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Top-1 linear classification accuracy of different methods at different stages using ResNet-50 as backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>(a) Top-1 accuracy with different k in knn. (b) Top-1 accuracy and computational complexity comparisons for different models. (c) Top-1 accuracy comparisons for different training epochs of CsMl and MoCo v2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure A. 1 .</head><label>1</label><figDesc>t-sne visualization of representation learned by MoCo v2, CsMl, and fully supervised learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Top-1 accuracy under linear classification on ImageNet with ResNet-50 as backbone.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>KNN classification accuracy on ImageNet. We report top-1 accuracy with 20 and 200 nearest neighbors, respectively.</figDesc><table><row><cell>Method</cell><cell>20-NN</cell><cell>200-NN</cell></row><row><cell>Supervised</cell><cell>75.0</cell><cell>73.2</cell></row><row><cell>NPID [31]</cell><cell>-</cell><cell>46.5</cell></row><row><cell>LA[37]</cell><cell>-</cell><cell>49.4</cell></row><row><cell>PCL[17]</cell><cell>54.5</cell><cell>-</cell></row><row><cell>MoCo v2[4]</cell><cell>62.0</cell><cell>59.0</cell></row><row><cell>SwAV[1]</cell><cell>65.7</cell><cell>62.7</cell></row><row><cell>CsMl</cell><cell>72.4</cell><cell>70.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Transfer learning accuracy (%) on COCO detection and COCO instance segmentation (averaged over 5 trials). Method Mask R-CNN, R50-FPN, Detection Mask R-CNN, R50-FPN, Segmentation AP bb AP bb 50 AP bb 75 AP S AP M AP L AP mk AP mk 50 AP mk 75 AP S AP M AP L</figDesc><table><row><cell>Supervised</cell><cell></cell><cell>38.9</cell><cell>59.6</cell><cell cols="4">42.0 23.0 42.9 49.9</cell><cell>35.4</cell><cell>56.5</cell><cell cols="4">38.1 17.5 38.2 51.3</cell></row><row><cell>MoCo v2 [4]</cell><cell></cell><cell>39.2</cell><cell>59.9</cell><cell cols="4">42.7 23.8 42.7 50.0</cell><cell>35.7</cell><cell>56.8</cell><cell cols="4">38.1 17.8 38.1 50.5</cell></row><row><cell>BYOL [10]</cell><cell></cell><cell>39.9</cell><cell>60.2</cell><cell cols="4">43.2 23.3 43.2 52.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">DenseCL[30]</cell><cell>40.3</cell><cell>59.9</cell><cell>44.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.4</cell><cell>57.0</cell><cell>39.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">CsMl(w/o multi-level) 39.5</cell><cell>60.1</cell><cell cols="4">43.1 24.1 42.8 50.9</cell><cell>35.9</cell><cell>56.9</cell><cell cols="4">38.6 18.3 38.1 51.4</cell></row><row><cell cols="2">CsMl(w/ multi-level)</cell><cell>40.3</cell><cell>61.1</cell><cell cols="4">43.8 25.0 43.6 50.8</cell><cell>36.6</cell><cell>58.1</cell><cell cols="4">39.1 18.8 39.0 51.2</cell></row><row><cell cols="6">Table 5. Transfer Learning results on PASCAL VOC detection (av-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">eraged over 5 trials).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Methods</cell><cell>AP50</cell><cell></cell><cell>AP75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Supervised</cell><cell>81.4</cell><cell></cell><cell>58.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MoCo v2 [4]</cell><cell>82.5</cell><cell></cell><cell>64.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CsMl</cell><cell>82.7</cell><cell></cell><cell>64.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6XSHUYLVHG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>&amp;V0,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>%&lt;2/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6Z$9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7RSDFF</cell><cell>0R&amp;RY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6WDJH</cell><cell></cell><cell>6WDJH</cell><cell></cell><cell>6WDJH</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Results of adding different modules.</figDesc><table><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>MoCo v2</cell><cell>67.5</cell></row><row><cell>q a + q p</cell><cell>70.4</cell></row><row><cell>q a + q p +q</cell><cell>71.6</cell></row><row><cell>q a + q p +q + multi-crop</cell><cell>74.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Classification accuracy with features of different stages and fine-tuning results with 1% labels.</figDesc><table><row><cell></cell><cell cols="4">Stage2 Stage3 Stage4 1% label</cell></row><row><cell>MoCo v2 [4]</cell><cell>44.5</cell><cell>60.2</cell><cell>71.1</cell><cell>52.4</cell></row><row><cell>Pull stage4</cell><cell>45.1</cell><cell>60.7</cell><cell>71.6</cell><cell>53.4</cell></row><row><cell>Pull stage3&amp;4</cell><cell>45.6</cell><cell>64.0</cell><cell>71.5</cell><cell>54.5</cell></row><row><cell>Pull stage2&amp;3&amp;4</cell><cell>46.0</cell><cell>64.0</cell><cell>71.6</cell><cell>54.8</cell></row><row><cell cols="5">(67.5% → 68.6%), which validate the effectiveness of se-</cell></row><row><cell cols="3">lecting positives via cross samples.</cell><cell></cell><cell></cell></row><row><cell cols="5">Effects of Pulling Multi-level Views. We analyze the</cell></row><row><cell cols="5">performance of the intermediate layers and validate how our</cell></row><row><cell cols="5">proposed hierarchical alignment strategy benefits the repre-</cell></row><row><cell cols="5">sentation. Except for the last layers of stage 4, we also ex-</cell></row><row><cell cols="5">plicitly pull similar samples in earlier layers such as stage 2</cell></row><row><cell>and stage 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A .</head><label>A</label><figDesc>1. Top-1 accuracy comparisons based on BYOL</figDesc><table><row><cell>Method</cell><cell cols="2">Accuracy(%)</cell></row><row><cell>BYOL 300 epochs</cell><cell cols="2">72.5</cell></row><row><cell>BYOL 1000 epochs</cell><cell cols="2">74.3</cell></row><row><cell>CsMl 300 epochs</cell><cell cols="2">75.3</cell></row><row><cell cols="3">Table A.2. Top-1 accuracy comparisons with more positive pairs</cell></row><row><cell>Method</cell><cell cols="2">Accuracy(%)</cell></row><row><cell>q a + q p</cell><cell>70.4</cell></row><row><cell>q a + q p1 + q p2</cell><cell>70.2</cell></row><row><cell>q a + q p +q</cell><cell>71.6</cell></row><row><cell cols="3">Table A.3. Comparisons of Mixup with CutMix augmentations.</cell></row><row><cell>Method</cell><cell cols="2">Accuracy(%)</cell></row><row><cell>MoCo v2</cell><cell></cell><cell>67.5</cell></row><row><cell>q a + q p</cell><cell></cell><cell>70.4</cell></row><row><cell cols="2">q a + q p + MixUp(q a , q p )</cell><cell>70.1</cell></row><row><cell cols="2">q a + q p + CutMix(q a , q p )</cell><cell>71.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Figure A.2. Top-1 accuracy comparisons on different categories of CsMl and fully supervised model. Here we compare most successful categories of each method.</figDesc><table><row><cell>8QVXSHUYLVHG6XSHUYLVHG</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">FDQRSHQHU VKRSSLQJEDVNHW VSRWOLJKW FULE SODWHUDFN ZRN PLWWHQ FUDGOH WKLPEOH KDPPHU UXOHU SHUIXPH FXUO\FRDWHGUHWULHYHU R[ KRZOHUPRQNH\ VKRYHO ODGOH ERRNFDVH QLJKWVQDNH OXPEHUPLOO ZHHYLO EREVOHG DUPDGLOOR FRPSXWHUNH\ERDUG $PHULFDQHJUHW EROHWH FRQIHFWLRQHU\ GUDJRQIO\ ILUHHQJLQH MLJVDZSX]]OH WULSRG FRUQ JRQGROD FUDQH RUDQJXWDQ 0H[LFDQKDLUOHVV HQWHUWDLQPHQWFHQWHU ZLQGRZVFUHHQ JDVPDVN WDQN %HUQHVHPRXQWDLQGRJ EHDYHU GULOOLQJSODWIRUP FKDLQPDLO PRUWDU ULQJOHW UHGZLQH SROLFHYDQ $PHULFDQVWDIIRUGVKLUHWHUULHU EXOOPDVWLII KHDGFDEEDJH VXQJODVVHV 6LEHULDQKXVN\ JRRVH %RUGHUFROOLH FRLO IODWZRUP QLSSOH *UDQQ\VPLWK SURMHFWLOHPLVVLOH PXGWXUWOH</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CsMl</cell></row><row><cell cols="5">shopping basket shopping basket shopping basket shopping basket shopping basket</cell></row><row><cell>shopping basket</cell><cell></cell><cell></cell><cell></cell><cell>Supervised</cell></row><row><cell>shower curtain</cell><cell>wardrobe</cell><cell>mailbag</cell><cell>wallet</cell><cell>pencil box</cell></row><row><cell>crib</cell><cell>crib</cell><cell>crib</cell><cell>crib</cell><cell>cradle</cell></row><row><cell>crib</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>artichoke</cell><cell>crib</cell><cell>artichoke</cell><cell>artichoke</cell><cell>artichoke</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our method is also applicable for other contrastive based method, see the appendix for the results of BYOL<ref type="bibr" target="#b9">[10]</ref> </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10542" to="10552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rethinking image mixture for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05438</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<pubPlace>Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10242</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09157</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised data augmentation for consistency training</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
