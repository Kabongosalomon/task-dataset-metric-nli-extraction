<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING DEEP AND COMPACT MODELS FOR GESTURE RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koustav</forename><surname>Mullick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Information Technology (CVIT)</orgName>
								<orgName type="department" key="dep2">International Institute of Information Technology</orgName>
								<address>
									<settlement>Hyderabad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><forename type="middle">M</forename><surname>Namboodiri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Information Technology (CVIT)</orgName>
								<orgName type="department" key="dep2">International Institute of Information Technology</orgName>
								<address>
									<settlement>Hyderabad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING DEEP AND COMPACT MODELS FOR GESTURE RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We look at the problem of developing a compact and accurate model for gesture recognition from videos in a deep-learning framework. Towards this we propose a joint 3DCNN-LSTM model that is end-to-end trainable and is shown to be better suited to capture the dynamic information in actions. The solution achieves close to state-of-the-art accuracy on the ChaLearn dataset, with only half the model size. We also explore ways to derive a much more compact representation in a knowledge distillation framework followed by model compression. The final model is less than 1 M B in size, which is less than one hundredth of our initial model, with a drop of 7% in accuracy, and is suitable for real-time gesture recognition on mobile devices.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION AND RELATED WORK</head><p>Gesture recognition is one of the key components in natural human-computer interfaces, especially for mobile devices, where interfaces like keyboard and mouse are impractical. However, the problem is challenging due to several factors like changes in background, lighting and variations in the performance and speed of the gestures. The users may have different appearance, pose and positioning relative to the camera as well as differences in the way they perform any specific gesture. In this work, we develop a solution that is accurate and robust enough to handle these variations, while being compact and fast to suit resource limited devices.</p><p>Deep Learning (DL) has had a major impact in the fields of computer vision, natural language processing and speech recognition over the past few years. However, one of the major issues with DL models are the large number of parameters involved which makes training time and memory requirements quite high. This makes it difficult to employ them in embedded devices. Keeping this in mind, we would like to explore ways to reduce the parameter space of such models without compromising a lot on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Action Recognition in Videos</head><p>Human action in video sequences consists of spatial information within individual frames as well as temporal information of motion across the frames. CNN models are very good at capturing the spatial information within an image as demonstrated by their success on ImageNet classification. Previous works have tried to follow two approaches in integrating temporal information into these frameworks. The first one is to extract features from videos that capture temporal information into images and then use image classification models. These include ideas such as dynamic image <ref type="bibr" target="#b0">[1]</ref> that produces a single image from a video obtained by applying rank pooling on the raw image pixels of a video, or the use of optical flow images <ref type="bibr" target="#b1">[2]</ref>.</p><p>A second approach is the use of deep-learning models that are better suited for capturing temporal information such as 3D-Convolutional Neural Network (3D-CNN) <ref type="bibr" target="#b2">[3]</ref> proposed by Ji et al., as well as Long Short Term Memory (LSTM) recurrent networks <ref type="bibr" target="#b3">[4]</ref>, which can use either pre-computed image features such as Bag of words and SIFT <ref type="bibr" target="#b4">[5]</ref>, regular CNN features including optical-flow <ref type="bibr" target="#b5">[6]</ref> or 3D-CNN features <ref type="bibr" target="#b6">[7]</ref>. Tran et al. <ref type="bibr" target="#b7">[8]</ref> proposed a simple, yet effective approach for spatio-temporal feature learning using deep 3D-CNN trained in a supervised fashion on a large scale video dataset. <ref type="bibr" target="#b5">[6]</ref> performed activity recognition by passing RGB and optical flow frames through pre-trained CNN networks and fed the learned features to an LSTM for final recognition. Varol et al. <ref type="bibr" target="#b8">[9]</ref> learned video representation using 3D-CNN and demonstrated the impact of different low-level features as input.</p><p>Most of the works discussed above either use pre-trained CNN models for feature extraction or trains the CNN on parts of videos, separately from the model that performs actual classification (like LSTM or SVM). We would like to develop an end-to-end trainable model that can automatically learn both spatial and temporal features in a complete gesture. Towards this we combine the strengths of 3D-CNNs that are good at capturing spatial and short-term temporal features and LSTMs that can capture long-term temporal signatures of actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Knowledge Distillation</head><p>The second aspect that we explore in this work is to develop models that are efficient in terms of memory and speed. Knowledge distillation, originally introduced by Caruana et al. <ref type="bibr" target="#b9">[10]</ref>, introduced a process of transferring knowledge from arXiv:1712.10136v1 [cs.CV] 29 Dec 2017 one model to another. In order to obtain a faster inference, Hinton et al. <ref type="bibr" target="#b10">[11]</ref> proposed a compression framework which trains a student network, from the softened output of a larger model or an ensemble of wider networks, called teacher network. The idea is to allow the student network to capture not only the information provided by the true labels, but also the finer structure learned by the pre-trained teacher network. This approach has been used for speech recognition <ref type="bibr" target="#b11">[12]</ref>, image classification <ref type="bibr" target="#b12">[13]</ref>, and network compression <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this work we show that the use of 3D-CNN with LSTM can improve the accuracy of action recognition in videos without increasing the model size, while the use of knowledge distillation allows us to develop a compact model that can be further compressed with minimal impact on accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OUR APPROACH</head><p>We describe our framework in three stages. As baseline models we use a 3D-CNN and an LSTM variant of RNN to classify each gesture. For 3D-CNN each gesture video needs to be represented using a fixed number of frames. For LSTM the raw frames are flattened and passed through a linear layer to obtain features that are then fed to the LSTM.</p><p>Next we present a framework that combines the 3D-CNN with LSTM. The 3D-CNN acts as an encoder at sub-gesture level and provides representations for groups of few frames. These are then fed as sequences to the LSTM to get the final prediction. This can be thought of as learning a two-step hierarchy, where gestures are made up of sub-gestures. The 3D-CNN learns representations at sub-gesture level followed by the LSTM which makes prediction by building on the subgesture features. As many gestures have considerable overlap in their sub-gestures (raising or lowering of hand), having a two-level learning of temporal evolution of the gesture helps in better modeling the gesture. Also, all gestures will not span across the same number of frames. This is a limitation while using a CNN for classification as it requires a fixed length input. A recurrent network allows us to process arbitrary length videos.</p><p>Finally, we use our trained baseline CNN as a teacher and use its softmax output to train much smaller variants of our joint CNN and LSTM models. The benefits of this approach is demonstrated in the experiments section. In the subsequent sub-sections we detail out the architectures used in each of the aforementioned approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Baseline 3D-CNN</head><p>Each gesture is represented using 32 consecutive frames, each of dimension 64 × 64 and two channels: depth and grayscale. For videos longer than 32 frames, the central 32 frames are extracted, while shorter videos are zero-padded. The model consists of six convolution layers, followed by two fully-connected layers (FCs). Each layer is followed by ReLU activation. Convolution layers are additionally followed by batch normalization. We do not use any pooling layer and sub-sampling is performed using padded convolution by varying the strides. For down-sampling by a factor of two, convolution is performed with filter size 4, stride 2 and padding 1 on each side. Whereas, to keep the size constant during convolution we use filter size 3, with stride 1 and padding 1 on each side. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Baseline LSTM</head><p>Each video is divided into chunks of 4 frames of dimension 64 × 64, for grayscale and depth channels respectively. After flattening, it is passed through a 512 dimensional linear layer, followed by ReLU activation. Sequences of 512 dimensional feature vectors are fed into the LSTM, containing 256 units, for final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Joint 3D-CNN and LSTM</head><p>The joint model consists of an encoder 3D-CNN and an LSTM for sequence classification. The encoder takes blocks of 4 × 64 × 64 frames of two channels (depth and grayscale) as input and produces a feature vector of it. The LSTM takes those vectors sequentially for an entire gesture and gives the final prediction. Architecture of the 3D-CNN is similar to the one used in our baseline model, except the number of frames in each block (4 versus 32), and the LSTM consists of 256 units. The complete model is shown in <ref type="figure" target="#fig_1">Figure 2</ref>, which is trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Knowledge Distillation from Baseline 3D-CNN Model to Joined Model</head><p>We train our baseline 3D-CNN model till convergence and obtain the softmax output for each training sample. As suggested in <ref type="bibr" target="#b13">[14]</ref>, for each gesture we obtain softened version of softmax using: where c is the number of classes and T is the temperature, set depending on how "soft" we want the distribution to be. We train smaller variants (referred to as medium and small) of our joint model obtained by reducing the number of feature maps (channels) in each layer by two and four times, respectively. The aim is to minimize the following loss function:</p><formula xml:id="formula_0">P i = e Z i T c j=1 e Z j T , ∀i ∈ {1, ...c},<label>(1)</label></formula><formula xml:id="formula_1">L = αL (sof t) + (1 − α)L (hard) ,<label>(2)</label></formula><p>where L (sof t) is the cross-entropy loss between pre-trained teacher's and student's softened softmax output, L (hard) is the cross-entropy loss between the actual class label and model output, and α is a weighting parameter (set as 0.5 in our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The Chalearn 2014 Looking at People Challenge (track 3) <ref type="bibr" target="#b14">[15]</ref> dataset is a vocabulary of 20 different Italian cultural/ anthropological signs, performed by 27 different users with variations in surroundings, clothing, lighting and gesture movement. The dataset consists of 7,754 gestures for training, 3,362 gestures for validation and 2,742 gestures for testing. The videos are recorded using Microsoft Kinect and consist of RGB, depth, user mask and skeleton/joint information for each frame of video as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Each video may contain multiple gestures, performed one after another. The training and validation data consists of soft segmented annotation for the gestures in the videos, but no such annotation is available for the testing data. Since the main aim of this work is gesture classification, not segmentation, we use the validation data as our testing data and report  accuracies on the same here-on. To maintain parity, we compare against the accuracies obtained on the validation data by other methods also. Further, we randomly choose 2,000 videos from training set to act as our validation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Input</head><p>For each video frame we use the depth and convert the RGB frames to grayscale and concatenate them to obtain twochannel inputs for our models. Since all the gestures occur in the upper-body region, we extract it using the skeleton information and resize it to 64 × 64. Thus inputs to our models are of dimension 2 × T × 64 × 64 , where T is 32 for the baseline 3D-CNN model and 4 for the joint model (as mentioned in section 2.1 and 2.3 respectively). Similar to <ref type="bibr" target="#b16">[17]</ref>, using skeleton information, we crop out the highest hand region for each gesture, resize to 64 × 64 and use them as input. <ref type="table" target="#tab_0">Table 1</ref> shows accuracies obtained with our baseline 3D-CNN model while using hand, upperbody and both combined, as input. Since the combination of upper-body and hand gives the best performance, all following experiments use that as input ( <ref type="figure">Figure 4</ref>). We also perform rotation, translation and zooming on the frames for data augmentation. <ref type="figure">Fig. 4</ref>. Left and right images are example grayscale and depth frames from the dataset. Center rows show the upper-body and hand input frames for our models obtained from it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method/Model</head><p>Accuracy(%) Baseline LSTM 86.6 Baseline 3D-CNN 90.1 3D-CNN + LSTM (ours) 93.2 Wu et al. <ref type="bibr" target="#b15">[16]</ref> 87.9 Pigou et al. <ref type="bibr" target="#b16">[17]</ref> 91.4 Neverova et al. <ref type="bibr" target="#b17">[18]</ref> 96.8 <ref type="table">Table 2</ref>. Accuracies obtained using our model compared with state-of-the-art methods. <ref type="table">Table 2</ref> shows the accuracies obtained using our models. The baseline LSTM performs the worst since the spatial and temporal information are not conserved while flattening out. Baseline 3D-CNN preserves such information across dimensions and performs better, but it is not suitable for capturing long-term dependencies. The best accuracy is obtained using the joint model which takes advantage of both 3D-CNN, for feature learning, and LSTM, for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results and Analysis</head><p>We compare our CNN + LSTM model against other stateof-the-art methods on the same dataset in <ref type="table">Table 2</ref>. Wu et al. <ref type="bibr" target="#b15">[16]</ref> described a novel method called Deep Dynamic Neural Networks, a semi-supervised hierarchical dynamic framework based on a Hidden Markov Model (HMM) and learned high-level spatio-temporal representations using a Deep Belief Network (DBN) and 3D-CNN, suited to the input modality. Pigou et al. <ref type="bibr" target="#b16">[17]</ref> used an approach similar to our baseline 3D-CNN approach, but their architecture is very different from our CNN. ModDrop <ref type="bibr" target="#b17">[18]</ref> used a multi-scale and multimodal deep learning approach. It performed careful initialization of individual modalities and fused multiple modalities at several spatial and temporal scales. Their architecture is a big cascade having individual branches for each hand, each modalities, and articulated pose based features. Our simpler architecture works directly with the raw pixels, without the need to perform any pre-training or initializations. <ref type="table" target="#tab_1">Table 3</ref>. Reducing the number of parameters limits the model's capacity to learn from the class labels directly, but it can be alleviated when trained according to Equation 2, using soft outputs from a larger pre-trained teacher network. We get almost at-par accuracy with the teacher, but at a much reduced training cost, making it both computationally and memory efficient. Further, training with Adam optimizer makes most of the parameters of the student have very low weights, which allows for further compression to obtain a sparse model. Removing weights having magnitude below 2 −100 got rid of ∼ 905K parameters out of 1.15M , of our small student network with no drop in performance. <ref type="table">Table 4</ref> compares this sparse model with our other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The effect of Knowledge Distillation is analyzed in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We show how joint 3D-CNN and LSTM model for gesture recognition from videos, leverages the best of both 3D convolution and recurrent network to model the sequential evolution of information in a video, while allowing to process arbitrary length videos.</p><p>We also show how information can be distilled from a larger model to models with 16× and 4× fewer parameters. Size of models could be further reduced using a sparse representation as discussed above. This not only benefits training time but also makes it possible to use them in low-memory and low-power embedded devices.  <ref type="table">Table 4</ref>. Reduction is size along with performance impact of the student model and sparse model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Baseline 3D-CNN architecture. Each block shows dimensions in the format (channels × number of frames × height × width). Green: Conv layers, Purple: Flattening, Blue: FC layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Joined 3D-CNN and LSTM architecture (best viewed in color). Each block shows dimensions in the format (channels × number of frames × height × width). Green: Conv layers, Purple: Flattening, Red: LSTM connection at each time-step, Blue: FC layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Example frame modalities from the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Baseline 3D-CNN on different input modalities.</figDesc><table><row><cell cols="2">Input mode Accuracy(%)</cell></row><row><cell>Hand</cell><cell>87.33</cell></row><row><cell>Upper-body</cell><cell>87.59</cell></row><row><cell>Combined</cell><cell>90.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Knowledge Distillation from baseline 3D-CNN to CNN + LSTM.</figDesc><table><row><cell></cell><cell></cell><cell cols="2"># of parameters</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell cols="2">(in millions)</cell><cell>Trained using</cell><cell></cell><cell>Accuracy(%)</cell></row><row><cell cols="2">Original 3D-CNN + LSTM</cell><cell>18.37</cell><cell></cell><cell>class labels</cell><cell></cell><cell>93.18</cell></row><row><cell>Teacher</cell><cell>3D-CNN</cell><cell>18.82</cell><cell></cell><cell>class labels</cell><cell></cell><cell>90.13</cell></row><row><cell>Student</cell><cell>3D-CNN + LSTM (medium) 3D-CNN + LSTM (small)</cell><cell>4.59 1.15</cell><cell cols="3">class labels class labels and softmax output of teacher class labels class labels and softmax output of teacher</cell><cell>86.18 88.35 81.50 86.05</cell></row><row><cell>Method</cell><cell cols="2"># of parameters (in millions)</cell><cell cols="4">Single-precision Model size (MB) Accuracy(%) Model size (MB) Accuracy(%) Half-precision</cell></row><row><cell cols="2">1. Teacher 3D-CNN</cell><cell>18.82</cell><cell>72</cell><cell>90.13</cell><cell>36</cell><cell>89.5</cell></row><row><cell cols="2">2. Original 3D-CNN + LSTM</cell><cell>18.37</cell><cell>71</cell><cell>93.18</cell><cell>35.5</cell><cell>93.18</cell></row><row><cell cols="2">3. Student 3D-CNN + LSTM</cell><cell>1.15</cell><cell>4.5</cell><cell>86.05</cell><cell>2.25</cell><cell>85.98</cell></row><row><cell cols="2">4. Sparse model of (3)</cell><cell>0.25</cell><cell>1.12</cell><cell>86.05</cell><cell>0.635</cell><cell>85.98</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic Image Networks for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two-Stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D Convolutional Neural Networks for Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action Classification in Soccer Videos with Long Short-term Memory Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atilla</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond Short Snippets: Deep Networks for Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Convolutional Sparse AutoEncoder for Sequence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atilla</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Longterm Temporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04494</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems, Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Transferring Knowledge from a RNN to a DNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FitNets: Hints for Thin Deep Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://www.ttic.edu/dl/dark14.pdf" />
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
	<note>Dark knowledge. Online; accessed 1</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ChaLearn Looking at People Challenge 2014: Dataset and Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Gonzàlez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meysam</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Víctor</forename><surname>Ponce-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam Do-Hoang</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni</forename><surname>Dambre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sign Language Recognition Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ModDrop: Adaptive Multi-modal Gesture Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
