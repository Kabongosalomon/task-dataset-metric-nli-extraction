<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
							<email>zhaoqijie@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Sheng</surname></persName>
							<email>shengtao@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tang</surname></persName>
							<email>tangzhi@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AI Labs</orgName>
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Cai</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AI Labs</orgName>
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
							<email>hbling@temple.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Computer and Information Sciences Department</orgName>
								<orgName type="institution">Temple University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature pyramids are widely exploited by both the state-ofthe-art one-stage object detectors (e.g., DSSD, RetinaNet, RefineDet) and the two-stage object detectors (e.g., Mask R-CNN, DetNet) to alleviate the problem arising from scale variation across object instances. Although these object detectors with feature pyramids achieve encouraging results, they have some limitations due to that they only simply construct the feature pyramid according to the inherent multiscale, pyramidal architecture of the backbones which are originally designed for object classification task. Newly, in this work, we present Multi-Level Feature Pyramid Network (MLFPN) to construct more effective feature pyramids for detecting objects of different scales. First, we fuse multi-level features (i.e. multiple layers) extracted by backbone as the base feature. Second, we feed the base feature into a block of alternating joint Thinned U-shape Modules and Feature Fusion Modules and exploit the decoder layers of each Ushape module as the features for detecting objects. Finally, we gather up the decoder layers with equivalent scales (sizes) to construct a feature pyramid for object detection, in which every feature map consists of the layers (features) from multiple levels. To evaluate the effectiveness of the proposed MLFPN, we design and train a powerful end-to-end onestage object detector we call M2Det by integrating it into the architecture of SSD, and achieve better detection performance than state-of-the-art one-stage detectors. Specifically, on MS-COCO benchmark, M2Det achieves AP of 41.0 at speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with multi-scale inference strategy, which are the new state-of-the-art results among one-stage detectors. The code will be made available on https://github.com/ qijiezhao/M2Det.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Scale variation across object instances is one of the major challenges for the object detection task <ref type="bibr" target="#b10">(Lin et al. 2017a;</ref><ref type="bibr" target="#b16">Singh and Davis 2018)</ref>, and usually there are two strategies to solve the problem arising from this challenge. The first one is to detect objects in an image pyramid (i.e. a series of resized copies of the input image) (Singh and Davis 2018), which can only be exploited at the testing time. Obviously, this solution will greatly increase memory and computational complexity, thus the efficiency of such object detectors drop dramatically. The second one is to detect objects in a feature pyramid extracted from the input image <ref type="bibr" target="#b13">(Liu et al. 2016;</ref><ref type="bibr" target="#b10">Lin et al. 2017a</ref>), which can be exploited at both training and testing phases. Compared with the first solution that uses an image pyramid, it has less memory and computational cost. Moreover, the feature pyramid constructing module can be easily integrated into the state-ofthe-art deep neural networks based detectors, yielding an end-to-end solution.</p><p>Although the object detectors with feature pyramids <ref type="bibr" target="#b13">(Liu et al. 2016;</ref><ref type="bibr" target="#b10">Lin et al. 2017a;</ref><ref type="bibr" target="#b11">Lin et al. 2017b;</ref> achieve encouraging results, they still have some limitations due to that they simply construct the feature pyramid according to the inherent multi-scale, pyramidal architecture of the backbones which are actually designed for object classification task. For example, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, SSD <ref type="bibr" target="#b13">(Liu et al. 2016)</ref> directly and independently uses two layers of the backbone (i.e. VGG16) and four extra layers obtained by stride 2 convolution to construct the feature pyramid; STDN <ref type="bibr" target="#b18">(Zhou et al. 2018)</ref> only uses the last dense block of DenseNet <ref type="bibr" target="#b9">(Huang et al. 2017)</ref> to construct feature pyramid by pooling and scale-transfer operations; FPN <ref type="bibr" target="#b10">(Lin et al. 2017a</ref>) constructs the feature pyramid by fusing the deep and shallow layers in a top-down manner. Generally speaking, the above-mentioned methods have the two following limitations. First, feature maps in the pyramid are not representa-tive enough for the object detection task, since they are simply constructed from the layers (features) of the backbone designed for object classification task. Second, each feature map in the pyramid (used for detecting objects in a specific range of size) is mainly or even solely constructed from single-level layers of the backbone, that is, it mainly or only contains single-level information. In general, high-level features in the deeper layers are more discriminative for classification subtask while low-level features in the shallower layers can be helpful for object location regression sub-task. Moreover, low-level features are more suitable to characterize objects with simple appearances while high-level features are appropriate for objects with complex appearances. In practice, the appearances of the object instances with similar size can be quite different. For example, a traffic light and a faraway person may have comparable size, and the appearance of the person is much more complex. Hence, each feature map (used for detecting objects in a specific range of size) in the pyramid mainly or only consists of single-level features will result in suboptimal detection performance.</p><p>The goal of this paper is to construct a more effective feature pyramid for detecting objects of different scales, while avoid the limitations of the existing methods as above mentioned. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, to achieve this goal, we first fuse multi-level features (i.e. multiple layers) extracted by backbone as base feature, and then feed it into a block of alternating joint Thinned U-shape Modules(TUM) and Feature Fusion Modules(FFM) to extract more representative, multilevel multi-scale features. It is worth noting that, decoder layers in each U-shape Module share a similar depth. Finally, we gather up the feature maps with equivalent scales to construct the final feature pyramid for object detection. Obviously, decoder layers that form the final feature pyramid are much deeper than the layers in the backbone, namely, they are more representative. Moreover, each feature map in the final feature pyramid consists of the decoder layers from multiple levels. Hence, we call our feature pyramid block Multi-Level Feature Pyramid Network (MLFPN).</p><p>To evaluate the effectiveness of the proposed MLFPN, we design and train a powerful end-to-end one-stage object detector we call M2Det (according to that it is built upon multilevel and multi-scale features) by integrating MLFPN into the architecture of SSD <ref type="bibr" target="#b13">(Liu et al. 2016)</ref>. M2Det achieves the new state-of-the-art result (i.e. AP of 41.0 at speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with multi-scale inference strategy), outperforming the onestage detectors on MS-COCO  benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Researchers have put plenty of efforts into improving the detection accuracy of objects with various scales -no matter what kind of detector it is, either an one-stage detector or a two-stage one. To the best of our knowledge, there are mainly two strategies to tackle this scale-variation problem.</p><p>The first one is featurizing image pyramids (i.e. a series of resized copies of the input image) to produce semantically representative multi-scale features. Features from images of different scales yield predictions separately and these predictions work together to give the final prediction.</p><p>In terms of recognition accuracy and localization precision, features from various-sized images do surpass features that are based merely on single-scale images. Methods such as <ref type="bibr" target="#b16">(Shrivastava et al. 2016</ref>) and SNIP (Singh and Davis 2018) employed this tactic. Despite the performance gain, such a strategy could be costly time-wise and memory-wise, which forbid its application in real-time tasks. Considering this major drawback, methods such as SNIP (Singh and Davis 2018) can choose to only employ featurized image pyramids during the test phase as a fallback, whereas other methods including Fast R-CNN (Girshick 2015) and Faster R-CNN  chose not to use this strategy by default.</p><p>The second one is detecting objects in the feature pyramid extracted from inherent layers within the network while merely taking a single-scale image. This strategy demands significantly less additional memory and computational cost than the first one, enabling deployment during both the training and test phases in real-time networks. Moreover, the feature pyramid constructing module can be easily revised and fit into state-of-the-art deep neural networks based detectors. MS-CNN <ref type="bibr" target="#b2">(Cai et al. 2016)</ref>, SSD <ref type="bibr" target="#b13">(Liu et al. 2016)</ref>, DSSD <ref type="bibr" target="#b4">(Fu et al. 2017)</ref>, FPN <ref type="bibr" target="#b10">(Lin et al. 2017a</ref>), YOLOv3 (Redmon and Farhadi 2018), RetinaNet <ref type="bibr" target="#b11">(Lin et al. 2017b)</ref>, and RefineDet <ref type="bibr" target="#b17">(Zhang et al. 2018)</ref> adopted this tactic in different ways.</p><p>To the best of our knowledge, MS-CNN <ref type="bibr" target="#b2">(Cai et al. 2016</ref>) proposed two sub-networks and first incorporated multiscale features into deep convolutional neural networks for object detection. The proposal sub-net exploited feature maps of several resolutions to detect multi-scale objects in an image. SSD <ref type="bibr" target="#b13">(Liu et al. 2016</ref>) exploited feature maps from the later layers of VGG16 base-net and extra feature layers for predictions at multiple scales. FPN <ref type="bibr" target="#b10">(Lin et al. 2017a)</ref> utilized lateral connections and a top-down pathway to produce a feature pyramid and achieved more powerful representations. DSSD <ref type="bibr" target="#b4">(Fu et al. 2017)</ref> implemented deconvolution layers for aggregating context and enhancing the highlevel semantics for shallow features. RefineDet <ref type="bibr" target="#b17">(Zhang et al. 2018</ref>) adopted two-step cascade regression, which achieves a remarkable progress on accuracy while keeping the efficiency of SSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>The overall architecture of M2Det is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. M2Det uses the backbone and the Multi-Level Feature Pyramid Network (MLFPN) to extract features from the input image, and then similar to SSD, produces dense bounding boxes and category scores based on the learned features, followed by the non-maximum suppression (NMS) operation to produce the final results. MLFPN consists of three modules, i.e. Feature Fusion Module (FFM), Thinned U-shape Module (TUM) and Scale-wise Feature Aggregation Module (SFAM). FFMv1 enriches semantic information into base features by fusing feature maps of the backbone. Each TUM generates a group of multi-scale features, and then the alternating joint TUMs and FFMv2s extract multi-level multiscale features. In addition, SFAM aggregates the features into the multi-level feature pyramid through a scale-wise feature concatenation operation and an adaptive attention mechanism. More details about the three core modules and    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-level Feature Pyramid Network</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, MLFPN contains three parts. Firstly, FFMv1 fuses shallow and deep features to produce the base feature, e.g., conv4 3 and conv5 3 of VGG (Simonyan and Zisserman 2015), which provide multi-level semantic information for MLFPN. Secondly, several TUMs and FFMv2 are stacked alternately. Specifically, each TUM generates several feature maps with different scales. The FFMv2 fuses the base feature and the largest output feature map of the previous TUM. And the fused feature maps are fed to the next TUM. Note that the first TUM has no prior knowledge of any other TUMs, so it only learns from X base . The output multi-level multi-scale features are calculated as:</p><formula xml:id="formula_0">[x l 1 , x l 2 , ..., x l i ] = T l (X base ), l = 1 T l (F(X base , x l−1 i )), l = 2...L ,<label>(1)</label></formula><p>where X base denotes the base feature, x l i denotes the feature with the i-th scale in the l-th TUM, L denotes the number of TUMs, T l denotes the l-th TUM processing, and F denotes FFMv1 processing. Thirdly, SFAM aggregates the multi-level multi-scale features by a scale-wise feature concatenation operation and a channel-wise attention mechanism.</p><p>FFMs FFMs fuse features from different levels in M2Det, which are crucial to constructing the final multi-level feature pyramid. They use 1x1 convolution layers to compress the channels of the input features and use concatenation operation to aggregate these feature maps. Especially, since FFMv1 takes two feature maps with different scales in backbone as input, it adopts one upsample operation to rescale the deep features to the same scale before the concatenation operation. Meanwhile, FFMv2 takes the base feature and the largest output feature map of the previous TUMthese two are of the same scale -as input, and produces the fused feature for the next TUM. Structural details of FFMv1 and FFMv2 are shown in <ref type="figure" target="#fig_3">Fig. 4 (a)</ref> and (b), respectively.</p><p>TUMs Different from FPN <ref type="bibr" target="#b10">(Lin et al. 2017a</ref>) and Reti-naNet <ref type="bibr" target="#b11">(Lin et al. 2017b</ref>), TUM adopts a thinner U-shape structure as illustrated in <ref type="figure" target="#fig_3">Fig. 4 (c)</ref>. The encoder is a series of 3x3 convolution layers with stride 2. And the decoder takes the outputs of these layers as its reference set of feature maps, while the original FPN chooses the output of the last layer of each stage in ResNet backbone. In addition, we add 1x1 convolution layers after upsample and elementwise sum operation at the decoder branch to enhance learning ability and keep smoothness for the features <ref type="bibr" target="#b12">(Lin, Chen, and Yan 2014)</ref>. All of the outputs in the decoder of each TUM form the multi-scale features of the current level. As a whole, the outputs of stacked TUMs form the multi-level multi-scale features, while the front TUM mainly provides shallow-level features, the middle TUM provides mediumlevel features, and the back TUM provides deep-level features.</p><p>SFAM SFAM aims to aggregate the multi-level multiscale features generated by TUMs into a multi-level feature pyramid as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The first stage of SFAM is to concatenate features of the equivalent scale together along the channel dimension. The aggregated feature pyramid can be presented as</p><formula xml:id="formula_1">X = [X 1 , X 2 , ..., X i ], where X i = Concat(x 1 i , x 2 i , ..., x L i ) ∈ R Wi×Hi×C refers to the features of the i-th largest scale.</formula><p>Here, each scale in the aggregated pyramid contains features from multi-level depths. However, simple concatenation operations are not adaptive enough. In the second stage, we introduce a channel-wise attention module to encourage features to focus on channels that they benefit most. Following SE block <ref type="bibr" target="#b8">(Hu, Shen, and Sun 2017)</ref>, we use global average pooling to generate channel-wise statistics z ∈ R C at the squeeze step. And to fully capture channel-wise dependencies, the following excitation step learns the attention mechanism via two fully connected layers:</p><formula xml:id="formula_2">s = F ex (z, W) = σ(W 2 δ(W 1 z)),<label>(2)</label></formula><p>where σ refers to the ReLU function, δ refers to the sigmoid function, W 1 ∈ R C r ×C , W 2 ∈ R C× C r , r is the reduction ratio (r = 16 in our experiments). The final output is obtained by reweighting the input X with activation s:</p><formula xml:id="formula_3">X c i = F scale (X c i , s c ) = s c · X c i ,<label>(3)</label></formula><formula xml:id="formula_4">whereX i = [X 1 i ,X 2 i , ...,X C i ]</formula><p>, each of the features is enhanced or weakened by the rescaling operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Configurations</head><p>We assemble M2Det with two kinds of backbones (Simonyan and Zisserman 2015; . Before training the whole network, the backbones need to be pre-trained on the ImageNet 2012 dataset <ref type="bibr" target="#b15">(Russakovsky et al. 2015)</ref>. All of the default configurations of MLFPN contain 8 TUMs, each TUM has 5 striding-Convs and 5 Upsample operations, so it will output features with 6 scales. To reduce the number of parameters, we only allocate 256 channels to each scale of their TUM features, so that the network could be easy to train on GPUs. As for input size, we follow the original SSD, RefineDet and RetinaNet, i.e., 320, 512 and 800.</p><p>At the detection stage, we add two convolution layers to each of the 6 pyramidal features to achieve location regression and classification respectively. The detection scale ranges of the default boxes of the six feature maps follow the setting of the original SSD. And when input size is 800×800, the scale ranges increase proportionally except keeping the minimum size of the largest feature map. At each pixel of the pyramidal features, we set six anchors with three ratios entirely. Afterward, we use a probability score of 0.05 as threshold to filter out most anchors with low scores. Then we use soft-NMS (Bodla et al. 2017) with a linear kernel for post-processing, leaving more accurate boxes. Decreasing the threshold to 0.01 can generate better detection results, but it will slow down the inference time a lot, we do not consider it for pursuing better practical values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we present experimental results on the bounding box detection task of the challenging MS-COCO benchmark. Following the protocol in MS-COCO, we use the trainval35k set for training, which is a union of 80k images from train split and a random 35 subset of images from the 40k image val split. To compare with state-ofthe-art methods, we report COCO AP on the test-dev split, which has no public labels and requires the use of the evaluation server. And then, we report the results of ablation studies evaluated on the minival split for convenience.</p><p>Our experiment section includes 4 parts: <ref type="formula" target="#formula_0">(1)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>For all experiments based on M2Det, we start training with warm-up strategy for 5 epochs, initialize the learning rate as 2 × 10 −3 , and then decrease it to 2 × 10 −4 and 2 × 10 −5 at 90 epochs and 120 epochs, and stop at 150 epochs. M2Det is developed with PyTorch v0.4.0 1 . When input size is 320 and 512, we conduct experiments on a machine with 4 NVIDIA Titan X GPUs, CUDA 9.2 and cuDNN 7.1.4, while for input size of 800, we train the network on NVIDIA Tesla V100 to get results faster. The batch size is set to 32 (16 each for 2 GPUs, or 8 each for 4 GPUs). On NVIDIA Titan Xp that has 12 GB memory, the training performance is limited if batch size on a single GPU is less than 5. Notably, for Resnet101, M2Det with the input size of 512 is not only limited in the batch size (only 4 is available), but also takes a long time to train, so we train it on V100.</p><p>For training M2Det with the VGG-16 backbone when input size is 320×320 and 512×512 on 4 Titan X devices, the total training time costs 3 and 6 days respectively, and with the ResNet-101 backbone when 320×320 costs 5 days. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation study</head><p>Since M2Det is composed of multiple subcomponents, we need to verify each of its effectiveness to the final performance. The baseline is a simple detector based on the original SSD, with 320×320 input size and VGG-16 reduced backbone.  TUM To demonstrate the effectiveness of TUM, we conduct three experiments. First, following DSSD, we extend the baseline detector with a series of Deconv layers, and the AP has improved from 25.8 to 27.5 as illustrated in the third column in <ref type="table" target="#tab_3">Table 2</ref>. Then we replace with MLFPN into it. As for the U-shape module, we firstly stack 8 s-TUMs, which is modified to decrease the 1×1 Convolution layers shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, then the performance has improved 3.1 compared with the last operation, shown in the forth column in <ref type="table" target="#tab_3">Table  2</ref>. Finally, replacing TUM by s-TUM in the fifth column has reached the best performance, it comes to AP of 30.8.</p><p>Base feature Although stacking TUMs can improve detection, but it is limited by input channels of the first TUM. That is, decreasing the channels will drop the abstraction of MLFPN, while increasing them will highly increase the parameters number. Instead of using base feature only once, We afferent base feature at the input of each TUM to alleviate the problem. For each TUM, the embedded base feature provides necessary localization information since it contains shallow features. The AP percentage increases to 32.7, as shown in the sixth column in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>SFAM As shown in the seventh column in <ref type="table" target="#tab_3">Table 2</ref>, compared with the architecture that without SFAM, all evaluation metrics have been upgraded. Specifically, all boxes including small, medium and large become more accurate.</p><p>Backbone feature As in many visual tasks, we observe a noticeable AP gain from 33.2 to 34.1 when we use welltested ResNet-101  instead of VGG-16 as the backbone network. As shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variants of MLFPN</head><p>The Multi-scale Multi-level Features have been proved to be effective. But what is the boundary of the improvement brought by MLFPN?</p><p>Step forward, how to design TUM and how many TUMs should be OK? We implement a group of variants to find the regular patterns. To be more specific, we fix the backbone as VGG-16 and the input image size as 320x320, and then tune the number of TUMs and the number of internal channels of each TUM. As shown in <ref type="table" target="#tab_5">Table 3</ref>, M2Det with different configurations of TUMs is evaluated on COCO minival set. Comparing the number of TUMs when fixing the channels, e.g.,256, it can be concluded that stacking more TUMs brings more promotion in terms of detection accuracy. Then fixing the number of TUMs, no matter how many TUMs are assembled, more channels consistently benefit the results. Furthermore, assuming that we take a version with 2 TUMS and 128 channels as the baseline, using more TUMs could bring larger improvement compared with increasing the internal channels, while the increase in parameters remains similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speed</head><p>We compare the inference speed of M2Det with state-ofthe-art approaches. Since VGG-16 (Simonyan and Zisserman 2015) reduced backbone has removed FC layers, it is very fast to use it for extracting base feature. We set the batch size to 1, take the sum of the CNN time and NMS time of 1000 images, and divide by 1000 to get the inference time of a single image. Specifically, we assemble VGG16reduced to M2Det and propose the fast version M2Det with the input size 320×320, the standard version M2Det with 512×512 input size and the most accurate version M2Det with 800×800 input size. Based on the optimization of PyTorch, M2Det can achieve accurate results with high speed. As shown in <ref type="figure">Fig. 5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We think the detection accuracy improvement of M2Det is mainly brought by the proposed MLFPN. On one hand, we fuse multi-level features extracted by backbone as the base feature, and then feed it into a block of alternating joint Thinned U-shape Modules and Feature Fusion Modules to extract more representative, multi-level multi-scale features, i.e. the decoder layers of each TUM. Obviously, these decoder layers are much deeper than the layers in the backbone, and thus more representative for object detection. Contrasted with our method, the existing detectors <ref type="bibr" target="#b17">(Zhang et al. 2018;</ref><ref type="bibr" target="#b10">Lin et al. 2017a;</ref><ref type="bibr" target="#b4">Fu et al. 2017</ref>) just use the layers of the backbone or extra layers with few depth increase. Hence, our method can achieve superior detection performance. On the other hand, each feature map of the multilevel feature pyramid generated by the SFAM consists of the decoder layers from multiple levels. In particular, at each scale, we use multi-level features to detect objects, which would be better for handling appearance-complexity variation across object instances.</p><p>To verify that the proposed MLFPN can learn effective feature for detecting objects with different scales and large appearance variation, we visualize the activation values of classification Conv layers along scale and level dimensions, such an example shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. The input image contains two persons, two cars and a traffic light. Moreover, the sizes of the two persons are different, as well as the two cars. And the traffic light, the smaller person and the smaller car have similar sizes. We can find that: 1) compared with the smaller person, the larger person has strongest activation value at the feature map of large scale, so as to the smaller car and larger car; 2) the traffic light, the smaller person and the smaller car have strongest activation value at the feature maps of the same scale; 3) the persons, the cars and the traffic light have strongest activation value at the highest-level, middlelevel, lowest-level feature maps respectively. This example presents that: 1) our method learns very effective features to handle scale variation and appearance-complexity variation across object instances; 2) it is necessary to use multi-level features to detect objects with similar size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, a novel method called Multi-Level Feature Pyramid Network (MLFPN) is proposed to construct effective feature pyramids for detecting objects of different scales. MLFPN consists of several novel modules. First, multi-level features (i.e. multiple layers) extracted by backbone are fused by a Feature Fusion Module (FFMv1) as the base feature. Second, the base feature is fed into a block of alternating joint Thinned U-shape Modules (TUMs) and Fature Fusion Modules (FFMv2s) and multi-level multi-scale features (i.e. the decoder layers of each TUM) are extracted. Finally, the extracted multi-level multi-scale features with the same scale (size) are aggregated to construct a feature pyramid for object detection by a Scale-wise Feature Aggregation Module (SFAM). A powerful end-to-end one-stage object detector called M2Det is designed based on the proposed MLFPN, which achieves a new state-of-the-art result (i.e. AP of 41.0 at speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with multi-scale inference strategy) among the one-stage detectors on MS-COCO benchmark. Additional ablation studies further demonstrate the effectiveness of the proposed architecture and the novel modules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustrations of four kinds of feature pyramids.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the proposed M2Det(320 × 320). M2Det utilizes the backbone and the Multi-level Feature Pyramid Network (MLFPN) to extract features from the input image, and then produces dense bounding boxes and category scores. In MLFPN, FFMv1 fuses feature maps of the backbone to generate the base feature. Each TUM generates a group of multi-scale features, and then the alternating joint TUMs and FFMv2s extract multi-level multi-scale features. Finally, SFAM aggregates the features into a multi-level feature pyramid. In practice, we use 6 scales and 8 levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of Scale-wise Feature Aggregation Module. The first stage of SFAM is to concatenate features with equivalent scales along channel dimension. Then the second stage uses SE attention to aggregate features in an adaptive way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Structural details of some modules. (a) FFMv1, (b) FFMv2, (c) TUM. The inside numbers of each block denote: input channels, Conv kernel size, stride size, output channels. network configurations in M2Det are introduced in the following.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>introducing implement details about the experiments; (2) demonstrating the comparisons with state-of-the-art approaches; (3) ablation studies about M2Det; (4) comparing different settings about the internal structure of MLFPN and introducing several version of M2Det.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Example activation values of multi-scale multilevel features. Best view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 1: Detection accuracy comparisons in terms of mAP percentage on MS COCO test-dev set.size is 512×512 on 2 V100 devices, it costs 11 days. The most accurate model is M2Det with the VGG backbone and 800×800 input size, it costs 14 days. M2Det is not entirely caused by the deepened depth of the model or the gained parameters, we compare with stateof-the-art one-stage detectors and two-stage detectors. Cor-nerNet with Hourglass has 201M parameters, Mask R-CNN withResNeXt-101-32x8d-FPN (Xie et al.  2017) has 205M parameters. By contrast, M2Det800-VGG has only 147M parameters. Besides, consider the comparison of depth, it is also not dominant.</figDesc><table><row><cell>Method of</cell><cell>Backbone</cell><cell>Input size</cell><cell>MultiScale</cell><cell>FPS</cell><cell cols="3">0.5:0.95 0.5 Avg. Precision, IoU: 0.75</cell><cell cols="3">S Avg. Precision, Area: M L</cell></row><row><cell>two-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Faster R-CNN (Ren et al. 2015)</cell><cell>VGG-16</cell><cell>∼1000×600</cell><cell>False</cell><cell>7.0</cell><cell>21.9</cell><cell>42.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OHEM++ (Shrivastava et al. 2016)</cell><cell>VGG-16</cell><cell>∼1000×600</cell><cell>False</cell><cell>7.0</cell><cell>25.5</cell><cell>45.9</cell><cell>26.1</cell><cell>7.4</cell><cell>27.7</cell><cell>40.3</cell></row><row><cell>R-FCN (Dai et al. 2016)</cell><cell>ResNet-101</cell><cell>∼1000×600</cell><cell>False</cell><cell>9</cell><cell>29.9</cell><cell>51.9</cell><cell>-</cell><cell>10.8</cell><cell>32.8</cell><cell>45.0</cell></row><row><cell>CoupleNet (Zhu et al. 2017)</cell><cell>ResNet-101</cell><cell>∼1000×600</cell><cell>False</cell><cell>8.2</cell><cell>34.4</cell><cell>54.8</cell><cell>37.2</cell><cell>13.4</cell><cell>38.1</cell><cell>50.8</cell></row><row><cell>Faster R-CNN w FPN (Lin et al. 2017a)</cell><cell>Res101-FPN</cell><cell>∼1000×600</cell><cell>False</cell><cell>6</cell><cell>36.2</cell><cell>59.1</cell><cell>39.0</cell><cell>18.2</cell><cell>39.0</cell><cell>48.2</cell></row><row><cell>Deformable R-FCN (Dai et al. 2017)</cell><cell>Inc-Res-v2</cell><cell>∼1000×600</cell><cell>False</cell><cell>-</cell><cell>37.5</cell><cell>58.0</cell><cell>40.8</cell><cell>19.4</cell><cell>40.1</cell><cell>52.5</cell></row><row><cell>Mask R-CNN (He et al. 2017)</cell><cell>ResNeXt-101</cell><cell>∼1280×800</cell><cell>False</cell><cell>3.3</cell><cell>39.8</cell><cell>62.3</cell><cell>43.4</cell><cell>22.1</cell><cell>43.2</cell><cell>51.2</cell></row><row><cell>Fitness-NMS (Tychsen-Smith and Petersson 2018)</cell><cell>ResNet-101</cell><cell>∼1024×1024</cell><cell>True</cell><cell>5.0</cell><cell>41.8</cell><cell>60.9</cell><cell>44.9</cell><cell>21.5</cell><cell>45.0</cell><cell>57.5</cell></row><row><cell>Cascade R-CNN (Cai and Vasconcelos 2018)</cell><cell>Res101-FPN</cell><cell>∼1280×800</cell><cell>False</cell><cell>7.1</cell><cell>42.8</cell><cell>62.1</cell><cell>46.3</cell><cell>23.7</cell><cell>45.5</cell><cell>55.2</cell></row><row><cell>SNIP (Singh and Davis 2018)</cell><cell>DPN-98</cell><cell>-</cell><cell>True</cell><cell>-</cell><cell>45.7</cell><cell>67.3</cell><cell>51.1</cell><cell>29.3</cell><cell>48.8</cell><cell>57.1</cell></row><row><cell>one-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSD300* (Liu et al. 2016)</cell><cell>VGG-16</cell><cell>300×300</cell><cell>False</cell><cell>43</cell><cell>25.1</cell><cell>43.1</cell><cell>25.8</cell><cell>6.6</cell><cell>25.9</cell><cell>41.4</cell></row><row><cell>RON384++ (Kong et al. 2017)</cell><cell>VGG-16</cell><cell>384×384</cell><cell>False</cell><cell>15</cell><cell>27.4</cell><cell>49.5</cell><cell>27.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DSSD321 (Fu et al. 2017)</cell><cell>ResNet-101</cell><cell>321×321</cell><cell>False</cell><cell>9.5</cell><cell>28.0</cell><cell>46.1</cell><cell>29.2</cell><cell>7.4</cell><cell>28.1</cell><cell>47.6</cell></row><row><cell>RetinaNet400 (Lin et al. 2017b)</cell><cell>ResNet-101</cell><cell>∼640×400</cell><cell>False</cell><cell>12.3</cell><cell>31.9</cell><cell>49.5</cell><cell>34.1</cell><cell>11.6</cell><cell>35.8</cell><cell>48.5</cell></row><row><cell>RefineDet320 (Zhang et al. 2018)</cell><cell>VGG-16</cell><cell>320×320</cell><cell>False</cell><cell>38.7</cell><cell>29.4</cell><cell>49.2</cell><cell>31.3</cell><cell>10.0</cell><cell>32.0</cell><cell>44.4</cell></row><row><cell>RefineDet320 (Zhang et al. 2018)</cell><cell>ResNet-101</cell><cell>320×320</cell><cell>True</cell><cell>-</cell><cell>38.6</cell><cell>59.9</cell><cell>41.7</cell><cell>21.1</cell><cell>41.7</cell><cell>52.3</cell></row><row><cell>M2Det (Ours)</cell><cell>VGG-16</cell><cell>320×320</cell><cell>False</cell><cell>33.4</cell><cell>33.5</cell><cell>52.4</cell><cell>35.6</cell><cell>14.4</cell><cell>37.6</cell><cell>47.6</cell></row><row><cell>M2Det (Ours)</cell><cell>VGG-16</cell><cell>320×320</cell><cell>True</cell><cell>-</cell><cell>38.9</cell><cell>59.1</cell><cell>42.4</cell><cell>24.4</cell><cell>41.5</cell><cell>47.6</cell></row><row><cell>M2Det (Ours)</cell><cell>ResNet-101</cell><cell>320×320</cell><cell>False</cell><cell>21.7</cell><cell>34.3</cell><cell>53.5</cell><cell>36.5</cell><cell>14.8</cell><cell>38.8</cell><cell>47.9</cell></row><row><cell>M2Det (Ours)</cell><cell>ResNet-101</cell><cell>320×320</cell><cell>True</cell><cell>-</cell><cell>39.7</cell><cell>60.0</cell><cell>43.3</cell><cell>25.3</cell><cell>42.5</cell><cell>48.3</cell></row><row><cell>YOLOv3 (Redmon and Farhadi 2018)</cell><cell>DarkNet-53</cell><cell>608×608</cell><cell>False</cell><cell>19.8</cell><cell>33.0</cell><cell>57.9</cell><cell>34.4</cell><cell>18.3</cell><cell>35.4</cell><cell>41.9</cell></row><row><cell>SSD512* (Liu et al. 2016)</cell><cell>VGG-16</cell><cell>512×512</cell><cell>False</cell><cell>22</cell><cell>28.8</cell><cell>48.5</cell><cell>30.3</cell><cell>10.9</cell><cell>31.8</cell><cell>43.5</cell></row><row><cell>DSSD513 (Fu et al. 2017)</cell><cell>ResNet-101</cell><cell>513×513</cell><cell>False</cell><cell>5.5</cell><cell>33.2</cell><cell>53.3</cell><cell>35.2</cell><cell>13.0</cell><cell>35.4</cell><cell>51.1</cell></row><row><cell>RetinaNet500 (Lin et al. 2017b)</cell><cell>ResNet-101</cell><cell>∼832×500</cell><cell>False</cell><cell>11.1</cell><cell>34.4</cell><cell>53.1</cell><cell>36.8</cell><cell>14.7</cell><cell>38.5</cell><cell>49.1</cell></row><row><cell>RefineDet512 (Zhang et al. 2018)</cell><cell>VGG-16</cell><cell>512×512</cell><cell>False</cell><cell>22.3</cell><cell>33.0</cell><cell>54.5</cell><cell>35.5</cell><cell>16.3</cell><cell>36.3</cell><cell>44.3</cell></row><row><cell>RefineDet512 (Zhang et al. 2018)</cell><cell>ResNet-101</cell><cell>512×512</cell><cell>True</cell><cell>-</cell><cell>41.8</cell><cell>62.9</cell><cell>45.7</cell><cell>25.6</cell><cell>45.1</cell><cell>54.1</cell></row><row><cell>CornerNet (Law and Deng 2018)</cell><cell>Hourglass</cell><cell>512×512</cell><cell>False</cell><cell>4.4</cell><cell>40.5</cell><cell>57.8</cell><cell>45.3</cell><cell>20.8</cell><cell>44.8</cell><cell>56.7</cell></row><row><cell>CornerNet (Law and Deng 2018)</cell><cell>Hourglass</cell><cell>512×512</cell><cell>True</cell><cell>-</cell><cell>42.1</cell><cell>57.8</cell><cell>45.3</cell><cell>20.8</cell><cell>44.8</cell><cell>56.7</cell></row><row><cell>M2Det (Ours)</cell><cell>VGG-16</cell><cell>512×512</cell><cell>False</cell><cell>18.0</cell><cell>37.6</cell><cell>56.6</cell><cell>40.5</cell><cell>18.4</cell><cell>43.4</cell><cell>51.2</cell></row><row><cell>M2Det (Ours)</cell><cell>VGG-16</cell><cell>512×512</cell><cell>True</cell><cell>-</cell><cell>42.9</cell><cell>62.5</cell><cell>47.2</cell><cell>28.0</cell><cell>47.4</cell><cell>52.8</cell></row><row><cell>M2Det (Ours)</cell><cell>ResNet-101</cell><cell>512×512</cell><cell>False</cell><cell>15.8</cell><cell>38.8</cell><cell>59.4</cell><cell>41.7</cell><cell>20.5</cell><cell>43.9</cell><cell>53.4</cell></row><row><cell>M2Det (Ours)</cell><cell>ResNet-101</cell><cell>512×512</cell><cell>True</cell><cell>-</cell><cell>43.9</cell><cell>64.4</cell><cell>48.0</cell><cell>29.6</cell><cell>49.6</cell><cell>54.3</cell></row><row><cell>RetinaNet800 (Lin et al. 2017b)</cell><cell>Res101-FPN</cell><cell>∼1280×800</cell><cell>False</cell><cell>5.0</cell><cell>39.1</cell><cell>59.1</cell><cell>42.3</cell><cell>21.8</cell><cell>42.7</cell><cell>50.2</cell></row><row><cell>M2Det (Ours)</cell><cell>VGG-16</cell><cell>800×800</cell><cell>False</cell><cell>11.8</cell><cell>41.0</cell><cell>59.7</cell><cell>45.0</cell><cell>22.1</cell><cell>46.5</cell><cell>53.8</cell></row><row><cell>M2Det (Ours)</cell><cell>VGG-16</cell><cell>800×800</cell><cell>True</cell><cell>-</cell><cell>44.2</cell><cell>64.6</cell><cell>49.3</cell><cell>29.2</cell><cell>47.9</cell><cell>55.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">al. 2017) is 37.5, AP of Faster R-CNN with FPN is 36.2. As-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">sembled with ResNet-101 can further improve M2Det, the</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">single-scale version obtains AP of 38.8, which is competi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">tive with state-of-the-art two-stage detectors Mask R-CNN</cell></row><row><cell>Comparison with State-of-the-art</cell><cell></cell><cell></cell><cell cols="8">(He et al. 2017). In addition, based on the optimization of</cell></row><row><cell cols="3">We compare the experimental results of the proposed M2Det with state-of-the-art detectors in Table 1. For these experi-ments, we use 8 TUMs and set 256 channels for each TUM. The main information involved in the comparison includes the input size of the model, the test method (whether it uses multi-scale strategy), the speed of the model, and the test results. Test results of M2Det with 10 different setting ver-sions are reported in Table 1, which are produced by testing it on MS-COCO test-dev split, with a single NVIDIA Titan X PASCAL and the batch size 1. Other statistical re-sults stem from references. It is noteworthy that, M2Det-320 with VGG backbone achieves AP of 38.9, which has sur-passed most object detectors with more powerful backbones</cell><cell cols="8">While for training M2Det with ResNet-101 when input PyTorch, it can run at 15.8 FPS. RefineDet (Zhang et al. 2018) inherits the merits of one-stage detectors and two-stage detectors, gets AP of 41.8, CornerNet (Law and Deng 2018) proposes key point regression for detection and bor-rows the advantages of Hourglass (Newell, Yang, and Deng 2016) and focal loss (Lin et al. 2017b), thus gets AP of 42.1. In contrast, our proposed M2Det is based on the regression method of original SSD, with the assistance of Multi-scale Multi-level features, obtains 44.2 AP, which exceeds all one-stage detectors. Most approaches do not compare the speed of multi-scale inference strategy due to different methods or tools used, so we also only focus on the speed of single-scale inference methods.</cell></row><row><cell cols="3">and larger input size, e.g., AP of Deformable R-FCN (Dai et</cell><cell cols="8">1 https://pytorch.org/ In addition, in order to emphasize that the improvement</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of M2Det. The detection results are evaluated on minival set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>, such observation remains true and consistent with other AP metrics.</figDesc><table><row><cell>TUMs</cell><cell>Channels</cell><cell>Params(M)</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell></row><row><cell>2</cell><cell>256</cell><cell>40.1</cell><cell>30.5</cell><cell>50.5</cell><cell>32.0</cell></row><row><cell>2</cell><cell>512</cell><cell>106.5</cell><cell>32.1</cell><cell>51.8</cell><cell>34.0</cell></row><row><cell>4</cell><cell>128</cell><cell>34.2</cell><cell>29.8</cell><cell>49.7</cell><cell>31.2</cell></row><row><cell>4</cell><cell>256</cell><cell>60.2</cell><cell>31.8</cell><cell>51.4</cell><cell>33.0</cell></row><row><cell>4</cell><cell>512</cell><cell>192.2</cell><cell>33.4</cell><cell>52.6</cell><cell>34.2</cell></row><row><cell>8</cell><cell>128</cell><cell>47.5</cell><cell>31.8</cell><cell>50.6</cell><cell>33.6</cell></row><row><cell>8</cell><cell>256</cell><cell>98.9</cell><cell>33.2</cell><cell>52.2</cell><cell>35.2</cell></row><row><cell>8</cell><cell>512</cell><cell>368.8</cell><cell>34.0</cell><cell>52.9</cell><cell>36.4</cell></row><row><cell>16</cell><cell>128</cell><cell>73.9</cell><cell>32.5</cell><cell>51.7</cell><cell>34.4</cell></row><row><cell>16</cell><cell>256</cell><cell>176.8</cell><cell>33.6</cell><cell>52.6</cell><cell>35.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Different configurations of MLFPN in M2Det. The backbone is VGG and input image is 320×320.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Soft-nms -improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bodla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
	<note>Deformable convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DSSD : Deconvolutional single shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
	<note>CoRR abs/1701.06659. [Girshick</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">ICCV 2017</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RON: reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>ECCV 2014</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan ;</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deng ; Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
	</analytic>
	<monogr>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ECCV 2016</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
	</analytic>
	<monogr>
		<title level="m">NIPS 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>Russakovsky et al. 2015. Imagenet large scale visual recognition challenge. IJCV 2015</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2016</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
	<note>Aggregated residual transformations for deep neural networks</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scale-transferrable object detection</title>
	</analytic>
	<monogr>
		<title level="m">CVPR 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="528" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Couplenet: Coupling global structure with local parts for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4146" to="4154" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
