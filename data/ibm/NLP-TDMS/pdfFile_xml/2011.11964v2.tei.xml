<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LiDAR-based Panoptic Segmentation via Dynamic Shifting Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LiDAR-based Panoptic Segmentation via Dynamic Shifting Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>(a) LiDAR-based Panoptic Segmentation (b) Dynamic Shifting Road Vegetation Building Stuff Classes Car #1 Car #2 Car #3 Car #4 Car #5 Car #6 Things Classes</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rapid advances of autonomous driving, it becomes critical to equip its sensing system with more holistic 3D perception. However, existing works focus on parsing either the objects (e.g. cars and pedestrians) or scenes (e.g. trees and buildings) from the LiDAR sensor. In this work, we address the task of LiDAR-based panoptic segmentation, which aims to parse both objects and scenes in a unified manner. As one of the first endeavors towards this new challenging task, we propose the Dynamic Shifting Network (DS-Net), which serves as an effective panoptic segmentation framework in the point cloud realm. In particular, DS-Net has three appealing properties: 1) strong backbone design. DS-Net adopts the cylinder convolution that is specifically designed for LiDAR point clouds. The extracted features are shared by the semantic branch and the instance branch which operates in a bottom-up clustering style. 2) Dynamic Shifting for complex point distributions. We observe that commonly-used clustering algorithms like BFS or DBSCAN are incapable of handling complex autonomous driving scenes with non-uniform point cloud distributions and varying instance sizes. Thus, we present an efficient learnable clustering module, dynamic shifting, which adapts kernel functions on-the-fly for different instances. 3) Consensus-driven Fusion. Finally, consensus-driven fusion is used to deal with the disagreement between semantic and instance predictions. To comprehensively evaluate the performance of LiDAR-based panoptic segmenta-tion, we construct and curate benchmarks from two largescale autonomous driving LiDAR datasets, SemanticKITTI and nuScenes. Extensive experiments demonstrate that our proposed DS-Net achieves superior accuracies over current state-of-the-art methods. Notably, we achieve 1st place on the public leaderboard of SemanticKITTI, outperforming 2nd place by 2.6% in terms of the PQ metric 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Autonomous driving, one of the most promising applications of computer vision, has achieved rapid progress in recent years. Perception system, one of the most important modules in autonomous driving, has also attracted extensive studies in previous research works. Admittedly, the classic tasks of 3D object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">32]</ref> and semantic segmentation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b35">35]</ref> have developed relatively mature solutions that support real-world autonomous driving prototypes. However, there still exists a considerable gap between the existing works and the goal of holistic perception which is essential for the challenging autonomous driving scenes. In this work, we propose to close the gap by exploring the task of LiDAR-based panoptic segmentation, which requires full-spectrum point-level predictions.</p><p>Panoptic segmentation has been proposed in 2D detection <ref type="bibr" target="#b14">[15]</ref> as a new vision task which unifies semantic and instance segmentation. Behley et al. <ref type="bibr" target="#b2">[3]</ref> extend the task to LiDAR point clouds and propose the task of LiDAR-based panoptic segmentation. As shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>, this task requires to predict point-level semantic labels for background (stuff ) classes (e.g. road, building and vegetation), while instance segmentation needs to be performed for foreground (things) classes (e.g. car, person and cyclist).</p><p>Nevertheless, the complex point distributions of LiDAR data make it difficult to perform reliable panoptic segmentation. Most existing point cloud instance segmentation methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref> are mainly designed for dense and uniform indoor point clouds. Therefore, decent segmentation results can be achieved through the center regression and heuristic clustering algorithms. However, due to the nonuniform density of LiDAR point clouds and varying sizes of instances, the center regression fails to provide ideal point distributions for clustering. The regressed centers usually form noisy strip distributions that vary in density and sizes. As will be analyzed in Section 3.2, several heuristic clustering algorithms widely used in previous works cannot provide satisfactory clustering results for the regressed centers of LiDAR point clouds. To tackle the above mentioned technical challenges, we propose Dynamic Shifting Network (DS-Net) which is specifically designed for effective panoptic segmentation of LiDAR point clouds.</p><p>Firstly, we adopt a strong backbone design and provide a strong baseline for the new task. Inspired by <ref type="bibr" target="#b37">[37]</ref>, the cylinder convolution is used to efficiently extract grid-level features for each LiDAR frame in one pass which are further shared by the semantic and instance branches.</p><p>Secondly, we present a novel Dynamic Shifting Module designed to cluster on the regressed centers with complex distributions produced by the instance branch. As illustrated in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, the proposed dynamic shifting module shifts the regressed centers to the cluster centers. The shift targets x i are adaptively computed by weighting across several shift candidates c ij which are calculated through kernel functions k j . The special design of the module makes the shift operation capable of dynamically adapting to the density or sizes of different instances and therefore shows superior performance on LiDAR point clouds. Further analysis also shows that the dynamic shifting module is robust and not sensitive to parameter settings.</p><p>Thirdly, the Consensus-driven Fusion Module is presented to unify the semantic and instance results to obtain panoptic segmentation results. The proposed consensusdriven fusion mainly solves the disagreement caused by the class-agnostic style of instance segmentation. The fusion module is highly efficient, thus brings negligible computation overhead.</p><p>Extensive experiments on SemanticKITTI demonstrate the effectiveness of our proposed DS-Net. To further illustrate the generalizability of DS-Net, we customize a LiDAR-based panoptic segmentation dataset based on nuScenes. As one of the first works for this new task, we present several strong baseline results by combining the state-of-the-art semantic segmentation and detection methods. DS-Net outperforms all the state-of-the-art methods on both benchmarks (1st place on the public leaderboard of SemanticKITTI).</p><p>The main contributions are summarized below: 1) To our best knowledge, we present one of the first attempts to address the challenging task of LiDAR-based panoptic segmentation.</p><p>2) The proposed DS-Net effectively handles the complex distributions of LiDAR point clouds, and achieves state-of-the-art performance on SemanticKITTI and nuScenes. 3) Extensive experiments are performed on large-scale datasets. We adapt existing methods to this new task for in-depth comparisons. Further statistical analyses are carried out to provide valuable observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Point Cloud Semantic Segmentation. According to the data representations of point clouds, most point cloud semantic segmentation methods can be categorized to pointbased and voxel-based methods. Based on PointNet <ref type="bibr" target="#b21">[22]</ref> and PointNet++ <ref type="bibr" target="#b22">[23]</ref>, KPConv <ref type="bibr" target="#b24">[25]</ref>, DGCNN <ref type="bibr" target="#b28">[28]</ref>, Point-Conv <ref type="bibr" target="#b31">[31]</ref> and Randla-Net <ref type="bibr" target="#b12">[13]</ref> can directly operate on unordered point clouds. However, due to space and time complexity, most point-based methods struggle on large-scale point clouds datasets e.g. ScanNet <ref type="bibr" target="#b8">[9]</ref>, S3DIS <ref type="bibr" target="#b0">[1]</ref>, and Se-manticKITTI <ref type="bibr" target="#b1">[2]</ref>. MinkowskiNet <ref type="bibr" target="#b6">[7]</ref> utilizes the sparse convolutions to efficiently perform semantic segmentation on the voxelized large-scale point clouds. Different from indoor RGB-D reconstruct point clouds, LiDAR point clouds have non-uniform and sparse point distributions which require special designs of the network. SqueezeSeg <ref type="bibr" target="#b30">[30]</ref> views LiDAR point clouds as range images while PolarNet <ref type="bibr" target="#b35">[35]</ref> and Cylinder3D <ref type="bibr" target="#b37">[37]</ref> divide the LiDAR point clouds under the polar and cylindrical coordinate systems. Compared to other data representations, cylindrical division is more suitable for sparse and non-uniform LiDAR point clouds because it fully utilizes the localization of point clouds while remains high efficiency. Point Cloud Instance Segmentation. Previous works have shown great progress in the instance segmentation of indoor point clouds. A large number of point-based methods (e.g. SGPN <ref type="bibr" target="#b26">[26]</ref>, ASIS <ref type="bibr" target="#b27">[27]</ref>, JSIS3D <ref type="bibr" target="#b20">[21]</ref> and JSNet <ref type="bibr" target="#b36">[36]</ref>) split the whole scene into small blocks and learn point-wise embeddings for final clustering. Although great efforts have been made on semantic and instance branch merging in order to boost each other's performance, these methods are limited by the heuristic post processing steps and the lack of perception. To avoid the problems, recent works (e.g. PointGroup <ref type="bibr" target="#b13">[14]</ref>, 3D-MPA <ref type="bibr" target="#b9">[10]</ref>, OccuSeg <ref type="bibr" target="#b11">[12]</ref>) abandon the block partition and use sparse convolutions to extract fea- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flat Kernel</head><p>Regressed Centers tures of the whole scene in one pass. As for LiDAR point clouds, there are a few previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b34">34]</ref> trying to tackle the problem. Wu et al. <ref type="bibr" target="#b30">[30]</ref> directly cluster on XYZ coordinates after semantic segmentation. Wong et al. <ref type="bibr" target="#b29">[29]</ref> builds the network in a top-down style and incorporates metric learning. Zhang et al. <ref type="bibr" target="#b34">[34]</ref> uses grid-level center voting to cluster points of interest in autonomous driving scenes. Although our work is not targeting the instance segmentation for LiDAR point clouds, the proposed DS-Net can provide some insights into the challenging task.</p><formula xml:id="formula_0">c 1 c 2 … c l</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>As one of the first attempts on the task of LiDAR-based panoptic segmentation, we first introduce a strong backbone to establish a simple baseline (Sec. 3.1), based on which two modules are further proposed. The novel dynamic shifting module is presented to tackle the challenge of the nonuniform LiDAR point clouds distributions (Sec. 3.2). The efficient consensus-driven fusion module combines the semantic and instance predictions and produces panoptic segmentation results (Sec. 3.3). The whole pipeline of the DS-Net is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Strong Backbone Design</head><p>To obtain panoptic segmentation results, it is natural to solve two sub-tasks separately, which are semantic and instance segmentation, and combine the results. As shown in the upper part of <ref type="figure" target="#fig_1">Fig. 2</ref>, the strong backbone consists of three parts: the cylinder convolution, a semantic branch, and an instance branch. High quality grid-level features are extracted by the cylinder convolution from raw Li-DAR point clouds and then shared by semantic and instance branches. Cylinder Convolution. Considering the difficulty presented by the task, we find that the cylinder convolution <ref type="bibr" target="#b37">[37]</ref> best meets the strict requirements of high efficiency, high performance and fully mining of 3D positional relationship. The cylindrical voxel partition can produce more even point distribution than normal Cartesian voxel partition and therefore leads to higher feature extraction efficiency and higher performance. Cylindrical voxel representation combined with sparse convolutions can naturally retain and fully explore 3D positional relationship. Thus we choose the cylinder convolution as our feature extractor. Semantic Branch. The semantic branch performs semantic segmentation by connecting MLP to the cylinder convolution to predict semantic confidences for each voxel grid. Then the point-wise semantic labels are copied from their corresponding grids. We use the weighted cross entropy and Lovasz Loss <ref type="bibr" target="#b3">[4]</ref> as the loss function of the semantic branch. Instance Branch. The instance branch utilizes center regression to prepare the things points for further clustering. The center regression module uses MLP to adapt cylinder convolution features and make things points to regress the centers of their instances by predicting the offset vectors O ∈ R M ×3 pointing from the points P ∈ R M ×3 to the instance centers C gt ∈ R M ×3 . The loss function for instance branch can be formulated as:</p><formula xml:id="formula_1">L ins = 1 M M i=0 O[i] − (C gt [i] − P [i]) 1 ,<label>(1)</label></formula><p>where M is the number of things points. The regressed centers O + P are further clustered to obtain the instance IDs, which can be achieved by either heuristic clustering algorithms or the proposed dynamic shifting module which are further introduced and analyzed in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dynamic Shifting</head><p>Point Clustering Revisit. Unlike indoor point clouds which are carefully reconstructed using RGB-D videos, the LiDAR point clouds have the distributions that are not suitable for normal clustering solutions used by indoor instance segmentation methods. The varying instance sizes, the sparsity and incompleteness of LiDAR point clouds make it difficult for the center regression module to predict the precise center location and would result in noisy long strips distribution as displayed in <ref type="figure" target="#fig_0">Fig. 1</ref> (b) instead of an ideal ballshaped cluster around the center. Moreover, as presented in <ref type="figure" target="#fig_3">Fig. 3 (a)</ref>, the clusters formed by regressed centers that are far from the LiDAR sensor have much lower densities than those of nearby clusters due to the non-consistent sparsity of LiDAR point clouds. Facing the non-uniform distribution of regressed centers, heuristic clustering algorithms struggle to produce satisfactory results. Four major heuristic clustering algorithms that are used in previous bottom-up indoor point clouds instance segmentation methods are analyzed below. The details of the following algorithms can be found in supplementary materials.</p><p>• Breadth First Search (BFS). BFS is simple and good enough for indoor point clouds as proved in <ref type="bibr" target="#b13">[14]</ref>, but not suitable for LiDAR point clouds. As discussed above, large density difference between clusters means that the fixed radius cannot properly adapt to different clusters. Small radius will over-segment distant instances while large radius will under-segment near instances.</p><p>• DBSCAN <ref type="bibr" target="#b10">[11]</ref> and HDBSCAN <ref type="bibr" target="#b5">[6]</ref>. As density-based clustering algorithms, there is no surprise that these two algorithms also perform badly on the LiDAR point clouds, even though they are proved to be effective for clustering indoor point clouds <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">33]</ref>. The core operation of DBSCAN is the same as that of BFS. While HDB-SCAN intuitively assumes that the points with lower density are more likely to be noise points which is not the case in LiDAR points.</p><p>• Mean Shift <ref type="bibr" target="#b7">[8]</ref>. The advantage of Mean Shift, which is used by <ref type="bibr" target="#b16">[17]</ref> to cluster indoor point clouds, is that the kernel function is not sensitive to density changes and robust to noise points which makes it more suitable than density-based algorithms. However, the bandwidth of the kernel function has great impact on the clustering results as shown in <ref type="figure" target="#fig_3">Fig. 3</ref> (b). The fixed bandwidth cannot handle the situation of large and small instances simultaneously which makes Mean Shift also not the ideal choice for this task.</p><p>Dynamic Shifting. As discussed above, it is a robust way of estimating cluster centers of regressed centers by iteratively applying kernel functions as in Mean Shift. However, the fixed bandwidth of kernel functions fails to adapt to varying instance sizes. Therefore, we propose the dynamic shifting module which can automatically adapt the kernel function for each LiDAR point in the complex autonomous driving scene so that the regressed centers can be dynamically, efficiently and precisely shifted to the correct cluster centers. In order to make the kernel function learnable, we first consider how to mathematically define a differentiable shift opration. Inspired by <ref type="bibr" target="#b15">[16]</ref>, the shift operation on the seeding points (i.e. points to be clustered) can be expressed as matrix operations if the number of iterations is fixed. Specifically, one iteration of shift operation can be formulated as follows. Denoting X ∈ R M ×3 as the M seeding points, X will be updated once by the shift vector S ∈ R M ×3 which is formulated as</p><formula xml:id="formula_2">X ← X + ηS,<label>(2)</label></formula><p>where η is a scaling factor which is set to 1 in our experiments. The calculation of the shift vector S is by applying kernel function f on X, and formally defined as S = f (X) − X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Forward Pass of the Dynamic Shifting Module</head><p>Input: <ref type="bibr" target="#b15">16</ref> return R Among various kinds of kernel functions, the flat kernel is simple but effective for generating shift target estimations for LiDAR points, which is introduced as follows. The process of applying flat kernel can be thought of as placing a query ball of certain radius (i.e. bandwidth) centered at each seeding point and the result of the flat kernel is the mass of the points inside the query ball. Mathematically, the flat kernel f (X) = D −1 KX is defined by the kernel matrix K = (XX T ≤ δ), which masks out the points within a certain bandwidth δ for each seeding point, and the diagonal matrix D = diag(K1) that represents the number of points within the seeding point's bandwidth.</p><formula xml:id="formula_3">Things Points P ∈ R M ×3 , Things Features F ∈ R M ×D , Things Regressed Centers C ∈ R M ×3 , Fixed number of iteration I ∈ N, Bandwidth candidates list L ∈ R l Output: Instance IDs of things points R ∈ R M ×1 1 mask = F P S(P ), P = P [mask] 2 X = C[mask], F = F [mask] 3 for i ← 1 to I do 4 W i = Sof tmax(M LP (F )) 5 acc = zeros like(X) 6 for j ← 1 to l do 7 K ij = (XX T ≤ L[j]) 8 D ij = diag(K ij 1) 9 acc = acc + W i [:, j] (D −1 ij K ij X) 10 end 11 X = acc 12 end 13 R = cluster(X) 14 index = nearest neighbour(P, P ) 15 R = R [index]</formula><p>With a differentiable version of the shift operation defined, we proceed to our goal of dynamic shifting by adapting the kernel function for each point. In order to make the kernel function adaptable for instances with different sizes, the optimal bandwidth for each seeding point has to be inferenced dynamically. A natural solution is to directly regress bandwidth for each seeding point, which however is not differentiable if used with the flat kernel. Even though Gaussian kernel can make direct bandwidth regression trainable, it is still not the best solution as analyzed in section 4.1. Therefore, we apply the design of weighting across several bandwidth candidates to dynamically adapt to the optimal one.</p><p>One iteration of dynamic shifting is formally defined as follows. As shown in the bottom half of <ref type="figure" target="#fig_1">Fig. 2</ref>, l bandwidth candidates L = {δ 1 , δ 2 , ..., δ l } are set. For each seed-ing point, l shift target candidates are calculated by l flat kernels with corresponding bandwidth candidates. Seeding points then dynamically decide the final shift targets, which are ideally the closest to the cluster centers, by learning the weights W ∈ R M ×l to weight on l candidate targets. The weights W are learned by applying MLP and Softmax on the backbone features so that l j=1 W [:, j] = 1. The above procedure and the new learnable kernel functionf can be formulated aŝ</p><formula xml:id="formula_4">f (X) = l j=1 W [:, j] (D −1 j K j X),<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">K j = (XX T ≤ δ j ) and D j = diag(K j 1).</formula><p>With the one iteration of dynamic shifting stated clearly, the full pipeline of the dynamic shifting module, which is formally defined in algorithm 1, can be illustrated as follows. Firstly, to maintain the efficiency of the algorithm, farthest point sampling (FPS) is performed on M things points to provide M seeding points for the dynamic shifting iterations (Lines 1-2). After a fixed number I of dynamic shifting iterations (Lines 3-12), all seeding points have converged to the cluster centers. A simple heuristic clustering algorithm is performed to cluster the converged seeding points to obtain instance IDs for each seeding point (Line 13). Finally, all other things points find the nearest seeding points and the corresponding instance IDs are assigned to them (Lines 14-15).</p><p>The optimization of dynamic shifting module is not intuitive since it is impractical to obtain the ground truth bandwidth for each seeding point. The loss function has to encourage seeding points shifting towards their cluster centers which have no ground truths but can be approximated by the ground truth centers of instances C gt ∈ R M ×3 . Therefore, the loss function for the ith iteration of dynamic shifting is defined by the manhattan distance between the ground truth centers C gt and the ith dynamically calculated shift targets X i , which can be formulated as</p><formula xml:id="formula_6">l i = 1 M M x=1 X i [x] − C gt [x] 1 .<label>(4)</label></formula><p>Adding up all the losses of I iterations gives us the loss function L ds for the dynamic shifting module:</p><formula xml:id="formula_7">L ds = I i=1 w i l i ,<label>(5)</label></formula><p>where w i are weights for losses of different iterations and are all set to 1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Consensus-driven Fusion</head><p>Typically, solving the conflict between semantic and instance predictions is one of the essential steps in panoptic  segmentation. The advantages of bottom-up methods are that all points with predicted instance IDs must be in things classes and one point will not be assigned to two instances. The only conflict needs to be solved is the disagreement of semantic predictions inside one instance, which is brought in by the class-agnostic way of instance segmentation. The strategy used in the proposed consensus-driven fusion is majority voting. For each predicted instance, the most appeared semantic label of its points determines the semantic labels for all the points inside the instance. This simple fusion strategy is not only efficient but could also revise and unify semantic predictions using instance information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments on two large-scale datasets: Se-manticKITTI <ref type="bibr" target="#b1">[2]</ref> and nuScenes <ref type="bibr" target="#b4">[5]</ref>. SemanticKITTI. SemanticKITTI is the first dataset that presents the challenge of LiDAR-based panoptic segmentation and provides the benchmark <ref type="bibr" target="#b2">[3]</ref>. SemanticKITTI contains 23,201 frames for training and 20,351 frames for testing. There are 28 annotated semantic classes which are remapped to 19 classes for the LiDAR-based panoptic segmentation task, among which 8 classes are things classes, and 11 classes are stuff classes. Each point is labeled with a semantic label and an instance id which will be set to 0 if the point belongs to stuff classes. nuScenes. In order to demonstrate the generalizability of DS-Net, we construct another LiDAR-based panoptic segmentation dataset from nuScenes. With the point-level semantic labels from the newly released nuScenes lidarseg challenge and the bounding boxes provided by the detection task, we could generate instance labels by assigning instance IDs to points inside bounding boxes. The simple strategy makes good enough panoptic segmentation annotations because in the autonomous driving scene, it is rare that something does not belong to an instance invades its bounding box. Following the definition of the nuScenes lidarseg challenge, we mark 10 foreground classes as things classes and 6 background classes as stuff classes out of all 16 semantic classes. Due to the annotation of the test split of nuScenes lidarseg challenge is held out, we could only validate and compare the results on the validation split. The training set of nuScenes has 28,130 frames and the validation set has 6,019 frames. Evaluation Metrics. As defined in <ref type="bibr" target="#b2">[3]</ref>, the evaluation metrics of LiDAR-based panoptic segmentation are the same as that of image panoptic segmentation defined in <ref type="bibr" target="#b14">[15]</ref> including Panoptic Quality (PQ), Segmentation Quality (SQ) and Recognition Quality (RQ) which are calculated across all classes. The above three metrics are also calculated separately on things and stuff classes which give PQ Th , SQ Th , RQ Th , and PQ St , SQ St , RQ St . PQ † is defined by swapping PQ of each stuff class to its IoU then averaging over all classes. In addition, mean IoU (mIoU) is also used to evaluate the quality of the sub-task of semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>Ablation on Overall Framework. To study on the effectiveness of the proposed modules, we sequentially add consensus-driven fusion module and dynamic shifting module to the bare backbone. The corresponding PQ and PQ Th are reported in <ref type="figure" target="#fig_5">Fig. 4</ref> (a) which shows that both modules contribute to the performance of DS-Net. The consensusdriven fusion improves the overall performance through unifying results from two parallel branches. The novel dynamic shifting module mainly boosts the performance of instance segmentation which are indicated by PQ Th where the DS-Net outperforms our backbone (with fusion module) by 3.2% in validation split. Ablation on Clustering Algorithms. In order to validate our previous analyses of clustering algorithms, we swap the dynamic shifting module for four other widely-used heuristic clustering algorithms: BFS, DBSCAN, HDBSCAN, and Mean Shift. The results are shown in <ref type="figure" target="#fig_5">Fig. 4 (b)</ref>. Consistent with our analyses in Sec. 3.2, the density-based clustering algorithms (e.g. BFS, DBSCAN, HDBSCAN) perform badly in terms of PQ and PQ Th while Mean Shift leads to the best results among the heuristic algorithms. Moreover, our dynamic shifting module shows the superiority over all four heuristic clustering algorithms. Ablation on Bandwidth Learning Style. In the dynamic shifting module, it is natural to directly regress bandwidth  for each point instead of learning weights for several candidates, as mentioned in Sec. 3.2. However, as shown in the <ref type="figure" target="#fig_5">Fig. 4 (c)</ref>, direct regression is hard to optimize in this case because the learning target is not straightforward. It is difficult to determine the best bandwidth for each point, and therefore impractical to directly apply supervision on the regressed bandwidth. Instead, we could only determine whether the shift takes the seeding point closer to the target center. Therefore, it is easier for the network to choose from and combine several bandwidth candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Comparisons on SemanticKITTI</head><p>Comparison Methods. Since its one of the first attempts on LiDAR-based panoptic segmentation, we provide several strong baseline results in order to validate the effectiveness of DS-Net. As proposed in <ref type="bibr" target="#b2">[3]</ref>, one good way of constructing strong baselines is to take the results from semantic segmentation methods and detection methods, and generate panoptic segmentation results by assigning instance IDs to all points inside predicted bounding boxes. <ref type="bibr" target="#b2">[3]</ref> has provided the combinations of KPConv [25] + PointPillars <ref type="bibr" target="#b17">[18]</ref>, and RangeNet++ [20] + PointPillars <ref type="bibr" target="#b17">[18]</ref>. To make the baseline stronger, we combine KPConv <ref type="bibr" target="#b24">[25]</ref> with PV-RCNN <ref type="bibr" target="#b23">[24]</ref> which is the state-of-the-art 3D detection method. In addition to the above baselines, we also adapt the stateof-the-art indoor instance segmentation method PointGroup <ref type="bibr" target="#b13">[14]</ref> using the official released codes to experiment on Se-manticKITTI. Moreover, LPASD <ref type="bibr" target="#b18">[19]</ref>, which is one of the earliest works in this area, is also included for comparison. Evaluation Results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Comparisons on nuScenes</head><p>Comparison Methods. Similarly, two strong semantic segmentation + detection baselines are provided for comparison on nuScenes. The semantic segmentation method is Cylinder3D <ref type="bibr" target="#b37">[37]</ref> and the detection methods are SECOND <ref type="bibr" target="#b32">[32]</ref> and PointPillars <ref type="bibr" target="#b17">[18]</ref>. For fair comparison, the detection networks are trained using single frames on nuScenes. The point-wise semantic predictions and predicted bounding boxes are merged in the following steps. First all points inside each bounding box are assigned a unique instance IDs across the frame. Then to unify the semantic predictions inside each instance, we assign the class labels of bounding boxes predicted by the detection network to corresponding instances. Evaluation Results. As shown in <ref type="table" target="#tab_4">Table 3</ref>, our DS-Net outperforms the best baseline method in most metrics. Especially, we surpass the best baseline method by 2.4% in PQ and 3.5% in PQ Th . Unlike SemanticKITTI, nuScenes is featured as extremely sparse point clouds in single frames which adds even more difficulties to panoptic segmentation. The results validate the generalizability and the effectiveness of our DS-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further Analysis</head><p>Robust to Parameter Settings. As shown in <ref type="table" target="#tab_5">Table 4</ref>, six sets of bandwidth candidates are set for independent training and the corresponding results are reported. The stable results show that DS-Net is robust to different parameter settings as long as the picked bandwidth candidates are  Interpretable Learned Bandwidths. By averaging the bandwidth candidates weighted by the learned weights, the learned bandwidths for every points could be approximated. The average learned bandwidths of different classes are shown in <ref type="figure">Fig. 5</ref>. The average learned bandwidths are roughly proportional to the instance sizes of corresponding classes, which is consistent with the expectation that dynamic shifting can dynamically adjust to different instance sizes.  redder points represents higher learned bandwidth and bluer points represents lower learned bandwidth. The seeding points farther away from the instance centers tend to learn higher bandwidths in order to quickly converge. While the well-learned regressed points tend to have lower bandwidths to maintain their positions. After four iterations, the seeding points have converged around the instance centers.</p><p>More visualization results could be found in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>With the goal of providing holistic perception for autonomous driving, we are one of the first to address the task of LiDAR-based panoptic segmentation. In order to tackle the challenge brought by the non-uniform distributions of LiDAR point clouds, we propose the novel DS-Net which is specifically designed for effective panoptic segmentation of LiDAR point clouds. Our DS-Net adopts strong baseline design which provides strong support for the consensusdriven fusion module and the novel dynamic shifting module. The novel dynamic shifting module adaptively shifts regressed centers of instances with different density and varying sizes. The consensus-driven fusion efficiently unifies semantic and instance results into panoptic segmentation results. The DS-Net outperforms all strong baselines on both SemanticKITTI and nuScenes. Further analyses show the robustness of the dynamic shifting module and the interpretability of the learned bandwidths.</p><p>In this appendix, we provide the following sections for a better understanding of the main paper. Firstly, the details of the heuristic clustering algorithms mentioned in the main paper are provided (Sec. 1). Then, we provide analyses of the differentiability of the dynamic shifting to give some insights into our design (Sec. 2). We further analyze the number of iterations in the dynamic shifting module (Sec. 3). Moreover, we report the implementation details of DS-Net for the reproducibility (Sec. 4). Last but not least, the per-class results are reported (Sec. 5), and more visualization examples are displayed (Sec. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Details of Heuristic Clustering Algorithms</head><p>Breadth First Search (BFS). BFS is simple but effective for indoor point clouds. For the points to be clustered, BFS first constructs a graph where edges connect point pairs that are closer than a given radius. Then each connected subgraph is considered a cluster. BFS has shown that it is capable of performing high-quality clustering for indoor point clouds <ref type="bibr" target="#b13">[14]</ref> which are dense, even and complete. However, it does not apply to LiDAR point clouds. As discussed in the main paper, large density difference within and between clusters means that the fixed radius can not properly adapt to different clusters. Therefore, it is not a good idea to use BFS as the clustering algorithm for autonomous driving scenes. DBSCAN <ref type="bibr" target="#b10">[11]</ref> and HDBSCAN <ref type="bibr" target="#b5">[6]</ref>. Both DBSCAN and HDBSCAN are density-based clustering algorithms which make them perform badly on LiDAR point clouds. Similar to BFS, DBSCAN also constructs a graph based on the mutual distances of the points. Although the new concept of core point is introduced to filter out noise points, the problem brought by the fixed radius also occurs in this algorithm. Moreover, the mechanism of noise points recognition also brings problems. For any points to be clustered, if there exists less than a certain number of points within a certain radius, the point is labeled as a noise point. However, the number and densities of points inside instances vary greatly, which makes it hard to determine the line between instances with little points and noise points.</p><p>HDBSCAN has a more complex rule of constructing graphs and is claimed to be more robust to density changing than DBSCAN. The mutual reachable distance replaced euclidean distance as the indicators of graph construction, which makes it more robust to density changes. Moreover, with the help of the cluster hierarchy, DBSCAN can automatically adapt to data with different distributions. However, by introducing the concept of mutual reachable distance, HDBSCAN intuitively assumes that the points with lower density are more likely to be seas (noise points) that separate lands (valid clusters), which is not the case in Li-DAR point clouds where low-density point clouds can also be valid instances that are far away from the LiDAR sensor. Thus DBSCAN and HDBSCAN can not provide highquality clustering results.</p><p>Mean Shift <ref type="bibr" target="#b7">[8]</ref>. Mean Shift performs clustering in a very different way than the above three algorithms. Firstly, seeding points are sampled from the points to be clustered. Then, seeding points are iteratively shifted towards cluster centers in order to obtain centers of all clusters. The positions where seeding points are shifted to is calculated by applying the kernel function on corresponding points. In our implementation, we use the flat kernel which takes the mean of all points within a query ball as the result. The radius of the ball is denoted bandwidth. After several iterations, all the shifted points have converged and the cluster centers are extracted from the converged points. All the points to be clustered are assigned to the nearest cluster centers, which produces the final instance IDs. The advantage of Mean Shift is that the kernel function is not sensitive to density changes and robust to noise points, which makes it more suitable than density-based clustering algorithms.</p><p>However, Mean Shift is not perfect. The choice of parameters of the kernel function, which is the bandwidth in this case, is not trivial. The bandwidth controls the range that the kernel function is applied on. Small bandwidth would mislead the regressed centers of a single instance shifting to several different cluster centers and cause oversegmentation. On the contrary, large bandwidth would mislead regressed centers of neighboring instances shifting to one cluster center and result in under-segmentation. The performance of the classes with relatively small size drops with the bandwidth increasing and vice versa. Therefore, the fixed bandwidth can not handle large and small instances simultaneously. Besides, assigning each point to the cluster centers is not reasonable for that nearby instances may have different sizes which would lead to a different degree of dispersion of regressed centers. For example, edge points of a large instance may be farther to its center than that of nearby instances. Although Mean Shift is not as bad as density-based clustering algorithms, there remains a lot of room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Gradient Calculation of Dynamic Shifting</head><p>In this chapter, we are going to show the gradients of one dynamic shifting iteration and that directly regressing bandwidth with flat kernel is not differentiable. The forward pass of dynamic shifting can be broken down to the following steps:</p><formula xml:id="formula_8">K j = (XX T ≤ δ j ) (6) D j = K j 1 (7) S j = D −1 j K j X (8) w j = f (F, p j )<label>(9)</label></formula><formula xml:id="formula_9">W j = w j 1 1×3 (10) Y j = W j S j (11) Y = l j=1 Y j ,<label>(12)</label></formula><p>where X represents the seeding points, f is the combination of Softmax and MLP, p j is the parameter of f , F is the backbone features, Y is the shifted seeding points, and denotes element-wise product. It is worth noting that S j , which is defined by equation 6, 7 and 8, is constant and therefore does not have gradients. Assuming c is the loss, backpropagation gradients are calculated as follows.</p><formula xml:id="formula_10">∂c ∂F = ∂f (F, p j ) ∂F ∂c ∂w j (13) ∂c ∂p j = ∂f (F, p j ) ∂p j ∂c ∂w j (14) ∂c ∂w j = ∂c ∂W j 1 3×1 (15) ∂c ∂W j = S j ∂c ∂Y j (16) ∂c ∂Y j = ∂c ∂Y<label>(17)</label></formula><p>However, if the bandwidth δ is learnable, then equation 6 will turn to:</p><formula xml:id="formula_11">K = (XX T ≤ δ),<label>(18)</label></formula><p>which unfortunately is not differentiable. Therefore, in our ablation study, in order to further demonstrate that direct regression is not a good strategy, we adopt the Gaussian kernel which is formally defined as:</p><formula xml:id="formula_12">K = exp(− XX T 2δ 2 ),<label>(19)</label></formula><p>where δ is the learnable bandwidth. With the Gaussian kernel, the direct regression version of dynamic shifting is now differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Further Analyses of the Number of Iterations</head><p>Number of Iterations Settings. In the dynamic shifting module, other than bandwidth candidates, the hyperparameter to tune is the number of iterations, which is essential for the final clustering quality because too few iterations would cause insufficient convergence while too many iterations would add unnecessary time and space complexity. As shown in <ref type="table" target="#tab_6">Table 5</ref>, we experiment on several different iteration number settings. The best result is achieved when the number of iterations is set to 4 which is the counterpoint of sufficient convergence and efficiency.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head><p>Backbone. For both datasets, each input point is represented as a 4 dimension vector including XYZ coordinates and the intensity. The backbone voxelizes a single frame to 480 × 360 × 32 voxels under the cylindrical coordinate system. For that we should not use the information of bounding boxes in this segmentation task, the ground truth center of each instance is approximated by the center of its tight box that parallel to axes which makes a better approximation than the mass centers of the incomplete point clouds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Per-class Evaluation Results</head><p>Detailed per-class PQ, RQ and SQ results are presented in table 6, 7 and 8 respectively. All the results are reported on the held-out test set of SemanticKITTI. " * " denotes the unpublished method which is in 2nd place on the public benchmark of SemanticKITTI (accessed on 2020- <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. Compared to semantic segmentation + detection baseline methods, our DS-Net has huge advantages in things classes in terms of PQ, RQ and SQ. With the help of the dynamic shifting module, our DS-Net surpasses the backbone (with fusion module) in all classes in all three metrics, which demonstrates the effectiveness of the novel dynamic shifting module. Moreover, our DS-Net shows superiority in most classes compared with 2nd place method "Polarnet seg". <ref type="figure">Fig. 8</ref> gives the screenshot of the public leaderboard of SemanticKITTI at 2020-11-16 and our DS-Net achieves 1st place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">More Visualization Results of the DS-Net</head><p>We further show the qualitative comparison of our backbone and DS-Net. As shown in <ref type="figure" target="#fig_0">Fig. 9, 10</ref> and 11, a total of 9 LiDAR frames from the validation set of SemanticKITTI are taken out for visualization. The left columns are the visualization of the results of our bare backbone with the consensus-driven fusion module. The middle columns show the results of the DS-Net and the right columns are the ground truth. For each frame, a region of interest (as framed in red) is zoomed in to show that our DS-Net is capable of correctly handling instances with different sizes and densities while our backbone method tends to either oversegment or undersegment in these complex cases. Please note that all the stuff classes points are colored following the official definition while the colors of things instances are randomly picked. To highlight the segmentation results of things classes in the regions of interest, we only color the things class points while the stuff class points are colored as gray. Since the order of the predicted instance IDs is different from that of the ground truth, the colors of the things instances cannot correspond in predictions and the ground truth.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DS-Net</head><p>Ground Truth 08/001104 08/001442 08/001702 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Shift Candidates c ij with Kernel Functions k j for Each Regressed Centers p i ii) Calculate Shift Targets x i As shown in (a), LiDAR-based panoptic segmentation requires instance-level segmentation for things classes and semantic-level segmentation for stuff classes. (b) shows the core operation of the proposed dynamic shifting where several shift candidates are weighted to obtain the optimal shift target for each regressed center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of the DS-Net. The DS-Net consists of the cylinder convolution, a semantic and an instance branch as shown in the upper part of the figure. The regressed centers provided by the instance branch are clustered by the novel dynamic shifting module which is shown in the bottom half. The consensus-driven fusion module unifies the semantic and instance results into the final panoptic segmentation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) counts the average number of regressed centers inside each valid voxel of instances at different distances. (b) shows the effect of Different Mean Shift Bandwidth on the Recognition Quality of Different Classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Ablation Study of the Overall Framework (b) Ablation Study of Clustering Algorithms (c) Ablation Study of Bandwidth Learning Style</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Ablation Study on the Validation Set of SemanticKITTI. The proposed two modules both contributes to the final performance of the DS-Net. The dynamic shifting module has advantages in clustering LiDAR point clouds. Weighting on bandwidth candidates is better than directly regressing bandwidth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Proportional Relationship Between Sizes and the Learned Bandwidths. The x-axis represents the classwise average size of regressed centers of instances while the y-axis stands for the average learned bandwidth of different things classes.Visualization of Dynamic Shifting Iterations. As visualized inFig. 6, the black points are the original point clouds of different instances including person, bicyclist and car. The seeding points are colored in spectral colors where the Visualization of Dynamic Shifting Iterations.The black points are the original LiDAR point clouds of instances. The colored points are seeding points. From left to right, with the iteration number increases, the seeding points converge to cluster centers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Relationship Between Iterations and the Learned Bandwidths. With number of iteration increases, the learned bandwidth decreases. At the 4th iteration, the learned bandwidths of most classes drop near the lower limit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Screenshot of the public leaderboard (https://competitions.codalab.org/competitions/ 24025) of SemanticKITTI at 2020-11-16. Our method achieves 1st place. Qualitative Comparison of the Backbone and DS-Net (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Qualitative Comparison of the Backbone and DS-Net (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>LiDAR-based panoptic segmentation results on the validation set of SemanticKITTI. All results in [%].</figDesc><table><row><cell>Method</cell><cell>PQ</cell><cell>PQ  †</cell><cell>RQ</cell><cell>SQ</cell><cell cols="7">PQ Th RQ Th SQ Th PQ St RQ St SQ St mIoU</cell></row><row><cell cols="5">KPConv [25] + PV-RCNN [24] 51.7 57.4 63.1 78.9</cell><cell>46.8</cell><cell>56.8</cell><cell>81.5</cell><cell>55.2</cell><cell>67.8</cell><cell>77.1</cell><cell>63.1</cell></row><row><cell>PointGroup [14]</cell><cell cols="4">46.1 54.0 56.6 74.6</cell><cell>47.7</cell><cell>55.9</cell><cell>73.8</cell><cell>45.0</cell><cell>57.1</cell><cell>75.1</cell><cell>55.7</cell></row><row><cell>LPASD [19]</cell><cell cols="2">36.5 46.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>28.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.7</cell></row><row><cell>DS-Net</cell><cell cols="4">57.7 63.4 68.0 77.6</cell><cell>61.8</cell><cell>68.8</cell><cell>78.2</cell><cell>54.8</cell><cell>67.3</cell><cell>77.1</cell><cell>63.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>LiDAR-based panoptic segmentation results on the test set of SemanticKITTI. All results in [%]. " * " denotes the unpublished method which is in the 2nd place on the public benchmark of SemanticKITTI (accessed on 2020-11-16).</figDesc><table><row><cell>Method</cell><cell>PQ</cell><cell>PQ  †</cell><cell>RQ</cell><cell>SQ</cell><cell cols="5">PQ Th RQ Th SQ Th PQ St RQ St SQ St mIoU</cell></row><row><cell>KPConv [25] + PointPillars [18]</cell><cell cols="4">44.5 52.5 54.4 80.0</cell><cell>32.7</cell><cell>38.7</cell><cell>81.5</cell><cell>53.1</cell><cell>65.9</cell><cell>79.0</cell><cell>58.8</cell></row><row><cell cols="5">RangeNet++ [20] + PointPillars [18] 37.1 45.9 47.0 75.9</cell><cell>20.2</cell><cell>25.2</cell><cell>75.2</cell><cell>49.3</cell><cell>62.8</cell><cell>76.5</cell><cell>52.4</cell></row><row><cell>KPConv [25] + PV-RCNN [24]</cell><cell cols="4">50.2 57.5 61.4 80.0</cell><cell>43.2</cell><cell>51.4</cell><cell>80.2</cell><cell>55.9</cell><cell>68.7</cell><cell>79.9</cell><cell>62.8</cell></row><row><cell>LPASD [19]</cell><cell cols="4">38.0 47.0 48.2 76.5</cell><cell>25.6</cell><cell>31.8</cell><cell>76.8</cell><cell>47.1</cell><cell>60.1</cell><cell>76.2</cell><cell>50.9</cell></row><row><cell>PolarNet seg  *</cell><cell cols="4">53.3 59.8 64.2 81.1</cell><cell>52.1</cell><cell>59.5</cell><cell>86.9</cell><cell>54.2</cell><cell>67.6</cell><cell>76.9</cell><cell>58.9</cell></row><row><cell>DS-Net</cell><cell cols="4">55.9 62.5 66.7 82.3</cell><cell>55.1</cell><cell>62.8</cell><cell>87.2</cell><cell>56.5</cell><cell>69.5</cell><cell>78.7</cell><cell>61.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>and 2 shows that the DS-Net</cell></row><row><cell>outperforms all baseline methods in both validation and test</cell></row><row><cell>splits by a large margin. The DS-Net surpasses the best</cell></row><row><cell>baseline method KPConv + PV-RCNN in most metrics and</cell></row></table><note>especially has the advantage of 6% and 15% in terms of PQ and PQ Th in validation split. In test split, the DS-Net outperforms KPConv + PV-RCNN by 5.7% and 11.9% in PQ and PQ Th . On the leaderboard provided by [3], our DS- Net achieves 1st place and surpasses 2nd method "Polar- Net seg" by 2.6% and 3.0% in PQ and PQ Th respectively. It is worth noting that PointGroup [14] performs poorly on the LiDAR point clouds which shows that indoor solutions are not suitable for challenging LiDAR point clouds. Further detailed results on SemanticKITTI can be found in supple- mentary materials.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>LiDAR-based panoptic segmentation results on the validation set of nuScenes. All results in [%].</figDesc><table><row><cell>Method</cell><cell>PQ</cell><cell>PQ  †</cell><cell>RQ</cell><cell>SQ</cell><cell cols="5">PQ Th RQ Th SQ Th PQ St RQ St SQ St mIoU</cell></row><row><cell cols="5">Cylinder3D [37] + PointPillars [18] 36.0 44.5 43.0 83.3</cell><cell>23.3</cell><cell>27.0</cell><cell>83.7</cell><cell>57.2</cell><cell>69.6</cell><cell>82.7</cell><cell>52.3</cell></row><row><cell>Cylinder3D [37] + SECOND [32]</cell><cell cols="4">40.1 48.4 47.3 84.2</cell><cell>29.0</cell><cell>33.6</cell><cell>84.4</cell><cell>58.5</cell><cell>70.1</cell><cell>83.7</cell><cell>58.5</cell></row><row><cell>DS-Net</cell><cell cols="4">42.5 51.0 50.3 83.6</cell><cell>32.5</cell><cell>38.3</cell><cell>83.1</cell><cell>59.2</cell><cell>70.3</cell><cell>84.4</cell><cell>70.7</cell></row><row><cell cols="4">comparable to the instance sizes. Unlike previous heuristic</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">clustering algorithms that require massive parameter adjust-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ment, DS-Net can automatically adjust to different instance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">sizes and point distributions and remains stable clustering</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">quality. Further analyses on the iteration number settings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>are shown in supplementary materials.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results of different bandwidth candidates settings.</figDesc><table><row><cell>All results in [%].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bandwidth Candidates (m)</cell><cell>PQ</cell><cell>PQ  †</cell><cell>RQ</cell><cell>SQ</cell><cell>mIoU</cell></row><row><cell>0.2, 1.1, 2.0</cell><cell cols="4">57.4 63.0 67.7 77.4</cell><cell>63.7</cell></row><row><cell>0.2, 1.3, 2.4</cell><cell cols="4">57.5 63.1 67.7 77.6</cell><cell>63.5</cell></row><row><cell>0.2, 1.5, 2.8</cell><cell cols="4">57.6 63.2 67.8 77.6</cell><cell>63.7</cell></row><row><cell>0.2, 1.7, 3.2</cell><cell cols="4">57.7 63.4 68.0 77.6</cell><cell>63.5</cell></row><row><cell>0.2, 1.9, 3.6</cell><cell cols="4">57.7 63.3 67.9 77.6</cell><cell>63.4</cell></row><row><cell>0.2, 2.1, 4.0</cell><cell cols="4">57.4 63.1 67.7 77.5</cell><cell>63.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Experiments on the number of iteration. All results in [%].Learned Bandwidths of Different Iterations. The average learned bandwidths of different iterations are shown inFig. 7. As expected, as the iteration rounds grow, points of the same instance gather tighter which usually require smaller bandwidths. After four iterations, learned bandwidths of most classes have dropped to 0.2, which is the lowest they can get, meaning that four iterations are enough for things points to converge to cluster centers, which further validates the conclusion made in the last paragraph.</figDesc><table><row><cell>Number of Iteration</cell><cell>PQ</cell><cell>PQ  †</cell><cell>RQ</cell><cell>SQ</cell><cell>mIoU</cell></row><row><cell>1</cell><cell cols="4">57.0 62.6 67.4 77.3</cell><cell>63.5</cell></row><row><cell>2</cell><cell cols="4">57.6 63.1 67.8 77.5</cell><cell>63.6</cell></row><row><cell>3</cell><cell cols="4">57.7 63.3 68.0 77.6</cell><cell>63.4</cell></row><row><cell>4</cell><cell cols="4">57.7 63.4 68.0 77.6</cell><cell>63.5</cell></row><row><cell>5</cell><cell cols="4">57.5 63.2 67.7 77.6</cell><cell>63.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The bandwidth of the Mean Shift used in our backbone method is set to 1.2. Adam solver is utilized to optimize the network. The minimum number of points in a valid instance is set to 50 for SemanticKITTI and 5 for nuScenes. Dynamic Shifting Module. The number of the FPS downsampled points in the dynamic shifting module is set to 10000. The final heuristic clustering algorithm used in the dynamic shifting module is Mean Shift with 0.65 bandwidth for SemanticKITTI and BFS with 1.2 radius for nuScenes. Bandwidth candidates are set to 0.2, 1.7 and 3.2 for both datasets. The number of Iterations is set to 4 for both datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Detailed per-class PQ results on the test set of SemanticKITTI. 72.5 17.2 9.2 30.8 19.6 29.9 59.4 22.8 84.6 60.1 34.1 8.8 80.7 77.6 53.9 42.2 49.0 46.2 46.8 RangeNet++[20] + PointPillars[18] 37.1 66.9 6.7 3.1 16.2 8.8 14.6 31.8 13.5 90.6 63.2 41.3 6.7 79.2 71.2 34.6 37.4 38.2 32.8 47.4 KPConv[25] + PV-RCNN[24] 50.2 84.5 21.9 9.9 34.2 25.6 51.1 67.9 43.8 84.9 63.6 37.1 8.4 83.7 78.3 57.5 42.3 51.1 51.0 57.4 PolarNet seg * 53.3 88.7 31.7 34.6 50.9 39.1 57.5 68.7 45.1 88.1 59.7 40.5 1.0 85.7 77.7 53.2 39.7 44.8 48.6 57.6 Backbone with Fusion 53.1 90.6 15.8 44.2 46.9 28.5 63.1 67.7 47.6 88.2 59.4 29.5 3.0 82.5 79.0 56.6 42.3 48.1 53.2 63.6 DS-Net 55.9 91.2 28.8 45.4 47.2 34.6 63.6 71.1 58.5 89.1 61.2 32.3 4.0 83.2 79.6 58.3 43.4 50.0 55.2 65.3</figDesc><table><row><cell>Method</cell><cell>PQ</cell><cell>car</cell><cell>truck</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>sidewalk</cell><cell>parking</cell><cell>other ground</cell><cell>building</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>fence</cell><cell>pole</cell><cell>traffic sign</cell></row><row><cell>KPConv[25] + PointPillars[18]</cell><cell>44.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Detailed per-class RQ results on the test set of SemanticKITTI. PV-RCNN[24] 61.4 94.5 26.9 14.5 43.6 32.0 70.0 76.8 52.6 91.5 78.7 47.7 11.4 90.2 94.1 76.4 56.5 66.8 68.8 74.1 PolarNet seg * 64.2 96.2 36.0 48.5 58.6 43.0 66.2 77.1 50.3 96.3 74.8 54.2 1.7 92.1 94.1 72.6 53.9 61.1 66.0 76.3 Backbone with Fusion 64.5 97.4 20.5 61.1 57.2 33.2 74.0 75.6 57.8 96.2 75.1 39.2 5.0 88.8 95.5 75.8 57.1 63.8 72.2 80.4 DS-Net 66.7 97.5 32.4 62.2 56.3 38.9 74.3 78.4 62.7 96.8 76.7 42.6 6.4 89.3 95.7 77.5 58.3 65.5 74.0 81.9</figDesc><table><row><cell>Method</cell><cell>RQ</cell><cell>car</cell><cell>truck</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>sidewalk</cell><cell>parking</cell><cell>other ground</cell><cell>building</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>fence</cell><cell>pole</cell><cell>traffic sign</cell></row><row><cell>KPConv[25] +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Detailed per-class SQ results on the test set of SemanticKITTI. PV-RCNN[24] 80.0 89.4 81.3 68.1 78.4 79.8 73.0 88.4 83.3 92.9 80.8 77.8 73.4 92.8 83.2 75.2 75.0 76.5 74.2 77.5 PolarNet seg * 81.1 92.2 88.0 71.4 86.9 90.8 87.0 89.1 89.8 91.4 79.9 74.8 55.4 93.1 82.6 73.3 73.8 73.2 73.6 75.5 Backbone with Fusion 80.3 93.0 77.0 72.3 81.9 85.8 85.3 89.5 82.5 91.8 79.1 75.1 60.2 92.9 82.7 74.7 74.0 75.5 73.6 79.1 DS-Net 82.3 93.6 88.9 73.0 83.8 89.0 85.6 90.7 93.3 92.0 79.8 75.8 61.4 93.2 83.2 75.2 74.4 76.3 74.5 79.7</figDesc><table><row><cell>Method</cell><cell>SQ</cell><cell>car</cell><cell>truck</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>sidewalk</cell><cell>parking</cell><cell>other ground</cell><cell>building</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>fence</cell><cell>pole</cell><cell>traffic sign</cell></row><row><cell>KPConv[25] +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Sven Behnke, Cyrill Stachniss, and Jurgen Gall</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A benchmark for lidar-based panoptic segmentation based on kitti</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02371</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The lovász-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11027</idno>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Density-based clustering based on hierarchical density estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jgb</forename><surname>Ricardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davoud</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Moulavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia conference on knowledge discovery and data mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorin</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bastian Leibe, and Matthias Nießner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kdd</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Occuseg: Occupancy-aware 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2940" to="2949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointgroup: Dual-set point grouping for 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent pixel embedding for instance grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9018" to="9028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Marc Pollefeys, and Martin R Oswald. 3d instance segmentation via multi-task metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9256" to="9266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lidar panoptic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Jsis3d: joint semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8827" to="8836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kpconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Associatively segmenting instances and semantics in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4096" to="4105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Identifying unknown instances for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Robot Learning (CORL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Spatial semantic embedding network: Fast 3d instance segmentation with deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junha</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Kyun</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">Min</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03169,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Instance segmentation of lidar point clouds. ICRA, Cited by</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<idno>2020. 3</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9601" to="9610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Jsnet: Joint instance and semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12951" to="12958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Cylinder3d: An effective 3d framework for driving-scene lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01550</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Qualitative Comparison of the Backbone and DS-Net</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
