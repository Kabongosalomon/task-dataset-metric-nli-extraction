<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-14">14 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeyu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-14">14 Jul 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic Segmentation</term>
					<term>Semantic Edge Detection</term>
					<term>3D Point Clouds</term>
					<term>3D Scene Understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation and semantic edge detection can be seen as two dual problems with close relationships in computer vision. Despite the fast evolution of learning-based 3D semantic segmentation methods, little attention has been drawn to the learning of 3D semantic edge detectors, even less to a joint learning method for the two tasks. In this paper, we tackle the 3D semantic edge detection task for the first time and present a new two-stream fully-convolutional network that jointly performs the two tasks. In particular, we design a joint refinement module that explicitly wires region information and edge information to improve the performances of both tasks. Further, we propose a novel loss function that encourages the network to produce semantic segmentation results with better boundaries. Extensive evaluations on S3DIS and ScanNet datasets show that our method achieves on par or better performance than the state-of-the-art methods for semantic segmentation and outperforms the baseline methods for semantic edge detection. Code release: https://github.com/hzykent/JSENet</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation (SS) and semantic edge detection (SED) are two fundamental problems for scene understanding. The former aims to parse a scene and assign a class label to each pixel in images or each point in 3D point clouds. The latter focuses on detecting edge pixels or edge points and classifying each of them to one or more classes. Interestingly, the SS and the SED tasks can be seen as two dual problems with even interchangeable outputs in an ideal case (see <ref type="figure" target="#fig_0">Fig. 1</ref>). While SS has been extensively studied in both 2D and 3D <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, SED has only been explored in 2D <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, to our best knowledge.</p><p>There are strong motivations to address both problems in a joint learning framework. On one hand, previous SS models tend to struggle in edge areas since these areas constitute only a small part of the whole scene and thus have little effect on the supervision during training <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Performing the complementary SED task simultaneously may help the network sharpen the boundaries of the predicted SS results <ref type="bibr" target="#b14">[15]</ref>. On the other hand, existing SED models are easily affected by non-semantic edges in the scene <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>, while a trained SS model is less sensitive to those edges <ref type="bibr" target="#b15">[16]</ref>. Information from the SS model thus may help SED models suppress network activations on the non-semantic edges. Despite the close relationships of the two tasks, there is no existing work tackling them jointly as far as we know.</p><p>Although not focusing on the SS and the SED tasks, in 2D, several existing works have already made fruitful attempts at proposing joint learning methods for complementary segmentation and edge detection tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">[17]</ref><ref type="bibr">[18]</ref><ref type="bibr">[19]</ref><ref type="bibr">[20]</ref>. These works often treat the two tasks na√Øvely by sharing parts of their networks and limit the interactions between them to the feature domain. Predicted segmentation masks and edge maps are used only for loss calculation and do not contribute to each other. Strong links between the outputs of the two tasks are not fully exploited.</p><p>In this work, we introduce the task of SED into the 3D field and propose JSENet, a new 3D fully-convolutional network (FCN) for joint SS and SED. The proposed network consists of two streams and a joint refinement module on top of them. Specifically, we use a classical encoder-decoder FCN for SS in one stream, which outputs semantic segmentation point (SSP) masks, and add an SED stream outputting semantic edge point (SEP) maps in parallel. The key to our architecture is the lightweight joint refinement module. It takes the output SSP masks and SEP maps as inputs and jointly refines them by explicitly exploiting the duality between them. Moreover, we further propose a novel loss function to encourage the predicted SSP masks to better align with the ground truth semantic edges.</p><p>To summarize, our contributions are threefold:</p><p>1. We introduce the task of SED into the 3D field and design an FCN-based architecture with enhanced feature extraction and hierarchical supervision to generate precise semantic edges.</p><p>2. We propose a novel framework for joint learning of SS and SED, named JSENet, with an effective lightweight joint refinement module that brings improvements by explicitly exploiting the duality between the two tasks. 3. We propose a dual semantic edge loss, which encourages the network to produce SS results with finer boundaries.</p><p>To build our FCN network in 3D, we resort to KPConv [21], a recently proposed kernel-based convolution method for point clouds, for its state-of-theart performance and ease of implementation. We conduct extensive experiments to demonstrate the effectiveness of our method. Since there is no existing 3D SED dataset, we construct a new 3D SED benchmark using S3DIS <ref type="bibr" target="#b11">[12]</ref> and ScanNet <ref type="bibr" target="#b6">[7]</ref> datasets. We achieve state-of-the-art performance for the SS task with IoU scores of 67.7% on S3DIS Area-5 and 69.9% on ScanNet test set. For the SED task, our method outperforms the baseline methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Semantic Segmentation</head><p>Based on different data representations, 3D SS methods can be roughly divided into three categories: multiview image-based, voxel-based, and point-based. Our method falls into the point-based category.</p><p>Although the multiview image-based methods easily benefit from the success of 2D CNN [22, 23], for SS, they suffer from occluded surfaces and density variations, and rely heavily on viewpoint selection. Based on powerful 3D CNN [24-30], voxel-based methods achieve the best performance on several 3D SS datasets, but they need intensive computation power.</p><p>Compared with the previously mentioned methods, point-based methods suffer less from information loss and thus achieve high point-level accuracy with less computation power consumption <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. They can be generally classified into four categories: neighboring feature pooling <ref type="bibr">[31]</ref><ref type="bibr">[32]</ref><ref type="bibr">[33]</ref><ref type="bibr" target="#b16">[34]</ref>, graph construction <ref type="bibr" target="#b17">[35]</ref><ref type="bibr" target="#b18">[36]</ref><ref type="bibr" target="#b19">[37]</ref><ref type="bibr" target="#b20">[38]</ref>, attention-based aggregation <ref type="bibr" target="#b21">[39]</ref>, and kernel-based convolution [21, <ref type="bibr" target="#b22">[40]</ref><ref type="bibr" target="#b23">[41]</ref><ref type="bibr" target="#b24">[42]</ref><ref type="bibr" target="#b25">[43]</ref><ref type="bibr" target="#b26">[44]</ref><ref type="bibr" target="#b27">[45]</ref><ref type="bibr" target="#b28">[46]</ref>. Among all the point-based methods, the recently proposed kernel-based method KPConv [21] achieves the best performance for efficient 3D convolution. Thus, we adopt KPConv to build our backbone and refinement network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">2D Semantic Edge Detection</head><p>Learning-based SED dates back to the early work of Prasad et al. <ref type="bibr" target="#b29">[47]</ref>. Later, Hariharan et al. <ref type="bibr" target="#b30">[48]</ref> introduced the first Semantic Boundaries Dataset. After the dawn of deep learning, the HFL method <ref type="bibr" target="#b31">[49]</ref> builds a two-stage prediction process by using two deep CNNs for edge localization and classification, respectively. More recently, CASENet <ref type="bibr" target="#b7">[8]</ref> extended the CNN-based class-agnostic edge detector HED <ref type="bibr" target="#b32">[50]</ref> to a class-aware semantic edge detector by combining lowand high-level features with a multi-label loss function for supervision. Later, several follow-up works <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b33">51]</ref> improved CASENet by adding diverse deep supervision and reducing annotation noises.</p><p>In 2D images, semantic edges of different classes are weakly related since they are essentially occlusion boundaries of projected objects. Based on this observation, 2D SED methods treat SED of different classes as independent binary classification problems and utilize structures that limit the interaction between different classes like group convolution modules. Unlike the ones in 2D, semantic edges in 3D are physical boundaries of objects and thus are highly related to each other. In this work, we study the problem of 3D SED for the first time. We adopt from 2D methods the idea of extracting enhanced features and construct a network that does not limit the interaction between different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Joint Learning of Segmentation and Edge Detection</head><p>For 2D images, several works have explored the idea of combining networks for complementary segmentation and edge detection tasks to improve the learning efficiency, prediction accuracy, and generalization ability <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">[17]</ref><ref type="bibr">[18]</ref><ref type="bibr">[19]</ref><ref type="bibr">[20]</ref>.</p><p>To be more specific, for salient object detection, researchers have exploited the duality between the binary segmentation and class-agnostic edge detection tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">20]</ref>. As for SS, such class-agnostic edges are used to build semantic segmentation masks with finer boundaries <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">[17]</ref><ref type="bibr">[18]</ref><ref type="bibr">[19]</ref>. In contrast, we tackle the problem of joint learning for SS and class-aware SED. Furthermore, unlike previous works, which limit the interactions between segmentation and edge detection to the sharing of features and network structures, our method exploits the close relationship between SSP masks and SEP maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">JSENet</head><p>In this section, we present our JSENet architecture for the joint learning of SS and class-aware SED. As depicted in <ref type="figure">Fig. 2</ref>, our architecture consists of two streams of networks with a shared feature encoder and followed by a joint refinement module. The first stream of the network, the SS stream, is a standard encoder-decoder FCN for SS using the same structure as presented in KPConv <ref type="bibr">[21]</ref>. The second stream, the SED stream, is another FCN with enhanced feature extraction and hierarchical supervision. We then fuse the outputs from the two streams using our carefully designed joint refinement module to produce refined SSP masks and refined SEP maps. Next, we will describe each of the modules in detail and then explain the supervisions used for joint learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic Segmentation Stream</head><p>We denote the SS stream as S Œ∏ (P) with parameters Œ∏, taking a point cloud P ‚àà R N √ó6 (x, y, z, r, g, b) with N points as input and outputting an SSP mask. More specifically, for a segmentation prediction of K semantic classes, it outputs a categorical distribution s ‚àà R N √óK : s(p|P, Œ∏) representing the probability of <ref type="figure">Fig. 2</ref>: JSENet architecture. Our architecture consists of two main streams. The SS stream can be any fully-convolutional network for SS. The SED stream extracts enhanced features through a skip-layer structure and is supervised by multiple loss functions. A joint refinement module later combines the information from the two streams and outputs refined SSP masks and SEP maps.</p><p>point p belonging to each of the K classes. We supervise this stream with the standard multi-class cross-entropy loss (L seg in <ref type="figure">Fig. 2</ref>). The SS stream can be any feedforward 3D fully-convolutional SS network such as InterpCNNs <ref type="bibr" target="#b28">[46]</ref> and SSCNs <ref type="bibr">[29]</ref>. In this work, we adopt KPConv [21] and use it as our backbone network for its efficient convolution operation and ability to build complex network structures. There are two types of KPConv: rigid and deformable. We use the rigid version in this work for its better convergence performance. In order to demonstrate the improvements introduced by joint learning, we use the same structure as described in KPConv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Edge Detection Stream</head><p>We denote the SED stream as E œÜ (P) with parameters œÜ, taking a point cloud P with N points as input and outputting SEP maps. For K defined semantic categories, this stream outputs K SEP maps {e 1 , ..., e K }, each having the same size as P with one channel. We denote e k (p|P, œÜ) as the network output, which indicates the computed edge probability of the k-th semantic category at point p. Note that one point may belong to multiple categories.</p><p>The SED stream shares the same feature encoder with the SS stream to force the network to prefer representations with a better generalization ability for both tasks. Our backbone is an FCN-based structure. However, one major drawback of an FCN-based structure is the loss in spatial information of the output after propagating through several alternated convolutional and pooling layers. This drawback would harm the localization performance, which is essential for our SED task. Besides, according to the findings of CASENet <ref type="bibr" target="#b7">[8]</ref>, the low-level features of CNN-based encoder are not suitable for semantic classification (due to the limited receptive fields) but are able to help augment top classifications by suppressing non-edge activations and providing detailed edge localization information. Based on these observations, we propose to extract enhanced features with hierarchical supervisions to alleviate the problem of spatial information loss and offer richer semantic cues for final classification.</p><p>Enhanced feature extraction. In detail, we extract the feature maps generated by the layers of the shared encoder ( <ref type="figure">Fig. 2(a)</ref>). We then reduce the numbers of their feature channels and deconvolve them to the size of the input point cloud. The features of different layers participate and augment the final SED through a skip-layer architecture, as shown in <ref type="figure">Fig. 2</ref></p><formula xml:id="formula_0">(b).</formula><p>Hierarchical supervision. As shown in <ref type="figure">Fig. 2(c)</ref>, from the extracted feature maps of the first three layers, we generate binary edge point maps indicating the probability of points belonging to the semantic edges of any classes. From the last two layers, we generate two SSP masks. All binary edge point maps are supervised by the weighted binary cross-entropy loss (L bce ) using ground-truth (GT) binary edges obtained from the GT SSP masks. The two SSP masks are supervised by the standard multi-class cross-entropy loss (L seg ). The output SEP maps of the SED stream are supervised by a weighted multi-label loss (L edge ) following the idea from CASENet <ref type="bibr" target="#b7">[8]</ref>. We will give details about different loss functions in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Joint Refinement Module</head><p>We denote the joint refinement module as R Œ≥ (s, e 1 , ..., e K ) with parameters Œ≥, taking as input the SSP mask s coming from the SS stream and the SEP maps {e 1 , ..., e K } generated by the SED stream. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, we construct a twobranch structure with simple feature fusion sub-modules ( <ref type="figure" target="#fig_1">Fig. 3(a)</ref>) and novel edge map generation sub-modules ( <ref type="figure" target="#fig_1">Fig. 3(b)</ref>). The upper branch is responsible for segmentation refinement, and the lower one is for edge refinement. We feed different joint features (described in detail below) to the feature fusion submodules of the two branches and generate refined SSP masks and SEP maps.</p><p>Feature fusion sub-module. To fuse the region and edge features, we construct two simple U-Net <ref type="bibr" target="#b34">[52]</ref> like feature fusion sub-modules for each refinement branch. In detail, each feature fusion sub-module consists of five encoding layers with channel sizes of 32 for all layers. For segmentation refinement, the feature fusion sub-module takes the concatenated SSP mask s ‚àà R N √óK and SEP maps {e 1 , ..., e K } (e i ‚àà R N ) as input and outputs a refined SSP mask directly. As for edge refinement, we find that adjusting the activation values of the SEP maps is more effective than asking the neural network to output refined SEP maps na√Øvely. Thus, we put the unrefined SEP maps through a sigmoid operation and concatenate them with the edge activation point maps {a 1 , ..., a K } (a i ‚àà R N ) generated by the edge map generation sub-module. The concatenated features are then fed to the feature fusion sub-module to generate auxiliary point maps, which are added to the unrefined SEP maps to adjust the edge activation values.</p><p>Edge map generation sub-module. In order to exploit the duality between two tasks, we design an edge map generation sub-module to convert an SSP mask to edge activation point maps. More formally, we denote the edge map generation sub-module as G(s), which takes a categorical distribution s ‚àà R N √óK as input and outputs edge activation point maps {a 1 , ..., a K } (a i ‚àà R N ), where a i (p) is a potential that represents whether a particular point p belongs to the semantic boundary of the i-th class. It is computed by taking a spatial derivative on the segmentation output as follows:</p><formula xml:id="formula_1">a i = col i (|M * Sof tmax(s) ‚àí Sof tmax(s)|),<label>(1)</label></formula><p>where col i denotes the i-th column and M denotes the Mean filter, which takes neighboring points within a small radius. As illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>, the i-th column of the output tensor of a softmax operation represents an activation point mask for class i, where a higher value indicates a higher probability of belonging to class i. On this mask, points near to the boundaries of class i will have neighbors with activation values of significant differences, and points far from the boundaries will have neighbors with similar activation values. Thus, after the mean filtering and subtraction, points nearer to the predicted boundaries will have larger activation values. The converted edge activation point maps are fed to the edge refinement branch and utilized for loss calculation as well.</p><p>Supervisions. For supervision, the output SSP masks are supervised by the multi-class cross-entropy loss (L seg ), and the output SEP maps are supervised by the weighted multi-label loss (L edge ). Additionally, we supervise the generated edge activation point maps with our proposed dual semantic edge loss (L dual ) to encourage the predicted SS results to align with the GT semantic edges correctly (see Section 3.4). After refinement, the final output SSP mask is normalized by a softmax operation. The final output SEP maps are normalized by a sigmoid operation and added element-wise with the final edge activation point maps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Joint Multi-task Learning</head><p>The key to joint learning of the SS and the SED tasks is to design a proper supervision signal. The total loss is formulated as:</p><formula xml:id="formula_2">L total = Œª 0 L seg + Œª 1 L edge + Œª 2 L bce + Œª 3 L dual .<label>(2)</label></formula><p>During training, Œª 0 is set to the number of semantic classes to balance the influences of the two tasks. The other weights are set to 1. We describe in detail all the loss functions used for supervision in our framework below. Multi-class cross-entropy loss. We use a standard multi-class cross-entropy loss, which denoted as L seg , on a predicted SSP mask s:</p><formula xml:id="formula_3">L seg (≈ù, s) = ‚àí k p≈ù k (p) log(s k (p)),<label>(3)</label></formula><p>where≈ù ‚àà R N √óK denotes GT semantic labels in a one-hot form.</p><p>Weighted multi-label loss. Following the idea proposed in CASENet <ref type="bibr" target="#b7">[8]</ref>, we address the SED problem for a point cloud by a multi-label learning framework and implement a point-wise weighted multi-label loss L edge . Suppose an input point cloud P has K SEP maps {e 1 , ..., e K } (e i ‚àà R N ) predicted by the network and K label point maps {√™ 1 , ...,√™ K } (√™ i ‚àà R N ), where√™ k is a binary point map indicating the ground truth of the k-th class semantic edges. The point-wise weighted multi-label loss L edge is formulated as:</p><formula xml:id="formula_4">L edge ({√™ 1 , ...,√™ K }, {e 1 , ..., e K }) = k p {‚àíŒ≤ k√™k (p) log(e k (p))‚àí (1 ‚àí Œ≤ k )(1 ‚àí√™ k (p)) log(1 ‚àí e k (p))},<label>(4)</label></formula><p>where Œ≤ k is the percentage of non-edge points in the point cloud of the k-th class to account for the skewness of sample numbers.</p><p>Weighted binary cross-entropy loss. To supervise the generated binary edge point maps in the SED stream, we implement a point-wise weighted binary crossentropy loss L bce . Let b denote the predicted binary edge point map, andb the GT binary edge point map converted from the GT SEP maps. The point-wise weighted cross-entropy loss is defined as:</p><formula xml:id="formula_5">L bce (b, b) = p {‚àíŒ≤b(p) log(b(p)) ‚àí (1 ‚àí Œ≤)(1 ‚àíb(p)) log(1 ‚àí b(p))},<label>(5)</label></formula><p>where Œ≤ is the percentage of non-edge points among all classes. Dual semantic edge loss. As mentioned above, inspired by the duality between SS and SED, we design an edge map generation sub-module to convert the predicted SSP mask s ‚àà R N √óK to edge activation point maps <ref type="figure" target="#fig_0">Equation 1</ref>). In a similar way, we can compute GT edge activation point maps {√¢ 1 , ...,√¢ K } from the GT semantic labels≈ù:</p><formula xml:id="formula_6">{a 1 , ..., a K } (a i ‚àà R N )(c.f.,</formula><formula xml:id="formula_7">a i = col i (|M * One hot(≈ù) ‚àí One hot(≈ù)|).<label>(6)</label></formula><p>Note that the softmax operation for predicted SSP mask s is changed to the one-hot encoding operation for GT semantic labels≈ù. Taking the converted GT edge activation point maps, we can define the loss function as follows:</p><formula xml:id="formula_8">L dual ({√¢ 1 , ...,√¢ K }, {a 1 , ..., a K }) = k p Œ≤(|√¢ k (p) ‚àí a k (p)|),<label>(7)</label></formula><p>where Œ≤ is the same weight as above. Intuitively, the network will get penalized when there are mismatches on edge points. It is worth noting that the loss function will not be dominated by the non-edge points since the calculated loss values on these points are zeros or very small numbers. The above dual loss is naturally differentiable and exploits the duality between SS and SED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To demonstrate the effectiveness of our proposed method, we now present various experiments conducted on the S3DIS <ref type="bibr" target="#b11">[12]</ref> and ScanNet <ref type="bibr" target="#b6">[7]</ref> datasets, for which GT SSP masks are available and we can generate GT SEP maps from them easily. We first introduce the dataset preparation and evaluation metrics in Section 4.1, and then present the implementation details for reproduction in Section 4.2. We report the results of our ablation studies in Section 4.3, and the results on the S3DIS and ScanNet datasets in Sections 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>We use S3DIS <ref type="bibr" target="#b11">[12]</ref> and ScanNet <ref type="bibr" target="#b6">[7]</ref> datasets for our experiments. There are two reasons for choosing these datasets: 1) they are both of high quality and 2) semantic edges are better defined on indoor data: compared to existing 3D outdoor datasets, in indoor scenes, more detailed semantic labels are defined and objects are more densely connected. The S3DIS dataset consists of 3D point clouds for six large-scale indoor areas captured from three different buildings. It has around 273 million points annotated with 13 semantic classes. The ScanNet dataset includes 1513 training scenes and 100 test scenes in a mesh format, all annotated with 20 semantic classes, for online benchmarking.</p><p>To generate the GT SEP maps, for S3DIS, we directly check the neighbors within a 2cm radius of each point in the dataset. For a specific point, if it has neighbors with different semantic labels, we label it as an edge point of all the semantic classes that appear in its neighborhood. As for the ScanNet dataset, following the work of KPConv [21], we first rasterize the training meshes by uniformly sampling points on the faces of meshes and then downsample the generated point clouds with 1cm grids. We use these point clouds for training and generate the GT SEP maps using the same way as described for the S3DIS dataset. During testing, we project the semantic edge labels to the vertices of the original meshes and test directly on meshes.</p><p>To evaluate the performance of SS and SED, we adopt the standard mean intersection over union (mIoU) for SS and use the mean maximum F-measure (MF) at the optimal dataset scale (ODS) for SED following the works in 2D <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. We generate thicker edges for point clouds than for images since a point cloud is much sparser than an image. Since we have thicker edges, the localization tolerance used in the 2D case is not introduced to our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>In this section, we discuss the implementation details for our experiments. JSENet is coded in Python and TensorFlow. All the experiments are conducted on a PC with 8 Intel(R) i7-7700 CPUs and a single GeForce GTX 1080Ti GPU.</p><p>Training. Since the 3D scenes in both datasets are of huge size, we randomly sample spheres with 2m radius in the training set and augment them with Gaussian noise, random scaling, and random rotation. Following the settings in KP-Conv, the input point clouds are downsampled with a grid size of 4cm. In all our experiments, unless explicitly stated otherwise, we use a Momentum gradient descent optimizer with a momentum of 0.98 and an initial learning rate of 0.01. The learning rate is scheduled to decrease exponentially. In particular, it is divided by 10 for every 100 epochs. Although the framework is end-to-end trainable, in order to clearly demonstrate the efficacy of the proposed joint refinement module, we first train our network without the joint refinement module for 350 epochs and then optimize the joint refinement module alone with the other parts fixed for 150 epochs.</p><p>Testing. Similar to the training process, during testing, we sample spheres with 2m radius from the testing set regularly and ensure each point to be sampled for multiple times. The predicted probabilities for each point are averaged through a voting scheme <ref type="bibr">[21]</ref>. All predicted values are projected to the original point clouds (S3DIS) or meshes (ScanNet) for evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we compare the performances of JSENet under different settings on the S3DIS dataset since it is originally presented in a point cloud format and all semantic labels are available. Following the common setting <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr">21,</ref><ref type="bibr">30,</ref><ref type="bibr" target="#b19">37,</ref><ref type="bibr" target="#b35">53,</ref><ref type="bibr" target="#b36">54]</ref>, we use Area-5 as a test scene and train our network on the other scenes. All experiments are conducted keeping all hyperparameters the same.</p><p>Network structures. In <ref type="table" target="#tab_0">Table 1</ref>, we evaluate the effectiveness of each component of our method. For the SS task, as shown in the table (Row 3), the performance of training our SS stream alone is 64.7% in terms of mIoU. Our SS stream shares the same architecture with KPConv and the reported score of KP-Conv is 65.4% in their paper. By na√Øvely combining the SS stream and the SED stream, we can improve the SS task by 1.5% (Row 2). From the joint refinement module, we further gain about 1.5% (Row 1) improvement in performance. We achieve about 3% improvement comparing to training our SS stream alone and still more than 2% improvement comparing to the result reported in KPConv. For the SED task, it can be seen from the table that the performance of training the SED stream alone without the enhanced feature extraction and hierarchical supervision is 29.4% (Row 6) in terms of mMF (ODS). We gain about 0.5% and 0.3% improvements in performance from the enhanced feature extraction (Row 5) and hierarchical supervision (Row 4), respectively. Na√Øvely combining the SS stream and the SED stream brings a further improvement of 0.3% (Row 2) in terms of mMF. By adding the joint refinement module, we can further improve the SED task by 0.5% (Row 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice of loss functions for hierarchical supervision.</head><p>To justify our choices of the loss functions in the SED stream for hierarchical supervision, we test the performances of SED using different settings of supervision. As shown in Table 2a, if all hierarchical supervisions are removed, the performance of our SED model will decrease by 0.3%. Among all the choices listed in the table, the one used in our network achieves the best result. Efficacy of dual semantic edge loss. We further showcase the effects of the dual semantic edge loss in terms of F-score for edge alignment of the predicted SSP masks in <ref type="table" target="#tab_1">Table 2b</ref>. We train our network without the dual semantic edge losses for the edge map generation sub-modules as the baseline. It is shown that the dual semantic edge losses bring an improvement of 0.4% in terms of F-score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on S3DIS &amp; ScanNet Datasets</head><p>To compare JSENet with the state-of-the-arts, for SS, we choose the latest methods <ref type="bibr">[3-5, 29, 30, 37, 42, 53-64]</ref> with reported results on the S3DIS or the ScanNet datasets as our baselines. For SED, since we cannot find any existing solutions in 3D, we extend CASENet <ref type="bibr" target="#b7">[8]</ref>, which is the state-of-the-art method for 2D SED to 3D for comparison. Besides, we build another baseline network using the same structure as the one presented in KPConv with a changed output layer. For the S3DIS dataset, we use the same train-test splitting setting as in Section 4.3. For the ScanNet dataset, since it is built for online benchmarking, the GT semantic labels for its test set are not available. Thus, for the SS task, we train our network using the 1513 training scenes and report the testing results on the 100 test scenes following the common setting. To evaluate the task of SED, as explained in Section 4.1, we obtain semantic edge labels from the training set and divide the 1513 training scenes into a new training set with 1201 scenes and a new test set with 312 scenes following the train-val splitting file offered by ScanNet. All the compared networks for SED are trained on the new training set and tested on the new test set. Qualitative results are shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Complexity comparison and more qualitative results can be found in supplementary.</p><p>Results of SS task. The results for the SS task are reported in <ref type="table" target="#tab_2">Table 3</ref>. The detailed IoU scores of each class for the S3DIS dataset can be found in the supplementary. The details for the ScanNet dataset can be found on the Scan-Net benchmarking website 3 . For the S3DIS dataset, JSENet achieves a 67.7% mIoU score and outperforms all the baseline methods. For the ScanNet dataset, JSENet ranks third with a 69.9% mIoU score. The first two (i.e., SparseConvNet and MinkowskiNet) are two voxel-based methods that require intense computation power. Compared to other point-based methods, JSENet achieves the best performance and consistently outperforms the results of KPConv.</p><p>Results of SED task. We present the results for the SED task in <ref type="table" target="#tab_3">Tables 4  and 5</ref>. It can be seen that our method outperforms the two baseline methods on both datasets. We find that the extended CASENet architecture performs worse than KPConv with the changed output layer. This result supports our previous argument (Section 2.2) that 3D semantic edges have stronger relevance between different classes (since they are physical boundaries of objects and the 2D semantic edges are occlusion boundaries of projected objects), and thus the network designs that enforce limitations between interactions of different classes would harm the edge detection performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we proposed a joint semantic segmentation and semantic edge detection network (JSENet), which is a new two-stream fully-convolutional architecture with a lightweight joint refinement module that explicitly wires region information and edge information for output refinement. We constructed a two-branch structure with simple feature fusion sub-modules and novel edge map generation sub-modules for the joint refinement module and designed the dual semantic edge loss that encourages the network to produce sharper predictions around object boundaries. Our experiments show that JSENet is an effective architecture that produces SSP masks and SEP maps of high qualities. JSENet achieves the state-of-the-art results on the challenging S3DIS and Scan-Net datasets, significantly improving over strong baselines. For future works, one straightforward direction is to explore the potential of joint improvement of these two tasks in the 2D field. Moreover, we notice that our SED method is easily affected by the noises in the GT labels introduced by human annotation. We believe that future works with special treatments on the misaligned GT semantic edges will further improve the performance of the SED task.  A Dataset selection and preparation.</p><p>In the main paper, we conduct all the experiments on two indoor-scene datasets: S3DIS <ref type="bibr" target="#b11">[12]</ref> and ScanNet <ref type="bibr" target="#b6">[7]</ref>. The main reason for choosing no outdoor-scene dataset is that we find semantic edges are not well defined in existing outdoorscene datasets. As shown in <ref type="figure" target="#fig_0">Fig. 1, compared</ref> to the indoor-scene datasets, existing outdoor-scene datasets suffer more from incompletion. Objects in an outdoor-scene are often not densely connected due to the missing parts in the point cloud. Therefore, it is hard to define meaningful semantic edges on these point clouds for our evaluation. We generate 3D semantic edges following the idea from 2D works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">47]</ref> with slight differences. In 2D, thin semantic edges of one or two pixels width are generated. In contrast, we generate thick semantic edges in 3D since points in a point cloud are much sparser than pixels in an image. Moreover, boundaries between an object and the background are considered as semantic edges in 2D images. However, these boundaries are meaningless in the 3D case. Thus, in 3D, we only consider semantic edges between different objects. In general, all semantic edge points will have two or more than two class labels. Since there are unconsidered classes in the ScanNet dataset, semantic edges between a considered class and an unconsidered class might have only one class label.</p><p>B Complexity of the network, in comparison with other works.</p><p>In this section, we present the comparison on the complexity of our network against state-of-the-art methods. All the experiments have been conducted on a PC with 8 Intel(R) i7-7700 CPUs and a single GeForce GTX 1080Ti GPU.</p><p>Training. We train KPConv and JSENet on the ScanNet dataset. Using the setting presented in their paper, KPConv takes about 0.7s for one training iteration and converges in 160K iterations, taking about 31h in total. Using the setting presented in our paper, in the first step, JSENet takes about 0.9s for one training iteration and converges in 170K iterations. In the second step, JSENet takes about 0.6s for one training iteration and converges in 40K iterations. The whole training takes about 49h. Inference. We compare KPConv, PointConv, MinkowskiNet, and JSENet for their runtime complexity given the same sets of points extracted from the Scan-Net dataset (13000 points each). Results are shown in <ref type="table">Table.</ref> 1. It can be seen that for both the inference time and the parameter size, JSENet is largely comparable to KPConv and is both more efficient and compact than PointConv (another recent point-based method) and MinkowskiNet (the SOTA voxel-based method).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Visualization.</head><p>In this section, we present more visualization results. More visualization results of our method on the ScanNet dataset are shown in <ref type="figure">Fig. 2</ref>. Qualitative comparison on the effects of joint refinement are shown in <ref type="figure" target="#fig_1">Fig. 3</ref> and <ref type="figure" target="#fig_2">Fig. 4</ref>. Black points in the GT SSP masks are unlabeled points or points of unconsidered classes. All semantic edges are thickened for visualization.  D Detailed semantic segmentation results.</p><p>In this section, we provide more details on our semantic segmentation experiments, for benchmarking purpose with future works. Detailed class scores for the S3DIS dataset and the ScanNet dataset are presented in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table  3</ref>, respectively. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(Left) Point cloud of a real-world scene from S3DIS [12]; (Middle) Semantic segmentation point (SSP) mask; (Right) Semantic edge point (SEP) map. For visualization, we paint an edge point to the color of one of its class labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of the joint refinement module, which consists of two branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Illustration of the edge map generation process on 2D points. (Left) An activation point mask, with dark colors representing high activation values. Three red points represent three different situations: far from the activated region, near the boundary, and within the activated region. (Middle) The activation point mask after the mean filtering; (Right) The generated edge activation point map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Mask Pred-SSP Mask GT-SEP Map Pred-SEP Map Qualitative results on S3DIS Area-5. For better visualization, we thickened all the semantic edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Acknowledgements. This work is supported by Hong Kong RGC GRF 16206819, 16203518 and Centre for Applied Computing and Interactive Media (ACIM) of School of Creative Media, City University of Hong Kong. 17. Cheng, D., Meng, G., Xiang, S., Pan, C.: Fusionnet: Edge aware deep convolutional networks for semantic segmentation of remote sensing harbor images. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 10(12) (2017) 5769-5783 18. Bertasius, G., Shi, J., Torresani, L.: Semantic segmentation with boundary neural fields. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2016) 3602-3610 19. Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J.: Large kernel matters-improve semantic segmentation by global convolutional network. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2017) 4353-4361 20. Su, J., Li, J., Zhang, Y., Xia, C., Tian, Y.: Selectivity or invariance: Boundaryaware salient object detection. In: Proceedings of the IEEE International Conference on Computer Vision. (2019) 3799-3808 21. Thomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B., Goulette, F., Guibas, L.J.: Kpconv: Flexible and deformable convolution for point clouds. In: Proceedings of the IEEE International Conference on Computer Vision. (2019) 6411-6420 22. Boulch, A., Le Saux, B., Audebert, N.: Unstructured point cloud semantic labeling using deep segmentation networks. 3DOR 2 (2017) 7 23. Lawin, F.J., Danelljan, M., Tosteberg, P., Bhat, G., Khan, F.S., Felsberg, M.: Deep projective 3d semantic segmentation. In: International Conference on Computer Analysis of Images and Patterns, Springer (2017) 95-107 24. Roynard, X., Deschaud, J.E., Goulette, F.: Classification of point cloud scenes with multiscale voxel deep network. arXiv preprint arXiv:1804.03583 (2018) 25. Ben-Shabat, Y., Lindenbaum, M., Fischer, A.: 3dmfv: Three-dimensional point cloud classification in real-time using convolutional neural networks. IEEE Robotics and Automation Letters 3(4) (2018) 3145-3152 26. Le, T., Duan, Y.: Pointgrid: A deep network for 3d shape understanding. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2018) 9204-9214 27. Meng, H.Y., Gao, L., Lai, Y., Manocha, D.: Vv-net: Voxel vae net with group convolutions for point cloud segmentation (2018) 28. Riegler, G., Osman Ulusoy, A., Geiger, A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 :</head><label>1</label><figDesc>(Left) An outdoor scene from Semantic3D<ref type="bibr" target="#b47">[65]</ref>; (Right) An indoor scene from ScanNet<ref type="bibr" target="#b6">[7]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Mask Pred-SSP Mask GT-SEP Map Pred-SEP Map Qualitative results on ScanNet val set. Some visualization comparison examples for semantic segmentation before and after joint refinement (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 :</head><label>4</label><figDesc>Some visualization comparison examples for semantic edge detection before and after joint refinement (best viewed in color). For better visualization, we thickened all the semantic edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation experiments of network structures on S3DIS Area-5. SEDS: semantic edge detection stream; EFE: enhanced feature extraction; HS: hierarchical supervision; SSS: semantic segmentation stream; JRM: joint refinement module. The results in some cells (with '-') are not available, since the corresponding models perform either SS or SED.</figDesc><table><row><cell>0 SEDS EFE</cell><cell>HS</cell><cell>SSS</cell><cell cols="2">JRM mIoU (%) mMF (ODS)(%)</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>67.7</cell><cell>31.0</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell>66.2</cell><cell>30.5</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell>64.7</cell><cell>-</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell>-</cell><cell>30.2</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell>-</cell><cell>29.9</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell>-</cell><cell>29.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>(a) Comparison of different supervision choices for SED. (b) Effects of the dual semantic edge loss in terms of boundary quality (F-score).</figDesc><table><row><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell></row><row><cell>Method</cell><cell>mMF (ODS) (%)</cell><cell>Method</cell><cell>F-score (%)</cell></row><row><cell>Lbce for all five layers</cell><cell>30.1</cell><cell>JSENet w/o dual loss</cell><cell>22.7</cell></row><row><cell>Lseg for all five layers</cell><cell>30.1</cell><cell>JSENet</cell><cell>23.1</cell></row><row><cell>No hierarchical supervision</cell><cell>29.9</cell><cell></cell><cell></cell></row><row><cell>Lbce for first three, Lseg for last two</cell><cell>30.2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>mIoU scores (%) of semantic segmentation task.</figDesc><table><row><cell>Method</cell><cell>S3DIS</cell><cell>ScanNet</cell></row><row><cell>TangentConv [54]</cell><cell>52.6</cell><cell>43.8</cell></row><row><cell>RNN Fusion [55]</cell><cell>53.4</cell><cell>-</cell></row><row><cell>SPGraph [56]</cell><cell>58.0</cell><cell>-</cell></row><row><cell>FCPN [57]</cell><cell>-</cell><cell>44.7</cell></row><row><cell>PointCNN [3]</cell><cell>57.3</cell><cell>45.8</cell></row><row><cell>ParamConv [58]</cell><cell>58.3</cell><cell>-</cell></row><row><cell>PanopticFusion [59]</cell><cell>-</cell><cell>52.9</cell></row><row><cell>TextureNet [60]</cell><cell>-</cell><cell>56.6</cell></row><row><cell>SPH3D-GCN [61]</cell><cell>59.5</cell><cell>61.0</cell></row><row><cell>HPEIN [37]</cell><cell>61.9</cell><cell>61.8</cell></row><row><cell>MCCNN [62]</cell><cell>-</cell><cell>63.3</cell></row><row><cell>MVPNet [4]</cell><cell>62.4</cell><cell>64.1</cell></row><row><cell>PointConv [42]</cell><cell>-</cell><cell>66.6</cell></row><row><cell>KPConv rigid [21]</cell><cell>65.4</cell><cell>68.6</cell></row><row><cell>KPConv deform [21]</cell><cell>67.1</cell><cell>68.4</cell></row><row><cell>SparseConvNet [29]</cell><cell>-</cell><cell>72.5</cell></row><row><cell>MinkowskiNet [30]</cell><cell>65.4</cell><cell>73.6</cell></row><row><cell>JSENet (ours)</cell><cell>67.7</cell><cell>69.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>MF (ODS) scores (%) of semantic edge detection on S3DIS Area-5. mean ceil. floor wall beam col. wind. door chair table book. sofa board clut. CASENet [8] 27.1 46.5 49.0 33.3 0.2 21.9 12.6 22.6 36.9 33.6 21.8 25.1 22.6 26.1 KPConv [21] 29.4 43.7 41.8 36.4 0.2 23.6 13.4 29.7 39.8 37.3 26.6 29.4 29.2 31.3 JSENet (ours) 31.0 44.5 43.2 38.8 0.2 24.1 13.2 36.7 37.7 36.3 29.1 34.0 33.3 32.4</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>MF (ODS) scores (%) of semantic edge detection on ScanNet val set. mean bath bed bksf cab chair cntr curt desk door floor othr pic ref show sink sofa tab toil wall wind CASENet [8] 32.3 38.2 55.9 29.9 36.0 36.0 36.8 28.1 28.5 19.1 26.6 25.1 32.2 28.6 26.6 23.9 19.0 45.7 27.1 54.6 27.4 KPConv [21] 34.8 40.5 55.9 33.6 38.5 39.3 38.0 32.9 31.2 22.0 25.1 28.5 36.2 30.8 30.9 22.7 22.8 46.5 33.3 55.2 32.3 JSENet (ours) 37.3 43.8 55.8 35.9 38.2 41.0 40.8 34.5 35.9 25.5 28.7 29.5 37.3 36.2 31.7 28.1 28.3 48.5 35.6 53.2 37.8</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Graham, B., Engelcke, M., van der Maaten, L.: 3d semantic segmentation with submanifold sparse convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2018) 9224-9232 30. Choy, C., Gwak, J., Savarese, S.: 4d spatio-temporal convnets: Minkowski convolutional neural networks. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (Jun 2019) 31. Li, J., Chen, B.M., Hee Lee, G.: So-net: Self-organizing network for point cloud analysis. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2018) 9397-9406 32. Huang, Q., Wang, W., Neumann, U.: Recurrent slice networks for 3d segmentation of point clouds. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (Jun 2018) 33. Zhao, H., Jiang, L., Fu, C.W., Jia, J.: Pointweb: Enhancing local neighborhood features for point cloud processing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2019) 5565-5573</figDesc><table><row><cell>: Octnet: Learning deep 3d representa-tions at high resolutions. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2017) 3577-3586 JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D Point Clouds Abstract. This supplementary document is organized as follows: -Section A explains in more detail about the dataset selection and preparation. -Section B compares the model sizes and speeds of our network with others. -Section C provides some qualitative comparison examples and more visualization results on the ScanNet [7] dataset. -Section D enumerates detailed semantic segmentation results with 29. Supplementary Material for class scores.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>Comparison on runtime complexity of JSENet against state-of-the-art methods.</figDesc><table><row><cell>Method</cell><cell cols="2">Average time (s) Parameters (m)</cell></row><row><cell>KPConv [21]</cell><cell>0.044</cell><cell>14.1</cell></row><row><cell>PointConv [42]</cell><cell>0.307</cell><cell>21.7</cell></row><row><cell>MinkowskiNet [30]</cell><cell>0.185</cell><cell>37.9</cell></row><row><cell>JSENet</cell><cell>0.097</cell><cell>16.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Detailed mIoU scores (%) of semantic segmentation on S3DIS Area-5.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://kaldir.vc.in.tum.de/scannet_benchmark/semantic_label_3d</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gated-scnn: Gated shape cnns for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5229" to="5238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on xtransformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view pointnet for 3d scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Casenet: Deep category-aware semantic edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5964" to="5973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Semantic edge detection with diverse deep supervision</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simultaneous edge alignment and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="388" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Devil is in the edges: Learning semantic boundaries from noisy annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11075" to="11083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7264" to="7273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10296" to="10305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10433" to="10441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic points agglomeration for hierarchical point sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7546" to="7555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Octree guided cnn with spherical kernels for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A-cnn: Annularly convolutional neural networks on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Komarichev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling local geometric structure of 3d point clouds using geo-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1578" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning class-specific edges for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="94" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel√°ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High-for-low and low-for-high: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Panoptic edge detection</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention MICCAI</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page">234241</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3d recurrent neural networks with context fusion for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="403" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fully-convolutional point networks for large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="596" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Panopticfusion: Online volumetric semantic mapping at the level of stuff and things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Seno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kaji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01177</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Texturenet: Consistent local parametrizations for learning from high-resolution signals on meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4440" to="4449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09287</idno>
		<title level="m">Spherical kernel for efficient graph convolution on 3d point clouds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>V√°zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">√Ä</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient convolutions for real-time semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno>IV-1-W1.</idno>
	</analytic>
	<monogr>
		<title level="m">SEMANTIC3D.NET: A new large-scale point cloud classification benchmark</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
	<note>ISPRS Annals of the Photogrammetry</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Method mIoU ceil. floor wall beam col. wind. door chair table book. sofa board clut</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Method mIoU bath bed bksf cab chair cntr curt desk door floor othr pic ref show sink sofa tab toil wall wind</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Detailed mIoU scores (%) of semantic segmentation on ScanNet test set. ScanNet [7] 30.6 20.3 36.6 50.1 31.1 52.4 21.1 0.2 34.2 18.9 78.6 14.5 10.2 24.5</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sparseconvnet</surname></persName>
		</author>
		<idno>29] 72.5 64.7 82.1 84.6 72.1 86.9 53.3 75.4 60.3 61.4 95.5 57.2 32.5 71.0 87.0 72.4 82.3 62.8 93.4 86.5 68.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jsenet</surname></persName>
		</author>
		<idno>ours) 69.9 88.1 76.2 82.1 66.7 80.0 52.2 79.2 61.3 60.7 93.5 49.2 20.5 57.6 85.3 69.1 75.8 65.2 87.2 82.8 64.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
