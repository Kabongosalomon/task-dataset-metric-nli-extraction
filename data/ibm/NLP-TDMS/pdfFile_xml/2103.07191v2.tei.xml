<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Are NLP Models really able to Solve Simple Math Word Problems?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkil</forename><surname>Patel</surname></persName>
							<email>arkil.patel@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Bhattamishra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navin</forename><surname>Goyal</surname></persName>
							<email>navingo@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Are NLP Models really able to Solve Simple Math Word Problems?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered "solved" with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-ofwords can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A Math Word Problem (MWP) consists of a short natural language narrative describing a state of the world and poses a question about some unknown quantities (see <ref type="table" target="#tab_0">Table 1</ref> for some examples). MWPs are taught in primary and higher schools. The MWP task is a type of semantic parsing task where given an MWP the goal is to generate an expression (more generally, equations), which can then be evaluated to get the answer. The task is challenging because a machine needs to extract relevant information from natural language text as well as perform mathematical reasoning to solve it. The complexity of MWPs can be measured along multiple axes, e.g., reasoning and linguistic PROBLEM: Text: Jack had 8 pens and Mary had 5 pens. Jack gave 3 pens to Mary. How many pens does Jack have now? Equation  complexity and world and domain knowledge. A combined complexity measure is the grade level of an MWP, which is the grade in which similar MWPs are taught. Over the past few decades many approaches have been developed to solve MWPs with significant activity in the last decade .</p><p>MWPs come in many varieties. Among the simplest are the one-unknown arithmetic word problems where the output is a mathematical expression involving numbers and one or more arithmetic operators (+, −, * , /). Problems in <ref type="table" target="#tab_0">Tables 1 and 6</ref> are of this type. More complex MWPs may have systems of equations as output or involve other operators or may involve more advanced topics and specialized knowledge. Recently, researchers have started focusing on solving such MWPs, e.g. multiple-unknown linear word problems <ref type="bibr" target="#b6">(Huang et al., 2016a)</ref>, geometry <ref type="bibr" target="#b20">(Sachan and Xing, 2017)</ref> and probability <ref type="bibr" target="#b0">(Amini et al., 2019)</ref>, believing that existing work can handle one-unknown arithmetic MWPs well <ref type="bibr" target="#b16">(Qin et al., 2020)</ref>. In this paper, we question the capabilities of the state-of-the-art (SOTA) methods to robustly solve even the simplest of MWPs suggesting that the above belief is not well-founded.</p><p>In this paper, we provide concrete evidence to show that existing methods use shallow heuristics to solve a majority of word problems in the benchmark datasets. We find that existing models are able to achieve reasonably high accuracy on MWPs from which the question text has been removed leaving only the narrative describing the state of the world. This indicates that the models can rely on superficial patterns present in the narrative of the MWP and achieve high accuracy without even looking at the question. In addition, we show that a model without word-order information (i.e., the model treats the MWP as a bag-of-words) can also solve the majority of MWPs in benchmark datasets.</p><p>The presence of these issues in existing benchmarks makes them unreliable for measuring the performance of models. Hence, we create a challenge set called SVAMP (Simple Variations on Arithmetic Math word Problems; pronounced swamp) of one-unknown arithmetic word problems with grade level up to 4 by applying simple variations over word problems in an existing dataset (see <ref type="table" target="#tab_0">Table 1</ref> for some examples). SVAMP further highlights the brittle nature of existing models when trained on these benchmark datasets. On evaluating SOTA models on SVAMP, we find that they are not even able to solve half the problems in the dataset. This failure of SOTA models on SVAMP points to the extent to which they rely on simple heuristics in training data to make their prediction.</p><p>Below, we summarize the two broad contributions of our paper.</p><p>• We show that the majority of problems in benchmark datasets can be solved by shallow heuristics lacking word-order information or lacking question text.</p><p>• We create a challenge set called SVAMP 1 for more robust evaluation of methods developed to solve elementary level math word problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Math Word Problems. A wide variety of methods and datasets have been proposed to solve MWPs; e.g. statistical machine learning <ref type="bibr" target="#b19">(Roy and Roth, 2018)</ref>, semantic parsing <ref type="bibr" target="#b5">(Huang et al., 2017)</ref> and most recently deep learning <ref type="bibr" target="#b22">(Wang et al., 2017;</ref><ref type="bibr" target="#b24">Xie and Sun, 2019;</ref>; see  for an extensive survey. Many papers have pointed out various deficiencies with previous datasets and proposed new ones to address them. <ref type="bibr" target="#b8">Koncel-Kedziorski et al. (2016)</ref> curated the MAWPS dataset from previous datasets which along with Math23k <ref type="bibr" target="#b22">(Wang et al., 2017)</ref> has been used as benchmark in recent works. <ref type="bibr">Recently, ASDiv (Miao et al., 2020)</ref> has been proposed to provide more diverse problems with annotations for equation, problem type and grade level. HMWP <ref type="bibr" target="#b16">(Qin et al., 2020)</ref> is another newly proposed dataset of Chinese MWPs that includes examples with muliple-unknown variables and requiring non-linear equations to solve them. Identifying artifacts in datasets has been done for the Natural Language Inference (NLI) task by <ref type="bibr" target="#b12">McCoy et al. (2019)</ref>, <ref type="bibr" target="#b15">Poliak et al. (2018), and</ref><ref type="bibr" target="#b4">Gururangan et al. (2018)</ref>. <ref type="bibr" target="#b18">Rosenman et al. (2020)</ref> identified shallow heuristics in a Relation Extraction dataset. <ref type="bibr" target="#b2">Cai et al. (2017)</ref> showed that biases prevalent in the ROC stories cloze task allowed models to yield state-of-the-art results when trained only on the endings. To the best of our knowledge, this kind of analysis has not been done on any Math Word Problem dataset.</p><p>Challenge Sets for NLP tasks have been proposed most notably for NLI and machine translation <ref type="bibr" target="#b1">(Belinkov and Glass, 2019;</ref><ref type="bibr" target="#b14">Nie et al., 2020;</ref><ref type="bibr" target="#b17">Ribeiro et al., 2020)</ref>. <ref type="bibr">Gardner et al. (2020)</ref> suggested creating contrast sets by manually perturbing test instances in small yet meaningful ways that change the gold label. We believe that we are the first to introduce a challenge set targeted specifically for robust evaluation of Math Word Problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We denote a Math Word Problem P by a sequence of n tokens P = (w 1 , . . . , w n ) where each token w i can be either a word from a natural language or a numerical value. The word problem P can be broken down into body B = (w 1 , . . . , w k ) and question Q = (w k+1 , . . . , w n ). The goal is to map P to a valid mathematical expression E P composed of numbers from P and mathematical operators from the set {+, −, /, * } (e.g. 3 + 5 − 4). The metric used to evaluate models on the MWP task is Execution Accuracy, which is obtained from com-  paring the predicted answer (calculated by evaluating E P ) with the annotated answer. In this work, we focus only on one-unknown arithmetic word problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets and Methods</head><p>Many of the existing datasets are not suitable for our analysis as either they are in Chinese, e.g. Math23k <ref type="bibr" target="#b22">(Wang et al., 2017)</ref> and HMWP <ref type="bibr" target="#b16">(Qin et al., 2020)</ref>, or have harder problem types, e.g. Dolphin18K <ref type="bibr" target="#b7">(Huang et al., 2016b)</ref>. We consider the widely used benchmark MAWPS <ref type="bibr" target="#b8">(Koncel-Kedziorski et al., 2016)</ref> composed of 2373 MWPs and the arithmetic subset of ASDiv <ref type="bibr" target="#b13">(Miao et al., 2020)</ref> called ASDiv-A which has 1218 MWPs mostly up to grade level 4 (MAWPS does not have grade level information). Both MAWPS and ASDiv-A are evaluated on 5-fold cross-validation based on pre-assigned splits. We consider three models in our experiments: (a) Seq2Seq consists of a Bidirectional LSTM Encoder to encode the input sequence and an LSTM decoder with attention <ref type="bibr" target="#b11">(Luong et al., 2015)</ref> to generate the equation. The performance of these models on both datasets is shown in <ref type="table" target="#tab_2">Table 2</ref>. We either provide RoBERTa <ref type="bibr" target="#b10">(Liu et al., 2019)</ref> pre-trained embeddings to the models or train them from scratch. Graph2Tree    datasets. Note that our implementations achieve a higher score than the previously reported highest score of 78% on ASDiv-A (Miao et al., 2020) and 83.7% on MAWPS . The implementation details are provided in Section B in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deficiencies in existing datasets</head><p>Here we describe the experiments that show that there are important deficiencies in MAWPS and ASDiv-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation on Question-removed MWPs</head><p>As mentioned in Section 3.1, each MWP consists of a body B, which provides a short narrative on a state of the world and a question Q, which inquires about an unknown quantity about the state of the world. For each fold in the provided 5-fold split in MAWPS and ASDiv-A, we keep the train set unchanged while we remove the questions Q from the problems in the test set. Hence, each problem in the test set consists of only the body B without any question Q. We evaluate all three models with RoBERTa embeddings on these datasets. The results are provided in <ref type="table" target="#tab_4">Table 3</ref>. The best performing model is able to achieve a 5-fold cross-validation accuracy of 64.4% on ASDiv-A and 77.7% on MAWPS. Loosely translated, this means that nearly 64% of the problems in ASDiv-A and 78% of the problems in MAWPS can be correctly answered without even looking at the question. This suggests the presence of patterns in the bodies of MWPs in these datasets that have a direct correlation with the output equation.</p><p>Some recent works have also demonstrated similar evidence of bias in NLI datasets <ref type="bibr" target="#b4">(Gururangan et al., 2018;</ref><ref type="bibr" target="#b15">Poliak et al., 2018)</ref>. They observed that NLI models were able to predict the correct label for a large fraction of the standard NLI datasets based on only the hypothesis of the input and without the premise. Our results on question-removed examples of math word problems resembles their observations on NLI datasets and similarly indicates the presence of artifacts that help statistical   <ref type="bibr">(2018)</ref>, we attempt to understand the extent to which SOTA models rely on the presence of simple heuristics in the body to predict correctly. We partition the test set into two subsets for each model: problems that the model predicted correctly without the question are labeled Easy and the problems that the model could not answer correctly without the question are labeled Hard. <ref type="table" target="#tab_6">Table 4</ref> shows the performance of the models on their respective Hard and Easy sets. Note that their performance on the full set is already provided in <ref type="table" target="#tab_2">Table 2</ref>. It can be seen clearly that although the models correctly answer many Hard problems, the bulk of their success is due to the Easy problems. This shows that the ability of SOTA methods to robustly solve word problems is overestimated and that they rely on simple heuristics in the body of the problems to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance of a constrained model</head><p>We construct a simple model based on the Seq2Seq architecture by removing the LSTM Encoder and replacing it with a Feed-Forward Network that maps the input embeddings to their hidden representations. The LSTM Decoder is provided with the average of these hidden representations as its initial hidden state. During decoding, an attention mechanism <ref type="bibr" target="#b11">(Luong et al., 2015)</ref> assigns weights to individual hidden representations of the input  tokens. We use either RoBERTa embeddings (noncontextual; taken directly from Embedding Matrix) or train the model from scratch. Clearly, this model does not have access to word-order information. <ref type="table" target="#tab_8">Table 5</ref> shows the performance of this model on MAWPS and ASDiv-A. The constrained model with non-contextual RoBERTa embeddings is able to achieve a cross-validation accuracy of 51.2 on ASDiv-A and an astounding 77.9 on MAWPS. It is surprising to see that a model having no word-order information can solve a majority of word problems in these datasets. These results indicate that it is possible to get a good score on these datasets by simply associating the occurence of specific words in the problems to their corresponding equations. We illustrate this more clearly in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analyzing the attention weights</head><p>To get a better understanding of how the constrained model is able to perform so well, we analyze the attention weights that it assigns to the hidden representations of the input tokens. As shown by <ref type="bibr" target="#b23">Wiegreffe and Pinter (2019)</ref>, analyzing the attention weights of our constrained model is a reliable way to explain its prediction since each hidden representation consists of information about only that token as opposed to the case of an RNN where each hidden representation may have information about the context i.e. its neighboring tokens.</p><p>We train the contrained model (with RoBERTa embeddings) on the full ASDiv-A dataset and observe the attention weights it assigns to the words of the input problems. We found that the model usually attends to a single word to make its prediction, irrespective of the context. <ref type="table" target="#tab_9">Table 6</ref> shows some representative examples. In the first example, the model assigns an attention weight of 1 to the representation of the word 'every' and predicts the correct equation. However, when we make a subtle change to this problem such that the corresponding equation changes, the model keeps on attending over the word 'every' and predicts the same equa-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Problem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted Equation Answer</head><p>John delivered 3 letters at every house. If he delivered for 8 houses, how many letters did John deliver? 3 * 8 24</p><p>John delivered 3 letters at every house. He delivered 24 letters in all. How many houses did John visit to deliver letters? 3 * 24 72</p><p>Sam made 8 dollars mowing lawns over the Summer. He charged 2 bucks for each lawn. How many lawns did he mow? 8 / 2 4 Sam mowed 4 lawns over the Summer. If he charged 2 bucks for each lawn, how much did he earn? 4 / 2 2 10 apples were in the box. 6 are red and the rest are green. how many green apples are in the box? 10 -6 4 10 apples were in the box. Each apple is either red or green. 6 apples are red. how many green apples are in the box? 10 / 6 1.67  <ref type="table" target="#tab_2">Table 26</ref> in the Appendix has more such examples. These examples represent only a few types of spurious correlations that we could find but there could be other types of correlations that might have been missed.</p><p>Note that, we do not claim that every model trained on these datasets relies on the occurrence of specific words in the input problem for prediction the way our constrained model does. We are only asserting that it is possible to achieve a good score on these datasets even with such a brittle model, which clearly makes these datasets unreliable to robustly measure model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SVAMP</head><p>The efficacy of existing models on benchmark datasets has led to a shift in the focus of researchers towards more difficult MWPs. We claim that this efficacy on benchmarks is misleading and SOTA MWP solvers are unable to solve even elementary level one-unknown MWPs. To this end, we create a challenge set named SVAMP containing simple one-unknown arithmetic word problems of grade level up to 4. The examples in SVAMP test a model across different aspects of solving word problems. For instance, a model needs to be sensitive to questions and possess certain reasoning abilities to correctly solve the examples in our challenge set. SVAMP is similar to existing datasets of the same level in terms of scope and difficulty for humans, but is less susceptible to being solved by models relying on superficial patterns.</p><p>Our work differs from adversarial data collection methods such as Adversarial NLI <ref type="bibr" target="#b14">(Nie et al., 2020)</ref> in that these methods create examples depending on the failure of a particular model while we create examples without referring to any specific model. Inspired by the notion of Normative evaluation (Linzen, 2020), our goal is to create a dataset of simple problems that any system designed to solve MWPs should be expected to solve. We create new problems by applying certain variations to existing problems, similar to the work of <ref type="bibr" target="#b17">Ribeiro et al. (2020)</ref>. However, unlike their work, our variations do not check for linguistic capabilities. Rather, the choice of our variations is motivated by the experiments in Section 4 as well as certain simple capabilities that any MWP solver must possess.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Creating SVAMP</head><p>We create SVAMP by applying certain types of variations to a set of seed examples sampled from the ASDiv-A dataset. We select the seed examples from the recently proposed ASDiv-A dataset since it appears to be of higher quality and harder than the MAWPS dataset: We perform a simple experiment to test the coverage of each dataset by training a model on one dataset and testing it on the other one. For instance, when we train a Graph2Tree model on ASDiv-A, it achieves 82% accuracy on MAWPS. However, when trained on MAWPS and tested on ASDiv-A, the model achieved only 73% accuracy. Also recall <ref type="table" target="#tab_2">Table 2</ref> where most models performed better on MAWPS. Moreover, AS-Div has problems annotated according to types and grade levels which are useful for us.</p><p>To select a subset of seed examples that sufficiently represent different types of problems in the ASDiv-A dataset, we first divide the examples into groups according to their annotated types. We dis-  Original: Allan brought two balloons and Jake brought four balloons to the park. How many balloons did Allan and Jake have in the park? Variation: Allan brought two balloons and Jake brought four balloons to the park. How many more balloons did Jake have than Allan in the park? Original: He then went to see the oranges being harvested. He found out that they harvest 83 sacks per day and that each sack contains 12 oranges. How many sacks of oranges will they have after 6 days of harvest?</p><p>Variation: He then went to see the oranges being harvested. He found out that they harvest 83 sacks per day and that each sack contains 12 oranges. How many oranges do they harvest per day?</p><p>Reasoning Ability </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invert Operation</head><p>Original: He also made some juice from fresh oranges. If he used 2 oranges per glass of juice and he made 6 glasses of juice, how many oranges did he use? Variation: He also made some juice from fresh oranges. If he used 2 oranges per glass of juice and he used up 12 oranges, how many glasses of juice did he make?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structural Invariance</head><p>Change order of objects Original: John has 8 marbles and 3 stones. How many more marbles than stones does he have? Variation: John has 3 stones and 8 marbles. How many more marbles than stones does he have?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Change order of phrases</head><p>Original: Matthew had 27 crackers. If Matthew gave equal numbers of crackers to his 9 friends, how many crackers did each person eat? Variation: Matthew gave equal numbers of crackers to his 9 friends. If Matthew had a total of 27 crackers initially, how many crackers did each person eat?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Add irrelevant information</head><p>Original: Jack had 142 pencils. Jack gave 31 pencils to Dorothy. How many pencils does Jack have now? Variation: Jack had 142 pencils. Dorothy had 50 pencils. Jack gave 31 pencils to Dorothy. How many pencils does Jack have now?  <ref type="table" target="#tab_11">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Variations</head><p>The variations that we make to each seed example can be broadly classified into three categories based on desirable properties of an ideal model:  termine a change in reasoning arising from subtle changes in the problem text. The different possible variations are as follows:</p><p>(a) Add relevant information: Extra relevant information is added to the example that affects the output equation.</p><p>(b) Change information: The information provided in the example is changed.</p><p>(c) Invert operation: The previously unknown quantity is now provided as information and the question instead asks about a previously known quantity which is now unknown.</p><p>3. Structural Invariance. Variations in this category check whether a model remains invariant to superficial structural changes that do not alter the answer or the reasoning required to solve the example. The different possible variations are as follows:</p><p>(a) Add irrelevant information: Extra irrelevant information is added to the problem text that is not required to solve the example.</p><p>(b) Change order of objects: The order of objects appearing in the example is changed.</p><p>(c) Change order of phrases: The order of numbercontaining phrases appearing in the example is changed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Protocol for creating variations</head><p>Since creating variations requires a high level of familiarity with the task, the construction of SVAMP is done in-house by the authors and colleagues, hereafter called the workers. The 100 seed examples (as shown in <ref type="table" target="#tab_11">Table 7</ref>) are distributed among the workers.</p><p>For each seed example, the worker needs to create new variations by applying the variation types discussed in Section 5.1.1. Importantly, a combination of different variations over the seed example can also be done. For each new example created, the worker needs to annotate it with the equation as well as the type of variation(s) used to create it. More details about the creation protocol can be found in Appendix C.</p><p>We created a total of 1098 examples. However, since ASDiv-A does not have examples with equations of more than two operators, we discarded 98 examples from our set which had equations consisting of more than two operators. This is to ensure that our challenge set does not have any unfairly difficult examples. The final set of 1000 examples was provided to an external volunteer unfamiliar with the task to check the grammatical and logical correctness of each example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dataset Properties</head><p>Our challenge set SVAMP consists of oneunknown arithmetic word problems which can be solved by expressions requiring no more than two operators. <ref type="table" target="#tab_16">Table 9</ref> shows some statistics of our dataset and of ASDiv-A and MAWPS. The Equation Template for each example is obtained by converting the corresponding equation into prefix form and masking out all numbers with a meta symbol. Observe that the number of distinct Equation Templates and the Average Number of Operators are similar for SVAMP and ASDiv-A and are considerably smaller than for MAWPS. This indicates that SVAMP does not contain unfairly difficult MWPs in terms of the arithmetic expression expected to be produced by a model.</p><p>Previous works, including those introducing MAWPS and ASDiv, have tried to capture the notion of diversity in MWP datasets. Miao et al.</p><p>(2020) introduced a metric called Corpus Lexicon Diversity (CLD) to measure lexical diversity. Their contention was that higher lexical diversity is correlated with the quality of a dataset. As can be seen from <ref type="table" target="#tab_16">Table 9</ref>, SVAMP has a much lesser CLD than ASDiv-A. SVAMP is also less diverse in terms of problem types compared to ASDiv-a. Despite this we will show in the next section that SVAMP is in fact more challenging than ASDiv-A for current models. Thus, we believe that lexical diversity is not a reliable way to measure the quality of MWP datasets. Rather it could depend on other factors such as the diversity in MWP structure which preclude models exploiting shallow heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments on SVAMP</head><p>We train the three considered models on a combination of MAWPS and ASDiv-A and test them on SVAMP. The scores of all three models with and without RoBERTa embeddings for various subsets of SVAMP can be seen in <ref type="table" target="#tab_0">Table 10</ref>.  The best performing Graph2Tree model is only able to achieve an accuracy of 43.8% on SVAMP. This indicates that the problems in SVAMP are indeed more challenging for the models than the problems in ASDiv-A and MAWPS despite being of the same scope and type and less diverse. Table 27 in the Appendix lists some simple examples from SVAMP on which the best performing model fails. These results lend further support to our claim that existing models cannot robustly solve elementary level word problems.</p><p>Next, we remove the questions from the examples in SVAMP and evaluate them using the three models with RoBERTa embeddings trained on combined MAWPS and ASDiv-A. The scores can be seen in <ref type="table" target="#tab_0">Table 11</ref>. The accuracy drops by half when compared to ASDiv-A and more than half compared to MAWPS suggesting that the problems in SVAMP are more sensitive to the information present in the question. We also evaluate the performance of the constrained model on SVAMP when trained on MAWPS and ASDiv-A. The best model achieves only 18.3% accuracy (see <ref type="table" target="#tab_0">Table 12</ref>   is marginally better than the majority template baseline. This shows that the problems in SVAMP are less vulnerable to being solved by models using simple patterns and that a model needs contextual information in order to solve them. We also explored using SVAMP for training by combining it with ASDiv-A and MAWPS. We performed 5-fold cross-validation over SVAMP where the model was trained on a combination of the three datasets and tested on unseen examples from SVAMP. To create the folds, we first divide the seed examples into five sets, with each type of example distributed nearly equally among the sets. A fold is obtained by combining all the examples in SVAMP that were created using the seed examples in a set. In this way, we get five different folds from the five sets. We found that the best model achieved about 65% accuracy. This indicates that even with additional training data existing models are still not close to the performance that was estimated based on prior benchmark datasets.</p><p>To check the influence of different categories of variations in SVAMP, for each category, we measure the difference between the accuracy of the best model on the full dataset and its accuracy on a subset containing no example created from that category of variations. The results are shown in    <ref type="table" target="#tab_0">Table 14</ref>.</p><p>We also check the break-up of performance of the best performing Graph2Tree model according to the number of numbers present in the text of the input problem. We trained the model on both ASDiv-A and MAWPS and tested on SVAMP and compare those results against the 5-fold crossvalidation setting of ASDiv-A. The scores are provided in <ref type="table" target="#tab_0">Table 15</ref>. While the model can solve many problems consisting of only two numbers in the input text (even in our challenge set), it performs very badly on problems having more than two numbers. This shows that current methods are incapable of properly associating numbers to their context. Also, the gap between the performance on ASDiv-A and SVAMP is high, indicating that the examples in SVAMP are more difficult for these models to solve than the examples in ASDiv-A even when considering the structurally same type of word problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Final Remarks</head><p>Going back to the original question, are existing NLP models able to solve elementary math word  problems? This paper gives a negative answer. We have empirically shown that the benchmark English MWP datasets suffer from artifacts making them unreliable to gauge the performance of MWP solvers: we demonstrated that the majority of problems in the existing datasets can be solved by simple heuristics even without word-order information or the question text. The performance of the existing models in our proposed challenge dataset also highlights their limitations in solving simple elementary level word problems. We hope that our challenge set SVAMP, containing elementary level MWPs, will enable more robust evaluation of methods. We believe that methods proposed in the future that make genuine advances in solving the task rather than relying on simple heuristics will perform well on SVAMP despite being trained on other datasets such as ASDiv-A and MAWPS.</p><p>In recent years, the focus of the community has shifted towards solving more difficult MWPs such as non-linear equations and word problems with multiple unknown variables. We demonstrated that the capability of existing models to solve simple one-unknown arithmetic word problems is overestimated. We believe that developing more robust methods for solving elementary MWPs remains a significant open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiments with Transformer</head><p>We additionally ran all our experiments with the Transformer <ref type="bibr" target="#b21">(Vaswani et al., 2017)</ref> model. The 5fold cross-validation accuracies of the Transformer on MAWPS and ASDiv-A are provided in <ref type="table" target="#tab_0">Table  16</ref>. The scores on Question-removed datasets are provided in <ref type="table" target="#tab_0">Table 17</ref> and on SVAMP challenge set is provided in <ref type="table" target="#tab_0">Table 18</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>We use 8 NVIDIA Tesla P100 GPUs each with 16 GB memory to run our experiments. The hyperparameters used for each model are shown in <ref type="table" target="#tab_0">Table  19</ref>. The hyperparameters used in for the Transformer model are provided in <ref type="table" target="#tab_2">Table 20</ref>. The best hyperparameters are highlighted in bold. Following the setting of , the arithmetic word problems from MAWPS are divided into five folds, each of equal test size. For ASDiv-A, we consider the 5-fold split <ref type="bibr">[238,</ref><ref type="bibr">238,</ref><ref type="bibr">238,</ref><ref type="bibr">238,</ref><ref type="bibr">266]</ref> provided by the authors <ref type="bibr" target="#b13">(Miao et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Creation Protocol</head><p>We create variations in template form. Generating more data by scaling up from these templates or by performing automatic operations on these templates is left for future work. The template form of an example is created by replacing certain words with their respective tags. <ref type="table" target="#tab_0">Table 21</ref> lists the various tags used in the templates.</p><p>The N U M tag is used to replace all the numbers and the N AM E tag is used to replace all the Names of Persons in the example. The OBJs and OBJp tags are used for replacing the objects in the example. The OBJs and OBJp tags with the same index represent the same object in singular and plural form respectively. The intention when using the OBJs or the OBJp tag is that it can be used as a placeholder for other similar words, which when entered in that place, make sense as per the context. These tags must    not be used for collectives; rather they should be used for the things that the collective represents. Some example uses of OBJs and OBJp tags are provided in <ref type="table" target="#tab_2">Table 22</ref>. Lastly, the M OD tag must be used to replace any modifier preceding the OBJs / OBJp tag. A preprocessing script is executed over the Seed Examples to automatically generate template suggestions for the workers. The script uses Named Entity Recognition and Regular Expression matching to automatically mask the names of persons and the numbers found in the Seed Examples. The outputs from the script are called the Script Examples. An illustration is provided in <ref type="table" target="#tab_2">Table 23</ref>.</p><p>Each worker is provided with the Seed Examples along with their respective Script Examples that have been alloted to them. The worker's task is to edit the Script Example by correcting any mistake made by the preprocessing script and adding any new tags such as the OBJs and the OBJp tags in order to create the Base Example. If a worker introduces a new tag, they need to mark it against its example-specific value. If the tag is used to mask objects, the worker needs to mark both the singular and plural form of the object in a commaseperated manner. Additionally, for each unique    index of OBJs / OBJp tag in the example, the worker must enter atleast one alternate value that can be used in that place. Similarly, the worker must enter atleast two modifier words that can be used to precede the principal OBJs / OBJp tags in the example. These alternate values are used to gather a lexicon which can be utilised to scale-up the data at a later stage. An illustration of this process is provided in <ref type="table" target="#tab_2">Table 24</ref>. In order to create the variations, the worker needs to check the different types of variations in <ref type="table" target="#tab_14">Table 8</ref> to see if they can be applied to the Base Example. If applicable, the worker needs to create the Variation Example while also making a note of the type of variation. If a particular example is the result of performing multiple types of variations, all types of variations should be listed according to their order of application from latest to earliest in a comma-seperated manner. For any variation, if a worker introduces a new tag, they need to mark it against its example-specific value as mentioned before. The index of any new tag introduced needs to be one more than the highest index already in use for that tag in the Base Example or its previously created variations.</p><p>To make the annotation more efficient and streamlined, we provide the following steps to be followed in order:   <ref type="table" target="#tab_2">Table 24</ref>. Note that two seperate examples were created through the 'Add irrelevant information' variation. The first by applying the variation on the Original Example and the second by applying it on a previously created example (as directed in Step-4).</p><p>To make sure that different workers following our protocol make similar types of variations, we hold a trial where each worker created variations from the same 5 seed examples. We observed that barring minor linguistic differences, most of the created examples were the same, thereby indicating the effectiveness of our protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analyzing Attention Weights</head><p>In <ref type="table" target="#tab_2">Table 26</ref>, we provide more examples to illustrate the specific word to equation correlation that the constrained model learns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Examples of Simple Problems</head><p>In <ref type="table" target="#tab_2">Table 27</ref>, we provide a few simple examples from SVAMP that the best performing Graph2Tree model could not solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Ethical Considerations</head><p>In this paper, we consider the task of automatically solving Math Word Problems (MWPs). Our work encourages the development of better systems that can robustly solve MWPs. Such systems can be deployed for use in the education domain. E.g., an application can be developed that takes MWPs as input and provides detailed explanations to solve them. Such applications can aide elementary school students in learning and practicing math.</p><p>We present a challenge set called SVAMP of oneunknown English Math Word Problems. SVAMP is created in-house by the authors themselves by applying some simple variations to examples from ASDiv-A <ref type="bibr" target="#b13">(Miao et al., 2020)</ref>, which is a publicly available dataset. We provide a detailed creation protocol in Section C. We are not aware of any risks associated with our proposed dataset.</p><p>To provide an estimate of the energy requirements of our experiments, we provide the details such as computing platform and running time in Section B. Also, in order to reduce carbon costs from our experiments, we first perform a broad hyperparameter search over only a single fold for the datasets and then run the cross validation experiment over a select few hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Excerpt of Example</head><p>Beth has 4 packs of red crayons and 2 packs of green crayons. Each pack has 10 crayons in it. Template Form N AM E1 has N U M 1 packs of M OD1 OBJp1 and N U M 2 packs of M OD2 OBJp1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Excerpt of Example</head><p>In a game, Frank defeated 6 enemies. Each enemy earned him 9 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Template Form</head><p>In a game N AM E1 defeated N U M 1 OBJp1 . Each OBJs1 earned him N U M 2 points. <ref type="table" target="#tab_2">Table 22</ref>: Example uses of tags. Note that in the first example, the word 'packs' was not replaced since it is a collective. In the second example, the word 'points' was not replaced because it is too instance-specific and no other word can be used in that place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seed Example Body</head><p>Beth has 4 packs of crayons. Each pack has 10 crayons in it. She also has 6 extra crayons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seed Example Question</head><p>How     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>had 8 pens and Mary had 5 pens. Jack gave 3 pens to Mary. How many pens does Mary have now? Equation: 5 + 3 = 8 REASONING ABILITY VARIATION: Text: Jack had 8 pens and Mary had 5 pens. Mary gave 3 pens to Jack. How many pens does Jack have now? Equation: 8 + 3 = 11 STRUCTURAL INVARIANCE VARIATION: Text: Jack gave 3 pens to Mary. If Jack had 8 pens and Mary had 5 pens initially, how many pens does Jack have now? Equation: 8 -3 = 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(c) GTS<ref type="bibr" target="#b24">(Xie and Sun, 2019)</ref> uses an LSTM Encoder to encode the input sequence and a tree-based Decoder to generate the equation.(d) Graph2Tree (Zhang et al., 2020) combines a Graph-based Encoder with a Tree-based Decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>many crayons does Beth have altogether? Seed Example Equation 4*10+6 Script Example Body N AM E1 has N U M 1 packs of crayons . Each pack has N U M 2 crayons in it . She also has N U M 3 extra crayons . Script Example Question How many crayons does N AM E1 have altogether ? Script Example Equation N U M 1 * N U M 2 + N U M 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>has N U M 1 packs of OBJp1 and N U M 4 packs of OBJp2 . Each pack has N U M 2 OBJp1 in it. She also has N U M 3 extra OBJp1 . Variation Question How many OBJp1 does N AM E1 have altogether ? Variation Equation N U M 1 * N U M 2 + N U M 3 Variation Body N AM E1 has N U M 1 packs of OBJp1 and N U M 4 packs of OBJp2 . Each pack has N U M 2 OBJp1 in it. She also has N U M 3 extra OBJp1 . Variation Question How many OBJp1 does N AM E1 have in packs? Variation Equation N U M 1 * N U M 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example of a Math Word Problem along with the types of variations that we make to create SVAMP.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: 5-fold cross-validation accuracies (↑) of base-</cell></row><row><cell>line models on datasets. (R) means that the model is</cell></row><row><cell>provided with RoBERTa pretrained embeddings while</cell></row><row><cell>(S) means that the model is trained from scratch.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: 5-fold cross-validation accuracies (↑) of base-</cell></row><row><cell>line models on Question-removed datasets.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results of baseline models on the Easy andHard test sets.</figDesc><table><row><cell>models predict the correct answer without com-</cell></row><row><cell>plete information. Note that even though the two</cell></row><row><cell>methods appear similar, there is an important dis-</cell></row><row><cell>tinction. In Gururangan et al. (2018), the model is</cell></row><row><cell>trained and tested on hypothesis only examples and</cell></row><row><cell>hence, the model is forced to find artifacts in the</cell></row><row><cell>hypothesis during training. On the other hand, our</cell></row><row><cell>setting is more natural since the model is trained in</cell></row><row><cell>the standard way on examples with both the body</cell></row><row><cell>and the question. Thus, the model is not explicitly</cell></row><row><cell>forced to learn based on the body during training</cell></row><row><cell>and our results not only show the presence of arti-</cell></row><row><cell>facts in the datasets but also suggest that the SOTA</cell></row><row><cell>models exploit them.</cell></row><row><cell>Following Gururangan et al.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>: 5-fold cross-validation accuracies (↑) of the constrained model on the datasets. (R) denotes that the model is provided with non-contextual RoBERTa pretrained embeddings while (S) denotes that the model is trained from scratch.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Attention paid to specific words by the constrained model.</figDesc><table /><note>tion, which is now incorrect. Similar observations can be made for the other two examples.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Distribution of selected seed examples across types.</figDesc><table><row><cell>CATEGORY</cell><cell>VARIATION</cell><cell>EXAMPLES</cell></row><row><cell></cell><cell>Same Object, Different</cell><cell></cell></row><row><cell></cell><cell>Structure</cell><cell></cell></row><row><cell>Question</cell><cell></cell><cell></cell></row><row><cell>Sensitivity</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>In a school, there are 542 girls and 387 boys. 290 more boys joined the school. How many pupils are in the school? Variation: In a school, there are 542 girls and 387 boys. 290 more boys joined the school. How many boys are in the school?</figDesc><table><row><cell>Different Object, Same Structure Original: Different Object,</cell></row><row><cell>Different Structure</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Every day, Ryan spends 4 hours on learning English and 3 hours on learning Chinese. How many hours does he spend on learning English and Chinese in all? Variation: Every day, Ryan spends 4 hours on learning English and 3 hours on learning Chinese. If he learns for 3 days, how many hours does he spend on learning English and Chinese in all?</figDesc><table><row><cell>Add relevant information Original: Change Information Original: Jack had 142 pencils. Jack gave 31 pencils to Dorothy. How many pencils does Jack have now? Variation: Dorothy had 142 pencils. Jack gave 31 pencils to Dorothy. How many pencils does Dorothy</cell></row><row><cell>have now?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>To control the complexity of resulting variations, we only consider those problems as seed examples that can be solved by an expression with a single operator. Then, within each group, we cluster examples using K-Means over RoBERTa sentence embeddings of each example. From each cluster, the example closest to the cluster centroid is selected as a seed example. We selected a total of 100 seed examples in this manner. The distribution of seed examples according to different types of problems can be seen in</figDesc><table /><note>Types of Variations with examples. 'Original:' denotes the base example from which the variation is created, 'Variation:' denotes a manually created variation.card types such as 'TVQ-Change', 'TVQ-Initial', 'Ceil-Division' and 'Floor-Division' that have less than 20 examples each. We also do not consider the 'Difference' type since it requires the use of an additional modulus operator. For ease of creation, we discard the few examples that are more than 40 words long.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>The principal object (i.e. object whose quantity is unknown) in the question is kept the same while the structure of the question is changed.(b) Different Object, Same Structure: The principal object in the question is changed while the structure of question remains fixed.</figDesc><table><row><cell>Dataset</cell><cell># Problems</cell><cell># Equation</cell><cell># Avg Ops</cell><cell>CLD</cell></row><row><cell></cell><cell></cell><cell>Templates</cell><cell></cell><cell></cell></row><row><cell>MAWPS</cell><cell>2373</cell><cell>39</cell><cell>1.78</cell><cell>0.26</cell></row><row><cell>ASDiv-A</cell><cell>1218</cell><cell>19</cell><cell>1.23</cell><cell>0.50</cell></row><row><cell>SVAMP</cell><cell>1000</cell><cell>26</cell><cell>1.24</cell><cell>0.22</cell></row></table><note>Question Sensitivity, Reasoning Ability and Structural Invariance. Examples of each type of variation are provided in Table 8.1. Question Sensitivity. Variations in this category check if the model's answer depends on the ques- tion. In these variations, we change the question in the seed example while keeping the body same. The possible variations are as follows: (a) Same Object, Different Structure:(c) Different Object, Different Structure: Both, the principal object in the question and the structure of the question, are changed.2. Reasoning Ability. Variations here check whether a model has the ability to correctly de-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Statistics of our dataset compared with MAWPS and ASDiv-A.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Results of models on the SVAMP challenge set. S indicates that the model is trained from scratch.</figDesc><table><row><cell>R indicates that the model was trained with RoBERTa</cell></row><row><cell>embeddings. The first row shows the results for the full</cell></row><row><cell>dataset. The next two rows show the results for subsets</cell></row><row><cell>of SVAMP composed of examples that have equations</cell></row><row><cell>with one operator and two operators respectively. The</cell></row><row><cell>last four rows show the results for subsets of SVAMP</cell></row><row><cell>composed of examples of type Addition, Subtraction,</cell></row><row><cell>Multiplication and Division respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table><row><cell>Model</cell><cell>SVAMP</cell></row><row><cell>FFN + LSTM Decoder (S)</cell><cell>17.5</cell></row><row><cell>FFN + LSTM Decoder (R)</cell><cell>18.3</cell></row><row><cell>Majority Template Baseline</cell><cell>11.7</cell></row></table><note>Accuracies (↑) of models on SVAMP without questions. The 5-fold CV accuracy scores for ASDiv-A without questions are restated for easier comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell>: Accuracies (↑) of the constrained model on</cell></row><row><cell>SVAMP. (R) denotes that the model is provided with</cell></row><row><cell>non-contextual RoBERTa pretrained embeddings while</cell></row><row><cell>(S) denotes that the model is trained from scratch.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 13 .</head><label>13</label><figDesc>Both the Question Sensitivity and Struc-</figDesc><table><row><cell>Removed Category</cell><cell># Removed</cell><cell>Change in</cell></row><row><cell></cell><cell>Examples</cell><cell>Accuracy (∆)</cell></row><row><cell>Question Sensitivity</cell><cell>462</cell><cell>+13.7</cell></row><row><cell>Reasoning Ability</cell><cell>649</cell><cell>-3.3</cell></row><row><cell>Structural Invariance</cell><cell>467</cell><cell>+4.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 13 :</head><label>13</label><figDesc>Change in accuracies when categories are removed. The Change in Accuracy ∆ = Acc(F ull −</figDesc><table><row><cell>Removed Variation</cell><cell># Removed</cell><cell>Change in</cell></row><row><cell></cell><cell>Examples</cell><cell>Accuracy (∆)</cell></row><row><cell>Same Obj, Diff Struct</cell><cell>325</cell><cell>+7.3</cell></row><row><cell>Diff Obj, Same Struct</cell><cell>69</cell><cell>+1.5</cell></row><row><cell>Diff Obj, Diff Struct</cell><cell>74</cell><cell>+1.3</cell></row><row><cell>Add Rel Info</cell><cell>264</cell><cell>+5.5</cell></row><row><cell>Change Info</cell><cell>149</cell><cell>+3.2</cell></row><row><cell>Invert Operation</cell><cell>255</cell><cell>-10.2</cell></row><row><cell>Change order of Obj</cell><cell>107</cell><cell>+2.3</cell></row><row><cell>Change order of Phrases</cell><cell>152</cell><cell>-3.3</cell></row><row><cell>Add Irrel Info</cell><cell>281</cell><cell>+6.9</cell></row></table><note>Cat) − Acc(F ull), where Acc(F ull) is the accuracy on the full set and Acc(F ull − Cat) is the accuracy on the set of examples left after removing all examples which were created using Category Cat either by itself, or in use with other categories.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 14</head><label>14</label><figDesc></figDesc><table><row><cell>: Change in accuracies when variations are re-</cell></row><row><cell>moved. The Change in Accuracy ∆ = Acc(F ull −</cell></row><row><cell>V ar) − Acc(F ull), where Acc(F ull) is the accuracy</cell></row><row><cell>on the full set and Acc(F ull − V ar) is the accuracy</cell></row><row><cell>on the set of examples left after removing all examples</cell></row><row><cell>which were created using Variation V ar either by itself,</cell></row><row><cell>or in use with other variations.</cell></row><row><cell>tural Invariance categories of variations show an</cell></row><row><cell>increase in accuracy when their examples are re-</cell></row><row><cell>moved, thereby indicating that they make SVAMP</cell></row><row><cell>more challenging. The decrease in accuracy for</cell></row><row><cell>the Reasoning Ability category can be attributed in</cell></row><row><cell>large part to the Invert Operation variation. This</cell></row><row><cell>is not surprising because most of the examples</cell></row><row><cell>created from Invert Operation are almost indistin-</cell></row><row><cell>guishable from examples in ASDiv-A, which the</cell></row><row><cell>model has seen during training. The scores for each</cell></row><row><cell>individual variation are provided in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 15 :</head><label>15</label><figDesc>Accuracy break-up according to the number of numbers in the input problem. 2 nums refers to the subset of problems which have only 2 numbers in the problem text. Similarly, 3 nums and 4 nums are subsets that contain 3 and 4 different numbers in the problem text respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 16</head><label>16</label><figDesc></figDesc><table><row><cell>: 5-fold cross-validation accuracies (↑) of</cell></row><row><cell>Transformer model on datasets. (R) means that the</cell></row><row><cell>model is provided with RoBERTa pretrained embed-</cell></row><row><cell>dings while (S) means that the model is trained from</cell></row><row><cell>scratch.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 17 :</head><label>17</label><figDesc>5-fold cross-validation accuracies (↑) of Transformer model on Question-removed datasets.</figDesc><table><row><cell></cell><cell cols="2">Transformer</cell></row><row><cell></cell><cell>S</cell><cell>R</cell></row><row><cell>Full Set</cell><cell>18.4</cell><cell>38.9</cell></row><row><cell>One-Op</cell><cell>18.6</cell><cell>40.5</cell></row><row><cell>Two-Op</cell><cell>17.8</cell><cell>33.9</cell></row><row><cell>ADD</cell><cell>22.3</cell><cell>36.3</cell></row><row><cell>SUB</cell><cell>17.1</cell><cell>37.5</cell></row><row><cell>MUL</cell><cell>17.9</cell><cell>28.3</cell></row><row><cell>DIV</cell><cell>18.6</cell><cell>53.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 18 :</head><label>18</label><figDesc>Results of Transformer model on the SVAMP challenge set. S indicates that the model is trained from scratch. R indicates that the model was trained with RoBERTa embeddings. The first row shows the results for the full dataset. The next two rows show the results for subsets of SVAMP composed of examples that have equations with one operator and two operators respectively. The last four rows show the results for subsets of SVAMP composed of examples of type Addition, Subtraction, Multiplication and Division respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 19 :</head><label>19</label><figDesc>Different hyperparameters and the values considered for each of them in the models. The best hyperparameters for each model for 5-fold cross-validation on ASDiv-A are highlighted in bold. Average Time/Epoch is measured in seconds.</figDesc><table><row><cell></cell><cell cols="2">Transformer</cell></row><row><cell>Hyperparameters</cell><cell>Scratch</cell><cell>RoBERTa</cell></row><row><cell>I/P and O/P Embedding Size</cell><cell>[128, 256]</cell><cell>[768]</cell></row><row><cell>FFN Size</cell><cell>[256, 384]</cell><cell>[256, 384]</cell></row><row><cell>heads</cell><cell>[2, 4]</cell><cell>[2, 4]</cell></row><row><cell>Number of Encoder Layers</cell><cell>[1, 2]</cell><cell>[1, 2]</cell></row><row><cell>Number of Decoder Layers</cell><cell>[1, 2]</cell><cell>[1, 2]</cell></row><row><cell>Learning Rate</cell><cell>[5e-5, 8e-5, 1e-4]</cell><cell>[5e-5, 8e-5, 1e-4]</cell></row><row><cell>Embedding LR</cell><cell>[5e-5, 8e-5, 1e-4]</cell><cell>[1e-5, 5e-6]</cell></row><row><cell>Batch Size</cell><cell>[4, 8]</cell><cell>[4, 8]</cell></row><row><cell>Dropout</cell><cell>[0.1]</cell><cell>[0.1]</cell></row><row><cell># Parameters</cell><cell>0.67M</cell><cell>132M</cell></row><row><cell>Epochs</cell><cell>100</cell><cell>100</cell></row><row><cell>Avg Time/Epoch</cell><cell>10</cell><cell>30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 20 :</head><label>20</label><figDesc>Different hyperparameters and the values considered for each of them in the Transformer model. The best hyperparameters for 5-fold cross-validation on ASDiv-A are highlighted in bold. Average Time/Epoch is measured in seconds.</figDesc><table><row><cell>Tag</cell><cell>Description</cell></row><row><cell>NUMx</cell><cell>Number</cell></row><row><cell>NAMEx</cell><cell>Names of Persons</cell></row><row><cell>OBJsx</cell><cell>Singular Object</cell></row><row><cell>OBJpx</cell><cell>Plural Object</cell></row><row><cell>MODx</cell><cell>Modifier</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 21 :</head><label>21</label><figDesc>List of tags used in annotated templates. x denotes the index of the tag.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head></head><label></label><figDesc>1. Apply the Question Sensitivity variations on the Base Example. 2. Apply the Invert Operation variation on the Base Example and on all the variations obtained so far. 3. Apply the Add relevant information variation on the Base Example. Then considering these variations as Base Examples, apply the Question Sensitivity variations. 4. Apply the Add irrelevant information variation on the Base Example and on all the variations obtained so far. 5. Apply the Change information variation on the Base Example and on all the variations obtained so far. 6. Apply the Change order of Objects and Change order of Events or Phrases variations on the Base Example and on all the variations obtained so far.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head>Table 25</head><label>25</label><figDesc>provides some variations for the example in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head>Table 23 :</head><label>23</label><figDesc>An example of suggested templates. Note that the preprocessing script could not succesfully tag crayons as OBJp1 . Example Body N AM E1 has N U M 1 packs of crayons . Each pack has N U M 2 crayons in it . She also has N U M 3 extra crayons . Script Example Question How many crayons does N AM E1 have altogether ? Base Example Body N AM E1 has N U M 1 packs of OBJp1 . Each pack has N U M 2 OBJp1 in it . She also has N U M 3 extra OBJp1 . Base Example Question How many OBJp1 does N AM E1 have altogether ?</figDesc><table><row><cell>Script OBJ1</cell><cell>crayon, crayons</cell></row><row><cell>Alternate for OBJ1</cell><cell>pencil, pencils</cell></row><row><cell>Alternate for M OD</cell><cell>small, large</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38"><head>Table 24 :</head><label>24</label><figDesc>An example of editing the Suggested Templates. The edits are indicated in green. Example Body N AM E1 has N U M 1 packs of OBJp1 . Each pack has N U M 2 OBJp1 in it. She also has N U M 3 extra OBJp1 Base Example Question How many OBJp1 does N AM E1 have altogether ? AM E1 has N U M 1 packs of OBJp1 . Each pack has N U M 2 OBJp1 in it. She also has N U M 3 extra OBJp1 .Variation QuestionHow many OBJp1 does N AM E1 have in packs?</figDesc><table><row><cell>Base Base Example Equation</cell><cell>N U M 1  *  N U M 2 + N U M 3</cell></row><row><cell>Category</cell><cell>Question Sensitivity</cell></row><row><cell>Variation</cell><cell>Same Object, Different Structure</cell></row><row><cell cols="2">Variation Body N Variation Equation N U M 1  *  N U M 2</cell></row><row><cell>Category</cell><cell>Structural Invariance</cell></row><row><cell>Variation</cell><cell>Add irrelevant information</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_39"><head>Table 25 :</head><label>25</label><figDesc></figDesc><table /><note>Example Variations</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The dataset and code are available at: https://github.com/arkilpatel/SVAMP</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their constructive comments. We would also like to thank our colleagues at Microsoft Research for providing valuable feedback. We are grateful to Monojit Choudhury for discussions about creating the dataset. We thank Kabir Ahuja for carrying out preliminary experiments that led to this work. We also thank Vageesh Chandramouli and Nalin Patel for their help in dataset construction.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Problem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted Equation Answer</head><p>Mike had 8 games. After he gave some to his friend he had 5 left . How many games did he give to his friend?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">-5 3</head><p>After Mike gave some games to his friend he had 5 left . If he had 8 games initially, how many games did he give to his friend?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">-8 -3</head><p>Jack bought 5 radios but only 2 of them worked. How many radios did not work? 5 -2 3 Jack bought 5 radios but only 2 of them worked. How many more radios did not work than those that did?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">-2 3</head><p>Ross had 6 marbles. He sold 2 marbles to Joey. How many marbles does Ross have now?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">-2 4</head><p>Ross had 6 marbles. Joey sold 2 marbles to Ross. How many marbles does Ross have now?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">-2 4</head><p>Bob collected 7 cans. He lost 3 of them. How many cans does Bob have now? 7 -3 4 Bob had 7 cans. He collected 3 more. How many cans does Bob have now? 7 -3 4</p><p>Joey had 9 pens. he used 4 of them. How many pens does he have now? 9 -4 5 Joey used 4 pens. If he had 9 pens intially, how many pens does he have now? 4 -9 -5</p><p>Jill read 30 pages in 10 days. How many pages did she read per day? 30 / 10 3 Jill can read 3 pages per day. How many pages can she read in 10 days? 3 / 10 0.33</p><p>Mary's hair was 15 inches long. After she did a haircut, it was 10 inches long . how much did she cut off ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15">-10 5</head><p>Mary cut off 5 inches of her hair. If her hair is now 10 inches long, how long was it earlier? 5 -10 -5 <ref type="table">Table 26</ref>: Attention paid to specific words by the constrained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Problem Correct Equation Predicted Equation</head><p>Every day ryan spends 6 hours on learning english and 2 hours on learning chinese. How many more hours does he spend on learning english than he does on learning chinese?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">-2 2 -6</head><p>In a school there are 34 girls and 841 boys. How many more boys than girls does the school have?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="841">-34 34 -841</head><p>David did 44 push-ups in gym class today. David did 9 more push-ups than zachary. How many push-ups did zachary do?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="44">-9 + 9</head><p>Dan has $ 3 left with him after he bought a candy bar for $ 2. How much money did he have initially?</p><p>3 + 2 3 -2 Jake has 11 fewer peaches than steven. If jake has 17 peaches. How many peaches does steven have?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">+ 17 17 -11</head><p>Kelly gives away 91 nintendo games. How many did she have initially if she still has 92 games left?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="91">+ 92 92 -91</head><p>Emily is making bead necklaces for her friends. She was able to make 18 necklaces and she had 6 beads. How many beads did each necklace need? An industrial machine can make 6 shirts a minute. It worked for 5 minutes yesterday and for 12 minutes today. How many shirts did machine make today? 6 * 12 5 + 12 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MathQA: Towards interpretable math word problem solving with operation-based formalisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1245</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2357" to="2367" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis methods in neural language processing: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00254</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="49" to="72" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pay attention to the ending:strong neural baselines for the ROC story cloze task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2097</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="616" to="622" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Basmova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Gottumukkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<editor>Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang</editor>
		<imprint/>
	</monogr>
	<note>and Ben Zhou. 2020. Evaluating models&apos; local decision boundaries via contrast sets</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="107" to="112" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning fine-grained expressions to solve math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1084</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="805" to="814" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How well do computers solve math word problems? large-scale dataset construction and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1084</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="887" to="896" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How well do computers solve math word problems? large-scale dataset construction and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1084</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="887" to="896" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MAWPS: A math word problem repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1152" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How can we accelerate progress towards human-like linguistic generalization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tal Linzen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.465</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5210" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1166</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1334</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3428" to="3448" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A diverse corpus for evaluating and developing English math word problem solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Chun</forename><surname>Shen-Yun Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Yih</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.92</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="975" to="984" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarial NLI: A new benchmark for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.441</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4885" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hypothesis only baselines in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparajita</forename><surname>Haldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S18-2023</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Seventh Joint Conference on Lexical and Computational Semantics<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="180" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semantically-aligned universal tree-structured solver for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond accuracy: Behavioral testing of NLP models with CheckList</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongshuang</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.442</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4902" to="4912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Shachar Rosenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Jacovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3702" to="3710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mapping to declarative knowledge for word problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00012</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="159" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to solve geometry problems from natural language demonstrations in textbooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-1029</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</title>
		<meeting>the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="251" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep neural solver for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1088</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is not not explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A goal-driven tree-structured neural model for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/736</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5299" to="5305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The gap of semantic parsing: A survey on automatic math word problem solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2914054</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2287" to="2305" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph-totree learning for solving math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.362</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3928" to="3937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Seq2Seq GTS Graph2Tree Constrained Hyperparameters Scratch RoBERTa Scratch RoBERTa Scratch RoBERTa Scratch RoBERTa Embedding Size</title>
		<imprint/>
	</monogr>
	<note>128, 256] [768] [128, 256] [768] [128, 256] [768] [128, 256</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Embedding</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>5e-4, 8e-4, 1e-3. 5e-6, 8e-6, 1e-5. 8e-4, 1e-3, 2e-3. 5e-6, 8e-6, 1e-5. 8e-4, 1e-3, 2e-3. 5e-6, 8e-6, 1e-5. 1e-3, 2e-3. 1e-3, 2e-3</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
