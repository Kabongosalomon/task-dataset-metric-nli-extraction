<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Direction-aware Spatial Context Features for Shadow Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Centre for Smart Health</orgName>
								<orgName type="department" key="dep2">School of Nursing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Centre for Smart Health</orgName>
								<orgName type="department" key="dep2">School of Nursing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Direction-aware Spatial Context Features for Shadow Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Shadow detection is a fundamental and challenging task, since it requires an understanding of global image semantics and there are various backgrounds around shadows. This paper presents a novel network for shadow detection by analyzing image context in a direction-aware manner. To achieve this, we first formulate the direction-aware attention mechanism in a spatial recurrent neural network (RNN) by introducing attention weights when aggregating spatial context features in the RNN. By learning these weights through training, we can recover direction-aware spatial context (DSC) for detecting shadows. This design is developed into the DSC module and embedded in a CNN to learn DSC features at different levels. Moreover, a weighted cross entropy loss is designed to make the training more effective. We employ two common shadow detection benchmark datasets and perform various experiments to evaluate our network. Experimental results show that our network outperforms state-of-the-art methods and achieves 97% accuracy and 38% reduction on balance error rate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Shadow is a monocular cue in human vision for depth and geometry perception. Knowing where the shadow is, on the one hand, allows us to acquire the lighting direction <ref type="bibr" target="#b13">[14]</ref> and scene geometry <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b9">10]</ref>, as well as the camera location and parameters <ref type="bibr" target="#b8">[9]</ref>. However, the presence of shadow, on the other hand, could deteriorate the performance of many fundamental computer vision tasks, such as object detection and tracking <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>. Hence, shadow detection has long been a fundamental problem in computer vision.</p><p>Existing methods detect shadows by developing physical models of color and illumination <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, or by using datadriven approaches based on hand-crafted features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref> or learned features <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b17">18]</ref>. While state-of-the-art meth- * Joint first authors <ref type="figure">Figure 1</ref>: In this example image, region B would give a stronger indication that A is a shadow compared to region C. This motivates us to analyze the global image context in a direction-aware manner for shadow detection.</p><p>ods have already achieved accuracy of 87% to 90% on two benchmark datasets <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref>, they may misrecognize black objects as shadows and miss unobvious shadows. These situations are revealed by the balance error rate (BER), which equally considers shadow and non-shadow regions; see Section 4 for quantitative comparison results.</p><p>Shadow detection requires an understanding of global image semantics, as shown very recently in <ref type="bibr" target="#b17">[18]</ref>. To improve the understanding, we propose to analyze the image (or spatial) context in a direction-aware manner. Taking region A in <ref type="figure">Figure 1</ref> as an example, comparing it with regions B and C, region B would give a stronger indication that A is a shadow as compared to region C. Hence, spatial contexts along different directions would give different amount of contributions in suggesting the presence of shadows.</p><p>To take directional variance into account when reasoning the spatial contexts, we first design a network module called the direction-aware spatial context (DSC) module, or DSC module for short, by adopting a spatial recurrent neural network (RNN) to aggregate spatial contexts in four principal directions, and formulating the direction-aware attention mechanism in the RNN to learn attention weights for each direction. Then, we embed multiple copies of this DSC module in a convolutional neural network to learn DSC features in different layers <ref type="bibr">(scales)</ref>. After that, we combine the DSC features with convolutional features to predict a score map for each layer, and fuse the score maps into the final shadow detection map. The whole network is trained in an end-to-end manner with a weighted cross entropy loss. We summarize the major contributions of this work below:</p><p>• First, we design a novel attention mechanism in a spatial RNN and construct the DSC module to learn spatial contexts in a direction-aware manner.</p><p>• Second, we present a new shadow detection network that adopts multiple DSC modules to learn directionaware spatial contexts in different layers. A weighted cross entropy loss is designed to balance the detection accuracy in shadow and non-shadow regions. This network has potential for use in other applications such as saliency detection and semantic segmentation.</p><p>• Third, we evaluate our network on two benchmark sets and compare it with several state-of-the-art methods on shadow detection, saliency detection, and semantic image segmentation. Results show that our network outperforms previous methods with over 97% accuracy and 38% reduction on the balance error rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we focus on discussing works on singleimage shadow detection rather than trying to be exhaustive.</p><p>Traditionally, single-image shadow detection is done by exploiting physical models of illumination and color <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b22">23]</ref>. This approach, however, tends to produce satisfactory results only for wide dynamic range images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. Another approach learns shadow properties using handcrafted features based on annotated shadow images. It first describes image regions by feature descriptors and then classifies the regions into shadow and non-shadow regions. Features like color <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25]</ref>, texture <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25]</ref>, edge <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b6">7]</ref> and T-junction <ref type="bibr" target="#b14">[15]</ref> are commonly used for shadow detection followed by classifiers like decision tree <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref> and SVM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25]</ref>. However, since handcrafted features have limited capability in describing shadows, this approach often fails for complex cases.</p><p>Convolutional neural network (CNN) is recently demonstrated to be a very powerful tool to learn features for detecting shadows, with results clearly outperforming previous approaches. Khan et al. <ref type="bibr" target="#b10">[11]</ref> used multiple CNNs to learn features in super pixels and along object boundaries, and fed the output features to a conditional random field to locate shadows. Shen et al. <ref type="bibr" target="#b19">[20]</ref> presented a deep structured shadow edge detector and employed structured labels to improve the local consistency of the predicted shadow map. Vicente et al. <ref type="bibr" target="#b23">[24]</ref> trained stacked-CNN using a large data set with noisy annotations. They minimized the sum of squared leave-one-out errors for image clusters to recover the annotations, and trained two CNNs to detect shadows.</p><p>Very recently, Hosseinzadeh et al. <ref type="bibr" target="#b5">[6]</ref> detected shadows using a patch-level CNN and a shadow prior map generated from hand-crafted features, while Nguyen et al. <ref type="bibr" target="#b17">[18]</ref> developed scGAN with a sensitivity parameter to adjust weights in the loss functions. Although the shadow detection accuracy keeps improving on the benchmark datasets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b23">24]</ref>, existing methods may still misrecognize black objects as shadows and miss unobvious shadows in the testing images. The most recent work by Nguyen et al. <ref type="bibr" target="#b17">[18]</ref> emphasized the importance of reasoning global semantics for shadow detection. Compared to this work, we suggest to consider the directional variance when analyzing the spatial context. Results show that our method can further outperform <ref type="bibr" target="#b17">[18]</ref> in terms of both the accuracy and the BER value. <ref type="figure" target="#fig_0">Figure 2</ref> presents the workflow of the overall shadow detection network that employs the DSC module (see <ref type="bibr">Figure 4)</ref> to learn direction-aware spatial context features. Our network takes the whole image as input and outputs the shadow detection map in an end-to-end manner. First, it begins by using a convolutional neural network (CNN) to extract a set of hierarchical feature maps, which encode fine details and semantic information in different scales over the CNN layers. Second, for each layer, we employ a DSC module to harvest spatial contexts in a direction-aware manner and produce DSC features. Third, we concatenate the DSC features with corresponding convolutional features, and upsample the concatenated feature map to the size of the input image. Moreover, we further combine the upsampled feature maps into the multi-level integrated features (MLIF) with a convolution layer (via a 1 × 1 kernel), and apply the deep supervision mechanism <ref type="bibr" target="#b26">[27]</ref> to impose the supervision signals to each layer as well as to the MLIF and predict a score map at each layer. Lastly, we fuse all the predicted score maps into the final shadow map output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In the following subsections, we first elaborate how the DSC module generates DSC features, and then introduce the training and testing strategies in the shadow detection network. <ref type="figure">Figure 4</ref> shows our DSC module architecture, which takes feature maps as input and outputs DSC features. In this subsection, we first describe the concept of spatial context features and the spatial RNN model (Section 3.1.1), and then elaborate how we formulate the direction-aware attention mechanism in a spatial RNN to learn attention weights and generate DSC features (Section 3.1.2).  <ref type="figure">Figure 4</ref>) to generate direction-aware spatial context (DSC) features for each layer; (iii) we concatenate the DSC features with convolutional features at each layer and upsample the concatenated feature maps to the size of the input image; (iv) we combine the upsampled feature maps into the multi-level integrated features (MLIF), and predict a score map based on the features for each layer by a deep supervision mechanism <ref type="bibr" target="#b26">[27]</ref>; and (v) lastly, we fuse the resulting score maps to produce the final shadow detection result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Direction-aware Spatial Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Spatial Context Features</head><p>Recurrent neural network (RNN) <ref type="bibr" target="#b15">[16]</ref> is an effective model to process 1D sequential data via an array of input nodes (to receive data), an array of hidden nodes (to update internal states based on past and present data), and an array of output nodes (to output data). There are three kinds of data translations in an RNN: from input nodes to hidden nodes, from hidden nodes to output nodes, and between adjacent hidden nodes. By iteratively performing the data translations, the data received at input nodes can propagate across the hidden nodes, and eventually produce results at the output nodes.</p><p>For processing image data with 2D spatial context, RNN has been extended to build the spatial RNN model <ref type="bibr" target="#b0">[1]</ref>; see the schematic illustration in <ref type="figure" target="#fig_1">Figure 3</ref>. Taking a 2D feature map from a CNN as input, the spatial RNN model first uses a 1×1 convolution to perform an input-to-hidden data trans-lation. Then, it applies four independent data translations to aggregate local spatial context along each principal direction (left, right, up, and down), and fuses the results into an intermediate feature map; see <ref type="figure" target="#fig_1">Figure 3</ref>(b). Lastly, the whole process is repeated to further propagate the aggregated spatial context in each principal direction, and then to generate the overall spatial context; see <ref type="figure" target="#fig_1">Figure 3</ref> Hence, by having two rounds of data translations, each pixel can obtain necessary global spatial context for learning features and solving the problem that the network is intended for.</p><p>To perform data translations in a spatial RNN, we follow the IRNN model <ref type="bibr" target="#b0">[1]</ref>, since it is fast, easy to train, and has a good performance for long-range data dependencies <ref type="bibr" target="#b0">[1]</ref>. Denote h i,j as the feature at pixel (i, j), we perform one round of data translations to the right (similarly for the other directions) by repeating the following operation n times:</p><formula xml:id="formula_0">h i,j = max( α right h i,j−1 + h i,j , 0 ) ,<label>(1)</label></formula><p>where n is the width of the feature map and α right is the weight parameter in the recurrent translation layer for the right direction. Note that α right , as well as weights for the other directions, are initialized to be an identity matrix and are learned by the training process automatically.  <ref type="figure">Figure 4</ref>: The schematic illustration of the direction-aware spatial context module (DSC module). We compute directionaware spatial context by adopting a spatial RNN to aggregate spatial contexts in four principal directions with two rounds of recurrent translations, and formulate the attention mechanism to generate maps of attention weights to combine context features for different directions. We use the same set of weights in both rounds of recurrent translations. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Direction-aware Spatial Context Features</head><p>To learn spatial context in a direction-aware manner, we formulate the direction-aware attention mechanism in a spatial RNN to learn attention weights and generate directionaware spatial context (DSC) features.</p><p>Direction-aware attention mechanism. The purpose of the direction-aware attention mechanism is to enable the spatial RNN to selectively leverage the spatial context aggregated along different directions by means of learning. See the top-left blocks in the DSC module shown in <ref type="figure">Figure 4</ref>. First, we employ two successive convolutional layers (with 3×3 kernels) followed by the ReLU <ref type="bibr" target="#b12">[13]</ref> non-linear operation, and then the third convolutional layer (with 1×1 kernels) to generate W. Then, we split W into four maps of attention weights denoted as W left , W down , W right , and W up . Mathematically, if we denote the above operators as f att and the input feature maps as X, we have</p><formula xml:id="formula_1">W = f att ( X ; θ ) ,<label>(2)</label></formula><p>where θ denotes the parameters to be learned by f att , and f att is also known as the attention estimator network. See again the DSC module shown in <ref type="figure">Figure 4</ref>. The four maps of weights are multiplied with the spatial context features (from the recurrent data translations) along different directions in an element-wise manner. Therefore, after we train the network with the shadow dataset, the network can learn θ for producing suitable attention weights to selectively leverage the spatial context in the spatial RNN.</p><p>Completing the DSC module. Next, we further provide additional details about the DSC module. As shown in <ref type="figure">Figure 4</ref>, after we multiply the spatial context features with the attention weights, we concatenate the results and use a 1×1 convolution to simulate a hidden-to-hidden data translation and reduce the feature dimensions to a quarter of the dimension size. Then, we perform the second round of recurrent translations and use the same set of attention weights to select spatial context. We empricially find that the network delivers higher performance, if we share the attention weights rather than using two separate sets of weights. Note that these attention weights are learnt based on the deep features extracted from the input images, and they may vary from images to images. Lastly, we utilize a 1 × 1 convolution followed by the ReLU <ref type="bibr" target="#b12">[13]</ref> non-linear operation on the concatenated feature maps to simulate the hidden-to-output translation and produce the output DSC features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training and Testing Strategies</head><p>Our network is built upon the VGG network <ref type="bibr" target="#b21">[22]</ref>, where we apply a DSC module to each layer, except for the first layer, which involves a large memory footprint.</p><p>Loss Function. In natural images, shadows usually occupy smaller areas than non-shadow regions. Hence, if the loss function simply aims for overall accuracy, it will incline to match the non-shadow regions, which have far more pixels. Therefore, we use a weighted cross-entropy loss to optimize the whole network in the training process.</p><p>In detail, assume that the ground truth value of a pixel is y (where y=1, if it is in shadow, and y=0, otherwise) and the prediction label of the pixel is p (where p ∈ [0, 1]). The weighted cross entropy loss L equals L 1 + L 2 :</p><formula xml:id="formula_2">L 1 = −( N n N p + N n )y log(p)−( N p N p + N n )(1−y) log(1−p) ,<label>(3)</label></formula><p>and</p><formula xml:id="formula_3">L 2 = −(1 − T P N p )y log(p) − (1 − T N N n )(1 − y) log(1 − p) ,<label>(4)</label></formula><p>where T P and T N are the number of true positives and true negatives, and N p and N n are the number of shadow and non-shadow pixels, respectively, so N p +N n is the total number of pixels. In practice, L 1 helps balance the detection of shadows and non-shadows; if the area of shadows is less than that of the non-shadow region, we will penalize misclassified shadow pixels more than misclassified nonshadow pixels. On the other hand, L 2 helps the network focus on learning the class (shadow or non-shadow) that is difficult to be classified <ref type="bibr" target="#b20">[21]</ref>. This can be achieved, since the weight in loss function for shadow (or non-shadow) class is large when the number of correctly-classified shadow (or non-shadow) pixels is small, and vice versa.</p><p>We use the above loss function for each layer in the shadow detection network presented in <ref type="figure" target="#fig_0">Figure 2</ref>. Hence, the overall loss function L overall is a summation of the individual loss on all the predicted score maps:</p><formula xml:id="formula_4">L overall = i w i L i + w m L m + w f L f ,<label>(5)</label></formula><p>where w i and L i denote the weight and loss of the i-th layer (level) in the overall network, respectively; w m and L m are the weight and loss of the MLIF layer; and w f and L f are the weight and loss of the fusion layer, which is the last layer in the overall network to produce the final shadow detection result; see <ref type="figure" target="#fig_0">Figure 2</ref>. Note that all the weights w i , w m and w f are empirically set to be 1.</p><p>Training parameters. To accelerate the training process while reducing over-fitting, we initialize parameters in the feature extraction layers (see the frontal part of the network in <ref type="figure" target="#fig_0">Figure 2</ref>) by the well-trained VGG network <ref type="bibr" target="#b21">[22]</ref> and the parameters in other layers by random noise. Stochastic gradient descent is used to optimize the whole network with a momentum value of 0.9 and a weight decay of 5×10 −4 . We set the learning rate as 10 −8 and terminate the learning process after 12k iterations. Moreover, we horizontally flip images for data argumentation. Note that we build the model on Caffe <ref type="bibr" target="#b7">[8]</ref> with a mini-batch size of 1.</p><p>Inference. In the testing process, our network produces one score map for each layer, including the MLIF layer and the fusion layer, with a supervision signal added to each layer. After that, we compute the mean of the score maps over the MLIF layer and the fusion layer to produce the final prediction map. Lastly, we apply the fully connected conditional field <ref type="bibr" target="#b11">[12]</ref> to improve the detection result by considering the spatial coherence between neighborhood pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>Benchmark datasets. Two benchmark datasets are employed in this work. The first one is the SBU Shadow Dataset <ref type="bibr" target="#b23">[24]</ref>, which is the largest publicly available annotated shadow dataset with 4089 training images and 638 testing images. It includes a wide variety of scenes, e.g., urban, beach, mountain, roads, parks, snow, animals, vehicles, and houses, and covers various types of pictures, e.g., aerial, landscape, close range, and selfies. The second benchmark dataset we employed is the UCF Shadow Dataset <ref type="bibr" target="#b29">[30]</ref>. It includes 145 training images and 76 testing images, and covers outdoor scenes with various backgrounds. We train our shadow detection network using the SBU training set.</p><p>Evaluation metrics. We employ two commonly-used metrics to quantitatively evaluate the shadow detection performance. The first one is the accuracy metric:</p><formula xml:id="formula_5">accuracy = T P + T N N p + N n ,<label>(6)</label></formula><p>where T P , T N , N p and N n are true positives, true negatives, number of shadow pixels, and number of non-shadow pixels, respectively, as defined in Section 3.2. Since N p is usually much smaller than N n in natural images, we employ the second metric called the balance error rate (BER) to obtain a more balanced evaluation by equally considering the shadow and non-shadow regions:</p><formula xml:id="formula_6">BER = (1 − 1 2 ( T P N p + T N N n )) × 100 .<label>(7)</label></formula><p>Note that unlike the accuracy metric, for BER, the lower its value, the better the detection result is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the State-of-the-art Shadow Detection Methods</head><p>We compare our method with four recent shadow detection methods: scGAN <ref type="bibr" target="#b17">[18]</ref>, stacked-CNN <ref type="bibr" target="#b23">[24]</ref>, patched-CNN <ref type="bibr" target="#b5">[6]</ref> and Unary-Pairwise <ref type="bibr" target="#b4">[5]</ref>. Among them, the first three are deep-learning-based methods, while the last one is based on hand-crafted features. For a fair comparison, the shadow detection results of other methods are obtained either directly from results provided by the authors, or by generating them using implementations provided by the authors with recommended parameter setting. input image ground truth DSC (ours) scGAN <ref type="bibr" target="#b17">[18]</ref> stkd'-CNN <ref type="bibr" target="#b23">[24]</ref> patd'-CNN <ref type="bibr" target="#b5">[6]</ref> SRM <ref type="bibr" target="#b25">[26]</ref> Amulet <ref type="bibr" target="#b27">[28]</ref> PSPNet <ref type="bibr" target="#b28">[29]</ref> Figure 5: Visual comparison of shadow maps produced by our method and other methods (4th-9th columns) against ground truths shown in 2nd column. Note that stkd'-CNN and patd'-CNN stand for stacked-CNN and patched-CNN, respectively. <ref type="table">Table 1</ref>: Comparing our method (DSC) with state-of-thearts methods for shadow detection (scGAN <ref type="bibr" target="#b17">[18]</ref>, stacked-CNN <ref type="bibr" target="#b23">[24]</ref>, patched-CNN <ref type="bibr" target="#b5">[6]</ref> and Unary-Pairwise <ref type="bibr" target="#b4">[5]</ref>), for saliency detection (SRM <ref type="bibr" target="#b25">[26]</ref> and Amulet <ref type="bibr" target="#b27">[28]</ref>), and for semantic image segmentation (PSPNet <ref type="bibr" target="#b28">[29]</ref>  <ref type="bibr" target="#b17">[18]</ref> 0.90 9.10 0.87 11.50 stacked-CNN <ref type="bibr" target="#b23">[24]</ref> 0.88 11.00 0.85 13.00 patched-CNN <ref type="bibr" target="#b5">[6]</ref> 0.88 11.56 --Unary-Pairwise <ref type="bibr" target="#b4">[5]</ref> 0.86 25.03 --SRM <ref type="bibr" target="#b25">[26]</ref> 0.96 7.25 0.94 9.81 Amulet <ref type="bibr" target="#b27">[28]</ref> 0.93 15.13 0.92 15.17 PSPNet <ref type="bibr" target="#b28">[29]</ref> 0.95 8.57 0.93 11.75 <ref type="table">Table 1</ref> reports the comparison results, where we can see that our method outperforms the others in terms of both accuracy and BER for both benchmark datasets. Note that our shadow detection network is trained using the SBU training set <ref type="bibr" target="#b23">[24]</ref>, but it still outperforms the others also for the UCF dataset, thus demonstrating its generalization ability.</p><p>We further provide visual comparison results in <ref type="figure">Figures 5 and 6</ref>, which show various challenging cases, e.g., a light shadow next to a dark shadow, shadows around complex backgrounds, and black objects around shadows. Without understanding the global image semantics, it is hard to locate these shadows, and non-shadow regions would be easily misrecognized as shadows. From the results, we can see that our method can effectively locate shadows and avoid false positives as compared to the others; for black objects misrecognized as shadows by other methods, our method could still recognize them as non-shadows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Saliency Detection and Semantic Segmentation Methods</head><p>In general, deep networks designed for saliency detection and semantic image segmentation may also be used for shadow detection by training the networks using datasets of annotated shadows. Hence, we conduct another experiment by using two recent deep models for saliency detection, i.e., SRM <ref type="bibr" target="#b25">[26]</ref> and Amulet <ref type="bibr" target="#b27">[28]</ref>, and a recent deep model for semantic image segmentation, i.e., PSPNet <ref type="bibr" target="#b28">[29]</ref>.</p><p>For a fair comparison, we re-train their models on the SBU training set using implementations provided by the authors, and adjust the training parameters to obtain the best shadow detection results. The last three rows in <ref type="table">Table 1</ref> report the comparison results in terms of the accuracy and BER metrics. Although these methods achieve good re-input image ground truth DSC (ours) scGAN <ref type="bibr" target="#b17">[18]</ref> stkd'-CNN <ref type="bibr" target="#b23">[24]</ref> patd'-CNN <ref type="bibr" target="#b5">[6]</ref> SRM <ref type="bibr" target="#b25">[26]</ref> Amulet <ref type="bibr" target="#b27">[28]</ref> PSPNet <ref type="bibr" target="#b28">[29]</ref> Figure 6: More visual comparison results (continue from <ref type="figure">Figure 5</ref>).  <ref type="bibr" target="#b23">[24]</ref>: "basic" denotes the architecture shown in <ref type="figure">Figure 4</ref> but without all DSC modules; "basic+context" denotes the "basic" network with spatial context but not direction-aware spatial context; and "DSC" is the overall network in <ref type="figure">Figure</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on the DCS Module</head><p>Component analysis. We perform an experiment to evaluate the effectiveness of the DSC module design. Here, we use the SBU dataset and consider two baseline networks.</p><p>The first baseline (denoted as "basic") is a network con- structed by removing all the DSC modules from the overall network shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The second baseline (denoted as "basic+context") considers spatial context but ignores the direction-aware attention weights. Compared with the first baseline, this network has all the DSC modules, but it removes the direction-aware attention mechanism inside the DSC modules, i.e., removing the computation of W and directly concatenating the context features without multiplying them with the attention weights; see <ref type="figure">Figure 4</ref>. This is equivalent to setting all the attention weights W to be one. <ref type="table" target="#tab_2">Table 2</ref> reports the comparison results, showing that our basic network with multi-scale features and the weighed cross entropy loss function can produce good results. Moreover, by considering spatial context and DSC features can lead to further obvious improvement. See also <ref type="figure" target="#fig_3">Figure 7</ref> for visual comparison results.</p><p>DSC architecture analysis. When we design the network structure in the DSC module, we encounter two questions: (i) how many rounds of recurrent translations we should employ in the spatial RNN; and (ii) whether to share the attention weights, or to use separate attention weights in different rounds of recurrent translations in the spatial RNN. We modify our network for these two parameters and produce the quantitative comparison results shown in <ref type="table" target="#tab_4">Table 3</ref>. From the results, we can see that having two rounds of recurrent translations and sharing the attention weights in both rounds produce the best detection result. We believe that when there is only one round of recurrent translations, the global context information cannot well propagate over the spatial domain, so there is insufficient information exchange for learning the shadows. On the other hand, when we have three rounds of recurrent translations or separate copies of attention weights, we will end up having too many parameters in the network, making it hard to be trained. and unconnected shadows; (c) no clear boundary between shadow and non-shadow regions; and (d) shadows of irregular shapes. Our method can still detect these shadows fairly well, but it fails in some extremely complex scenes: (a) a scene with many small shadows (see 1 st row in <ref type="figure">Figure 9)</ref>, where the features in deep layers lose the detail information and features in shallow layers lack the semantic information for the shadow context; (b) a scene with a large black region (see 2 nd row in <ref type="figure">Figure 9</ref>), where there are insufficient surrounding context to indicate whether it is a shadow or simply a black object; and (c) a scene with soft shadows (see 3 rd row in <ref type="figure">Figure 9</ref>), where the difference between the soft shadow regions and the non-shadow regions is small. The code, trained model, and more shadow detection results on the datasets are publicly available at https://xw-hu.github.io/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">More Shadow Detection Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a novel network for single-image shadow detection by harvesting direction-aware spatial context. Our key idea is to analyze multi-level spatial context in a direction-aware manner by formulating a direction-aware attention mechanism in a spatial RNN. In our mechanism, the network can automatically learn the attention weights for leveraging and composing the spatial context in different directions in the spatial RNN. In this way, we can produce direction-aware spatial context (DSC) features and formulate the DSC module for the task. Further, we adopt multiple DSC modules in a multi-layer convolutional neural network to predict score maps in different scales, and design a weighted cross entropy loss function to make effective the training process. In the end, we test our network on two benchmark datasets, compare it with various state-of-theart methods, and show the superiority of our network over the others in terms of the accuracy and BER metrics.</p><p>In future, we plan to explore the potential of our network for other applications such as saliency detection and semantic segmentation, and further enhance its capability for detecting time-varying shadows in videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The schematic illustration of the overall shadow detection network: (i) we extract features in different scales over the CNN layers from the input image; (ii) we embed a DSC module (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The schematic illustration of how spatial context information propagates in a two-round spatial RNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(c). Comparing with Figure 3(c), each pixel in Figure 3(a) knows only its local spatial context, while each pixel in Figure 3(b) further knows the spatial context along the four principal directions after the first round of data translations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Visual comparison results of component analysis.sults for both metrics, our method still outperforms them for both benchmark datasets. Please also refer to the last three columns inFigures 5 and 6for visual comparison results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>More results produced from our method. Failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8</head><label>8</label><figDesc>shows more shadow detection results: (a) light and dark shadows locate next to each other; (b) small</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Component analysis. We train three networks using the SBU training set and test them using the SBU testing set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>DSC architecture analysis. By varying the parameters in the DSC architecture (see 2nd and 3rd columns below), we can have produce a slightly different overall network and explore their performance (see last column).</figDesc><table><row><cell cols="3">number of rounds shared W? BER</cell></row><row><cell>1</cell><cell>-</cell><cell>5.85</cell></row><row><cell>2</cell><cell>Yes</cell><cell>5.59</cell></row><row><cell>3</cell><cell>Yes</cell><cell>5.85</cell></row><row><cell>2</cell><cell>No</cell><cell>6.02</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work is supported by the National </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detecting moving objects, ghosts, and shadows in video streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1337" to="1342" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entropy minimization for shadow removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="57" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the removal of shadows from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Hordley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="68" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single-image shadow detection and removal using paired regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fast shadow detection from a single image using a patched convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hosseinzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.09283</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What characterizes a shadow boundary under the sun and sky</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="898" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimating geo-temporal location of stationary cameras using shadow trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="318" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rendering synthetic objects into legacy photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno>157:1-157:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic feature learning for robust shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1939" to="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating natural illumination from a single outdoor image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting ground shadows in outdoor consumer photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="322" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Physical models for moving shadow and object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nadimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1079" to="1087" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shadow detection with conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4510" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attached shadow coding: Estimating surface normals from shadows under unknown reflectance and lighting conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1693" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shadow optimization from structured deep edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Wee</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2067" to="2074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">New spectrum ratio properties and features for shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="96" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale training of shadow detectors with noisilyannotated shadow examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Leaveone-out kernel optimization for shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3388" to="3396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to recognize shadows in monochromatic natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="230" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
