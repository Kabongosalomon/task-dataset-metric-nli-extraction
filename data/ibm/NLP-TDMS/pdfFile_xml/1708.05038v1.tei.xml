<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ConvNet Architecture Search for Spatiotemporal Feature Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
							<email>trandu@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Applied Machine Learning</orgName>
								<address>
									<settlement>Facebook</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
							<email>jamieray@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Applied Machine Learning</orgName>
								<address>
									<settlement>Facebook</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Applied Machine Learning</orgName>
								<address>
									<settlement>Facebook</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ConvNet Architecture Search for Spatiotemporal Feature Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning image representations with ConvNets by pretraining on ImageNet has proven useful across many visual understanding tasks including object detection, semantic segmentation, and image captioning. Although any image representation can be applied to video frames, a dedicated spatiotemporal representation is still vital in order to incorporate motion patterns that cannot be captured by appearance based models alone. This paper presents an empirical ConvNet architecture search for spatiotemporal feature learning, culminating in a deep 3-dimensional (3D) Residual ConvNet. Our proposed architecture outperforms C3D by a good margin on Sports-1M, UCF101, HMDB51, THU-MOS14, and ASLAN while being 2 times faster at inference time, 2 times smaller in model size, and having a more compact representation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Improving the design of ConvNet architectures has spurred significant progress in image understanding, with AlexNet <ref type="bibr" target="#b15">[17]</ref> succeeded by VGG <ref type="bibr" target="#b32">[34]</ref> and GoogleNet <ref type="bibr" target="#b36">[38]</ref>, then ResNet <ref type="bibr" target="#b7">[8]</ref>. While video understanding is another fundamental problem in computer vision, the progress in architectures for video classification [14, <ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b31">33]</ref> and representation learning <ref type="bibr" target="#b39">[41]</ref> is slower. There are three sources of friction impeding development of strong architectures for video. First, compared with image models, video Con-vNets have higher computation and memory cost. For example, according to <ref type="bibr" target="#b39">[41]</ref>, it takes 3 to 4 days to train a 3D ConvNet on UCF101 and about two months on Sports-1M, which makes extensive architecture search difficult even on UCF101. Second, there is not a standard benchmark to use for video architecture search. In the static image setting, ConvNets can be trained on ImageNet <ref type="bibr" target="#b26">[28]</ref> within a reasonable amount of time, and architectures that perform well on Imagenet have been shown to generalize to other tasks like object detection and segmentation. In the video domain, Sports-1M is shown to be helpful for generic feature learning <ref type="bibr" target="#b39">[41]</ref>, but is still too large to conduct archi-tecture search. In contrast, while UCF101 has a similar number of frames to ImageNet, they are highly correlated and the setting is tightly controlled. As a result, models trained on this benchmark overfit easily; experiments in <ref type="bibr" target="#b39">[41,</ref><ref type="bibr">14]</ref> showed that ConvNets trained from scratch can obtain 41 − 44% while finetuning from Sports1M can improve accuracy to 82% [41] on UCF101. Third, designing a video classification model is nontrivial; there are many choices to which resulting performance is sensitive. These include how to sample and pre-process the input, what type of convolutions, how many layers to use, and how to model the temporal dimension (e.g. joint spatiotemporal modeling or decouple spatial and temporal dimensions). Thus, while it's clear that progress in the image domain should be incorporated in video modeling, a naive transfer of image models to video classification (e.g. simply applying a 2D Resnet to video frames) is suboptimal.</p><p>In this paper, we address these issues by conducting a carefully-designed architecture search on a small benchmark (UCF101). One might argue that the generalization of these findings is limited by the bias of the dataset -essentially, overfitting the search to UCF101. We tackle this difficulty via two efforts. First, we constrain networks to have similar capacity (number of parameters) -they will still overfit, but the improvements in accuracy can be more confidently attributed to a single change in architecture, not capacity. Second, the observations on this small-dataset architecture search lead us to an efficient deep 3D Residual ConvNet architecture (that we term Res3D), which we show to be effective when trained on a much larger dataset (Sports-1M), producing strong results on different video benchmarks. In summary, this paper makes the following contributions:</p><p>• We conduct a ConvNet architecture search across multiple dimensions by training on the UCF101 action recognition task, and present empirical observations of sensitivity to each dimension (section 3).</p><p>• We propose (to our best knowledge) the first deep 3D Residual network and train it on a large-scale video benchmark for spatiotemporal feature learning.  <ref type="table">Table 1</ref>. Comparison between Res3D and C3D. Res3D outperforms C3D across different benchmarks by a good margin. Res3D achieves the best performance on different benchmarks including Sports-1M (among methods without long-term modeling), UCF101 and HMDB51 (among methods using only RGB input), THUMOS14, and ASLAN. We note that the 4.5% improvement gap on Sports1M is significant as random chance on this benchmark is about 0.2%.</p><p>• Our spatiotemporal representation achieves state-ofthe-art results on Sports-1M (when no long-term modeling is used), UCF101 and HMDB51 (when considering only RGB input), and competative performance on THUMOS14, and ASLAN.</p><p>• Our model is 2 times faster, 2 times smaller, and more compact than current deep video features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video understanding is one of the core computer vision problems and has been studied for decades. Many research contributions in video understanding have focused on developing spatiotemporal features for videos. Some proposed video representations include spatiotemporal interest points (STIPs) <ref type="bibr" target="#b17">[19]</ref>, SIFT-3D <ref type="bibr" target="#b28">[30]</ref>, HOG3D <ref type="bibr" target="#b13">[15]</ref>, Cuboids <ref type="bibr" target="#b1">[2]</ref>, and ActionBank <ref type="bibr" target="#b27">[29]</ref>. These representations are hand-designed and use different feature encoding schemes like feature histograms or pyramids. Among hand-crafted representations, improved Dense Trajectories (iDT) <ref type="bibr" target="#b40">[42]</ref> is known as the current state-of-the-art hand-crafted feature with strong results on different video classification problems.</p><p>Since the deep learning breakthrough in computer vision <ref type="bibr" target="#b15">[17]</ref> presented at the ImageNet Challenge 2012 <ref type="bibr" target="#b26">[28]</ref>, many ConvNet-based methods <ref type="bibr" target="#b19">[21]</ref> were proposed for image recognition. Simonyan and Zisserman proposed to stack multiple small 3 × 3 kernels to approximate bigger kernels (e.g. 5 × 5 or 7 × 7) with more non-linear RELU units in between, and obtained good image classification performance <ref type="bibr" target="#b32">[34]</ref> with an ConvNet architecture known as VGG. Various techniques have been developed to improve image classification including Batch Normalization <ref type="bibr" target="#b9">[10]</ref>, Parameterized-RELU <ref type="bibr" target="#b6">[7]</ref>, Spatial Pyramid Pooling <ref type="bibr" target="#b5">[6]</ref>. Inspired by the idea of Network in Network <ref type="bibr" target="#b20">[22]</ref>, different GoogleNet (a.k.a. Inception) models were proposed with strong performance on ImageNet <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b36">38]</ref>. Recently, He et al. proposed deep residual networks (Resnets), which won multiple tracks in the ImageNet 2015 challenges <ref type="bibr" target="#b7">[8]</ref>. By using residual connections, deeper ConvNets can be trained with much less overfitting.</p><p>Deep learning has also been applied to video understanding. 3D ConvNets were proposed for recognizing human actions in videos <ref type="bibr" target="#b10">[11]</ref>, and 3D convolutions were also used in Restricted Boltzmann Machines <ref type="bibr" target="#b38">[40]</ref> and stacked ISA <ref type="bibr" target="#b18">[20]</ref> to learn spatiotemporal features. Karpathy et al. <ref type="bibr">[14]</ref> proposed different fusion methods for video classification. Simonyan and Zisserman <ref type="bibr" target="#b31">[33]</ref> used two-stream networks to achieve high accuracy on action recognition. Feichtenhofer et al. enhanced these two-stream networks with Resnet architectures and additional connections between streams <ref type="bibr" target="#b3">[4]</ref>. Some two-stream-network based approaches including Temporal Segment Networks <ref type="bibr" target="#b42">[44]</ref>, Action Transformations <ref type="bibr" target="#b43">[45]</ref>, and Convolutional Fusion <ref type="bibr" target="#b4">[5]</ref> were proposed and achieved the best accuracy for human action recognition. Recently, Tran et al. proposed to train a deep 3D ConvNet architecture, called C3D, on a largescale dataset for spatiotemporal feature learning <ref type="bibr" target="#b39">[41]</ref>. The C3D features have strong performance on various tasks, including action recognition <ref type="bibr" target="#b39">[41]</ref>, action detection <ref type="bibr" target="#b30">[32]</ref>, video captioning <ref type="bibr" target="#b24">[26]</ref>, and hand gesture detection <ref type="bibr" target="#b21">[23]</ref>.</p><p>Our approach in this paper is mostly related to C3D <ref type="bibr" target="#b39">[41]</ref> and Resnet <ref type="bibr" target="#b7">[8]</ref>. Similar to C3D, we use 3D ConvNets to learn spatiotemporal features. However, while the work in <ref type="bibr" target="#b39">[41]</ref> is limited to searching for the 3D convolution temporal kernel length, we consider many other dimensions of architecture design. Furthermore, our search is designed to compare different architectures while constraining the model capacity (number of parameters). Our work is also related to Resnet <ref type="bibr" target="#b7">[8]</ref> in the way we constrain our search to Resnet architectures. However, we emphasize that the application of Resnet to video representation is challenging as we need to consider many nontrivial questions, which we answer empirically via carefully-designed experiments (section 3). Our newly proposed architecture (Res3D) outperforms C3D by a good margin across 5 different benchmarks while being 2x faster in run-time and 2x smaller in model size (section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture Search</head><p>In this section we present a large-scale search for ConvNet architectures amenable to spatiotemporal feature learning. We start with C3D <ref type="bibr" target="#b39">[41]</ref> as it is commonly used as a deep representation for videos. We also constrain our search space to deep residual networks (Resnet <ref type="bibr" target="#b7">[8]</ref>) owing to their good performance and simplicity. Due to the high computational cost of training deep networks, we conduct our architecture search on UCF101 [35] split 1. We also note that the high memory consumption of these networks, coupled with the need for a large minibatch to compute batch normalization statistics, prohibits exploring some parts of model space. The observations from these experiments are collectively adopted in designing our final proposed ConvNet architecture. We later train our final ar-chitecture on Sports-1M [14] and show the benefits of the learned spatiotemporal features across different video understanding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Remark on Generality</head><p>These are empirical findings on a small benchmark (UCF101). We attempt to limit the confounding effect of overfitting by constraining the capacity of each network in the search, so that differences in performance can be more confidently attributed to the design of a network, rather than its size. Nevertheless, one might ask whether these findings can generalize to other datasets (in particular, larger ones like Sports-1M). While it is prohibitive to replicate each experiment at larger scale, we will choose a few in following sections to show that the findings are consistent. We stress that while this protocol isn't ideal, it is practical and the resulting intuitions are valuable -nevertheless, we encourage the development of a benchmark more suited to architecture search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D Residual Networks</head><p>Notations: For simplicity we omit the channels, and denote input, kernel, and output as 3D tensors of L × H × W , where L, H, and W are temporal length, height, and width, respectively.</p><p>Basic architectures: Our basic 3D Resnet architectures are presented in <ref type="table" target="#tab_2">Table 2</ref>. These networks use an 8×112×112 input, the largest that can fit within GPU memory limits and maintain a large enough mini-batch. However, we skip every other frame, making this equivalent to the using C3D input and dropping the even frames. In summary, we modify 2D-Resnets by: changing the input from 224 × 224 to 8 × 112 × 112; changing all convolutions from d × d to 3 × d × d with all downsampling convolution layers using stride 2×2×2 except for conv1 with stride 1×2×2; and removing the first max-pooling layer.</p><p>Training and evaluation: We train these networks on UCF101 train split 1 using SGD with mini-batch size of 20. Similar to C3D, video frames are scaled to 128 × 171 and randomly cropped to 112 × 112. Batch normalization is applied at all convolution layers. The learning rate is set to 0.01 and divided by 10 after every 20k of iterations. Training is done at 90K iterations (about 15 epochs). We also conduct 2D-Resnet baselines where we replace the input with a single frame cropped to 112 × 112 and all 3D operations (convolution and pooling) with 2D analogues. <ref type="table">Table 3</ref> presents the clip accuracy of different networks on UCF101 test split 1. Compared to the 2D reference models, the 3D-Resnets perform better, and the deeper networks (34 layers) show little gain over the 18-layer ones for 2D or 3D. We note that these findings are not conclusive as the networks have different number of parameters. In the following architecture search experiments, all compared models use a   <ref type="table">Table 3</ref>. Accuracy on UCF101, varying Resnet. 3D-Resnets achieve better accuracy compared to 2D ones, however the finding is not conclusive because the 3D-Resnets have many more parameters. similar number ( 33M) of parameters.</p><p>Simplified networks: We note that by reducing the input size, we can further reduce the complexity of the network and the memory consumption of network training, thus accelerating architecture search. With a smaller input of 4 × 112 × 112, we have to adjust the stride of conv5 1 to 1 × 2 × 2. This simplification reduces the complexity of 3D-Resnet18 from 19.3 billion floating point operations (FLOPs) to 10.3 billion FLOPs while maintaining accuracy on UCF101 (within the margin of random chance, 0.96%). From now on we denote this network architecture as SR18 (the Simplified 3D-Resnet18), and use it as a baseline to modify for our following architecture search experiments.</p><p>Observation 1. Using 4 frames of input and a depth-18 network (SR18) achieves good baseline performance and fast training on UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">What are good frame sampling rates?</head><p>We use SR18 and vary the temporal stride of the input frames in the following set {1, 2, 4, 8, 16, 32}. At the lowest extreme the input is 4 consecutive frames which is roughly a 1/8-second-long clip. On the other hand, using stride 32 the input clip is coarsely sampled from a 128-frame long clip (∼ 4.5 to 5 seconds). <ref type="table">Table 4</ref> presents the accuracy of SR18 trained on inputs with different sampling rates.  <ref type="table">Table 4</ref>. Accuracy on UCF101, varying sampling rates. Temporal strides between 2 and 4 are reasonable, while 1 is a bit less accurate, suggesting these clips are too short and lack context. On the other hand, strides beyond 4 are significantly worse, maybe because they lack coherence and make it hard for 3D kernels to learn temporal information.</p><p>Input resolution (crop size) 64 <ref type="formula">(56) 128 (112)</ref>   <ref type="table">Table 5</ref>. Accuracy on UCF101, varying input resolution. With input resolution of 64, SR18 accuracy drops 9.5%, and 128 gives higher accuracy and less computation than 256. It's possible that at high resolution the larger conv1 kernels are harder to learn.</p><p>Observation 2. For video classification, sampling one frame out of every 2-4 (for videos within 25-30fps), and using clip lengths between 0.25s and 0.75s yields good accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">What are good input resolutions?</head><p>In <ref type="bibr" target="#b39">[41]</ref>, Tran et al. conducted an experiment to explore the input resolutions. However, their networks have different numbers of parameters, thus different levels of overfitting can be expected. We conduct a similar experiment to determine a good input resolution for video classification, but again constrain our networks to use a similar number of parameters. We experiment with 3 different input resolutions of 224 × 224, 112 × 112, and 56 × 56 with re-scaled frame size 256 × 342, 128 × 171, and 64 × 86, respectively. We adjust the kernel size of the conv1 layers of these networks so that they have similar receptive fields, using 3 × 11 × 11, 3 × 7 × 7, 3 × 5 × 5 with stride of 1 × 4 × 4, 1 × 2 × 2, 1 × 1 × 1 for input resolution 224, 112, and 56, respectively. The rest of these three networks remains the same, thus the only difference in parameters comes from the conv1 layer which is small compared with the total number of parameters. <ref type="table">Table 15</ref> reports the accuracy of these different input resolutions.</p><p>Observation 3. An input resolution of 128 (crop 112) is ideal both for computational complexity and accuracy of video classification given the GPU memory constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">What type of convolutions?</head><p>There are many conjectures about the type of convolutions used for video classification. One can choose to use 2D ConvNets as in two stream networks <ref type="bibr" target="#b31">[33]</ref> or fully 3D ConvNets as in C3D <ref type="bibr" target="#b39">[41]</ref>, or even a ConvNet with mixtures of 2D and 3D operations <ref type="bibr">[14]</ref>. In this section, we compare a mixed 3D-2D ConvNet and mixed 2D-1D ConvNet with a full-3D ConvNet (SR18) to address these conjectures.</p><p>Mixed 3D-2D ConvNets. One hypothesis is that we only need motion modeling at some low levels (early layers) while at higher levels of semantic abstraction (later layers), motion or temporal modeling is not necessary. Thus some plausible architectures may start with 3D convolutions and switch to using 2D convolutions at the top layers. As SR18 has 5 groups of convolutions, our first variation is to replace all 3D convolutions in group 5 by 2D ones. We denote this variant as MC5 (mixed convolutions). Similarly, we replace group 4 and 5 with 2D convolutions, and name the variation is MC4 (meaning from group 4 and deeper layers all convolutions are 2D). Following this pattern, we also create MC3, MC2, and MC1 variations. We note that MC1 is equivalent to a 2D ConvNet applied on clip inputs, which differs slightly from the architecture presented in <ref type="table" target="#tab_2">Table 2</ref>, which is a 2D-Resnet18 applied on frame inputs. In order to constrain the model capacity, we fix the number of filters in each group of conv1 x to conv5 x to be k, k, 2k, 4k, 8k, respectively, and call k the network width. When 3D convolution kernels (3×d×d) are replaced by 2D convolutions</p><formula xml:id="formula_0">(1 × d × d),</formula><p>there is a reduction in the number of parameters; we can then adjust k as needed to keep the capacity comparable to SR18.</p><p>2.5D ConvNets. Another hypothesis is that 3D convolutions aren't needed at all, as 3D convolutions can be approximated by a 2D convolution followed by a 1D convolution -decomposing spatial and temporal modeling into separate steps. We thus design a network architecture that we call a 2.5D ConvNet, where we replace each 3 × d × d 3D convolution having n input and m output channels by a 2.5D block consisting of a 1×d×d 2D convolution and a 3×1×1 1D convolution layer having i internal channel connections such that the number of parameters are comparable <ref type="figure">(Figure 1</ref>). If the 3D convolution has spatial and temporal striding (e.g. downsampling), the striding is also decomposed according to its corresponding spatial or temporal convolutions. We choose i = 3mnd 2 nd 2 +3m so that the number of parameters in the 2.5D block is approximately equal to that of the 3D convolution. This is similar to the bottleneck block in <ref type="bibr" target="#b7">[8]</ref> which factorized two 2D convolutions into two 1D and one 2D convolutions. <ref type="table">Table 16</ref> reports the accuracy of these convolution variations.</p><p>Observation 4. Using 3D convolutions across all layers seems to improve video classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Is your network deep enough?</head><p>Here, we use SR18 as a reference model and vary the network depth with the same constraint on number of parameters. We again design the network width k such that all convolution layers in group 1 and 2 (conv1 and conv2 x) have k output filters, conv 3x, conv 4x, and conv 5x Convolution layer with n input channels and m output channels. b) a mixed 2D and 1D convolution block with the same n input and m output channels. d is the spatial size of the kernels. i is the output channels for the 2D convolution layer and also the input channels for 1D convolution layer, and is chosen to make the number of parameters in both blocks equal. have 2k, 4k, and 8k output filters, respectively. By changing the number of blocks in each group we obtain networks of different depth, and again adjust k to match the number of parameters. <ref type="table">Table 17</ref> reports the effects of model depth on accuracy. We note that He et al. <ref type="bibr" target="#b7">[8]</ref> conducted a similar investigation of depth for ImageNet classification, but did not constrain the number of parameters of the comparing networks. Their experiments are on ImageNet which is large-scale, while UCF101 exhibits much more overfitting and is thus more sensitive to the model capacity. Although D16, D18, and D26 provide a similar accuracy, D18 has a lower complexity than D16, and consumes less memory than D26.</p><p>Observation 5. A network depth of 18 layers gives a good trade-off between accuracy, computational complexity, and memory for video classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Spatiotemporal feature learning with 3D Resnets</head><p>We now apply the observations of section 3 to design networks for spatiotemporal feature learning on a large-scale dataset (Sports-1M). We then compare the learned features with the current C3D features on a variety of video tasks in section 5.</p><p>Architecture. We select the 3D-Resnet18 architecture (shown in <ref type="table" target="#tab_2">Table 2</ref>) as suggested by the empirical observations in section 3. In contrast to SR18, we use an input of 8 × 112 × 112 frames, because large-scale training can benefit from the additional information. All other observations are adopted: temporal stride of 2, input resolution of 112 × 112, full-3D convolutions with depth of 18. We denote this architecture as Res3D from now on.</p><p>Training. Similar to C3D, we train our Res3D on Sports-1M [14] to learn spatiotemporal features. Sports-1M is a large-scale dataset with about 1.1M videos of 487 fine-grained sports categories, and includes a public train and test split. We randomly extract five 2-secondlong clips from each training video. Clips are resized to 128 × 171 resolution. Random spatiotemporal jittering is applied as data augmentation to randomly crop the input clips to 8 × 112 × 112 (sampling stride 2). Training uses SGD on 2 GPUs with a mini-batch size of 40 examples (20 per GPU). The initial learning rate is 0.01 and is divided by 2 every 250k iterations, finishing at 3M iterations. We also train a 2D-Resnet18 baseline with the same procedure.</p><p>Results on Sports1M. <ref type="table" target="#tab_6">Table 8</ref> presents the classification results of our Res3D compared with current methods. For top-1 clip accuracy we use a single center-cropped clip and a single model. For video top-1 and top-5 accuracy, we use 10 center-cropped clips and average their predictions to make a video-level prediction. Res3D achieves state-of-theart performance when compared with single models that do not use long-term modeling. It outperforms the previous state of the art, C3D <ref type="bibr" target="#b39">[41]</ref>, by 2.7%, 4.5%, and 2.6% on top-1 clip, top-1 video, and top-5 video accuracy respectively. Compared with 2D ConvNets, Res3D improves 2% and 0.7% over AlexNet and GoogleNet on top-1 video accuracy. We note that these methods use 240 crops where Res3D uses only 10 crops. These improvements are significant compared to random chance (only 0.2%) on this large- We note that our method does not involve any long-term modeling because our main objective is to learn atomic spatiotemporal features. In fact, the orthogonal direction of long-term modeling with LSTM or convolution pooling can be applied on our Res3D model to further improve sports classification accuracy. <ref type="figure" target="#fig_1">Figure 2</ref> visualizes the learned conv1 filters of both Res3D and the 2D-Resnet baseline. These two networks are trained on Sports-1M; the only difference is that 3D convolutions are replaced by 2D ones. We observe that: 1) All 3D filters change in the time dimension, meaning each encodes spatiotemporal information, (not just spatial); 2) For most of the 2D filters, we can find a 3D filter with a similar appearance pattern (mostly at the center of the kernel). This may indicate that 3D filters are able to cover the appearance information in 2D filters but can also capture useful motion information. This confirms our finding (consistent with <ref type="bibr" target="#b39">[41]</ref>) that 3D convolutions can capture appearance and motion simultaneously, and are thus well-suited for spatiotemporal feature learning.</p><p>Model size and complexity. Res3D is about 2 times smaller and also 2 times faster than C3D. Res3D has 33.2 million parameters and 19.3 billion FLOPs while C3D has 72.9 million parameters and 38.5 billion FLOPs.  <ref type="table">Table 9</ref>. Action recognition results on HMDB51 and UCF101. Res3D outperforms C3D by 3.5% on UCF101 and 3.3% on HMDB51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Res3D as Spatiotemporal Features</head><p>In this section, we evaluate our Res3D model pre-trained on Sports-1M as a spatiotemporal feature extractor for video understanding tasks and compare with other representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Action recognition</head><p>Datasets. In this experiment, we use UCF101 <ref type="bibr" target="#b33">[35]</ref> and HMDB51 <ref type="bibr" target="#b16">[18]</ref>, which are popular public benchmarks for human action recognition. UCF101 has 13,320 videos and 101 different human actions. HMDB51 has about 7,000 videos and 51 human action categories. Both have 3 train/test splits, so we evaluate with 3-fold cross validation.</p><p>Models. There is one publicly available deep spatiotemporal feature: C3D <ref type="bibr" target="#b39">[41]</ref>. We compare our Res3D with C3D and the 2D-Resnet baseline which was trained on Sports-1M frames. We use the fc6 activations of C3D as suggested by the authors <ref type="bibr" target="#b39">[41]</ref>. For Res3D and 2D-Resnet, we tried both res5b and pool5. The res5b features are slightly better than pool5 for both 2D and 3D cases, but the gap is small; here we report the result using res5b. We represent a video by extracting clip features and average pooling them, then L2-normalizing the resulting vector. A linear SVM is used to classify the actions. We also fine-tune C3D and Res3D on both UCF101 and HMDB51 and find out that the performance gaps for fine-tuned models are bigger, suggesting that a stronger architecture benefits more from fine-tuning. <ref type="table">Table 9</ref> shows the result on human action recognition on UCF101 and HMDB51 of Res3D compared with C3D <ref type="bibr" target="#b39">[41]</ref> and the 2D-Resnet baseline. Our Res3D outperforms C3D by 3.5% and 3.3% on UCF101 and HMDB51, respectively. It achieves the best accuracy of methods that use only RGB input (see <ref type="table">Table 10</ref>). Temporal Segment Networks (TSNs) also achieves very high accuracy of 85.7% on UCF101 when using only RGB. TSNs can even achive higher accuracy of 87.3% when using with two modalities (e.g. RGB and RGB Difference). We note that these numbers are evaluated on split 1 only, thus not directly compparable to our results. Although the direct comparion is not possible here, we can still conjecture that TSNs and Res3D architecture are in par when applied on the same RGB modality. On HMDB51, Res3D achieves the best performance among  <ref type="figure">7 × 7)</ref> presented as a group of 3 images (upscaled by 4x). The lower images are 2D-Resnet filters (7 × 7) upscaled by 4x. We see that both models capture appearance information (color, edges, texture), but the 3D filters also incorporate temporal dynamics, thus they are well-suited for spatiotemporal features. Best viewed in color. GIF annimations of 3D filters can be viewed here: http://goo.gl/uES8Ma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>UCF101 HMDB51 Slow Fusion <ref type="bibr">[14]</ref> 65.4 -Spatial Stream <ref type="bibr" target="#b31">[33]</ref> 73.0 40.5 LSTM Composite Model <ref type="bibr" target="#b34">[36]</ref> 75.8 44.1 Action Transformations <ref type="bibr" target="#b43">[45]</ref> 80.  <ref type="table">Table 10</ref>. Comparison with state-of-the-art methods on HMDB51 and UCF101 considering methods that use only RGB input. We note that the current state of the art is about 94%-98% and 69% on UCF101 and HMDB51 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref> when using optical flow and improved dense trajectories <ref type="bibr" target="#b40">[42]</ref>. *Results are computed on only split 1.</p><p>the methods using only RGB input. State of the art models <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref> augment RGB input with expensive optical flow or iDT <ref type="bibr" target="#b43">[45]</ref>, but it is worth noting that the high computational cost of such augmentations prohibits their application at a large scale (e.g. Sports-1M). The very recent work, I3D <ref type="bibr" target="#b0">[1]</ref> (concurrent with this work), achieves very good performance on UCF101 (98%) and HMDB51 (80%) with their two-stream I3D model using both RGB and optical flow inputs, and an Imagenet pre-trained model. When using only RGB, their results on UCF101 and HMDB51 are 84.5% and 49.8% which are 1.3% and 5.1% worse than ours. We note these results are not directly comparable as their number on only split 1 of UCF101 and HMDB51. <ref type="figure">Figure 3</ref> compares the accuracy (on UCF101) of Res3D with that of C3D at low dimensions. We use PCA to project the spatiotemporal features of C3D (fc6) and Res3D (res5b)  <ref type="figure">Figure 3</ref>. Res3D is more compact than C3D. Res3D outperforms C3D by 4 to 5% after PCA projection to low dimensions, showing that its feature is much more compact than C3D.</p><p>to a low dimensionality and then classify them with a linear SVM. Res3D outperforms C3D by 3.9% (56.7 vs. 52.8) at 10 dimensions, and improves about 5% over C3D at 50, 100, 200, and 300 dimensions. This indicates that Res3D is much more compact compared with C3D. In fact, Res3D has accuracy of 82.9% at only 300 dimensions which is already better than C3D using the full 4,096 dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Action similarity labelling</head><p>Dataset. The ASLAN dataset <ref type="bibr" target="#b14">[16]</ref> has 3,631 videos from 432 action classes. The task is to predict if a given pair of videos contain the same or different action. We use the public 10-fold cross validation protocol.</p><p>Model. In this experiment, we follow the protocol of <ref type="bibr" target="#b39">[41]</ref>, and extract 3 different features of Res3D: res5b, pool5, and prob. We again average clip features across videos and L2-normalize each feature. For each pair of videos, we compute different distances (12 distance metrics were used in <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b14">16]</ref>  <ref type="table">Table 11</ref>. Action similarity labeling results on ASLAN. Res3D outperforms C3D and other methods and achieves the best accuracy on ASLAN. sional vectors. These vectors are used to train a linear SVM to predict whether the two videos contain the same action. <ref type="table">Table 11</ref> presents the results of action similarity labeling using our Res3D features compared with C3D, 2D-Resnet features, as well as AlexNet features. Our Res3D features give a small improvement over C3D, but large improvements compared with 2D baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Action detection</head><p>Dataset. We use the THUMOS'14 <ref type="bibr" target="#b11">[12]</ref> temporal action localization task, a standard benchmark for action detection in long, untrimmed videos. Each video can contain multiple action instances with 20 total action categories. For training, we use all 2,755 trimmed training videos and 1,010 untrimmed validation videos (containing 3,007 action instances). For testing, we use all 213 test videos (containing 3,358 action instances) that are not entirely background videos. The temporal action localization task is to predict action instances with action categories and start/end time. Following the conventional metrics used in <ref type="bibr" target="#b11">[12]</ref>, we evaluate mean average precision (mAP) and do not allow redundant detections. A prediction is only correct if its category prediction is correct and its temporal overlap Intersectionover-Union (IoU) with the ground truth is larger than a threshold used during evaluation.</p><p>Models. Shou et al. <ref type="bibr" target="#b30">[32]</ref> proposed a Segment-based 3D CNN framework (S-CNN) for action localization, and showed superior performance over other methods when using C3D (denoted as C3D + S-CNN). In this experiment we replace C3D with Res3D (denoted as Res3D + S-CNN) to determine whether a better spatiotemporal feature can improve performance. All 3 networks (proposal, classification, localization) of S-CNN are fine-tuned in an endto-end manner following the protocol of <ref type="bibr" target="#b30">[32]</ref>: the learning rate is set to 0.001 for all layers except 0.01 for the last layer; the learning rate is divided by 2 after every 4 epochs; and training is done at 16 epochs. All other settings are the same as <ref type="bibr" target="#b30">[32]</ref>. As shown in <ref type="table" target="#tab_2">Table 12</ref>, Res3D + S-CNN outperforms its direct comparable baseline, C3D+S-CNN, on THUMOS'14 (e.g. 3.5% mAP gain over C3D + S-CNN when IoU threshold is 0.5). Some baselines in <ref type="bibr">[13,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b2">3]</ref> are SVM classifiers trained on a pool of features that do not specifically address the detection problem. Others are based on iDT with Fisher Vector <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">27]</ref> or RNN/LSTM <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b46">48]</ref>. Unlike 3D ConvNets (C3D and Res3D), those methods cannot explicitly model appearance and motion information simultaneously. We note that there are some concurrent work with us, e.g. R-C3D <ref type="bibr" target="#b44">[46]</ref> and CDC <ref type="bibr" target="#b29">[31]</ref> achiving better performance on THUMOS'14. These models use C3D as their base network, we hope a similar gain can be achieved when replacing C3D by Res3D on these systems. --13.9 --Oneata et al. <ref type="bibr" target="#b23">[25]</ref> 28. <ref type="bibr" target="#b7">8</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We have presented an empirical architecture search for video classification on UCF101. We showed that our observations are useful for spatiotemporal feature learning on the large-scale Sports-1M dataset. Our proposed model, Res3D, achieves the best performance on Sports-1M, when compared with models applied on a single crop and without long-term modeling. Our Res3D outperforms C3D by a good margin across 5 different benchmarks: Sports-1M, UCF101, HMDB51, ASLAN, and THUMOS14. In addition, Res3D is 2 times faster in run-time, 2 times smaller in model size, and more compact than C3D, e.g. outperforming C3D by 5% on low dimensions on UCF101.</p><p>Although the current video benchmarks and machine capacity (e.g. GPU memory) are not ideal for ConvNet architecture search, we showed that under careful experimental settings architecture search is still valuable. This paper also provides empirical evidence, consistent with the finding in <ref type="bibr" target="#b39">[41]</ref>, that 3D convolutions are more suitable for spatiotemporal feature learning than 2D convolutions. Various video understanding applications such as action recognition, action similarity labeling, and action detection were significantly improved by using the Res3D architecture. We hope that other tasks can also benefit from Res3D, as happened with C3D. For the sake of reproducible research, the source code and pre-trained models are available at http://github.com/facebook/C3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings.</head><p>We conduct additional experiments to verify our architecture search on HMDB51 <ref type="bibr" target="#b16">[18]</ref>. In this section we use the same architectures as in our architecture search experiments on UCF101. Because HMDB51 is about 2 times smaller than UCF101, we adjust the training scheme accordingly. More specific, we keep the initial learning rate the same as in UCF101 which is 0.01, but divided by 10 at every 10K iterations (instead of 20K), and training is stopped at 45K (instead of 90K). We use train and test split 1. As HMDB51 is smaller than UCF101, more overfitting is expected. The absolute accuracy is not important, but we would like verify if the relative ranking between architectures remains consistent with the search in UCF101. If so, we can be more confident about our empirical observations. We re-run most of architecture search experiments as done on UCF101, except for the experiment with varying sampling rates. The main reason is that HMDB51 has many short videos, when applying on higher sampling rate (equivalent to longer input clips), the number of training and testing examples are significantly dropped. More specific, HMDB51 will loose about 20%, 45%, and 82% of its examples when input clip length is increased to 32, 64, and 128 frames (sampling rate of 8, 16, and 32), respectively.</p><p>Results. <ref type="table">Table 15</ref> presents HMDB51 accuracy of SR18 with different input resolutions. We observe that the results are consistent with our experiments on UCF101. It shows that, training on a higher resolution and with a bigger receptive field is even harder on a smaller benchmark. <ref type="table">Table 16</ref> reports the accuracy of different mixedconvolution architectures on HMDB51 along with UCF101 results. We observe that all of the mixed-convolution architectures perform consistently worse than 3D, while C2.5D is comparable to 3D (SR18). <ref type="table">Table 17</ref> presents the HMDB51 accuracy of different architectures with varying the network depth along with the UCF101 results. We found that the relative ranking between architectures are consistent on both datasets. This confirms that, for 3D Resnets, depth of 18 is a good trade-off for video classification. <ref type="figure">Figure 5</ref> plots the accuracy versus computation cost (FLOPs) for different architectures on both UCF101 and HMDB51. Different architectures are visualized by different color dots. We observe that, although the performance gaps are different, the distribtion of the color dots are consistent for UCF101 and HMDB51. This fact further confirms that under a careful design, architecture search on small benchmarks is still relevant.  <ref type="table">Table 17</ref>. Accuracy of different depth architectures on UCF101 and HMDB51. On HMDB51, D34 performs slightly worse than the others, D16 and D26 are also among the best, which is consistent with UCF101. D18 performs slightly worse than D16 and D26 but at lower computation complexity and less memory requirement.    <ref type="figure">Figure 5</ref>. Architecture search on UCF101 vs. HMDB51. We plot the accuracy versus computation (FLOPs) for different architectures on UCF101 and HMDB51. The left plot shows results on UCF101 while the right plot shows the results on HMBD51. Different colors are used for different architectures. The distribution of the color dots is consistent for UCF101 and HMDB51.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 . 2 .</head><label>12</label><figDesc>5D Convolution Block vs. 3D Convolution. a) A 3D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Res3D vs. 2D-Resnet learned filters. Visualization of the Res3D 64 filters in its conv1 layer compared with those of the 2D-Resnet baseline. Both networks are trained on Sports1M. The upper images are Res3D filters (3 ×</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Res3D architecture. Downsampling strides are denoted as t × s where t and s are temporal and spatial stride, respectively. Dotted lines are residual connections with downsampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Basic 3D-Resnet architectures. Building blocks are shown in brackets, with the numbers of blocks stacked.</figDesc><table><row><cell>Down-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>) 43.8 46.3 46.1 38.8 38.1 36.9</figDesc><table><row><cell>Input Stride</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16 32</cell></row><row><cell>Accuracy (%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 Table 7 .</head><label>67</label><figDesc>×10 6 ) 32.5 32.5 32.8 32.4 33.0 33.2 33.2 FLOPs (×10 9 ) 10.2 9.7 20.8 22.6 18.7 10.3 10.3 Accuracy (%) 43.3 44.5 44.8 44.3 45.1 44.6 46.1 Accuracy on UCF101, varying depth.The second column denotes the numbers of blocks in each convolution group. k is purposely selected to make the networks have similar number of parameters. D18 is our SR18 reference model. There is no clear winner among these networks, except that D34 and D10 are somewhat worse. This suggests that for the same number of parameters, the depth of 16-26 layers is good enough for video classification.</figDesc><table><row><cell>Net</cell><cell cols="5">MC1 MC2 MC3 MC4 MC5 2.5D 3D</cell></row><row><cell>k</cell><cell></cell><cell>109 109 108 103</cell><cell>90</cell><cell>-</cell><cell>64</cell></row><row><cell cols="6"># params (%)</cell></row><row><cell cols="2">D10 [1 1 1 1] 98</cell><cell>33.7</cell><cell>10.8</cell><cell></cell><cell>45.3</cell></row><row><cell cols="2">D16 [2 2 2 1] 85</cell><cell>33.6</cell><cell>16.7</cell><cell></cell><cell>46.8</cell></row><row><cell cols="2">D18 [2 2 2 2] 64</cell><cell>33.2</cell><cell>10.3</cell><cell></cell><cell>46.1</cell></row><row><cell cols="2">D26 [2 3 4 3] 50</cell><cell>33.8</cell><cell>8.4</cell><cell></cell><cell>46.3</cell></row><row><cell cols="2">D34 [3 4 6 3] 46</cell><cell>32.8</cell><cell>10.0</cell><cell></cell><cell>45.8</cell></row></table><note>. Accuracy on UCF101, varying convolution mixtures. SR18 with fully 3D convolution layers performs the best. The ac- curacy gap is not very large, but still significant relative to random chance of 0.96%. k is the network width, which we adjust so that all comparing networks have similar number of parameters. Net Blocks k # params (×10 6 ) FLOPs (×10 9 ) Acc (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="3">Clip@1 Video@1 Video@5</cell></row><row><cell cols="3">single model, no long-term modeling</cell><cell></cell></row><row><cell>DeepVideo [14]</cell><cell>41.9</cell><cell>60.9</cell><cell>80.2</cell></row><row><cell>C3D [41]</cell><cell>46.1</cell><cell>61.1</cell><cell>85.2</cell></row><row><cell>AlexNet [24]</cell><cell>N/A</cell><cell>63.6</cell><cell>84.7</cell></row><row><cell>GoogleNet [24]</cell><cell>N/A</cell><cell>64.9</cell><cell>86.6</cell></row><row><cell>2D-Resnet*</cell><cell>45.5</cell><cell>59.4</cell><cell>83.0</cell></row><row><cell>Res3D (ours)*</cell><cell>48.8</cell><cell>65.6</cell><cell>87.8</cell></row><row><cell cols="2">with long-term modeling</cell><cell></cell><cell></cell></row><row><cell>LSTM+AlexNet [24]</cell><cell>N/A</cell><cell>62.7</cell><cell>83.6</cell></row><row><cell>LSTM+GoogleNet [24]</cell><cell>N/A</cell><cell>67.5</cell><cell>87.1</cell></row><row><cell>Conv pooling+AlexNet [24]</cell><cell>N/A</cell><cell>70.4</cell><cell>89.0</cell></row><row><cell cols="2">Conv pooling+GoogleNet [24] N/A</cell><cell>71.7</cell><cell>90.4</cell></row></table><note>Results on Sports-1M. Upper table presents the sports classification accuracy of different methods when a single model is used. The lower table presents the results of the methods that use multiple crops and long-term modeling. Our Res3D achieves state-of-the-art accuracy when compared with methods that do not employ long-term modeling. The results of the other methods are quoted directly from the relevant papers. Note that random chance on this dataset is only 0.2%. * 2D-Resnet and Res3D use only 112 × 112 resolution while the others use 224 × 224.scale benchmark. Compared to methods that use long-term modeling, e.g. LSTM or Convolution Pooling [24], our Res3D is 2.9% and 4.2% better than an LSTM trained on AlexNet fc features, and comparable with an LSTM trained on GoogleNet fc features. The only methods with higher ac- curacy use convolutional pooling for long-term modeling.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>) which results in 3 × 12 = 36 dimen-</figDesc><table><row><cell>Model</cell><cell cols="3">2D-Resnet AlexNet C3D Res3D</cell></row><row><cell>Accuracy (%)</cell><cell>77.2</cell><cell>67.5</cell><cell>78.3 78.8</cell></row><row><cell>AUC</cell><cell>85.0</cell><cell>73.8</cell><cell>86.5 86.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>Temporal action localization mAP on THUMOS'14 with the overlap IoU threshold used in evaluation varied from 0.3 to 0.7. -indicates that results are unavailable in the corresponding papers.</figDesc><table><row><cell></cell><cell cols="2">21.8 15.0 8.5 3.2</cell></row><row><cell cols="2">Richard and Gall [27] 30.0 23.2 15.2 -</cell><cell>-</cell></row><row><cell>Yeung et al. [47]</cell><cell>36.0 26.4 17.1 -</cell><cell>-</cell></row><row><cell>Yuan et al. [48]</cell><cell>33.6 26.1 18.8 -</cell><cell>-</cell></row><row><cell>Shou et al. [31]</cell><cell cols="2">40.1 29.4 23.3 13.1 7.9</cell></row><row><cell>Xu et al. [46]</cell><cell>44.8 35.6 28.9 -</cell><cell>-</cell></row><row><cell cols="3">C3D + S-CNN [32] 36.3 28.7 19.0 10.3 5.3</cell></row><row><cell>Res3D + S-CNN</cell><cell cols="2">40.6 32.6 22.5 12.3 6.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 .Table 14 .Table 15 .Table 16 .</head><label>13141516</label><figDesc>Resnet architectures with different mixtures of 3D and 2D convolutions. Building blocks are shown in brackets, with the numbers of blocks stacked. Downsampling is performed by conv1 and conv5 1 with a stride of 1 × 2 × 2, and conv3 1 and conv4 1 with a stride of 2 × 2 × 2. For better contrasting the difference, 3D convolutions and 2D convolutions are presented in bold red and underlined blue text, repsectively. 3D-Resnet architectures with different number of layers. Building blocks are shown in brackets, with the numbers of blocks stacked. Downsampling is performed by conv1 and conv5 1 with a stride of 1 × 2 × 2, and conv3 1 and conv4 1 with a stride of 2 × 2 × 2. *D18 is equivalent to SR18. For better contrasting the difference, number of filters and number of blocks are presented in bold red and underlined blue text, repsectively. Accuracy on UCF101 and HMDB, varying input resolution. The experimental result on HMDB51 is consistent with the observation on UCF101. This confirms resolution 128 is a good choice for video classification.[13] S. Karaman, L. Seidenari, and A. D. Bimbo. Fast saliency based pooling of fisher encoded dense trajectories. In ECCVNet MC1 MC2 MC3 MC4 MC5 2.5D 3D UCF101 43.3 44.5 44.8 44.3 45.1 44.6 46.1 HMDB51 19.7 19.4 19.9 20.4 20.9 21.4 21.5 Accuracy of different mixed-convolution architectures accuracy on UCF101 and HMDB51. We further verify the observations on HMDB51 and confirm that most of the observations are also hold for HMDB51. THUMOS Workshop, 2014. 8 [14] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, Net D10 D16 D18 D26 D34 UCF101 45.3 46.8 46.1 46.3 45.8 HMDB51 21.3 22.8 21.5 22.0 20.1</figDesc><table><row><cell cols="2">layer name output size</cell><cell>MC1</cell><cell></cell><cell>MC2</cell><cell></cell><cell>MC3</cell><cell></cell><cell>MC4</cell><cell></cell><cell>MC5</cell><cell></cell></row><row><cell>conv1</cell><cell>4×56×56</cell><cell cols="2">1 × 7 × 7, 109</cell><cell cols="2">3 × 7 × 7, 109</cell><cell cols="2">3 × 7 × 7, 108</cell><cell cols="2">3 × 7 × 7, 103</cell><cell>3 × 7 × 7, 90</cell><cell></cell></row><row><cell cols="2">conv2 x 4×56×56</cell><cell>1×3×3, 109 1×3×3, 109</cell><cell>×2</cell><cell>1×3×3, 109 1×3×3, 109</cell><cell>×2</cell><cell>3×3×3, 108 3×3×3, 108</cell><cell>×2</cell><cell>3×3×3, 103 3×3×3, 103</cell><cell>×2</cell><cell>3×3×3, 90 3×3×3, 90</cell><cell>×2</cell></row><row><cell cols="2">conv3 x 2×28×28</cell><cell>1×3×3, 218 1×3×3, 218</cell><cell>×2</cell><cell>1×3×3, 218 1×3×3, 218</cell><cell>×2</cell><cell>1×3×3, 216 1×3×3, 216</cell><cell>×2</cell><cell>3×3×3, 206 3×3×3, 206</cell><cell>×2</cell><cell>3×3×3, 180 3×3×3, 180</cell><cell>×2</cell></row><row><cell cols="2">conv4 x 1×14×14</cell><cell>1×3×3, 436 1×3×3, 436</cell><cell>×2</cell><cell>1×3×3, 436 1×3×3, 436</cell><cell>×2</cell><cell>1×3×3, 432 1×3×3, 432</cell><cell>×2</cell><cell>1×3×3, 412 1×3×3, 412</cell><cell>×2</cell><cell>3×3×3, 360 3×3×3, 360</cell><cell>×2</cell></row><row><cell>conv5 x</cell><cell>1×7×7</cell><cell>1×3×3, 872 1×3×3, 872</cell><cell>×2</cell><cell>1×3×3, 872 1×3×3, 872</cell><cell>×2</cell><cell>1×3×3, 864 1×3×3, 864</cell><cell>×2</cell><cell>1×3×3, 824 1×3×3, 824</cell><cell>×2</cell><cell>1×3×3, 720 1×3×3, 720</cell><cell>×2</cell></row><row><cell>pool5</cell><cell>1×1×1</cell><cell></cell><cell></cell><cell></cell><cell cols="3">average pool, 101-d fc, softmax</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FLOPs (×10 9 )</cell><cell>10.2</cell><cell></cell><cell>9.7</cell><cell></cell><cell>20.8</cell><cell></cell><cell>22.6</cell><cell></cell><cell>18.7</cell><cell></cell></row><row><cell cols="2"># of parameters (×10 6 )</cell><cell>32.5</cell><cell></cell><cell>32.5</cell><cell></cell><cell>32.8</cell><cell></cell><cell>32.4</cell><cell></cell><cell>33.0</cell><cell></cell></row><row><cell cols="2">layer name output size</cell><cell>D10</cell><cell></cell><cell>D16</cell><cell></cell><cell>D18*</cell><cell></cell><cell>D26</cell><cell></cell><cell>D34</cell><cell></cell></row><row><cell>conv1</cell><cell>4×56×56</cell><cell>3 × 7 × 7, 98</cell><cell></cell><cell>3 × 7 × 7, 85</cell><cell></cell><cell>3 × 7 × 7, 64</cell><cell></cell><cell>3 × 7 × 7, 50</cell><cell></cell><cell>3 × 7 × 7, 46</cell><cell></cell></row><row><cell cols="2">conv2 x 4×56×56</cell><cell>3×3×3, 98 3×3×3, 98</cell><cell>×1</cell><cell>3×3×3, 85 3×3×3, 85</cell><cell>×2</cell><cell>3×3×3, 64 3×3×3, 64</cell><cell>×2</cell><cell>3×3×3, 50 3×3×3, 50</cell><cell>×2</cell><cell>3×3×3, 46 3×3×3, 46</cell><cell>×3</cell></row><row><cell cols="2">conv3 x 2×28×28</cell><cell>3×3×3, 196 3×3×3, 196</cell><cell>×1</cell><cell>3×3×3, 170 3×3×3, 170</cell><cell>×2</cell><cell>3×3×3, 128 3×3×3, 128</cell><cell>×2</cell><cell>3×3×3, 100 3×3×3, 100</cell><cell>×3</cell><cell>3×3×3, 92 3×3×3, 92</cell><cell>×4</cell></row><row><cell cols="2">conv4 x 1×14×14</cell><cell>3×3×3, 392 3×3×3, 392</cell><cell>×1</cell><cell>3×3×3, 340 3×3×3, 340</cell><cell>×2</cell><cell>3×3×3, 256 3×3×3, 256</cell><cell>×2</cell><cell>3×3×3, 200 3×3×3, 200</cell><cell>×4</cell><cell>3×3×3, 184 3×3×3, 184</cell><cell>×6</cell></row><row><cell>conv5 x</cell><cell>1×7×7</cell><cell>3×3×3, 784 3×3×3, 784</cell><cell>×1</cell><cell>3×3×3, 680 3×3×3, 680</cell><cell>×1</cell><cell>3×3×3, 512 3×3×3, 512</cell><cell>×2</cell><cell>3×3×3, 400 3×3×3, 400</cell><cell>×3</cell><cell>3×3×3, 368 3×3×3, 368</cell><cell>×3</cell></row><row><cell>pool5</cell><cell>1×1×1</cell><cell></cell><cell></cell><cell></cell><cell cols="3">average pool, 101-d fc, softmax</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FLOPs (×10 9 )</cell><cell>10.8</cell><cell></cell><cell>16.7</cell><cell></cell><cell>10.3</cell><cell></cell><cell>8.4</cell><cell></cell><cell>10.0</cell><cell></cell></row><row><cell cols="2"># of parameters (×10 6 )</cell><cell>33.7</cell><cell></cell><cell>33.6</cell><cell></cell><cell>33.2</cell><cell></cell><cell>33.8</cell><cell></cell><cell>32.8</cell><cell></cell></row><row><cell cols="6">Input resolution (crop size) 64 (56) 128 (112) 256 (224)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>UCF101</cell><cell>37.6</cell><cell>46.1</cell><cell>44.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">HMDB51</cell><cell>18.3</cell><cell>21.5</cell><cell>17.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment: we would like to thank Kaiming He and colleagues at Facebook AI Research and Applied Machine Learning for valuable feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A: Architechture Details <ref type="table">Table 13</ref> and <ref type="table">Table 14</ref> provide further details of the architectures used in our architecture search with mixedconvolutions and varying network depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Filter Visualizations</head><p>Appendix C: Architecture Search Validation on HMDB51</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV VS-PETS</title>
		<meeting>ICCV VS-PETS</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The action similarity labeling challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kliper-Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time-series. Brain Theory and Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Online detection and classification of dynamic hand gestures with recurrent 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The lear submission at thumos 2014</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action bank: A high-level representation of activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CDC: convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Actions˜transforma-tions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">R-C3D: region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1703.07814</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
