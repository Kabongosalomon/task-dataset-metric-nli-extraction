<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Making History Matter: History-Advantage Sequence Training for Visual Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Making History Matter: History-Advantage Sequence Training for Visual Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the multi-round response generation in visual dialog, where a response is generated according to a visually grounded conversational history. Given a triplet: an image, Q&amp;A history, and current question, all the prevailing methods follow a codec (i.e., encoder-decoder) fashion in a supervised learning paradigm: a multimodal encoder encodes the triplet into a feature vector, which is then fed into the decoder for the current answer generation, supervised by the ground-truth. However, this conventional supervised learning does NOT take into account the impact of imperfect history, violating the conversational nature of visual dialog and thus making the codec more inclined to learn history bias but not contextual reasoning. To this end, inspired by the actor-critic policy gradient in reinforcement learning, we propose a novel training paradigm called History-Advantage Sequence Training (HAST). Specifically, we intentionally impose wrong answers in the history, obtaining an adverse critic, and see how the historic error impacts the codec's future behavior by History Advantagea quantity obtained by subtracting the adverse critic from the gold reward of ground-truth history. Moreover, to make the codec more sensitive to the history, we propose a novel attention network called History-Aware Co-Attention Network (HACAN) which can be effectively trained by using HAST. Experimental results on three benchmarks: VisDial v0.9&amp;v1.0 and GuessWhat?!, show that the proposed HAST strategy consistently outperforms the state-of-the-art supervised counterparts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual dialog is one of the most comprehensive task for benchmarking the AI's comprehension of natural language grounded by a visual scene <ref type="bibr" target="#b5">[6]</ref>. A good visual dialog agent should accomplish a complex series of reasoning sub-tasks: contextual visual perception <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33]</ref>, language modeling <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4]</ref>, and co-reference resolution in dia- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>History A0: A couple sharing a knife to cut a large cake. Q1: Is the couple male and female? A1: Yes. Q2: What color is the cake? A2: White with blue lettering, but hard to see it.</p><p>Question Q3: Anything written on the cake? <ref type="figure" target="#fig_2">Figure 1</ref>. (a) A typical visual dialog task. Particularly, the initial answer A 0 denotes the given image captioning. (b) The conventional training process at round t: given an image I, a history H t , and question Q t , the loss is supervised by the groundtruth answer A t gt . (c) The proposed History-Advantage Sequence Training (HAST) paradigm: the reward (i.e. − loss) is a History-Advantage, which is more focused on the impact caused by a wrong answerĀ t to the future round t , by comparing the difference between the Gold Reward from gold history H t g and the Adverse Critic from fake history H t a .</p><p>log history <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b19">20]</ref> (e.g., identify what is "that"). Thanks to the end-to-end deep neural networks in their respective subtasks, state-of-the-art visual dialog systems can be built by assembling them into a codec framework <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref>. The encoder encodes a triplet input -history question-answer sentences, an image, and a current question sentence -into a vector representation; then, the decoder fuses those vectors and decodes them into answer sentences (e.g., by generative language models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref> or discriminative candidate ranking <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>). So far, one may identify that the key difference between Visual Dialog (VisDial) and the well-known Visual Ques-tion&amp;Answering (VQA) <ref type="bibr" target="#b2">[3]</ref> is the exploitation of the history. As shown in <ref type="figure" target="#fig_2">Figure 1</ref>(a) and <ref type="figure" target="#fig_2">Figure 1(b)</ref>, VisDial can be cast into a multi-round VQA given additional language context of history question-answering pairs. Essentially, at each round, the response generated by the agent is "thrown away" and the history is artificially "corrected" by the ground-truth answers <ref type="bibr" target="#b5">[6]</ref> for the next round. Note that this ground-truth history setting is reasonable because it steers the conversation to be evaluable; otherwise, any other response may digress the conversation into a never-ending open-domain chitchat <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14]</ref>. However, we argue that by only exploiting the ground-truth history is ineffective for the codec training. For example, the ground-truth answer only tells the model that "you can not see it" is good, but neglect to show how badly other answers will impact the dialog. Therefore, the resultant model is easily over-grounded in the insufficient ground-truth data, but not learning visual reasoning <ref type="bibr" target="#b21">[22]</ref>.</p><p>In this paper, we propose a novel training strategy that utilizes the history response in a more efficient way, that is, to make the codec model more sensitive to the history dialog such as co-reference resolution and context. As illustrated in <ref type="figure" target="#fig_2">Figure 1</ref>(c), we intentionally impose wrong answers in the "tamperred" history (e.g. replacing "White with blue lettering, but hard to see it." with " Yellow" in the history) and see how the model behaves as compared to the "gold" history. Specifically, suppose that we are going to train a model at the t-th round, to gain more insights about the wrong answers, we maintain two parallel lines of dialogs: one with the ground-truth answer A t gt , and the other one with a probable mistake A t . Then, we run the two lines to a future round t (both of them are filled with groundtruth answers from t + 1 to t ). Thus, we can collect two rewards at round t : 1) Gold Reward (GR): a conventional ground-truth history reward for answer A t gt , the larger the better , and 2) Adverse Critic (AC): a proposed critic for fake history reward of A t , small AC implies large future impact by the mistake answer. Interestingly, their difference HA = GR − AC, which we call History-Advantage (HA), will tell the model how to reward A t gt : if HA &gt; 0, large HA implies large impact of fake history, i.e., very small AC; thus, we need to strengthen the training signal of gold history with A t gt ; if HA ≤ 0, it implies the ineffectiveness of the gold history, i.e., the current model cannot accurately rank correct answer candidates; thus, the gradient for A t gt will be in the opposite direction. In this way, we can collect a sequence of history-advantage training losses from t + 1 to T . Therefore, we call the proposed training paradigm: History-Advantage Sequence Training (HAST).</p><p>Although the application scenario of HAST is regardless of specific codec models, for more effective training, we develop a novel codec dubbed: History-Aware Co-Attention (HACAN) encoder to address the essential co-reference and visual context in history encoding. In a nutshell, HACAN is a sequential model that contains two novel co-attention modules and one history-aware gate. Equipped with the the proposed History-Advantage Sequence Training (HAST), we achieve a new state-of-the-art single-model on the real-world VisDial benchmarks: 0.6792 MRR on VisDial v0.9, 0.6422 MRR on VisDial v1.0, and 66.8% accuracy on GuessWhat?! <ref type="bibr" target="#b6">[7]</ref>. We also achieve a top performing 0.5717 NDCG score on the official VisDial online challenge server. More ablative studies, qualitative examples, and detailed results are discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Dialog. Visual dialog is recently proposed in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b6">[7]</ref>, which is a more challenging vision-language problem. Most vision-and-language problems are based on a single-round language interaction (e.g. image caption <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2]</ref> and visual question answering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25]</ref>). On the contrary, visual dialog task involves a multi-round dialog which is more complex. Das et al. <ref type="bibr" target="#b5">[6]</ref> proposed a large-scale free-form visual dialog dataset, which consists of sequential open-ended questions and answers about arbitrary objects in the image. Another visual dialog task GuessWhat?! proposed by <ref type="bibr" target="#b6">[7]</ref> focuses on a different aspect, which aims at object discovery with a set of yes/no questions. We apply the first setting in this paper. The proposed approaches for Visual Dialog are based on encoder-decoder structure, and can be categorized into three groups based on the design of encoder: (1) Fusion-based Models (LF <ref type="bibr" target="#b5">[6]</ref>, HRE <ref type="bibr" target="#b5">[6]</ref>, Sync <ref type="bibr" target="#b11">[12]</ref>), the methods fuse image, question, and history features at different stages. (2) Attention-based Models (MN <ref type="bibr" target="#b5">[6]</ref>, HCIAE <ref type="bibr" target="#b23">[24]</ref>, CoAtt <ref type="bibr" target="#b24">[25]</ref>), the methods establish attention mechanisms over image, question and history. (3) Approaches that deal with visual reference resolution (AMEM <ref type="bibr" target="#b31">[32]</ref>, CorefNMN <ref type="bibr" target="#b19">[20]</ref>, RvA <ref type="bibr" target="#b16">[17]</ref>, DAN <ref type="bibr" target="#b16">[17]</ref>), the methods focus on explicit visual co-reference resolution in visual dialog.</p><p>Attention Mechanism. Attention mechanisms are widely used in vision-and-language problems, and have achieved inspiring. For visual question answering, the attentionbased model may attend to both the relevant regions in the image and the phrases in the questions. A number of approaches <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33]</ref> have been proposed that apply question-based attention module on image features. Attention models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> which attend to the phrases or words in the questions are developed in later studies. We apply co-attention mechanism in our proposed model HACAN.</p><p>Reinforcement Learning with Baseline. Reinforcement learning with baseline is widely applied in language generation, e.g., image caption <ref type="bibr" target="#b30">[31]</ref> and visual dialog <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref>. However, the historical response rounds in dialog are not considered as a sequence, which is in contrast our work focuses on. In particular, we follow the spirit of sequence training and design a history-advantage as a baseline to reward or penalize the future dialog rounds based on the current gold and fake history. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Codec Model</head><p>In this section, we formally introduce the visual dialog task and describe the details of our proposed model. We follow the definition introduced by Das et al. <ref type="bibr" target="#b5">[6]</ref>. Given input as: 1) an image I, 2) a dialog history with a caption A 0 of the image and t − 1 rounds of the dialog</p><formula xml:id="formula_0">{A 0 , (Q 1 , A 1 ), ..., (Q t−1 , A t−1 )}, where (Q i , A i )</formula><p>is the ith round of the "ground-truth" question and answer pair, 3) a follow-up question Q t , and 4) a list of 100 candidate answer options {A t 1 , ..., A t 100 } which contains one correct answer A t gt . The visual dialog model needs to sort the answer options and choose the right one when given the inputs. To perform response generation given the above task, as illustrated in <ref type="figure">Figure 2</ref>, our codec model includes three modules: 1) feature representation (Section 3.1), 2) the proposed History-Aware Co-Attention Network (HACAN) for the encoder (Section 3.2), and 3) a discriminative decoder for response generation by ranking (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Representation</head><p>Image Feature. We follow bottom-up attention mechanism to extract region-based image features as in <ref type="bibr" target="#b1">[2]</ref>. We train Faster-RCNN <ref type="bibr" target="#b29">[30]</ref> based on the ResNet-101 backbone <ref type="bibr" target="#b12">[13]</ref> on Visual Genome <ref type="bibr" target="#b20">[21]</ref> dataset. We choose top-K regions from each image and encode the regions as the visual fea-</p><formula xml:id="formula_1">ture V t ,</formula><p>where t is the current round in the dialog. For fair comparison with some methods without region proposal network <ref type="bibr" target="#b29">[30]</ref>, we replace bottom-up attention mechanism with VGG model in the ablation study. More details are discussed in Section 5.</p><p>Language Feature. 1) Question Feature: We first embed the words of the follow-up question. LSTM is applied to generate a sequence of hidden states. Words play different roles in the question. The operative words can tell the model the kind of the question and which instance attribute to consider. Especially in visual dialog task, history and the current question may have relationships and make word contribution mean more. As illustrated in <ref type="figure" target="#fig_2">Figure 1</ref>(a), the word "written" in the current question has relationship with "blue lettering" and helps the model focus on the last round in the history. Meanwhile, the last round reminds the model to pay more attention to the word "written". From this, we use the whole hidden state sequence generated by LSTM instead of last hidden state as some prior works do, which is denoted as Q t . 2) Answer Feature: We apply another LSTM to the word embeddings of the candidates, and use the the whole hidden state sequence as the answer feature.</p><p>3) History Feature: Each round of the question and the answer in the history is concatenated into a long "sentence". Another LSTM is applied to each round of the history after word embedding. We use the last hidden state of each round as the question-answer pair feature. The history feature is denoted as H t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">History-Aware Co-Attention Network</head><p>We propose a novel attention-based model called History-Aware Co-Attention Network (HACAN) to encode the input features described above with the co-attention mechanism <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>As illustrated in <ref type="figure">Figure 2</ref>, HACAN is composed by a sequence of attention blocks, each of them contains two attention-based modules: Feature-Wise Co-Attention module (FCA) and Element-Wise Co-Attention module (ECA). Given the input triplet {V t , Q t , H t }, FCA aims to attend to the relevant features in one set of inputs with the guidances from the other two inputs. ECA takes the outputs of FCA as the inputs. It aims to activate the relevant elements and restrain the irrelevant ones with the guidances from the other two inputs before the final fusion. We now describe the two modules in details. Feature-Wise Co-Attention (FCA). We use additive attention function to compute the feature-wise attention, returning the attended feature as output. Without loss of generality, we take the FCA for V as an example, which is denoted</p><formula xml:id="formula_2">as Attend f (V t , g Q , g H ). Take the visual feature of image regions V t = {v 1 , ...v K } as input, the visual attention is formulated as: v i = tanh(W f 1 v t i + W f 2 g Q + W f 3 g H ),<label>(1)</label></formula><formula xml:id="formula_3">α i = softmax(W T v i ),<label>(2)</label></formula><formula xml:id="formula_4">v t f = K i=1 α i v i i = 1, ..., K,<label>(3)</label></formula><p>where g Q , g H ∈ R d are the guidances from Q t and</p><formula xml:id="formula_5">H t . W f 1 , W f 2 , W f 3 ∈ R d×d , W ∈ R d×1</formula><p>and d is the feature dimension. As the formats of Q t and H t are consistent with V t , we can simply apply Eq.</p><formula xml:id="formula_6">(1)-(3) to Attend f (Q t , g H , g V ) and Attend f (H t , g V , g Q ),</formula><p>and compute three modules in parallel. Element-Wise Co-Attention (ECA). The outputs of FCAs are the sum of attended features. However, the features itself are not attended yet. We introduce an element-wise attention mechanism to attend the sum of features. It is worth noting that, each element in the feature (e.g., v i ∈ R d in V t ) is a response activation of the neural networks, and reflects some attributes of the instance in a sense. We apply the attention mechanism in an element-wise manner with the guidance from the other two inputs. It can be regarded as selecting the relevant semantic attributes and discarding the irrelevant ones based on the guidances from other domains. We take Attend e (v t f , q t f , h t f ) as an example, and ECA is formulated as:</p><formula xml:id="formula_7">V e = tanh(W e 1 ⊗ v t f + (W e 2 q t f )E T + (W e 3 h t f )E T ), (4) v t e = σ(W T V e ) v t f ,<label>(5)</label></formula><p>where W e 1 ∈ R m which is learnable and all elements are initialized with value 1, W e 2 , W e 3 ∈ R d×d and W ∈ R m . E ∈ R m is a vector with all elements set to 1. ⊗ represents the outer product of vectors and represents the element-wise product. We use W e 1 to broadcast v t to m times, and perform the attention function in parallel. It can be viewed as the additional-attention of multi-head attention in <ref type="bibr" target="#b36">[37]</ref>. We compute</p><formula xml:id="formula_8">Attend e (q t f , h t f , v t f ) and Attend e (h t f , v t f , q t f )</formula><p>in the same way. Gated History-Awareness. We observe that an ambiguous question often has relationship with its latest round in the history. From this, the encoding feature of the latest his-</p><formula xml:id="formula_9">tory v t−1 e , h t−1 e</formula><p>is a good choice to initialize the guidances in FCAs, which can be viewed as history-awareness. However, sometimes the current question has nothing to do with its latest history. A simple solution is to apply a gate function to control the history-awareness, which is formulated as:</p><formula xml:id="formula_10">q t s = Attend f (Q t , 0, 0),<label>(6)</label></formula><formula xml:id="formula_11">o = M LP ([f q g (q t s ), f h g (h t−1 e )]),<label>(7)</label></formula><formula xml:id="formula_12">h t−1 e = σ(o)h t−1 e , v t−1 e = σ(o)v t−1 e ,<label>(8)</label></formula><p>where q t s is self-attention of the current question, [·] is a concatenation operation and σ(o) is the gate value. With the gated history-awareness, each first FCA is guided by its latest history feature and update the attended features efficiently. For the follow-up modules, we take the outputs of ECAs as the guidance inputs in Eq. (1), so we can connect two attention modules recurrently and update the encoding feature v t e , h t e , q t e , as illustrated in <ref type="figure">Figure 2</ref>. The shortcut connections in the layers help the model consider different levels in multi-hop visual reasoning. The outputs of final round of attention blocks, v t e , h t e , q t e , are used to generate the answer response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Response Generation</head><p>Now we introduce how to generate the answers for the visual dialog task. We concatenate the three features v t e , h t e , q t e together and use a linear transform followed by a tangent activation:</p><formula xml:id="formula_13">z = tanh(W e [v t e , h t e , q t e ]),<label>(9)</label></formula><p>where [, ] is a concatenation operation. We encode candidate answer features using a self-attention mechanism. The self-attention mechanism is formulated like Eq. (6). We dot product the candidate answer features and z to calculate the similarities.</p><p>We sort the answer candidates by the similarities and choose the top one with highest similarity as the prediction.</p><p>In the task of GuessWhat?!, the information of answer candidates are the localizations and categories of the objects. We concatenate the localizations and categories and embed them with a linear transform to obtain the answer option features. We also calculate the similarities of answer features and the final encoding features by dot product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">History-Advantage Sequence Training</head><p>HACAN described in Section 3.2 encodes the "ground truth" triplet and generates the responds. However, by only using the conventional supervised learning, HACAN does not take into account the contribution of "gold" history and the impact of imperfect history, which makes the model history-aware and explore semantic information in the history. To disentangle the individual contribution of a specific round in the history and make the codec model more sensitive to the history, an intuitive solution is to replace the default answer of the specific round in the history with imperfect answers. To this end, besides utilizing "gold" history, we intentionally impose wrong answers chosen by the model to generate the "tamperred" history, and see how much better the model performs with "gold" history than the one with "tamperred" history. We first describe the policy gradient for visual dialog in Section 4.1, and then we describe the history-advantage in the policy gradient in Section 4.2. Finally, we briefly introduce the training process in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Policy Gradient for Visual Dialog</head><p>We transform visual dialog task to a simple two-step decision-making game: in step 1, given an image I, t-round history with caption {A 0 , (Q 1 , A 1 ), ..., (Q t−1 , A t−1 )} and the follow-up question Q t , the agent needs to choose an answer from the candidates. In step 2, besides the inputs above and the choice of the agent, a judge is given the remaining rounds from t + 1 to t , and needs to mark by answering the current question Q t (t' is anywhere from t + 1 to 10) correctly. The goal of the game is to choose the right answer of Q t , which is the same as visual dialog task described in Section 3. The difference is the choice of Q t may be imperfect and impact the score of the game. We formulate the game as a decision-making problem. The action space is the answer candidates denoted as {A t 1 , ..., A t 100 } and the state space is {V t , Q t , H t }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">History-Advantage</head><p>We use the metric of the visual dialog task to compute the reward (e.g. MRR). We denote the gold reward (GR) as R(V t , Q t , H t g ) and use R(V t , Q t , H t a,i ) to represent the adverse critic (AC) when the agent choose the i-th negative answer in round t. As illustrated in <ref type="figure" target="#fig_2">Figure 1</ref>(c), their difference GR − AC can reflect the influence of the "gold" round t when answering Q t . We definite history-advantage of the "gold" round t as:</p><formula xml:id="formula_14">GR − AC = A(t ) = A(V t , Q t , H t ) = R(V t , Q t , H t g ) − p(Ā t i )R(V t , Q t , H t a,i ),<label>(10)</label></formula><p>where A(t ) can estimate the contribution of the "gold" answer of round t on answering Q t . It plays a similar role as the "advantage" in actor-critic methods, and AC is a baseline in the history-advantage. When the total adverse critic is lower than gold reward, A(t ) is positive. It means the mistake does impact negatively on the future and the influence of the "gold" round is positive. The baseline can reduce the variance of gradient estimation in the training part as well and help the model take the contribution of the history into account. As t is anywhere from t + 1 to 10, we compute A(t ) with different value of t , which can reduce the variance of gradient estimation as well. The sum of A(t ) can also be viewed as the contribution of the "gold" round in the whole dialog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training</head><p>Inspired by the policy gradient theorem <ref type="bibr" target="#b35">[36]</ref>, the historyadvantage gradient can be simply denoted as:</p><formula xml:id="formula_15">∇ θ J g = ∇ θ log p(A t gt ; θ) · ( 1 10 − t 10 t =t+1</formula><p>A(t )), <ref type="bibr" target="#b10">(11)</ref> Following previous policy gradient works that use a supervised pre-training step as model initialization, we train our model with two-stage training. In the first training stage, we use a metric-learning multi-class N-pair loss L np <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>In HAST, we denote the final gradient as:</p><formula xml:id="formula_16">∇ θ J = ∇ θ J g − α∇ θ L np ,<label>(12)</label></formula><p>where we incorporate N-pair loss (weighted by a trade-off α) for an end-to-end training. The whole training process is reviewed in Algorithm. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In the following we evaluate our proposed approach on three visual dialog datasets, VisDial v0.9 <ref type="bibr" target="#b5">[6]</ref>, VisDial v1.0 <ref type="bibr" target="#b5">[6]</ref> and GuessWhat?! <ref type="bibr" target="#b6">[7]</ref>. We first present the details about the datasets, evaluation metrics and the implementation details. Then we provide qualitative results and compare our methods with the state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Discriminative Model with History-Advantage Sequence Training</head><p>Require: Supervised Pre-trained Model HACAN 1: for Round t = 1, · · · , T − 1 do 2:</p><p>Compute</p><formula xml:id="formula_17">{p(Ā t 1 ), · · · , p(Ā t N −1 )} ∼ HACAN(V t , Q t , H t )</formula><p>where N is the number of answer candidates. <ref type="bibr">3:</ref> for t = t + 1, · · · , T do 4:</p><p>for i = 1, · · · , N − 1 do 5:</p><p>Fake the history H t withĀ t i 6: Compute the gradient ∇ θ J with Eq. <ref type="formula" target="#formula_2">(12)</ref> 10:</p><formula xml:id="formula_18">Compute R(V t , Q t , H t g ) and R(V t , Q t ,</formula><p>θ ← θ + δ∇ θ J 11: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>VisDial v0.9 <ref type="bibr" target="#b5">[6]</ref> contains about 123k image-captiondialog tuples. The images are all from MS COCO <ref type="bibr" target="#b22">[23]</ref> with multiple objects. The dialog of each image has 10 question-answer pairs, which were collected by pairing two people on Amazon Mechanical Turk to chat with each other about the image. Specifically, the "questioner" is required to "imagine the scene better" by sequentially asking questions about the hidden image. The "answerer" then observes the picture and answers questions.</p><p>VisDial v1.0 <ref type="bibr" target="#b5">[6]</ref> is an extension of VisDial v0.9 <ref type="bibr" target="#b5">[6]</ref>. Images for the training set are all from COCO train2014 and val2014. The dialogs in validation and test sets were collected on about 10k COCO-like images from Flickr. The test set is split into two parts, 4k images for test-std and 4k images for test-challenge. Answers are already provided for the train and val set, but not in the test set. For the test-std and test-challenge phases, the results must be submitted to the evaluation server.</p><p>We also evaluated our proposed model on GuessWhat?! dataset <ref type="bibr" target="#b6">[7]</ref>. The dataset contains 67k images collected from MS COCO <ref type="bibr" target="#b22">[23]</ref> and 155k dialogs including about 820k question-answer pairs. The guesser game in GuessWhat?! is to predict the correct object in object options through a multi-round dialog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Metrics</head><p>For VisDial v0.9, we followed the evaluation protocol established in <ref type="bibr" target="#b5">[6]</ref> and used the retrieval setting to evaluate the responses at each round in the dialog. Specifically, for each question we sorted the answer options and used Recall@k, mean reciprocal rank (MRR) and mean rank of the ground truth answer to evaluate the model. Recall@k is the per-centage of questions for which the correct answer option is ranked in the top k predictions of a model. Mean rank is the average rank of the ground truth answer option. Mean reciprocal rank is the average of 1/rank of the ground truth answer option. For the test set of VisDial v1.0, we also evaluated our model using the newly introduced normalized discounted cumulative gain (NDCG). NDCG is invariant to the order of options with identical relevance and to the order of options outside of the top K, where K is the number of answers marked as correct by at least one annotator. For GuessWhat?! dataset, we used classification accuracy to evaluate our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation Details</head><p>Language Processing. We first tokenized the questions and answers using the Python NLTK toolkit and constructed a vocabulary of words that appear at least 5 times in the training split. All the words were embedded to a 300-dimension vector initialized by pre-trained GloVe <ref type="bibr" target="#b27">[28]</ref> embeddings. The LSTMs of the question and history were two-layered, while they were one-layered for the answers. The hidden states in all LSTM were 512-d. Training Details. We pretrained our codec using the supervised training for 15 epochs before starting HAST. We used Adam optimizer <ref type="bibr" target="#b18">[19]</ref> and started the supervised training with the base learning rate of 1×10 −3 and decreasing to 5×10 −4 after 10 epochs. In HAST, the base learning rate was 1×10 −4 , and decayed every 5 epochs with an exponential rate of 0.5. We set the hyper-parameter m to 16, and W e 1 ∈ R 16 in Eq. (4). We set α to 1 in Eq. (12). In Eq. (10), we needed to sum over all the negative answer candidates to calculate the advantage for HAST. It cost quite a lot of time to train (about 99 evaluations for each incorrect answers). However, we noticed that only a few negative answers have non-ignorable probabilities. For reducing the time cost, we made a trade-off between the accuracy and the speed and summed over the top-5 negative answers chosen by our model. The experiment showed that we saved 95% training time with a slight performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablative Study</head><p>Components in HACAN. We present a few variants of our model to verify the contribution of each component:   attention modules three times in a residual manner.</p><p>The first and second row in <ref type="table" target="#tab_3">Table 3</ref> show that, HA-CAN w/o FCA uses question-guided image feature (like the style in VQA task), but is not history-aware. Benefiting from FCA and Gated History-Awareness, the model takes history-aware features and achieves approximately 3.5% improvements on MRR, more in line with the conversational nature of visual dialog. In HACAN w/o ECA-16, two types of attention modules guide the model not only which features to focus (e.g. feature-level attended histories), but also the attended attributes of features, advancing the attention mechanism to a more fine-grained level. The hyper-parameter m can be viewed as the number of heads in multi-head attention <ref type="bibr" target="#b36">[37]</ref>, and improves the performance by approximately 0.5 point on MRR. We stacked two attention modules up to three layers in a residual manner to obtain multi-level abstraction of history-aware and question-aware features, this achieved the best performance in our ablative experiments. We believe the ablative experiments demonstrate that: (1) different from the VQA task, the model for visual dialog task relies on history-aware representations.</p><p>(2) FCA and ECA are efficient to compute both reliable question-aware and history-aware visual attention.</p><p>Effectiveness of HAST. We did ablative experiments to demonstrate the effectiveness of HAST. Results are showed in <ref type="table" target="#tab_4">Table 4 and Table 5</ref>. Our model achieves approximately another 1 point improvement on R@1 and 0.5% on MRR on VisDial dataset. Beyond that, we applied HAST to some additional ablative models with official codes 1 . We denote HCIAE-D-MLE <ref type="bibr" target="#b23">[24]</ref> and HCIAE-D-NP-ATT <ref type="bibr" target="#b23">[24]</ref> as HCIAE-M and HCIAE. The encoder models are attentionbased. Interestingly, we found that both models achieve improvements with our proposed HAST. The results validate that: taking the impact of imperfect history into account, the history-aware models explore the contributions of "gold" history and deal with the relationship between visual and linguistic representations better.</p><p>The qualitative results shown in <ref type="figure" target="#fig_3">Figure 3</ref> and <ref type="figure" target="#fig_4">Figure 4</ref> demonstrate the following advantages of our HACAN model with HAST: History Sensitive. Our model is sensitive to the history. In <ref type="figure" target="#fig_3">Figure 3</ref>, the upper half part shows that, with historical changes, our model HACAN generates different responses. In more details, with the contribution of "gold" history, the "ground truth" answer achieves a higher rank order. Thanks to FCA and ECA, the visual attention influenced by historical changes is more precise when given the "gold" history. Rather, with historical changes, HCIAE behaves the same and does not benefit from the contribution of "gold" history. Reliable Contextual Reasoning. Our model addresses contextual reasoning reliably using HAST. Focusing more on the influences of the different histories in the dialog, HA-CAN learns more contextual reasoning. In <ref type="figure" target="#fig_4">Figure 4</ref>, with different facts, HACAN chooses the corresponding regions in the image correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with the State-of-the-art</head><p>We compared our model with the state-of-the-art methods on VisDial v0.9 and v1.0. Early methods use grid-based CNN(e.g. VGG-16 <ref type="bibr" target="#b33">[34]</ref>) features. For fair comparison, we replaced the bottom-up attention features <ref type="bibr" target="#b1">[2]</ref> with ImageNet pre-trained VGG-16 features. The upper half of <ref type="table" target="#tab_1">Table 1</ref> reports the results of the methods with the VGG-16 features, and the bottom half reports the results with bottom-up attention features. Compared Methods. The state-of-the-art methods can be categorized into three groups based on the design of encoder: (1) Fusion-based Models (LF <ref type="bibr" target="#b5">[6]</ref>, HRE <ref type="bibr" target="#b5">[6]</ref>, Sync <ref type="bibr" target="#b11">[12]</ref>).</p><p>(2) Attention-based Models (MN <ref type="bibr" target="#b5">[6]</ref>, HCIAE <ref type="bibr" target="#b23">[24]</ref>, CoAtt <ref type="bibr" target="#b24">[25]</ref>). (3) Approaches that deal with visual reference resolution in VisDial (AMEM <ref type="bibr" target="#b31">[32]</ref>, CorefNMN <ref type="bibr" target="#b19">[20]</ref>, RvA <ref type="bibr" target="#b16">[17]</ref>, DAN <ref type="bibr" target="#b16">[17]</ref>)</p><p>Our method outperforms the state-of-the-art methods across most of the metrics. Specifically, our method achieves more than 1 point improvement on R@1, and 1% increase on MRR. We also achieve a new state-of-theart single-model on the official VisDial online challenge server 2 . Furthermore, we conducted supplementary experi-    <ref type="table">Table 5</ref>. Performance of ablative models on the guesser game of GuessWhat?!. HAST indicates the usage of History-Advantage Sequence Training. ments on the guesser task of GuessWhat?!. <ref type="table" target="#tab_2">Table 2</ref> shows that our method is comparable to the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we develop a codec model equipped with History-Aware Co-Attention Network (HACAN) for the visual dialog task. HACAN contains Feature-Wise Co-Attention module and Element-Wise Co-Attention module to address the co-reference and visual context in question and history encoding. We propose a novel training strategy dubbed History-Advantage Sequence Training (HAST) that utilizes the history response to make the codec model more sensitive to the history dialog. Extensive experiments on the real-world datasets, VisDial and GuessWhat?!, achieve a new state-of-the-art single-model on the benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 . 1 e and v t− 1 e</head><label>211</label><figDesc>The framework of our proposed codec model. H t , Q t and V t are the input triplets (Section 3.1) extracted by CNNs and LSTMs. t denotes the current round of the dialog. Feature-Wise Co-Attention and Element-Wise Co-Attention are applied as blocks in parallel to encode the input triplets and generate the guidances of the follow-up attention layer. The history-awareness, h t−, are used to initialize the guidances in the first block (Section 3.2). The final outputs of the encoder update the history-awareness and feed to the decoder as well. The decoder finally generates the response and ranks candidate answer options (Section 3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 1 )</head><label>1</label><figDesc>HACAN w/o FCA is a baseline without FCA and ECA. Image features are guided by the question, the attention weight is computed by Eq. (1) with only question guidance. (2) HACAN w/o ECA-1 is a model with one FCA and Gated History-Awareness. The model has no ECA. (3) HACAN w/o ECA-16 is a model with one FCA, followed by one ECA with m = 1 in Eq. (5). The ECA is not multihead. (4) HACAN w/o RS is a model with one FCA, followed by one ECA with m = 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results on VisDial dataset. We visualize the behaviors of our model and HCIAE with historical changes. Incorrect history and the region chosen by model with tamperred history are marked with red. GT rank denotes the rank of groundtruth answer in the sorted list.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results of our model on GuessWhat?!. The green bounding boxes highlight the right predictions with gold histories. The red bounding boxes highlight the wrong predictions with tamperred histories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The numbers of multihead is 16. (5) HACAN is a whole model that we stack two Retrieval performance of discriminative models on the test-standard split of VisDial v1.0 and the validation set of VisDial v0.9. RPN indicates the usage of region proposal network.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">VisDial v1.0(test-std)</cell><cell></cell><cell></cell><cell cols="2">VisDial v0.9(val)</cell><cell></cell></row><row><cell></cell><cell>NDCG</cell><cell>MRR</cell><cell cols="4">R@1 R@5 R@10 Mean</cell><cell>MRR</cell><cell cols="3">R@1 R@5 R@10 Mean</cell></row><row><cell>LF w/o RPN [6]</cell><cell cols="4">0.4531 0.5542 40.95 72.45</cell><cell>82.83</cell><cell>5.95</cell><cell cols="3">0.5807 43.82 74.68</cell><cell>84.07</cell><cell>5.78</cell></row><row><cell>HRE [6]</cell><cell cols="4">0.4546 0.5416 39.93 70.45</cell><cell>81.50</cell><cell>6.41</cell><cell cols="3">0.5846 44.67 74.50</cell><cell>4.22</cell><cell>5.72</cell></row><row><cell>MN [6]</cell><cell cols="4">0.4750 0.5549 40.98 72.30</cell><cell>83.30</cell><cell>5.92</cell><cell cols="3">0.5965 45.55 76.22</cell><cell>85.37</cell><cell>5.46</cell></row><row><cell>HCIAE [24]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.6222 48.48 78.75</cell><cell>87.59</cell><cell>4.81</cell></row><row><cell>AMEM [32]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.6227 48.53 78.66</cell><cell>87.43</cell><cell>4.86</cell></row><row><cell>CoAtt [25]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">0.6398 50.29 80.71</cell><cell>88.81</cell><cell>4.47</cell></row><row><cell>CorefNMN [20]</cell><cell cols="4">0.5470 0.6150 47.55 78.10</cell><cell>88.80</cell><cell>4.40</cell><cell cols="3">0.6410 50.92 80.18</cell><cell>88.81</cell><cell>4.45</cell></row><row><cell cols="5">RvA w/o RPN [27] 0.5176 0.6060 46.25 77.88</cell><cell>87.83</cell><cell>4.65</cell><cell cols="3">0.6436 50.40 81.36</cell><cell>89.59</cell><cell>4.22</cell></row><row><cell>Ours w/o RPN</cell><cell cols="4">0.5281 0.6174 47.91 78.59</cell><cell>87.81</cell><cell>4.63</cell><cell cols="3">0.6451 50.72 81.18</cell><cell>89.23</cell><cell>4.32</cell></row><row><cell>LF [6]</cell><cell cols="4">0.5163 0.6041 46.18 77.80</cell><cell>87.30</cell><cell>4.75</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RvA [27]</cell><cell cols="4">0.5559 0.6303 49.03 80.40</cell><cell>89.83</cell><cell>4.18</cell><cell cols="3">0.6634 52.71 82.97</cell><cell>90.73</cell><cell>3.93</cell></row><row><cell>Sync[12]</cell><cell cols="4">0.5732 0.6220 47.90 80.43</cell><cell>89.95</cell><cell>4.17</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DAN [17]</cell><cell cols="4">0.5759 0.6320 49.63 79.75</cell><cell>89.35</cell><cell>4.30</cell><cell cols="3">0.6638 53.33 82.42</cell><cell>90.38</cell><cell>4.04</cell></row><row><cell>Ours</cell><cell cols="4">0.5717 0.6422 50.88 80.63</cell><cell>89.45</cell><cell>4.20</cell><cell cols="3">0.6792 54.76 83.03</cell><cell>90.68</cell><cell>3.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results on the guesser game of GuessWhat?!.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">Train err Val err Test err</cell></row><row><cell>LSTM [7]</cell><cell></cell><cell cols="2">27.9%</cell><cell>37.9% 38.7%</cell></row><row><cell>HRED [7]</cell><cell></cell><cell cols="2">32.6%</cell><cell>38.2% 39.0%</cell></row><row><cell cols="2">LSTM+VGG [7]</cell><cell cols="2">26.1%</cell><cell>38.5% 39.2%</cell></row><row><cell cols="2">HRED+VGG [7]</cell><cell cols="2">27.4%</cell><cell>38.4% 39.6%</cell></row><row><cell>ATT [8]</cell><cell></cell><cell cols="2">26.7%</cell><cell>33.7% 34.2%</cell></row><row><cell>Ours</cell><cell></cell><cell cols="2">26.1%</cell><cell>32.3% 33.2%</cell></row><row><cell>Model</cell><cell></cell><cell>MRR</cell><cell>R@1 R@5 R@10 Mean</cell></row><row><cell>HACAN w/o FCA</cell><cell cols="3">0.5837 44.52 74.77 84.84</cell><cell>5.56</cell></row><row><cell>HACAN w/o ECA-1</cell><cell cols="3">0.6181 48.29 78.23 87.76</cell><cell>4.77</cell></row><row><cell cols="4">HACAN w/o ECA-16 0.6285 49.26 79.41 88.72</cell><cell>4.53</cell></row><row><cell>HACAN w/o RS</cell><cell cols="3">0.6323 49.61 79.96 89.05</cell><cell>4.40</cell></row><row><cell>HACAN</cell><cell cols="3">0.6391 50.44 80.67 89.71</cell><cell>4.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance of ablative models on the validation set of VisDial v1.0.</figDesc><table><row><cell>Model</cell><cell>MRR</cell><cell cols="2">R@1 R@5 R@10 Mean</cell></row><row><cell cols="3">HCIAE-M w/o HAST [24] 0.6156 47.67 78.50 87.54</cell><cell>4.68</cell></row><row><cell>HCIAE-M [24]</cell><cell cols="2">0.6177 47.95 78.70 87.97</cell><cell>4.61</cell></row><row><cell>HCIAE w/o HAST [24]</cell><cell cols="2">0.6227 48.58 79.19 88.16</cell><cell>4.58</cell></row><row><cell>HCIAE [24]</cell><cell cols="2">0.6243 48.71 79.14 88.72</cell><cell>4.53</cell></row><row><cell>HACAN w/o HAST</cell><cell cols="2">0.6391 50.44 80.67 89.71</cell><cell>4.32</cell></row><row><cell>HACAN</cell><cell cols="2">0.6445 51.20 80.76 89.92</cell><cell>4.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Performance of ablative models on the validation set of VisDial v1.0. HAST indicates the usage of History-Advantage Sequence Training.</figDesc><table><row><cell>Model</cell><cell cols="2">Train err Val err Test err</cell></row><row><cell>HRED w/o HAST [7]</cell><cell>32.6%</cell><cell>38.2% 39.0%</cell></row><row><cell>HRED [7]</cell><cell>31.8%</cell><cell>37.7% 38.4%</cell></row><row><cell>HRED+VGG w/o HAST [7]</cell><cell>27.4%</cell><cell>38.4% 39.6%</cell></row><row><cell>HRED+VGG [7]</cell><cell>26.8%</cell><cell>37.7% 38.9%</cell></row><row><cell>HACAN w/o HAST</cell><cell>26.9%</cell><cell>33.6% 34.1%</cell></row><row><cell>HACAN</cell><cell>26.1%</cell><cell>32.3% 33.2%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/jiasenlu/visDial.pytorch 2 https://evalai.cloudcv.org/web/challenges/challengepage/103/leaderboard/298</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sort story: Sorting jumbled images and captions into stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Human attention in visual question answering: Do humans and deep networks look at the same regions? Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="90" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual dialog</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual grounding via accumulated attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-question-answer synergistic network for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards an open-domain conversational system fully based on natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Higashinaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Two can play this game: visual dialog with discriminative question generation and answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dual attention networks for visual reference resolution in visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09368</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual coreference resolution in visual dialog using neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BEHAV BRAIN SCI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recursive visual attention in visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Designing an interactive open-domain question answering system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Quarteroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>NAT LANG ENG</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual reference resolution using attention memory for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Are you talking to me? reasoned visual dialog generation through adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structured attentions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
