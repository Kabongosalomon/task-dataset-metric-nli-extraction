<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Multilayer Channel Features for Pedestrian Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Learning Multilayer Channel Features for Pedestrian Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Pedestrian Detection</term>
					<term>Multilayer Channel Fea- tures (MCF)</term>
					<term>HOG+LUV</term>
					<term>CNN</term>
					<term>NMS</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pedestrian detection based on the combination of Convolutional Neural Network (i.e., CNN) and traditional handcrafted features (i.e., HOG+LUV) has achieved great success. Generally, HOG+LUV are used to generate the candidate proposals and then CNN classifies these proposals. Despite its success, there is still room for improvement. For example, CNN classifies these proposals by the full-connected layer features while proposal scores and the features in the inner-layers of CNN are ignored. In this paper, we propose a unifying framework called Multi-layer Channel Features (MCF) to overcome the drawback. It firstly integrates HOG+LUV with each layer of CNN into a multi-layer image channels. Based on the multi-layer image channels, a multi-stage cascade AdaBoost is then learned. The weak classifiers in each stage of the multi-stage cascade is learned from the image channels of corresponding layer. With more abundant features, MCF achieves the state-of-the-art on Caltech pedestrian dataset (i.e., 10.40% miss rate). Using new and accurate annotations, MCF achieves 7.98% miss rate. As many non-pedestrian detection windows can be quickly rejected by the first few stages, it accelerates detection speed by 1.43 times. By eliminating the highly overlapped detection windows with lower scores after the first stage, it's 4.07 times faster with negligible performance loss.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P EDESTRIAN detection based on Convolutional Neural Network (i.e., CNN) has achieved great success recently <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The main process of CNN based methods can be divided into two steps: proposal extraction and CNN classification. Firstly, the candidate proposals are extracted by the traditional pedestrian detection algorithm (e.g., ACF <ref type="bibr" target="#b10">[11]</ref> and LDCF <ref type="bibr" target="#b21">[22]</ref>). Then, these proposals are classified into pedestrian or non-pedestrian by the CNN <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b0">[1]</ref>.</p><p>Despite its great success, it still exists some room to improve it. 1) Most methods only use the last layer features in CNN with softmax or SVM to classify the proposals. In fact, different layers in CNN represents the different image characteristic. The first few layers can better describe the image local variance, whereas the last few layers abstract the image global structure. It means that each layer in CNN contains different discriminative features, which can be used for learning the classifier. 2) Some methods only use the Y. <ref type="bibr">Pang</ref>  traditional methods based on the handcrafted features (i.e., HOG+LUV <ref type="bibr" target="#b9">[10]</ref>) to generate the candidate proposals while ignoring the proposal scores. 3) Due to the large amount of convolutional operations, the methods based very deep CNN (e.g., VGG16 <ref type="bibr" target="#b24">[25]</ref>) run very slowly on the common CPU (e.g., about 8s).</p><p>Recently, researchers have done some work to solve the above problems. Li et al. <ref type="bibr" target="#b34">[35]</ref> proposed to train the cascaded multiple CNN models of different resolutions. As the low resolution CNN can early reject many background regions, it avoids scanning the full image with high resolution CNN and then reduces the computation cost. However, the training process of multiple CNN models is relatively complex. Cai et al. <ref type="bibr" target="#b0">[1]</ref> proposed the complexity-aware cascade to seamlessly integrate handcrafted features and the last layer features in CNN into a unifying detector. However, it still does not make full use of the multi-layer features in CNN. Bell et al. <ref type="bibr" target="#b18">[19]</ref> concatenated the multiple layers of CNN into the fixedsize ROI pooling. With more abundant feature abstraction, it outperforms fast-RCNN <ref type="bibr" target="#b17">[18]</ref>. Though its success, it needs complex operations of L2-normalized, concatenated, scaled, and dimension-reduced. Moreover, it ignores the scores of the proposals.</p><p>In this paper, we propose a unifying framework, which is called Multi-layer Channel Features (MCF). Firstly, it integrates handcrafted image channels (i.e., HOG+LUV) and each layer of CNN into the multi-layer image channels. HOG+LUV image channels are set as the first layer, which contains 10 image channels. The layers in CNN correspond to the remaining layers, respectively. Secondly, zero-order, one-order, and high-order features are extracted to generate a large number of candidate feature pools in each layer. Finally, a multi-stage cascade AdaBoost is used to select the discriminative features and efficiently classify object and background. The weak classifiers in each stage of multi-stage cascade are learned based on the candidate features from corresponding layer. To further accelerate detection speed, the highly overlapped detection windows with lower scores are eliminated after the first stage. Overall, the contributions of this paper and the merits of the proposed methods (MCF) can be summarized as follows:</p><p>1) The unifying framework MCF is proposed. MCF seamlessly integrates HOG+LUV image channels and each layer of CNN into a unifying multi-layer image channels. Due to the diverse characteristic of different layers, these layers can provide more rich feature abstractions.  <ref type="bibr" target="#b36">[37]</ref> of the test set, MCF achieves 7.98% miss rate, which is superior to other methods. The rest of the paper is organized as follows. Firstly, we give a review about pedestrian detection. Then, our methods are introduced in Sec. III. Sec. IV shows the experimental results. Finally, we conclude this paper in Sec. V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>According to whether or not CNN is used, pedestrian detection can be divided into two main manners: the handcrafted channels based methods and CNN based methods. Handcrafted channels based methods are relatively simple and efficient, whereas CNN based methods are much more effective but inefficient. We firstly give a review about the handcrafted channels based methods and then introduce some methods based on CNN.</p><p>Haar features based cascade AdaBoost detector is one of the most famous object detection methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b32">[33]</ref>. It can quickly reject a large number of non-object detection windows by the early stages of the cascade. Dalal and Triggs <ref type="bibr" target="#b11">[12]</ref> proposed to use the Histogram of Oriented Gradients (HOG) to describe the image local variance. It can work very well with a linear SVM. To handle pose variations of objects, Felzenszwalb et al. <ref type="bibr" target="#b12">[13]</ref> proposed the Deformable Part Model (DPM) based on HOG features, which is a mixture of six deformable part models and one root model.</p><p>By integrating cascade AdaBoost <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref> and HOG features <ref type="bibr" target="#b11">[12]</ref>, Dollár et al. <ref type="bibr" target="#b9">[10]</ref> proposed Integral Channel Features (ICF). Firstly, it extracts the local sum features from HOG channels and LUV color channels (i.e., HOG+LUV). Then, cascade AdaBoost <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b35">[36]</ref> is used to learn the classifier. To further speedup the detection, Dollár et al. <ref type="bibr" target="#b10">[11]</ref> then proposed Aggregated Channel Features (ACF), which downsamples the image channels by a factor of 4.</p><p>Following ICF <ref type="bibr" target="#b9">[10]</ref>, SquaresChnFtrs <ref type="bibr" target="#b8">[9]</ref>, InformedHaar <ref type="bibr" target="#b20">[21]</ref>, LDCF <ref type="bibr" target="#b21">[22]</ref>, Filtered Channel Features (FCF) <ref type="bibr" target="#b6">[7]</ref>, and NNNF <ref type="bibr" target="#b30">[31]</ref> have also been proposed. They all employ the same image channels (i.e., HOG+LUV) as ICF. In SquaresChnFtrs <ref type="bibr" target="#b8">[9]</ref>, the pixel sums of local square regions in each channel are used for learning the classifier. InformedHaar <ref type="bibr" target="#b20">[21]</ref> incorporates the statistical pedestrian model into the design of simple haar-like features. Inspired by <ref type="bibr" target="#b22">[23]</ref>, Nam et al. <ref type="bibr" target="#b21">[22]</ref> proposed to calculate the decorrelated channels by convolving the PCA-like <ref type="bibr" target="#b23">[24]</ref> filters with HOG+LUV image channels. Recently, Zhang et al. <ref type="bibr" target="#b6">[7]</ref> proposed to put the above different types of channel features into a unifying framework (i.e., FCF). FCF generates the candidate feature pool by convolving a filter bank (RadomFilters, Checkerboards, etc) with HOG+LUV image channels. It's found that using the simple Checkboards filters could achieve very good performance. Based on the appearance constancy and shape symmetry, Cao et al. <ref type="bibr" target="#b30">[31]</ref> proposed NNNF features.</p><p>Recently, deep Convolutional Neural Network (CNN) based methods have also achieved great success in object detection <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Generally speaking, it firstly generates the candidate object proposals <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b15">[16]</ref> and then uses the trained CNN model <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b14">[15]</ref> to classify these proposals. Hosang et al. <ref type="bibr" target="#b5">[6]</ref> generalized CNN model for pedestrian detection after using the handcrafted features based methods to extract the candidate pedestrian proposals. To eliminate the number of hard negative proposals in the background, Tian et al. <ref type="bibr" target="#b2">[3]</ref> proposed to jointly optimize pedestrian detection with semantic tasks. Recently, Tian et al. <ref type="bibr" target="#b1">[2]</ref> proposed to learn deep strong part models to handle the problem of pedestrian occlusion. Li et al. <ref type="bibr" target="#b3">[4]</ref> proposed the scale-aware fast-RCNN by incorporating a large scale sub-network and a small scale sub-network into a unifying architecture.</p><p>Despite the success of CNN based pedestrian detection, it still exists some room to improve it. Firstly, the score information of the candidate proposals can be used to boost the detection performance. Secondly, each layer in CNN contains some discriminative features, which can be used for learning classifier and rejecting non-pedestrian detection windows early. Cai et al. <ref type="bibr" target="#b0">[1]</ref> proposed to seamlessly integrate CNN and handcrafted features. Though it uses the proposal score information, it still ignores features of the inner layer of CNN. Sermanet et al. <ref type="bibr" target="#b4">[5]</ref> proposed to concatenate the first layer feature and the second layer together. Bell et al. <ref type="bibr" target="#b18">[19]</ref> proposed to use skip pooling to integrate multiple layers. It's called skip-layer connections. Despite its success, there is still some problems: 1) It ignores the proposals scores;</p><p>2) The proposal should pass through the whole CNN before classification; 3) The skip-layer operations in <ref type="bibr" target="#b18">[19]</ref> is relatively complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-layer Channel Features (MCF)</head><p>The layers in CNN represent the different and diverse image characteristic. The image channels in the first few layers can better describe the image local variance, while the image channels in the last few layers can abstract the image global structure. Meanwhile, the handcrafted image channels (e.g., HOG+LUV) can be also able to describe the image variations very well. HOG channels can describe the image local edge directions and variances. LUV channels capture the image color information. Compared to the layers in CNN, the handcrafted image channels are very simple and efficient. In this paper, we integrate HOG+LUV and the layers of CNN to construct Multi-layer Channel Features (MCF). First of all, we give an overview about our proposed Multilayer Channel Features (i.e., MCF). <ref type="figure" target="#fig_0">Fig. 1</ref> show the basic architecture of MCF. It can be divided into three parts: 1) Firstly, multi-layer image channels from L1 to LN are generated. The traditional handcrafted image channels (i.e., HOG+LUV) are used for the first layer (i.e., L1). The convolutional layers from C1 to C(N-1) in CNN construct the remaining layers from L2 to LN. In each layer, there are multiple image channels.</p><p>2) The second step is feature extraction. Zero-order, oneorder, and high-order features can be calculated in the image channels of each layer. 3) Finally, the multi-stage cascade AdaBoost is learned from the candidate features of each layer one after another. The weak classifiers in each stage of multi-stage cascade are learned from the candidate features of corresponding layer. For example, the weak classifiers in Stage 2 (i.e., S2) are learned from candidate features F2 of Layer 2 (i.e., L2). <ref type="figure" target="#fig_1">Fig. 2</ref> shows the test process of MCF. Give the test image, the image channels in L1 (i.e., HOG+LUV) are firstly computed. Detection windows are generated by scanning the test image. These detection windows are classified by S1 using the weak classifiers learned from L1. Some detection windows will be rejected by S1. For the detection windows accepted by S1, the image channels in L2 are computed. Then the accepted detection windows are classified by S2 using the weak classifiers learned from L2. The above process is repeated from L1 to LN. Finally, the detection windows accepted by all the stages (i.e., S1 to SN) will be merged by NMS. The merged detection windows are the final pedestrian windows.</p><p>Multi-layer Image Channels Row 1 in <ref type="figure" target="#fig_0">Fig. 1</ref> shows the multi-layer image Channels. It consists of N layers. In each layer, there are multiple image channels. <ref type="table" target="#tab_3">Table I</ref>     For example, a five-layer image channels can be generated by HOG+LUV and C2-C5 of VGG16. C1 of VGG16 is not used. The corresponding MCF are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Feature Extraction Features can be divided into three classes: zero-order feature, one-order feature, and high-order feature. In zero-order feature extraction, a single pixel itself is used as a feature and no neighboring pixels are used. Oneorder feature is defined as the pixel sums or averages in the local or non-local regions in each channel. High-order feature defined by the difference of the sums or averages of two or more different regions. For L1 (i.e., HOG+LUV), there are many successful methods for feature extraction, including ICF <ref type="bibr" target="#b9">[10]</ref>, ACF <ref type="bibr" target="#b10">[11]</ref>, SquaresChnFtrs <ref type="bibr" target="#b8">[9]</ref>, Informed- Haar <ref type="bibr" target="#b20">[21]</ref>, LDCF <ref type="bibr" target="#b21">[22]</ref>, FCF <ref type="bibr" target="#b6">[7]</ref>, and NNNF <ref type="bibr" target="#b30">[31]</ref>. ICF, ACF, and SquaresChnFtrs can be seen as one-order features. In-formedHaar, LDCF, FCF, and NNNF are high-order features. Compared to the other features, ACF has the fastest detection speed. NNNF has the best trade-off between detection speed and detection performance. Due to the simplicity and effectiveness, ACF and NNNF are used for feature exaction in L1. The number of image channels from CNN is relatively large. For example, the fourth convolutional layer (i.e., C4) in VGG16 has 512 image channels (see <ref type="table" target="#tab_3">Table I</ref>). To reduce the computation cost and avoid a very large number of candidate features, only zero-order feature is used. It means that each pixel value in image channels of each layer is used for the candidate feature. The specific feature extraction in multi-layer image channels can be seen in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>Multi-stage Cascade AdaBoost Cascade AdaBoost is a popular method for object detection. Based on multi-layer image channels, we propose the multi-stage cascade AdaBoost for pedestrian detection. <ref type="figure">Fig. 5</ref> gives the specific explanations about multi-stage cascade. The features in Si are learned from the candidate features Fi of Li, where i=1, 2, ..., N. Firstly, k 1 weak classifiers in S1 are learned from the candidate features F1 extracted from L1. Based on the hard negative samples and positive samples, k 2 weak classifiers in S2 are then learned from F2. The remaining stages are trained in the same manner. Finally, multi-stage (i.e., N-stage) cascade AdaBoost classifier can be obtained. This strong classifier H(x) can be expressed as the following equation:</p><formula xml:id="formula_0">H(x) = k1 j=1 h j 1 (x) + ... + ki j=1 h j i (x) + ... + k N j=1 h j N (x) = N i=1 ki j=1 h j i (x),<label>(1)</label></formula><p>where x represents the samples (windows), h j i (x) represents the j-th weak classifier in Stage i. k 1 , k 2 , ..., k N are the number of weak classifiers in each stage, respectively. How to set the value of k 1 , k 2 , ..., k N is an open problem. In this paper, one of simple structure is used as the follows: where N All represents the number of the total weak classifiers. Based on soft-cascade <ref type="bibr" target="#b19">[20]</ref>, the reject thresholds are set after each weak classifier.</p><formula xml:id="formula_1">k 1 = N All /2, k 2 = k 3 = ... = k N = N All /(2 × (N − 1)),<label>(2)</label></formula><p>The advantages about the multi-stage cascade AdaBoost structure can be concluded as the following: 1) Firstly, it avoids learning classifier from a very large feature pooling (e.g., more than one million); 2) Secondly, it makes full use of the information from multi-layer image channels. Thus, it can enrich the feature abstraction. 3) Finally, many non-pedestrian detection windows can be quickly rejected by the features in the first few layers. Thus, it avoids the computation cost of the remaining layers in CNN and accelerates the detection speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Elimination of Highly Overlapped Windows</head><p>Pedestrian detection is a multiple instance problem. Generally, the adjacent area around the pedestrian exists many positive detection windows. Many of these positive detection windows around pedestrians highly overlap. Though multi-stage cascade AdaBoost structure can reject many non-pedestrian detection windows, it cannot reject the positive detection windows around pedestrians. When the cascade classifier based on very deep CNN (e.g., VGG16), the computation cost of these positive detection windows are large.</p><p>In fact, there is no need to put all the highly overlapped windows accepted by first stage into the remaining stages. Detection windows accepted by the first stage each have a classification score. The highly overlapped windows with lower scores can be eliminated after the first stage. To eliminate these highly overlapped windows with lower scores, Non-Maximum Suppression (i.e., NMS) is used after the first stage. The overlap ratio O(w 1 , w 2 ) of detection windows can be defined in the following:</p><formula xml:id="formula_2">O(w 1 , w 2 ) = area(w 1 ∩ w 2 ) area(w 1 ∪ w 2 ) ,<label>(3)</label></formula><p>where w 1 and w 2 are two detection windows. If O(w 1 , w 2 ) &gt; θ, it means that w 1 and w 2 highly overlap. Then the detection window with lower score will be eliminated. Instead of the standard threshold θ = 0.5, a larger threshold is used here. Experimental results show that θ = 0.8 can accelerate the detection speed with little performance loss. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the specific test process of MCF by eliminating highly overlapped detection windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The challenging Caltech pedestrian detection dataset <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b33">[34]</ref> is employed for the evaluation. It consists of 11 videos. The first 6 videos are used for training and the rest videos are used for testing. The raw training images are formed To enlarge the training samples, Caltech10x is used. It samples one image per 3 frames in the training videos. As a result, there are 42,782 images in which there are 16,376 positive samples. Please note that the testing data is same as <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b33">[34]</ref> whenever the Caltech or Caltech10x is used. It contains 4024 images in which there are 1014 pedestrians.</p><p>The first layer in MCF is HOG+LUV image channels <ref type="bibr" target="#b9">[10]</ref>, which contains one normalized gradient magnitude channel, six histograms of oriented gradient channels, and three color channels. Two popular CNN models (i.e., AlexNet <ref type="bibr" target="#b25">[26]</ref> and VGG16 <ref type="bibr" target="#b24">[25]</ref>) are used for constructing the remaining layers in MCF. Instead of using original input size 227 × 227 or 224 × 224, we use the size 128 × 64 for pedestrian detection. For AlexNet, stride 4 in the first convolutional layer is replaced by stride 2. The input size 6×6 in the first full connected layer is replaced by the size 8 × 4. For VGG16, the input size 7 × 7 of the first full connected layer is replaced by the size 4 × 2. The other initial parameters follow the pre-trained models on ImageNet. The final parameters in AlexNet and VGG16 are fine-tuned on the Caltech10x.</p><p>Feature extraction in L1 (i.e., HOG+LUV) is ACF <ref type="bibr" target="#b10">[11]</ref> (Section IV.A) or NNNF <ref type="bibr" target="#b30">[31]</ref> (Section IV.B). Feature extraction in the remaining layers (i.e., the layers of CNN) is zero-order feature (single pixel). The final classifier consists of 4096 level-2 or level-4 decision trees. The decision tree number of each stage are k 1 = 2048, k 2 = k 3 = ... = k N = 2048/(N − 1), respectively. N is the number of the layers in MCF. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Self-Comparison of MCF</head><p>In this section, some intermediate experimental results on original Caltech training set are reported to show how to setup the effective and efficient MCF. Some specific experimental setup is as follows. HOG+LUV are used for the first layer. The convolutional layers in CNN (i.e., AlexNet or VGG16) correspond to the remaining layers. The feature extraction in HOG+LUV is ACF. Feature extraction in the layers of CNN is just zero-order feature (single pixel). To speed up the training, negative samples are generated by five round training of original ACF <ref type="bibr" target="#b10">[11]</ref>, where the number of trees in each round are 32, 128, 512, 1024, and 2048, respectively. Finally, multi-stage cascade which consists of 4096 level-2 decision trees is learned based on these negative samples and positive samples. The first stage contains the first 2048 decision trees. The remaining stages equally split the remaining 2048 decision trees. For example, HOG+LUV and C2 to C5 of CNN construct a five-layer image channels. Then, the corresponding five-stage cascade can be learned. The first stage S1 has 2048 weak classifiers. The remaining stages (i.e., S2-S5) each have 512 weak classifiers. Miss Rates are log-averaged over the range of FPPI = [10 −2 ,10 0 ], where FPPI represents False Positive Per Image. <ref type="table" target="#tab_3">Table II</ref> shows Miss Rates (MR) of MCF based on HOG+LUV and the different layers in CNN. The results based on AlexNet and VGG16 are both shown here.</p><p>√ means that the corresponding layer is used for MCF. HOG+LUV image channels are always used for the first layer. The layers (i.e., C1, C2, ... or C5) are used for the remaining layers. MCF-N means that there are N layers in MCF. For example, MCF-3 in Row 3 are generated by HOG+LUV, C4 and C5 of AlexNet. The first layer is HOG+LUV image channels. The second layer is the fourth convolutional layer (i.e., C4) of AlexNet. The last layer is the fifth layer (i.e., C5) of AlexNet. Based on multi-layer image channels, the corresponding multi-stage cascade is learned. There are the following observations from Table II: 1) Compared to MCF-2, MCF-N (N&gt;2) usually achieves the better performance. For example, the miss rate of MCF-6 based on VGG16 in the last row is lower than that of MCF-2 by 4.21%; 2) Generally, with increase of the layer number, the miss rate of MCF becomes lower and the detection performance becomes better. The above observations demonstrate that the middle layers in CNN can enrich the feature abstraction. It means that each layer in CNN contains some discriminative features, which can be used for classification. <ref type="table" target="#tab_3">Table III</ref> shows the average number and the ratio of detection windows rejected by each stage in MCF-6. MCF-6 is based on HOG+LUV and all the five convolutional layers in CNN. Thus, the multi-stage cascade AdaBoost in MCF has six stages from S1 to S6. '*' means that the average number of detection windows accepted by stage 1, instead of that rejected by stage 1, is shown. As the weak classifiers in S1 are both learned from HOG+LUV, the number of detection windows accepted by S1 are same (i.e., 159). Among the 159 accepted detection windows, about 71% and 76.1% detection windows are rejected by the cascade based on AlexNet and that based on VGG16, respectively. Overall, the multi-stage cascade based on VGG16 can reject more detection windows. Specifically, the first two stage stages (i.e., S2 and S3) based on AlexNet reject more detection windows than that based on VGG16. The middle two stages (i.e., S4 and S5) based on VGG16 can reject more detection windows than that based on AlexNet.</p><p>As multi-stage cascade can reject many detection windows by the first few stages, MCF can accelerate the detection speed. <ref type="table" target="#tab_3">Table IV</ref> compares the detection time and detection performance between MCF-2 and MCF-6. MCF-2 uses HOG+LUV and C5 in CNN to construct two-layer image channels. Then two-stage cascade is learned. MCF-6 uses HOG+LUV and all the five convolutional layers from C1 to C5 in CNN to .69s and 5.37s, respectively. Thus, the miss rate of MCF-6 is lower than that of MCF-2 by 4.21%, while the speed of MCF-6 is 1.43 times faster than that of MCF-2. The reasons can be explained as the following: 1) As MCF-6 uses all the layers in CNN to learn the classifier, it can learn more abundant features. Thus, it has a better performance. 2) MCF-2 needs to calculate all the layers of CNN (i.e., C1 to C5) before classifying the detection windows accepted by S1. MCF-6 just needs to calculate the i-th layer of CNN before classifying the detection windows by Si (i=2,3,...,6). In the <ref type="table" target="#tab_3">Table III</ref>, MCF-6 reject 66.7% detection windows before S6. Thus, MCF-6 has faster detection speed than MCF-2.</p><p>Though the speed of MCF-6 is faster than that of MCF-2, it's still very slow. To further accelerate the detection speed, the highly overlapped detection windows with lower scores accepted by the first stage (i.e., S1) are eliminated by NMS. As stated in section III.B, the threshold θ is an important factor to balance detection speed and detection performance. <ref type="table" target="#tab_7">Table  V</ref> shows that miss rates and detection time vary with θ. MCF-2 and MCF-6 based on HOG+LUV and AlexNet in <ref type="table" target="#tab_3">Table IV</ref> are used for the baseline (i.e., θ =INF). When θ = 0.5, the detection speed is very fast, but the detection performance drops rapidly. For MCF-6, the detection speed of MCF-6 with θ = 0.5 is 6.76 times faster than original MCF, while the miss rate of MCF with θ = 0.5 is higher than original MCF by 3.36%. Thus, it's not a good choice. When θ = 0.9, the detection performance is almost no loss, while the detection speed is not significantly improved. Thus, the trade-off choice is θ = 0.8. With little performance loss (e.g., 0.77%) in MCF-6, it's 2.67 times faster than original MCF. In the following section, MCF with θ = 0.8 are called MCF-f. <ref type="table" target="#tab_3">Table VI</ref> summarizes MCF-2, MCF-6 and MCF-6-f. MCF-6-f is the fast version of MCF-6, where the highly overlapped detection windows are eliminated after the first stage. There are the following observations: 1) MCF-6 and MCF-6-f both have the lower miss rates. Specifically, MCF-6 and MCF-6f based on AlexNet have lower miss rates than MCF-2 by 2.79% and 2.02%, respectively. MCF-6 and MCF-6-f based on VGG16 have lower miss rates than MCF-2 by 4.21% and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with the state-of-the-Art</head><p>In this section, MCF is based on HOG+LUV and all the five convolutional layers (i.e., C1-C5) in VGG16. Thus, MCF contains six-layer image channels. The features extracted in the first layer (i.e., HOG+LUV) are NNNF <ref type="bibr" target="#b30">[31]</ref>, which is one of the state-of-the-art features. The features extracted in the remaining five layers (i.e., C1-C5) are zero-order feature (single pixel). Caltech10x is used for training the final classifier. To speedup the training process, negative samples are accumulated by five rounds of original NNNF, where the number of the trees in each round is 32, 128, 512, 2048, and 4096, respectively. The resulting classifier contains 4096 level-4 decision trees. S1 contains 2048 decision trees. S2-S6 each have 409 decision trees. In <ref type="bibr" target="#b36">[37]</ref>  <ref type="figure">Fig. 7</ref> compares MCF with some state-of-the-art methods on the original annotations of the test set. ACF <ref type="bibr" target="#b10">[11]</ref>, LDCF <ref type="bibr" target="#b21">[22]</ref>, Checkboards <ref type="bibr" target="#b6">[7]</ref>, NNNF <ref type="bibr" target="#b30">[31]</ref>, DeepParts <ref type="bibr" target="#b1">[2]</ref>, and CompACT-Deep <ref type="bibr" target="#b0">[1]</ref> are used. ACF <ref type="bibr" target="#b9">[10]</ref> are trained on INRIA dataset <ref type="bibr" target="#b11">[12]</ref>. The other methods are trained based on Caltech10x dataset. MCF achieves the state-of-the-art performance, which outperforms CompACT-Deep <ref type="bibr" target="#b0">[1]</ref>, DeepParts <ref type="bibr" target="#b1">[2]</ref>, NNNF <ref type="bibr" target="#b30">[31]</ref>, and Checkboards [7] by 1.35%, 1.49%, 6.38%, and 8.07%, respectively.</p><p>Miss rates and Frames Per Second (FPS) of some methods based on CNN are visualized in <ref type="figure" target="#fig_6">Fig. 8</ref>. Detection time of the methods are all tested on the common CPU (i.e., Intel Core i7-3700). The best choice is that the miss rate is as small as possible while FPS is as large as possible. Though ACF <ref type="bibr" target="#b10">[11]</ref> has very fast detection speed (9.49 fps), miss rate of ACF are very large. Fast RCNN reported in <ref type="bibr" target="#b3">[4]</ref> has the better      performance (11.82%), but the speed is very slow. MCF-f is the fast version of MCF with little performance loss (0.65%). Compared to Fast RCNN <ref type="bibr" target="#b3">[4]</ref>, MCF is 4.5 times faster than it, while MCF has also lower miss rate than it by 0.77%. Therefore, MCF has a better trade-off between detection time and detection performance. Based on the new and accurate annotations of the Caltech test set, <ref type="figure" target="#fig_8">Fig. 9</ref> compares MCF with some state-of-the-art meth-ods: CompACT-Deep <ref type="bibr" target="#b0">[1]</ref>, DeepParts <ref type="bibr" target="#b1">[2]</ref>, Checkboards <ref type="bibr" target="#b6">[7]</ref>, and NNNF <ref type="bibr" target="#b30">[31]</ref>. Miss rates log-averaged over the FPPI range of [10 −2 ,10 0 ] and the FPPI range of [10 −4 ,10 0 ] are shown. They are represented by M R −2 and M R −4 . M R −2 (M R −4 ) are shown in the legend. MCF is trained based on the Caltech10x with the new annotations. M R −2 and M R −4 of MCF achieve 7.98% and 15.45%, respectively. They are superior to all the other methods. Specifically, M R −2 of MCF is 1.17%, 4.92%, and 6.41% lower than that of CompACT-Deep <ref type="bibr" target="#b0">[1]</ref>, DeepParts <ref type="bibr" target="#b1">[2]</ref>, and NNNF <ref type="bibr" target="#b30">[31]</ref>. Compared to M R −2 of MCF, M R − 4 of MCF has the better performance. Specifically, M R −4 of MCF is 3.39%, 9.70%, and 10.05% lower than that of CompACT-Deep <ref type="bibr" target="#b0">[1]</ref>, DeepParts <ref type="bibr" target="#b1">[2]</ref>, and NNNF <ref type="bibr" target="#b30">[31]</ref>. It means that MCF stably outperforms the other state-of-the-art methods.</p><p>V. CONCLUSION In this paper, we have proposed a unifying framework, which is called Multi-layer Channels Features (MCF). Firstly, the handcrafted image channels and the layers in CNN construct the multi-layer image channels. Then a multi-stage cascade are learned from the features extracted in the layers, respectively. The weak classifiers in each stage are learned from the corresponding layer. On the one hand, due to the much more abundant candidate features, MCF achieves the state-of-the-art performance on Caltech pedestrian dataset (i.e., 10.40% miss rate). Using the new and accurate annotations of the Caltech pedestrian dataset, miss rate of MCF is 7.98%, which is superior to other methods. On the other hand, due to the cascade structure, MCF rejects many detection windows by the first few stages and then accelerates the detection speed. To further speedup the detection, the highly overlapped detection windows are eliminated after the first stage. Finally, MCF with VGG16 can run on the CPU by 0.54 fps.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The basic architecture of MCF. It can be divided into three steps: multi-layer channel generation, feature extraction, and multi-stage cascade AdaBoost classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Test process of basic MCF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>MCF are generated by HOG+LUV and C2-C5 of VGG16. C1 of VGG16 are not used for constructing MCF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Feature Extraction in Multi-layer image channels. (a) feature extraction in L1 (HOG+LUV), where one-order (ACF) and high-order features (NNNF) are used. (b) feature extraction in L2-LN (the layers of CNN), where zeroorder features are extracted. Zero-order feature means that a single pixel value in each channel is used as a feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Test process of the fast version of MCF where the technique of NMS is used to eliminating highly overlapped detection windows with lower scores. by sampling one image per 30 frames. It results in 4250 images for training, where there are 1631 positive samples. The corresponding training data is called Caltech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Miss rates and FPS on Caltech pedestrian dataset are shown. Detection time of the methods are all tested on the common CPU (i.e., Intel Core i7-3700).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>ROC of Caltech test set using the new and accurate annotations<ref type="bibr" target="#b36">[37]</ref>. Miss rates log-averaged over the FPPI range of[10 −2 ,10 0 ] and the FPPI range of [10 −4 ,10 0 ] are shown. They are represented by M R −2 and M R −4 , respectively. M R −2 (M R −4 ) are shown in the legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and J. Cao are with the School of Electronic Information Engineering, Tianjin University, Tianjin 300072, China. e-mails: {pyw,connor}@tju.edu.cn X. Li is with the Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an 710119, Shaanxi, P. R. China. e-mail: xuelong li@opt.ac.cn.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>feature abstractions and quickly reject many detection windows by the first few stages.3) The highly overlapped detection windows with lower scores are eliminated after the first stage. Thus, it can reduce the large computation cost of CNN operations. With very little performance loss, it's 4.07 times faster.</figDesc><table><row><cell>Finally, it's possible that MCF with very deep CNN</cell></row><row><cell>(e.g., VGG16 [25]) can run at 0.54 fps on the common</cell></row><row><cell>CPU, while it achieves 11.05% miss rate on original</cell></row><row><cell>Caltech pedestrian set.</cell></row><row><cell>4) MCF achieves the state-of-the-art performance on Cal-</cell></row><row><cell>tech pedestrian dataset (the log-average miss rate is</cell></row><row><cell>10.40%), which outperforms CompACT-Deep [1] by</cell></row><row><cell>1.35%. Using new and more accurate annotations</cell></row></table><note>2) Multi-stage cascade AdaBoost is learned from multi- layer image channels. It can achieve better performance with more abundant</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>shows the specific parameters of multi-layer image channels based on HOG+LUV and VGG16. It contains six layers from L1 to L6. L1 is the handcrafted image channels (i.e., HOG+LUV). L2-L6 are five convolutional layers (i.e., C1-C5) in VGG16.Row  3 shows the image size in each layer. The image size in L1 is 128 × 64. The sizes of L2-L6 are 64 × 32, 32 × 16, 16 × 8, 8 × 4, and 4 × 2, respectively. Row 4 shows the number of the channels in each layer. L1 contains 10 image channels. L2-L6 each have 64, 128, 256, 512, and 512 image channels, respectively. InTable I, all the convolutional layers in CNN (i.e., C1 to C5) are used for constructing the multilayer image channels. In fact, only part convolutional layers in CNN can also construct the multi-layer image channels.</figDesc><table><row><cell>Test Image</cell><cell></cell><cell></cell></row><row><cell>Compute image channels in L1</cell><cell></cell><cell></cell></row><row><cell>Generate detection windows</cell><cell></cell><cell></cell></row><row><cell>Classify detection windows by S1 Compute image channels in LN Compute image channels in L2 Classify detection windows by S2 ... Accept Accept</cell><cell>Reject Reject ...</cell><cell>Non-Pedestrian windows</cell></row><row><cell>Classify detection windows by SN</cell><cell>Reject</cell><cell></cell></row><row><cell>Accept</cell><cell></cell><cell></cell></row><row><cell>NMS</cell><cell></cell><cell></cell></row><row><cell>Pedestrian windows</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I MULTI</head><label>I</label><figDesc>-LAYER IMAGE CHANNELS. THE FIRST LAYER IS HOG+LUV, THE REMAINING LAYERS ARE THE CONVOLUTIONAL LAYERS (I.E., C1 TO C5) IN VGG16.</figDesc><table><row><cell>Layer</cell><cell>L1</cell><cell>L2</cell><cell>L3</cell><cell>L4</cell><cell>L5</cell><cell>L6</cell></row><row><cell>Name</cell><cell>HOG LUV</cell><cell>C1</cell><cell>C2</cell><cell>VGG16 C3</cell><cell>C4</cell><cell>C5</cell></row><row><cell>Size</cell><cell cols="4">128 × 64 64 × 32 32 × 16 16 × 8</cell><cell cols="2">8 × 4 4 × 2</cell></row><row><cell>Num</cell><cell>10</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>512</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Fig. 5. Multi-stage cascade AdaBoost. Each stage learns the classifiers from the candidate features of corresponding layer.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">F1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Feature Extraction</cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">…</cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multi-stage Cascade</cell><cell></cell><cell cols="3">... S1 + +</cell><cell></cell><cell>+</cell><cell></cell><cell cols="2">... + + S2</cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell cols="2">... + + S3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell>…</cell><cell>+</cell><cell></cell><cell cols="3">... + + SN</cell><cell></cell><cell></cell></row><row><cell>( ) H x</cell><cell>1 1 h</cell><cell>+</cell><cell>...</cell><cell>+</cell><cell>1 k 1 h</cell><cell>+</cell><cell>1 2 h</cell><cell>+</cell><cell>...</cell><cell>+</cell><cell>2 k h</cell><cell>2</cell><cell>+</cell><cell>h</cell><cell>1 3</cell><cell>+</cell><cell>...</cell><cell>+</cell><cell>h</cell><cell>3 k</cell><cell>3</cell><cell>+</cell><cell>...</cell><cell>+</cell><cell>1 N h</cell><cell>+</cell><cell>...</cell><cell>+</cell><cell>1 k h</cell><cell>N</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II MISS</head><label>II</label><figDesc>RATES (MR) OF MCF BASED ON HOG+LUV AND THE DIFFERENT LAYERS IN CNN. √ MEANS THAT THE CORRESPONDING LAYER IS USED. HOG+LUV IS ALWAYS USED FOR THE FIRST LAYER. THE LAYERS IN ALEXNET OR VGG16 ARE USED FOR THE REMAINING LAYERS.</figDesc><table><row><cell>Name</cell><cell>HOG</cell><cell></cell><cell></cell><cell>AlexNet</cell><cell></cell><cell></cell><cell cols="2">MR (%) ∆ MR (%)</cell></row><row><cell></cell><cell>LUV</cell><cell cols="5">C1 C2 C3 C4 C5</cell><cell></cell><cell></cell></row><row><cell>MCF-2 MCF-3 MCF-4 MCF-5 MCF-6</cell><cell>√ √ √ √ √</cell><cell>√</cell><cell>√ √</cell><cell>√ √ √</cell><cell>√ √ √ √</cell><cell>√ √ √ √ √</cell><cell>20.08 18.43 17.40 18.01 17.29</cell><cell>N/A 1.65 2.68 2.07 2.79</cell></row><row><cell>Name</cell><cell>HOG</cell><cell></cell><cell></cell><cell>VGG16</cell><cell></cell><cell></cell><cell cols="2">MR (%) ∆ MR (%)</cell></row><row><cell></cell><cell>LUV</cell><cell cols="5">C1 C2 C3 C4 C5</cell><cell></cell><cell></cell></row><row><cell>MCF-2 MCF-3 MCF-4 MCF-5 MCF-6</cell><cell>√ √ √ √ √</cell><cell>√</cell><cell>√ √</cell><cell>√ √ √</cell><cell>√ √ √ √</cell><cell>√ √ √ √ √</cell><cell>18.52 17.14 15.40 14.78 14.31</cell><cell>N/A 1.38 3.12 3.74 4.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III REJECTED</head><label>III</label><figDesc>NUMBER AND REJECTED RATIO BY THE STAGES IN MCF-6 ARE SHOWN. '*' MEANS THAT THE AVERAGE NUMBER OF DETECTION WINDOWS ACCEPTED BY STAGE 1 ARE SHOWN.</figDesc><table><row><cell>Stage</cell><cell cols="2">HOG+LUV and AlexNet Number Ratio</cell><cell cols="2">HOG+LUV and VGG16 Number Ratio</cell></row><row><cell>S1</cell><cell>159*</cell><cell>N/A</cell><cell>159*</cell><cell>N/A</cell></row><row><cell>S2</cell><cell>35</cell><cell>22.0%</cell><cell>23</cell><cell>14.5%</cell></row><row><cell>S3</cell><cell>35</cell><cell>22.0%</cell><cell>21</cell><cell>13.2%</cell></row><row><cell>S4</cell><cell>21</cell><cell>13.2%</cell><cell>33</cell><cell>20.8%</cell></row><row><cell>S5</cell><cell>14</cell><cell>8.8%</cell><cell>29</cell><cell>18.2%</cell></row><row><cell>S6</cell><cell>8</cell><cell>5.0%</cell><cell>15</cell><cell>9.4%</cell></row><row><cell>Total</cell><cell>113</cell><cell>71.0%</cell><cell>121</cell><cell>76.1%</cell></row><row><cell></cell><cell></cell><cell>TABLE IV</cell><cell></cell><cell></cell></row><row><cell cols="5">MISS RATES (MR) AND DETECTION TIME OF MCF-2 AND MCF-6.</cell></row><row><cell cols="5">MCF-2 IS BASED ON HOG+LUV AND C5 OF CNN. MCF-6 IS BASED ON</cell></row><row><cell></cell><cell cols="3">HOG+LUV AND C1-C5 OF CNN.</cell><cell></cell></row><row><cell></cell><cell cols="2">HOG+LUV and AlexNet</cell><cell cols="2">HOG+LUV and VGG16</cell></row><row><cell></cell><cell>MCF-2</cell><cell>MCF-6</cell><cell>MCF-2</cell><cell>MCF-6</cell></row><row><cell>MR (%)</cell><cell>20.08</cell><cell>17.29</cell><cell>18.52</cell><cell>14.31</cell></row><row><cell>Time (s)</cell><cell>2.99</cell><cell>2.30</cell><cell>7.69</cell><cell>5.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V MISS</head><label>V</label><figDesc>RATES AND DETECTION TIME VARY WITH θ. MCF USED HERE IS BASED ON HOG+LUV AND ALEXNET.</figDesc><table><row><cell>θ</cell><cell>MR (%)</cell><cell>MCF-2</cell><cell>Time (s)</cell><cell>MR (%)</cell><cell>MCF-6</cell><cell>Time (s)</cell></row><row><cell>INF</cell><cell>20.08</cell><cell></cell><cell>2.99</cell><cell>17.29</cell><cell></cell><cell>2.30</cell></row><row><cell>0.50</cell><cell>23.70</cell><cell></cell><cell>0.44</cell><cell>21.65</cell><cell></cell><cell>0.34</cell></row><row><cell>0.80</cell><cell>20.97</cell><cell></cell><cell>1.15</cell><cell>18.06</cell><cell></cell><cell>0.86</cell></row><row><cell>0.85</cell><cell>20.30</cell><cell></cell><cell>1.76</cell><cell>17.34</cell><cell></cell><cell>1.35</cell></row><row><cell>0.90</cell><cell>19.82</cell><cell></cell><cell>2.03</cell><cell>17.32</cell><cell></cell><cell>1.57</cell></row><row><cell cols="7">construct six-layer image channels. Then six-stage cascade is</cell></row><row><cell cols="7">learned. The detection time shown in Table IV is based on the</cell></row><row><cell cols="7">common CPU (i.e., Intel Core i7-3700). No matter CNN model</cell></row><row><cell cols="7">is AlexNet or VGG16, MCF-6 have the better performance and</cell></row><row><cell cols="7">the faster detection speed. For example, based on VGG16, the</cell></row><row><cell cols="7">miss rates of MCF-2 and MCF-6 are 18.52% and 14.31%,</cell></row><row><cell cols="7">respectively. The detection times of MCF-2 and MCF-6 are</cell></row><row><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI MISS</head><label>VI</label><figDesc>RATE (MR) AND DETECTION TIME OF MCF-2, MCF-6, AND MCF-6-F. MCF-2 IS BASED ON HOG+LUV AND C5 IN CNN. MCF-6 IS BASED ON HOG+LUV AND C1-C5 IN CNN. MCF-6-F IS THE FAST VERSION OF MCF-6.</figDesc><table><row><cell></cell><cell></cell><cell>HOG+LUV and AlexNet</cell><cell></cell></row><row><cell></cell><cell>MCF-2</cell><cell>MCF-6</cell><cell>MCF-6-f</cell></row><row><cell>MR (%)</cell><cell>20.08</cell><cell>17.29</cell><cell>18.06</cell></row><row><cell>Time (s)</cell><cell>2.99</cell><cell>2.30</cell><cell>0.86</cell></row><row><cell></cell><cell></cell><cell>HOG+LUV and VGG16</cell><cell></cell></row><row><cell></cell><cell>MCF-2</cell><cell>MCF-6</cell><cell>MCF-6-f</cell></row><row><cell>MR (%)</cell><cell>18.52</cell><cell>14.31</cell><cell>14.89</cell></row><row><cell>Time (s)</cell><cell>7.69</cell><cell>5.37</cell><cell>1.89</cell></row><row><cell cols="4">3.63%, respectively. 2) MCF-6 and MCF-6-f is faster than</cell></row><row><cell cols="4">MCF-2. For example, detection time of MCF-2 based VGG16</cell></row><row><cell cols="4">is 7.69s and that of MCF-6-f based on VGG16 is 1.89s. It</cell></row><row><cell cols="4">means that detection speed of MCF-6-f is 4.07 times faster</cell></row><row><cell cols="4">than that of MCF-2. 3) With little performance loss, MCF-6-f</cell></row><row><cell cols="4">have faster detection speed than MCF-6. The loss of MCF-6-f</cell></row><row><cell cols="4">based on AlexNet is 0.77%, and the loss of MCF-6-f based</cell></row><row><cell cols="2">on VGG16 is 0.58%.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>,</head><label></label><figDesc>Zhang et al. provided a new, high quality ground truth for the training and test sets. The new annotations of Caltech10x is also used for training MCF. Original Caltech test set and new Caltech test set are both used for the evaluations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Fig. 7. ROC of Caltech test set (reasonable).</figDesc><table><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>.64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>miss rate</cell><cell>.20</cell><cell></cell><cell cols="2">94.73% VJ</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>.10</cell><cell></cell><cell cols="3">68.46% HOG 63.26% LatSvm−V2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">44.22% ACF−Caltech</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">24.80% LDCF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>05</cell><cell></cell><cell cols="4">18.47% Checkerboards 16.78% NNNF</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">11.89% DeepParts</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">11.75% CompACT−Deep</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">10.40% MCF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">false positives per image</cell></row><row><cell></cell><cell>10 1.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[9.49fps/44.20%] ACF</cell></row><row><cell>log−average miss rate</cell><cell>10 1.3 10 1.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[14.10fps/53.90%] Crosstalk [0.16fps/21.90%] LatSvm−L2 [0.63fps/34.60%] InformedHaar [0.12fps/29.20%] SpatialPooling [3.62fps/24.80%] LDCF [0.12fps/21.90%] SpatialPooling+ [0.50fps/18.47%] Checkboards [1.14fps/16.78%] NNNF [0.12fps/11.82%] Fast RCNN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[0.54fps/11.05%] MCF−f</cell></row><row><cell></cell><cell>10 1.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1/16</cell><cell>1/8</cell><cell>1/4</cell><cell>1/2</cell><cell>1/1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">frames per second</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Complexity-Aware Cascades for Deep Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Learning Strong Parts for Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pedestrian Detection Aided by Deep Learning Semantic Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Scale-Aware Fast R-CNN for Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>CoRR abs/1601.04798</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pedestrian Detection with Unsupervised Multi-Stage Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Taking a Deeper Look at Pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Filtered Channel Features for Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ten Years of Pedestrian Detection, What Have We Learned?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Seeking the Strongest Rigid Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Integral Channel Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast Feature Pyramids for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gradients for Human Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object Detection with Discriminatively Trained Part Based Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust Real-Time Face Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Selective Search for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BING: Binarized Normed Gradients for Objectness Estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno>CoRR abs/1512.04143</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust Object Detection Via Soft Cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Informed Haar-like Features Improve Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local Decorrelation for Improved Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process</title>
		<meeting>Adv. Neural Inf. ess</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative Decorrelation for Clustering and Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Regularized LDA by Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2191" to="2201" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process</title>
		<meeting>Adv. Neural Inf. ess</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cosaliency Detection Based on Intrasaliency Prior Transfer and Deep Intersaliency Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Neural Network for Structural Prediction and Lane Detection in Traffic Scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process</title>
		<meeting>Adv. Neural Inf. ess</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional Channel Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pedestrian Detection Inspired by Appearance Constancy and Shape Symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR abs/1511.08058</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning Sampling Functions for Efficient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transcations on Cybernetics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pedestrian Detection: An Evaluation of the State of the Art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Convolutional Neural Network Cascade for Face Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiple-Instance Pruning For Learning Efficient Cascade Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process</title>
		<meeting>Adv. Neural Inf. ess</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">How Far are We from Solving Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>CoRR abs/1602.01237</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
