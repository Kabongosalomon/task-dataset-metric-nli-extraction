<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IKSHANA : A THEORY OF HUMAN SCENE UNDERSTANDING MECHANISM A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-27">January 27, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkata</forename><forename type="middle">Satya</forename><surname>Sai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Blekinge Insitute of Technology Karlskrona</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Daliparthi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Blekinge Insitute of Technology Karlskrona</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IKSHANA : A THEORY OF HUMAN SCENE UNDERSTANDING MECHANISM A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-27">January 27, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, deep neural networks achieved state-of-the-art performance on many computer vision tasks. The two most commonly observed drawbacks of these deep neural networks are: the requirement of a massive amount of labeled data and a vast number of parameters. In this work, we propose a theory named Ikshana, to explain the functioning of the human brain, while humans understand a natural scene/image. We have designed an architecture named IkshanaNet and evaluated on the Cityscapes pixel-level semantic segmentation benchmark, to show how to implement our theory in practice. The results showed that the Ikshana theory could perform with less training data. Also, through some experiments evaluated on the validation set, we showed that the Ikshana theory can significantly reduce the number of parameters of the network. In conclusion, a deep neural network designed by following the Ikshana theory will learn better vector representations of the image, useful for any computer vision task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The human brain can seamlessly perceive several perceptual and semantic information regarding the natural scene/image during a glance. The visual scene information perceived during/after a glance refers to the gist (a summary) of the scene/image. The gist includes all the visual information from the low-level (e.g., colors and contours) to the high-level (e.g., shapes and activation). Due to this reason, some studies <ref type="bibr" target="#b0">[1]</ref> suggested that the gist can be investigated at both the perceptual and conceptual level. The structural representation of the image refers to the perceptual gist, and the semantic information of the image refers to the conceptual gist. However, the conceptual gist is more refined and modified than the perceptual gist. Depending on the situation and the environment, the human brain can seamlessly grasp the required information regarding the scene/image, by recognizing the objects and observing their structure. On the other hand, for a computer to do the same is the fundamental goal of the computer vision field. In recent years, deep learning methods have shown a significant improvement over traditional handcrafted techniques on several computer vision tasks. Though these deep neural networks achieved state-of-the-art performance in many cases, the two major drawbacks of these networks are: the requirement of a massive amount of labeled data and a vast number of parameters. The collection of labeled data is an expensive and time taking process. Further, the increasing number of parameters will make it difficult for the deep neural networks to scale during deployment. Particularly in the fields such as autonomous driving, the computer inside the vehicle should provide efficient real-time results. Also, in the embedded systems, the size of the network plays a crucial role in the performance. Several few-shot learning methods and real-time methods addressed these drawbacks of the deep neural networks. Even though the deep neural networks are said to be inspired by the functioning of the human brain, is this how human brain learns to perform any visual task? NO. Because, the human brain does not require a massive amount of labeled data to perform any visual task, and it has the ability to perform with few data samples. However, we cannot observe a similar phenomenon in the case of many deep neural networks. In this work, we argue that there is a disconnection between the current deep learning methods and neuroscience. Several works <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b2">[3]</ref>[4] <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b5">[6]</ref> in neuroscience have arXiv:2101.10837v1 [cs.CV] 21 Jan 2021 addressed the fundamental question, i.e.,"how does the human brain perform the visual tasks?" by investigating through conceptual and perceptual gist. They conducted several experiments and proposed various theories to explain the functioning of the human brain. However, there was no general principle that explains the functioning of the human brain and its mechanisms. Even though there is a general principle, we expect that to be different from human-to-human. Motivated by the fact that a better learning algorithm will have the ability to perform with few data samples. In this work, we addressed the common drawbacks of the deep neural networks, i.e., the requirement of a vast number of parameters and labeled data, by proposing a theory of human scene understanding mechanism named Ikshana. The word Ikshana was derived from the Sanskrit language, which has many synonyms such as the eye, sight, look, etc. The idea is that "To understand the conceptual gist of a given image, humans look at the image multiple times recurrently, at different scales." We selected the high-level computer vision task semantic segmentation and the Cityscapes benchmark to show how to implement the Ikshana theory in practice. Semantic segmentation is the task of assigning a class label to every pixel in the given image, which has many applications in various fields such as medical, autonomous driving, robotic navigation, localization, and scene understanding. We designed a simple deep neural network architecture by following the Ikshana theory named IkshanaNet, which has only 4M parameters. Without any pre-training, we trained the IkshanaNet on the Cityscapes test set in three different ways: IkshanaNet-1 was trained on the entire training set (2975 images), IkshanaNet-2 was trained the half of the training set (1487 images), and IkshanaNet-3 was trained on the quarter of the training set (743 images). We evaluated these variants on the Cityscapes benchmark and observed that the variants trained on less training data shown considerable performance. We observed only a 4.21 IOU percentage points difference after losing half of the training data. During our initial architectural search of the IkshanaNet, the networks designed by following the Ikshana theory, have shown similar phenomena when trained on less training data. Also, through some experiments evaluated on the validation set, we showed that the Ikshana theory would help in reducing the number of parameters of the deep neural networks. In this work, we argue that using image classification networks (such as VGGNet <ref type="bibr" target="#b6">[7]</ref>, ResNet <ref type="bibr" target="#b7">[8]</ref>, DenseNet <ref type="bibr" target="#b8">[9]</ref>, and etc.) as a backbone to learn representations from the image is the bottleneck causing the disconnection with neuroscience. Because, the most common architectural pattern observed in many deep neural network architectures was to learn a representation with 32/64 filters (say f (x)) from the input image and go deeper on top of that representation (f (x)) until the network achieves adequate performance. According to our Ikshana theory, this commonly observed architectural pattern was like teaching the computers to learn by looking at the image once. Even though these deep neural network architectures are learning 32/64 filters from the input image, we argue that they are not sufficient for many high-level computer vision tasks, where the spatial location of the objects and their relation plays a crucial role in the output. Instead of that, the Ikshana theory suggests looking at the image multiple times recurrently by learning representations from the input image every time. The aim of this work is to find a better learning algorithm to perform with a few data samples and parameters. Here, we are neither addressing the semantic segmentation task or competing with state-of-the-art techniques. Hence, we are not comparing our method to any other works. The Ikshana theory is the main contribution of this work and not the IkshanaNet architecture. The architecture is an example to show how to implement the Ikshana theory in practice. Further investigation is needed to develop a new backbone architecture by following the Ikshana theory, which will contribute to several computer vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In Neurological terms, all the low-level and high-level computer vision tasks come under a single term called human scene understanding. In human scene understanding, a scene was a view of a real-world environment that contains multiple surfaces and objects, organized in a meaningful way. The gist (a summary) refers to the visual information perceived after/ during a glance at scene <ref type="bibr" target="#b0">[1]</ref>. Some early works <ref type="bibr" target="#b9">[10]</ref> [11] on conceptual gist explained that a typical scene fixation of 275 to 300ms was often sufficient to understand the gist of the image. However, many prominent works <ref type="bibr">[</ref>  <ref type="bibr" target="#b5">[6]</ref> in neurosciene investigated the perceptual gist. Several findings have shown that, how the modelling and shape of the scene was done in the human brain. However, in most of the cases, we need the neural networks to understand the semantic information of the given image. So, our theory focuses on the conceptual gist rather than perceptual gist. Some works <ref type="bibr" target="#b2">[3]</ref>[13] empirically showed that a spatial resolution of eight cycles/image and four cycles/image was sufficient to identify the objects in gray-scale and color images, respectively. Neural networks were there since 90's <ref type="bibr" target="#b13">[14]</ref>[15][16] <ref type="bibr" target="#b16">[17]</ref> but some prominent works <ref type="bibr" target="#b17">[18]</ref>[19] <ref type="bibr" target="#b7">[8]</ref> made them popular during recent years. In our work, we used the convolutional neural network (CNN) architecture <ref type="bibr" target="#b19">[20]</ref> to learn representations from the images, which itself was inspired by a work on human receptive fields <ref type="bibr" target="#b20">[21]</ref>. The architecture of the IkshanaNet was inspired by VGG <ref type="bibr" target="#b6">[7]</ref> and Densenet <ref type="bibr" target="#b8">[9]</ref>. The first prominent work on Semantic segmentation using deep learning was the fully convolutional networks (FCN) <ref type="bibr" target="#b21">[22]</ref>. Later on, many semantic segmentation networks followed the idea of the skip-connections introduced by FCN. The total prominent works on deep learning-based semantic segmentation methods can be roughly classified into five categories. They are encoder-decoder based methods (DeconvNet <ref type="bibr" target="#b22">[23]</ref>, SegNet <ref type="bibr" target="#b23">[24]</ref>, and U-Net[25]), regional proposal based methods (MaskRCNN <ref type="bibr" target="#b25">[26]</ref>, FPN <ref type="bibr" target="#b26">[27]</ref>, PANet <ref type="bibr" target="#b27">[28]</ref>, and PSPNet <ref type="bibr" target="#b28">[29]</ref>), dilated convolutions and increased resolution of feature map methods (DeepLab <ref type="bibr" target="#b29">[30]</ref>, DeepLab V3+ <ref type="bibr" target="#b30">[31]</ref>, and HRNet <ref type="bibr" target="#b31">[32]</ref>), context information methods (DANet <ref type="bibr" target="#b32">[33]</ref>, HANet <ref type="bibr" target="#b33">[34]</ref>, and OCR <ref type="bibr" target="#b34">[35]</ref>), and boundary refinement methods (Gated-SCNN <ref type="bibr" target="#b35">[36]</ref> and SegFix <ref type="bibr" target="#b36">[37]</ref>). Most of these above mentioned works, achieved state-of-the-art performance at a certain time in the past. However, these state-of-the-art models had complex architectures and requires a huge number of parameters. Due to this reason, most of these methods fail to perform in real-time. Especially, in the fields such as the autonomous driving, where the goal was to provide efficient results in real-time. Real-time semantic segmentation methods such as ENet <ref type="bibr" target="#b37">[38]</ref>, ICNet <ref type="bibr" target="#b28">[29]</ref>, ESPNet <ref type="bibr" target="#b38">[39]</ref>, HardNet <ref type="bibr" target="#b39">[40]</ref>, SwiftNet <ref type="bibr" target="#b40">[41]</ref> and BiseNet <ref type="bibr" target="#b41">[42]</ref> were introduced to deal with scalability drawbacks of the S-O-T-A models. Another common drawback of the deep neural networks was the requirement of a massive amount of labeled data. Few-shot segmentation methods such as PANet <ref type="bibr" target="#b42">[43]</ref>, Attention-based <ref type="bibr" target="#b43">[44]</ref>, Texture-bias <ref type="bibr" target="#b44">[45]</ref>, etc. were introduced to address the massive labelled data requirement. Almost in many computer vision tasks, we can observe this similar kind of phenomena, such as the state-of-the-art methods, the real-time methods, and the few-shot learning methods. To our knowledge, our work was inspired and related to the above-mentioned works. There might be a chance that some works may be directly/indirectly related to this work and were not mentioned here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ikshana (The Eye) Theory</head><p>In her prominent work <ref type="bibr" target="#b9">[10]</ref>, professor Mary C. Potter found that an average human can understand the gist of the image in between the time interval of 275 to 300ms. During that time interval, the Ikshana theory approximates the functioning of the human brain. The Ikshana theory states that "To understand the conceptual gist of a given image, humans look at the image multiple times recurrently, at different scales." The word Ikshana was derived from the Sanskrit language, which has many synonyms such as the eye, sight, look, etc. An example to explain the Ikshana theory is presented in <ref type="figure" target="#fig_1">figure 1</ref>, where there is an image (x) on the left side and the human brain's mechanism on the right side. According to Ikshana theory, for a human to understand the conceptual gist of the given image the following process is occuring in the human brain: At a time step (t), during the first glance (Φ), the human brain learns the first representation (f (x)) from the image and stores that representation in the memory (M ), as shown in the equation 1.</p><formula xml:id="formula_0">f (x) = Φ(x); M = f (x) (1)</formula><p>At a time step (t + 1), during the second glance (Φ), the human brain holds the first representation (f (x)) in the memory and learns the second representation (g(x)) from the image. Then the brain stores both the representation (g(x)) along with (f (x)) in the memory(M ), as show in the equation 2.</p><formula xml:id="formula_1">g(x) = Φ(x, f (x)); M = f (x), g(x)<label>(2)</label></formula><p>At a time step (t + 2), during the third glance (Φ), the human brain holds the first and the second representations (f (x) and g(x)) in the memory and learns the third representation (h(x)) from the image. Then the brain stores the representation (h(x)) along with (f (x) and g(x)) in the memory(M ), as show in the equation 3.</p><formula xml:id="formula_2">h(x) = Φ(x, f (x), g(x)); M = f (x), g(x), h(x)<label>(3)</label></formula><p>This kind of recurrent process occurs at (t + n) times at a single image scale. Depending upon the given task (T ), by combing all the information stored in the memory until (t + n)'th time step, human understands the conceptual gist (y 1 ) of the image at a single scale, as shown in the equation 4.</p><formula xml:id="formula_3">y 1 = T (f (x), g(x), h(x)...........n(x))<label>(4)</label></formula><p>This process occurs at different scales (1, 2, 3...N ) and generates the outputs (Y 1 , Y 2 ,Y 3 ,....Y n ). By considering all the outputs at different scales, the human brain selects some of those representations and forgets the remaining representations. In this way, the human brain learns(∆) the final output (Y ) of the given visual task (T ), as shown in the equation 5.  several factors such as the given task, age, intelligence, memory, etc. However, the number of glances required by a neural network to learn the semantic information from the given image, depends upon the given visual task and performance requirement in the application field.</p><formula xml:id="formula_4">Y = ∆(Y 1 , Y 2 , Y 3 , ....Y N )<label>(5)</label></formula><p>Note: According to the Ikshana theory, the most commonly used architectures for learning representations from the input image, such as VGGNet <ref type="bibr" target="#b6">[7]</ref>, ResNet <ref type="bibr" target="#b7">[8]</ref>, DenseNet <ref type="bibr" target="#b8">[9]</ref>, and etc., are like learning (f (x)) with 32/64 filters from the image and then going deep on top of that (f (x)) representation until the network achieves adequate performance. Instead of that, our theory suggests going deeper by learning 32/64 representations from the input image and preceding layers every-time, until the network achieves adequate performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture</head><p>To implement the Ikshana theory in practice, a novel architecture named IkshanaNet was introduced. Humans can look at the image and seamlessly learn various representations <ref type="bibr" target="#b9">[10]</ref> regarding the image. However, for a computer to do the same, the convolutional neural network architecture was used to learn representations from the image. Humans need the spatial resolutions around eight cycles/images to understanding the gist of the given image <ref type="bibr" target="#b2">[3]</ref> <ref type="bibr" target="#b12">[13]</ref>. However, for a computer to do the same, three scales are selected in the IkshaNet architecture. The network size was 16.1MB and had 4M parameters. The entire architecture of the IkshanaNet consists of three basic building blocks; they are (1) the glance module, (2) the projection module, and (3) a 1x1 convolutional layer, as shown in <ref type="figure">figure 2</ref>. The glance module consists of three 3x3 convolutional layers (with different dilation rates), and it was used to learn representations from the given feature map. Even though the number of input filters passed into the glance module varies several times, the output feature map always returns a 32 filtered feature map. The projection module consists of three 3x3 convolutional layers that take in and returns the same number of filters every time. Every convolutional layer in the glance and projection modules was followed by batch normalization and a ReLU activation layer. The 1x1 convolution layers were used to reduce the number of filters in a given future map. Except for the last 1x1 convolutional layer, that returns the output, every 1x1 convolutional layer in the architecture was followed by batch normalization and a ReLU activation layer. The entire architecture of IkshanaNet could be briefly divided into four parts; they are the scale 1, the scale 2, the scale 3, and the output. At scale 1, the input image was passed through a glance module with a dilation rate (d= 1), that returns a feature map with 32 filters. Then the input image was concatenated with the previously learned feature map (32 + 3 = 35). The concatenation of the input image in the feature map was the crucial step, to make sure that we are learning representations from the input image every time. Then, the feature map was passed through another glance module with a dilation rate (d= 2), that returns another feature map with 32 filters. Then the resulting feature map was concatenated with the feature-maps from the preceding layers (32 + 32 + 3 = 67) and passed through another glance module with dilation rate (d= 3), which takes in 67 filters and returns 32 filters. Again, the resulting feature-map was concatenated with feature-maps from the preceding layers (32 + 32 + 32 + 3 = 99). At this step, the input image was removed from the feature map through tensor slicing.This step was crucial to make sure that the input image was removed from the feature map. Then the resulting feature map will have (32 + 32 + 32 = 96) filters learned from three glances modules. In this way, the network followed the Ikshana theory and had three glances at the input image, multiple times recurrently. Then that feature map was passed through a projection module. Here, there was a side branch, a 1x1 convolutional layer that takes the 96 filtered feature map as input returns a feature map with 20 filters, named side one output (Y 1 ). Parallelly, the feature map was passed through an average pooling layer, which reduces the size of the feature map by a factor of two. The resulting feature map was then passed to the scale 1 part. At scale 2 part, the input image was down-sampled by a factor of two and concatenated with the feature map from the scale 1 (96 + 3 = 99). The feature map with 99 filters was then passed through three glance modules with different dilation rates <ref type="figure" target="#fig_1">(d=1,2,3)</ref>, which returns a feature map with 195 filters (99 + 32 + 32 <ref type="bibr">+ 32 = 195)</ref>. Then the image in the feature map was removed from the feature map and then passed through a projection module. Here, there was another side branch, a 1x1 convolutional layer, that takes the 192 filtered feature map as input returns a feature map with 20 filters, named side two output (Y 2 ). Parallelly, the feature map was passed through an average pooling layer, which reduces the size of the feature map by a factor of two. From here, the feature map was passed to the scale 3 part. At scale 3 part, the input image was down-sampled by a factor of four and concatenated with the feature map from the above layers <ref type="bibr">(192 + 3 = 195)</ref>. The feature map with 195 filters was then passed through three glance modules with different dilation rates (d=1,2,3), and result in a feature map with 288 filters (96 + 96 + 32 + 32 + 32 = 288). Then the image in the feature map was removed from the feature map and then passed through a projection module. Here, there was a final side branch, a 1x1 convolutional layer, that takes the 288 filtered feature map as input and returns a feature map with 20 filters, named side three output (Y 3 ). Finally, the outputs from two scales (y 2 and y 3 ) are up-sampled to match with the size of the input image and then concatenated <ref type="bibr">(20 + 20 + 20)</ref>. From, that a feature map with 20 filters was selected through a 1x1 convolutional layer,i.e, the output Y = ∆(Y 1 , Y 2 , Y 3 ) of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The Cityscapes <ref type="bibr" target="#b45">[46]</ref> pixel-level semantic labeling dataset was selected as a benchmark to evaluate the IkshanaNet. By using the preparation scripts, 35 classes in the dataset were converted into 20 classes. The cityscapes fine-labels dataset was the only dataset used for training, and no other dataset was used for pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-Processing</head><p>The images were resized to half of the original resolution, i.e., 512 x 1024. The mean values of (0.485, 0.456, 0.406) and standard deviation values of (0.229, 0.224, 0.225) were used to normalize the image data. These are the pre-processing steps, and no data augmentation steps were involved in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyper-Parameters</head><p>In this experiment, the following hyper-parameters were used while training the models:</p><p>• Criterion: Cross entropy loss • Learning rate: The learning rate scheduler, ReduceLROnPlateau (mode = min, factor = 0.5, patience = 20, verbose = 1) was used with the initial learning rate of 1e − 06. • Optimizer: SGD + Momentum (0.7) • Epochs: 300 • Batch size: 2</p><p>Note: Different momentum values such as 0.5, 0.6, 0.7, 0.8, and 0.9 were tuned and found that the IkshanaNet performs better with the momentum value of 0.7. This was the only hyper-parameter tuning step involved in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Specifications</head><p>•  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Independent and Dependent Variables</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Independent Variables</head><p>Independent Variables of this experiment were the three variants of the IkshanaNet, as follows:</p><p>1. The IkshanaNet-1 was trained with 2975 training set images, and 500 validation set images.</p><p>2. The IkshanaNet-2 was trained with 1487 training set images (half of the original training set), and 500 validation set images.</p><p>3. The IkshanaNet-3 was trained with 743 training set images (the quarter of the original training set), and 500 validation set images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Dependent Variables</head><p>Dependent Variables of this experiment were the class-wise IOU score and category-wise IOU scores. All three independent variables were evaluated on 1525 test set images of the Cityscapes pixel-level semantic segmentation benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Quantitative results</head><p>The qualitative results of all three independent variables were provided in table 1. Only average class IOU and category IOU results were present in table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The following interpretations and observations were made after conducting the experiments:</p><p>1. From table 1, it was observed that the difference between the IkshanaNet-2 (trained on half of the training data) and the IkshanaNet-1 (trained on full training data) was around 4.21 IOU percentage points. The IkshanaNet-3 (trained on the quarter part of the training data) achieved 4.7 IOU points lesser than the IkshanaNet-2 and 8.95 IOU points lesser than the IkshanaNet-1. Even after losing the training data, our method managed to achieve considerable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>We observed a similar phenomenon many times during our initial experiments evaluated on the validation set (We provided some of them in the supplementary material, in the folder named Ikshana Theory). The reason behind the occurrence of this phenomenon was the Ikshana theory of human scene understanding mechanism.</p><p>3. Humans can easily perceive and understand a variety of information by having a quick glance at the images. However, for computers, the images are nothing but a bunch of numbers arranged in a matrix. As of our knowledge, placing multiple CNN layers in the network will perform multiple matrix multiplications on the input image to transform it into the desired output. In this work, we argue that if the input image has lots of useful information regarding the scene, then why not use that information multiple times, rather than using it once. 4. As of our knowledge, every time the image was passed through a convolutional layer, we were losing some information regarding the image and learning some useful representations from it. Most of the current literature on deep neural networks discussed in such a way that, "the ability of CNNs to effectively down-sample the input image into a latent vector space was an advantage for low-level tasks such as image classification (where output was just a number representing the class of the image). On another side, it becomes a disadvantage for using CNNs for high-level tasks such as semantic segmentation (where output was of the same size transformed image)". However, we partially agree with the current literature. Instead, we argue that Professor Yann LeCun did an exceptional work by replicating the human receptive fields for the machines. The CNN architecture might be sufficient for dealing with many computer vision tasks. 5. In semantic segmentation, early works such as FCN addressed the loss of information problem by CNNs through introducing skip-connections from higher-layers to the lower-layers in the network. Later on, many other works on semantic segmentation using deep neural networks followed the idea of skip connections and proposed many encoder-decoder architectures. However, in this work, we are proposing a completely different approach for semantic segmentation and other computer vision tasks. Several empirically proved best-performing concepts in semantic segmentation using deep neural networks, such as skip-connections, Atrous Spatial pyramid pooling (ASPP), Object contextual representations (OCR), Attention Based multi-scale networks, SegFix and etc. can be easily integrated to a network designed by following the Ikshana theory. 6. In this work, we are suggesting that learning representations from the input image every time will help the network to learn better vector representations of the image, instead of the traditional way (of learning a representation with 32/64 filters from the input image and going deep on top of that representation until the network achieves adequate performance). 7. During our initial experiments, the networks designed by following the Ikshana theory showed considerable performance with less number of parameters. In table 2, we presented the results of a few different architectures designed by following our theory, evaluated on the validation set. Note: In table 2, the method named IkshanaNet-main was the architecture that we discussed in section 3.2. Here, the images were resized to the resolution of 256 x 512, and the number of training epochs was 200. However, all these methods were trained with the same hyper-parameters. (In the supplementary material, we provided the source code and class-wise IOU results for all of those architectures.) 8. In table 2, we can observe that a network with 88K parameters shown considerable performance compared to the IkshanaNet-M with 4M parameters. We expect our theory to help in designing lightweight architectures suitable for embedded and mobile devices, where there was limited computational power available. 9. The universal approximation theorem states that a deep neural network with large enough depth will have the ability to approximate any complex function. However, in real-time methods, it was evident that even small networks were approximating these complex functions. In this work, we suggest going deeper by learning multiple representations from the input image rather than the traditional method. 10. In this work, we are not making any claims, such as the current deep neural networks are not the correct way of learning. Instead, we think that the current deep learning methods were like training a super-intelligent neural network that can look at the image once and perform many computer vision tasks, and even achieved state-of-the-art performance in most cases. However, we are doing that at the cost of millions and even sometimes billions of parameters. Through this work, we suggest letting the network look at the images multiple times recurrently, i.e., as humans do, to scale these deep neural networks to real-world applications.</p><p>6 Validity threats 1. During training, the input images were resized to the resolution of 512 x 1024, exactly half of the original resolution, which will impact the performance of the model. The less batch size of 2 might also affect the performance. 2. The IkshanaNet-2 and IkshanaNet-3 were trained on a few training samples but evaluated on the same test set.</p><p>The performance of these two methods depends on how well the selected data samples represent the whole training set. So, an exact comparison of these methods to the main method IkshanaNet-1 may not be valid. 3. The IkshanaNet architecture might have defects and the exact architecture may not be suitable for other computer vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future work 7.1 Conclusions</head><p>In conclusion, we empirically showed that a network designed by following the Ikshana theory would have the ability to perform with a few data samples. We also presented some empirical evidence to show that our theory will help in reducing the number of parameters of the network. The main contribution of this work is the Ikshana theory and not the IkshanaNet. The IkshanaNet architecture is just an example to show how to implement our theory in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Future Work</head><p>Further investigation is required to design a new backbone architecture, by following the Ikshana theory, to learn representations from the images, which will contribute to every computer vision task. Along with that, additional investigation is required to address the following questions:</p><p>1. To what extent does a network designed by following the Ikshana theory would generalize to other datasets? Is there any improvement compared to the current state-of-the-art methods?</p><p>2. To what extent does a network designed by following the Ikshana theory would be resistant to adversarial attacks compared to currently available networks?</p><p>3. Some works <ref type="bibr" target="#b2">[3]</ref>[13] in neuroscience suggests that spatial resolutions around eight cycles/image are sufficient for humans to understand the basic gist. In this work, we used three spatial resolutions of the image. What are the different spatial resolutions required by a neural network to capture the semantic information of the given image? Do these required spatial resolutions changes from one task to another task?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>IkshanaNet-1 IkshanaNet-2 IkshanaNet-3 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>From the equations (1), (2), (3), (4), and (5), this is how the Ikshana theory approximates the functioning of the human brain, while human understands the conceptual gist of the image. The time taken(/number of glances required) by an average human to understand the gist of the image depends upon</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Ikshana Theory at Single Scale</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Quantitative results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Val Set Results</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gist of the scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurobiology of attention</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="251" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eye movements in reading and information processing: 20 years of research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">372</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diagnostic colors mediate scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><forename type="middle">G</forename><surname>Schyns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="176" to="210" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human gaze control during real-world scene perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="498" to="504" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Perception of objects in natural scenes: is it really attention free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Treisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1476</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What do we perceive in a glance of a real-world scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asha</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="10" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Very deep convolutional neural network based image classification using small training sample size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="730" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Meaning in visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="issue">4180</biblScope>
			<biblScope unit="page" from="965" to="966" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rapid conceptual identification of sequentially presented pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helene</forename><surname>Intraub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">604</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From blobs to boundary edges: Evidence for time-and spatial-scale-dependent scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Philippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Schyns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A logical calculus of the ideas immanent in nervous activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The bulletin of mathematical biophysics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The perceptron: a probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">386</biblScope>
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">E. hinton, geoffrey imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">3065386</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Torsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cars can&apos;t fly up in the sky: Improving urban-scene segmentation via height-driven attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungha</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanne</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9373" to="9383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gated-scnn: Gated shape cnns for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segfix: Model-agnostic boundary refinement for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="489" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision (ECCV)</title>
		<meeting>the european conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="552" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hardnet: A low memory traffic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yang</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Hsiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youn-Long</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3552" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12607" to="12616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9197" to="9206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention-based multicontext guiding for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8441" to="8448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">On the texture bias for few-shot cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Abdur R Fayjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Kauffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04052</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
