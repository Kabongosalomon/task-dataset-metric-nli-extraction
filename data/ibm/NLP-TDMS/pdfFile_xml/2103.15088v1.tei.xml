<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACSNet: Action-Context Separation Network for Weakly Supervised Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Liu</surname></persName>
							<email>liuziyi@stu.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence and Robotics</orgName>
								<orgName type="institution">Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence and Robotics</orgName>
								<orgName type="institution">Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">HERE Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
							<email>tangw@uic.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<email>jsyuan@buffalo.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">The State University of New York at Buffalo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
							<email>nnzheng@mail.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence and Robotics</orgName>
								<orgName type="institution">Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
							<email>ganghua@gmail.com</email>
							<affiliation key="aff4">
								<orgName type="department">Wormpex AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ACSNet: Action-Context Separation Network for Weakly Supervised Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The object of Weakly-supervised Temporal Action Localization (WS-TAL) is to localize all action instances in an untrimmed video with only video-level supervision. Due to the lack of frame-level annotations during training, current WS-TAL methods rely on attention mechanisms to localize the foreground snippets or frames that contribute to the video-level classification task. This strategy frequently confuse context with the actual action, in the localization result. Separating action and context is a core problem for precise WS-TAL, but it is very challenging and has been largely ignored in the literature. In this paper, we introduce an Action-Context Separation Network (ACSNet) that explicitly takes into account context for accurate action localization. It consists of two branches (i.e., the Foreground-Background branch and the Action-Context branch). The Foreground-Background branch first distinguishes foreground from background within the entire video while the Action-Context branch further separates the foreground as action and context. We associate video snippets with two latent components (i.e., a positive component and a negative component), and their different combinations can effectively characterize foreground, action and context. Furthermore, we introduce extended labels with auxiliary context categories to facilitate the learning of action-context separation. Experiments on THU-MOS14 and ActivityNet v1.2/v1.3 datasets demonstrate the ACSNet outperforms existing state-of-the-art WS-TAL methods by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Temporal Action Localization (TAL) aims to localize temporal starts and ends of specific action categories in a video. It serves as a fundamental tool for several practical applications such as action retrieval, intelligent surveillance and video summarization <ref type="bibr" target="#b11">(Lee, Ghosh, and Grauman 2012;</ref><ref type="bibr" target="#b26">Vishwakarma and Agrawal 2013;</ref><ref type="bibr" target="#b0">Asadiaghbolaghi et al. 2017;</ref><ref type="bibr" target="#b9">Kang and Wildes 2016;</ref><ref type="bibr" target="#b32">Yao, Lei, and Zhong 2019)</ref>. Although fully supervised TAL methods have recently achieved remarkable progress <ref type="bibr" target="#b1">(Buch et al. 2017;</ref><ref type="bibr" target="#b29">Xu, Das, and Saenko 2017;</ref><ref type="bibr" target="#b6">Gao et al. 2017;</ref><ref type="bibr" target="#b29">Xu, Das, and Saenko 2017;</ref><ref type="bibr" target="#b2">Chao et al. 2018;</ref><ref type="bibr" target="#b13">Lin et al. 2018</ref><ref type="bibr" target="#b12">Lin et al. , 2019</ref><ref type="bibr" target="#b35">Zeng et al. 2019)</ref>, manually annotating the precise temporal boundaries  <ref type="figure">Figure 1</ref>: The illustration of action, context and background in terms of frames and points in feature space. The green dashed line is the desired boundary for the localization task. However, based on the given video-level categorical labels, the blue dashed line is learned, due to the high co-occurrence and visual similarity of action and context. Existing methods frequently identify both red and green dots as actions. The main challenge in WS-TAL is how to isolate context from action instances with merely video-level categorical labels of action instances in untrimmed videos is time-consuming and challenging. This limitation motivates the weakly supervised setting where only video-level categorical labels are provided for model training. Compared with temporal boundary annotations, video-level categorical labels are easier to collect, and they help avoid the localization bias introduced by human annotators. Existing weakly-supervised temporal action localization (WS-TAL) methods <ref type="bibr" target="#b17">Nguyen et al. 2018;</ref><ref type="bibr" target="#b19">Paul, Roy, and Roy-Chowdhury 2018;</ref><ref type="bibr" target="#b18">Nguyen, Ramanan, and Fowlkes 2019)</ref> leverage attention mechanisms to categorize snippets or sampled frames into foreground and background based on their contribution to the video-level classification task, i.e., to find the blue dashed line in <ref type="figure">Figure 1</ref>. Then temporal action localization is reformulated as select-   <ref type="figure">Figure 2</ref>: An overview of our main idea, i.e., using extended label with auxiliary context categories to guide the training of action/context attentions. Unfortunately, such an idea is nontrivial to implement due to "lack of explicit actioncontext constraint" and "lack of explicit supervision".</p><p>ing consecutive foreground snippets belonging to each category. However, the foreground localized through video-level categorization involves not only the actual action instance but also its surrounding context. As illustrated in <ref type="figure">Figure 1</ref>, context is snippets or frames that frequently co-occur with the action instances of a specific category but should not be included in their localization. Different from background, which is class-agnostic, context provides strong evidence for action classification and thus can be easily confused with the action instances. We believe separating the action instances and their context is a core problem in WS-TAL, and it is very challenging due to the co-occurrence nature. The goal of this paper is to address the action-context separation (ACS) problem in the weakly-supervised setting so as to achieve more precise action localization. We first introduce auxiliary context categories for each action class during training. As shown in <ref type="figure">Figure 2</ref>, each video-level category is divided into two sub-categories, respectively corresponding to the actual action and its context. Prior methods exploit foreground attention to achieve foreground-background separation. However, this simple idea is not applicable to actioncontext separation due to two difficult issues. (1) Lack of explicit action-context constraints: The sum-to-one constraint <ref type="bibr" target="#b18">(Nguyen, Ramanan, and Fowlkes 2019)</ref> of the foreground and background attention scores does not apply to actioncontext separation. (2) Lack of explicit supervision: Both action and context can contribute to action classification, so the only available video-level categorical labels cannot provide direct supervision for them.</p><p>To address these two difficult issues, we introduce the Action-Context Separation Network (ACSNet). As illustrated in <ref type="figure">Figure 3</ref>, it consists of two branches, i.e., the Foreground-Background branch (FB branch) and the Action-Context branch (AC branch). The FB branch divides an untrimmed video into foreground and background based on whether a snippet supports the video-level classifica-tion. This is achieved via snippet-level categorical predictions (SCPs) and snippet-level attention predictions (SAPs), e.g., foreground attention in <ref type="figure">Figure 2</ref>. Subsequently, the AC branch further divides the obtained foreground into action and context by associating each video snippet with two latent components, i.e., a positive component and a negative component. Different combinations of these two components respectively characterize the foreground, action and context. This enables effective action-context separation with only video-level supervision. Finally, the output of AC branch facilitates the TAL by providing (1) temporal action proposals with more accurate boundaries and (2) more reliable proposal confidence scores.</p><p>The contribution of this paper is summarized below.</p><p>1. Prior WS-TAL approaches take it for granted that the foreground localized via the classification attention is equivalent to the actual action instance, and thus they unavoidably include the co-occurring context in the localization result. We address this challenge via a novel actioncontext separation network (ACSNet), which not only distinguishes foreground from background but also separates action and context within the foreground to achieve more precise action localization.</p><p>2. The proposed ACSNet features a novel Action-Context branch. It can individually characterize foreground, action and context using different combinations of two latent components, i.e., the positive component and the negative component.</p><p>3. We propose novel extended labels with auxiliary context categories. By explicitly decoupling the actual action and its context, this new representation facilitates effective learning of action-context separation.</p><p>4. Extensive experimental results indicate the proposed AC-SNet can effectively perform action-context separation. It significantly outperforms state-of-the-art methods on three benchmarks, and it is even comparable to recent fully-supervised methods.</p><p>2 Related Work of WS-TAL Different from action recognition which is essentially a classification task <ref type="bibr" target="#b5">(Feichtenhofer, Pinz, and Zisserman 2016;</ref><ref type="bibr" target="#b22">Simonyan and Zisserman 2014;</ref><ref type="bibr" target="#b28">Wang et al. 2016;</ref><ref type="bibr" target="#b7">Ji et al. 2013;</ref><ref type="bibr" target="#b24">Sun et al. 2015b;</ref><ref type="bibr" target="#b25">Tran et al. 2015;</ref><ref type="bibr" target="#b4">Feichtenhofer et al. 2019)</ref>, TAL requires finer-grained predictions with temporal boundaries of the target action instances. WS-TAL methods address it without temporal annotations, which is first introduced in <ref type="bibr" target="#b23">(Sun et al. 2015a</ref>). To distinguish action instances from background, the attention mechanism is widely adopted for foreground-background separation. Untrimmed-Net ) formulates the attention mechanism as a soft selection module to localize target action, and the final localization is achieved by thresholding the snippets' action scores. STPN <ref type="bibr" target="#b17">(Nguyen et al. 2018</ref>) proposes a sparsity loss based on the soft selection module of Untrimmed-Net, which can facilitate the selection of action instances. Nguyen et al. <ref type="bibr" target="#b18">(Nguyen, Ramanan, and Fowlkes 2019)</ref> characterize background by an additional background loss and introduce other losses to guide the attention. For better evaluation of temporal action proposals, W-TALC (Paul, Roy, and Roy-Chowdhury 2018) proposes a co-activity loss to enforce the feature similarity among localized instances. Au-toLoc <ref type="bibr" target="#b21">(Shou et al. 2018)</ref> uses an "outer-inner-contrastive loss" to predict and regress temporal boundaries. Liu et al. <ref type="bibr" target="#b14">(Liu, Jiang, and Wang 2019</ref>) exploit a multi-branch neural network to discover distinctive action parts and fuse them to ensure completeness. CleanNet <ref type="bibr" target="#b16">(Liu et al. 2019b</ref>) designs a "contrast score" by leveraging temporal contrast in SCPs to achieve end-to-end training of localization. However, driven by the video-level classification labels, the existing attention mechanism is merely able to capture the difference between foreground and background for classification, instead of action and non-action for localization. The proposed ACSNet manages to distinguish action instances from their surrounding context, and we extend labels by introducing auxiliary context categories to make the framework trainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Action-Context Separation Network</head><p>In this section, we introduce the extended video-level labels with auxiliary context categories (Section 3.1) and the proposed Action-Context Separation Network (ACSNet). As illustrated in <ref type="figure">Figure 3</ref>, the ACSNet consists of two branches, i.e., Foreground-Background branch (FB branch) and Action-Context branch (AC branch). After feature extraction from the given video (Section 3.2), FB branch distinguishes the foreground from background (Section 3.2). The obtained foreground contains both action and context. Subsequently, AC branch localizes the actual temporal action instances by performing action-context separation within the foreground (Section 3.3). To guide the training of ACS, additional losses are introduced (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extending Video-Level Labels</head><p>Suppose we are given a video V with a video-level categorical label y = [y(0), y(1), . . . , y(N )], where y(n) = 1 if V contains the n-th action category. N is the total number of action categories, y(0) represents the background category. To guide the division of foreground into action and context, we extend y with auxiliary context categories as</p><formula xml:id="formula_0">y = [ya(1), . . . , ya(N ), yc(1), . . . , yc(N )], y ∈ R 2N , (1)</formula><p>where y a (n) and y c (n) denote the n-th action category and its corresponding context, respectively. As shown in <ref type="figure">Figure</ref> </p><formula xml:id="formula_1">3, y ∈ R N +1 is used in FB branch and y ∈ R 2N is used in AC branch.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline Modules</head><p>This section introduces the baseline modules used in AC-SNet, including feature extraction and FB branch based on the attention mechanism. While they are not our main contribution, we introduce them for completeness. Similar modules have been explored and adopted by existing methods <ref type="bibr" target="#b17">(Nguyen et al. 2018;</ref><ref type="bibr" target="#b19">Paul, Roy, and Roy-Chowdhury 2018;</ref><ref type="bibr" target="#b18">Nguyen, Ramanan, and Fowlkes 2019;</ref><ref type="bibr" target="#b10">Lee, Uh, and Byun 2020)</ref>.</p><p>Feature Extraction The input of the feature extraction module is the given video V = {s t } T t=1 , which is divided into T non-overlapping snippets. The outputs are the corresponding features of each snippet. For each snippet s t , the corresponding D-dimensional features are extracted from two streams, i.e., the spatial stream (RGB) and the temporal stream (optical flow), denoted as</p><formula xml:id="formula_2">F rgb (t) ∈ R D and F flow (t) ∈ R D , respectively. Afterwards, the video V is rep- resented as F rgb ∈ R D×T and F flow ∈ R D×T .</formula><p>For notational simplicity, we use superscript "s" to indicate the notations used in both streams in the rest of the paper. The notations of the spatial/temporal stream can be obtained by substituting the superscript "s" with "rgb/flow". For example, F s can represent either F rgb or F flow .</p><p>Foreground-Background Branch The goal of the FB branch is to divide the entire video into two parts, i.e., foreground and background, which can be trained by the videolevel categorical label y = [y(0), y(1), . . . , y(N )].</p><p>The inputs of FB branch are the features F s ∈ R D×T , and the outputs are the snippet-level attention predictions (SAPs, ϕ ϕ ϕ ∈ R 1×T ) and the snippet-level classification predictions (SCPs, Ψ Ψ Ψ ∈ R (N+1)×T ). Accordingly, FB branch consists of two sub-modules, i.e., attention module (m s a ) and Foreground-Background classification module (m s ). The SAPs and SCPs of each stream are obtained by</p><formula xml:id="formula_3">ϕ ϕ ϕ s = m s a (F s ), ϕ ϕ ϕ s ∈ R 1×T ,<label>(2)</label></formula><formula xml:id="formula_4">Ψ Ψ Ψ s = m s (F s ), Ψ Ψ Ψ s ∈ R (N +1)×T .</formula><p>(3) Subsequently, the outputs of two streams are weighted to get the final SAPs and SCPs as</p><formula xml:id="formula_5">ϕ ϕ ϕ = αϕ ϕ ϕ rgb + (1 − α)ϕ ϕ ϕ flow ,<label>(4)</label></formula><formula xml:id="formula_6">Ψ Ψ Ψ = αΨ Ψ Ψ rgb + (1 − α)Ψ Ψ Ψ flow ,<label>(5)</label></formula><p>where α = 0.5 by default in our experiments. We implement m s a with a fully-connected (FC) layer followed by a sigmoid activation function. And m s is implemented by an FC layer.</p><p>To train m s a and m s with only video-level label, videolevel prediction is needed. Therefore, we calculate the videolevel foreground feature as</p><formula xml:id="formula_7">f s fg = 1 T T t=1 ϕ ϕ ϕ s (t)F s (t), f s fg ∈ R D .<label>(6)</label></formula><p>Similarly, the video-level background feature is obtained by</p><formula xml:id="formula_8">f s bg = 1 T T t=1 (1 − ϕ ϕ ϕ s (t))F s (t), f s bg ∈ R D .<label>(7)</label></formula><p>After obtaining f s fg and f s bg , we feed them into m s to obtain the video-level prediction, i.e., the foreground prediction</p><formula xml:id="formula_9">(p s fg ∈ R N +1 ) and background prediction (p s bg ∈ R N +1 ), defined as p s fg = m s (f s fg ), p s bg = m s (f s bg ).</formula><p>(8) Given video-level predictions in Eq.(8), the FB branch can be trained via regular cross-entropy loss. For p s fg , its label is y, where y(n) = 1 if V contains the n-th action category, as shown in <ref type="figure">Figure 3</ref>. While for p s bg , assuming that all videos contain background snippets, its label is always y(0) = 1 and y(n) = 0, n = 1, 2, ...N . Extended Label <ref type="figure">Figure 3</ref>: The framework of the proposed ACSNet, which has two branches, i.e., Foreground-Background branch and Action-Context branch. The input video is first processed by the feature embedding to get features from both spatial and temporal streams. The FB branch focuses on foreground-background separation while the AC branch focuses on action-context separation. Video-level labels are extended to facilitate the action-context separation.</p><formula xml:id="formula_10">C Label rgb F flow F Foreground-Background Attention Module s F fg fg ( ) s s s m  p f bg s f s  s  - 1 f g s f bg bg ( ) s s s m  p f Latent Components Combination Action-Context Classification fĝ ˆa ˆc  rgb flow 2 [ , ], D T    F F F F rgb flow + + &amp; m m rgb flow &amp; a a m m 1- s  rgb flow &amp; m m   s  s  fĝ fˆa fˆc f fg fĝˆ( ) m  p f )ˆ( a a m  p fˆ( ) c c m  p f ( ) 0, (0) 1 n   y y ( ) ( ) 0.5 a c n n   y y ( ) 1, ( ) 0 a c n n   y y ( ) 0, ( ) 1 a c n n   y y Action-Context Branch</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Action-Context Branch</head><p>The attention mechanism trained by y will be distracted by context because both action and context can support videolevel classification. To avoid such distraction, after distinguishing the foreground from background, we further separate action and context within the foreground to locate the actual action instances in this section. The inputs of the AC branch are features from two streams (F s obtained in Section 3.2) and SAPs (ϕ ϕ ϕ obtained in Section 3.2). The AC branch consists of three sub-modules, i.e., latent components generation, latent components combination, and action-context separation.</p><p>Latent Components Generation. We introduce the con- </p><formula xml:id="formula_11">L s + = m s + (F s ), L s − = m s − (F s ).<label>(9)</label></formula><p>m s + and m s − share the same architecture (parameters are not shared), with two temporal convolution (Conv1d) layers followed by a ReLU and a sigmoid activation function for the first and the second layer, respectively. Latent Components Combination. Given L s + and L s − , we use the combination of them to construct the snippetlevel foreground attention ( ϕ ϕ ϕ fg ∈ R 1×T ), action attention ( ϕ ϕ ϕ a ∈ R 1×T ), and context attention ( ϕ ϕ ϕ c ∈ R 1×T ). Specifically, for each stream, we have ϕ ϕ ϕ</p><formula xml:id="formula_12">s fg = σ(L s + + L s − ),<label>(10)</label></formula><formula xml:id="formula_13">ϕ ϕ ϕ s a = σ(L s + ),<label>(11)</label></formula><formula xml:id="formula_14">ϕ ϕ ϕ s c = σ(L s − − L s + ),<label>(12)</label></formula><p>where σ(·) denotes the sigmoid function. Subsequently, the outputs from two streams are fused by weighted average similar to Eq. <ref type="formula" target="#formula_5">(4)</ref>,</p><formula xml:id="formula_15">ϕ ϕ ϕ z = α ϕ ϕ ϕ rgb z + (1 − α) ϕ ϕ ϕ flow z , ϕ ϕ ϕ z z ∈ {fg, a, c},<label>(13)</label></formula><p>where ϕ ϕ ϕ z ∈ R 1×T . For notational simplicity, we use subscript "z" to denote either "fg", "a" or"c" if necessary. By substituting the subscript "z" with "fg/a/c", ϕ ϕ ϕ fg / ϕ ϕ ϕ a / ϕ ϕ ϕ c are obtained following Eq.(13). Instead of directly imposing simple constrains like foreground and background following <ref type="bibr">(Nguyen, Ramanan</ref> Action-Context Separation. After obtaining ϕ ϕ ϕ fg , ϕ ϕ ϕ a and ϕ ϕ ϕ c , we can start the action-context separation by leveraging label with auxiliary context categories (i.e., y ∈ R 2N introduced in Section 3.1). First of all, we select all temporal indices corresponding to foreground snippets as</p><formula xml:id="formula_16">I = {t | ϕ ϕ ϕ(t) &gt; 0.5}, |I| = T ,<label>(14)</label></formula><p>where |·| denotes the cardinality (number of elements). Subsequently, the video-level feature representations of foreground, action and context are obtained as</p><formula xml:id="formula_17">f z = 1 T t∈I ϕ ϕ ϕ z (t)F(t), z ∈ {fg, a, c},<label>(15)</label></formula><p>where f z ∈ R 2D×1 and F(t) = F rgb (t), F flow (t) (F(t) ∈ R 2D×1 ) is the concatenated feature from both streams and · means concatenation. By substituting the subscript "z" with "fg/a/c", f fg , f a and f c are calculated following Eq.(15). Afterwards, they are fed into the action-context classification module m to get the video-level action-context prediction as</p><formula xml:id="formula_18">p z = m( f z ), p z ∈ R 2N , z ∈ {fg, a, c}.<label>(16)</label></formula><p>Different from the video-level prediction from FB branch (i.e., p s fg ∈ R N +1 in Eq.(8)), p z ∈ R 2N provides predictions on both action and context categories. Specifically, if the video contains the n-th category, the label for p fg is y = [y a (1), . . . , y a (N ), y c (1), . . . , y c (N )], where y a (n) = y c (n) = 0.5. While for p a and p c , the labels are (y a (n) = 1, y c (n) = 0) and (y a (n) = 0, y c (n) = 1), respectively, as shown in <ref type="figure">Figure 3</ref>. After obtaining p z and the corresponding labels, the AC branch is also trained via regular cross-entropy loss.</p><p>Applying m to each snippet, the snippet-level actioncontext predictions are obtained as</p><formula xml:id="formula_19">Ψ Ψ Ψ = m(F), Ψ Ψ Ψ ∈ R 2N ×T ,<label>(17)</label></formula><p>where F ∈ R 2D×T is the concatenated feature. Ψ Ψ Ψ is leveraged to promote the action and suppress the context, by defining an "action-context offset ( Ψ Ψ Ψ ∈ R N ×T )" as</p><formula xml:id="formula_20">Ψ Ψ Ψ(n, t) = Ψ Ψ Ψ (n, t)−Ψ Ψ Ψ (2n, t) if t ∈ I, 0 otherwise,<label>(18)</label></formula><p>where Ψ Ψ Ψ (n, t) (or Ψ Ψ Ψ (2n, t)) is the prediction of the n-th action (or corresponding context) of the t-th snippet. Intuitively, Ψ Ψ Ψ(n, t) means "offsets" for the n-th class of the t-th snippet, compared the prediction of action (Ψ Ψ Ψ (n, t)) with context (Ψ Ψ Ψ (2n, t)).</p><p>In summery, the AC branch outputs snippet-level action score ( ϕ ϕ ϕ a ∈ R 1×T ) and the action-context offset ( Ψ Ψ Ψ ∈ R N ×T ) for the subsequent localization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Additional Losses</head><p>In addition to the regular cross-entropy losses, more constrains are required to train the ACSNet successfully, since there are neither temporal annotations nor action/context annotations available. In this section, we introduce two additional losses to provide extra guidance for ACSNet training, i.e., L g and L mse .</p><p>For guidance loss L g , due to the lack of ground truth labeled action or context categories, confusion between action and context (e.g., ϕ ϕ ϕ a and ϕ ϕ ϕ c , L s + and L s − ) will occur due to symmetry. Therefore, additional guidance should be introduced to distinguish action from context, which is achieved by minimizing L g . Specifically, the differences between two streams are leveraged. We adopt weighted binary logistic regression loss function L r to guide ϕ ϕ ϕ a and ϕ ϕ ϕ c , where L r (p, q) is denoted as</p><formula xml:id="formula_21">Lr(p, q) = − l i=1 qi · log(pi) l + + (1−qi) · log(1−pi) l − ,<label>(19)</label></formula><p>where p, q ∈ R 1×l and q is a binary vector indicating positive and negative samples (snippets). p is the prediction to be regressed. l + = q i and l − = (1 − q i ). For action attention ϕ ϕ ϕ a , positive time index set (P a ) and negative time index set (N a ) are defined as</p><formula xml:id="formula_22">P a = {t | ϕ ϕ ϕ rgb a (t) &gt; θ h &amp; ϕ ϕ ϕ flow a (t) &gt; θ h },<label>(20)</label></formula><formula xml:id="formula_23">N a = {t | ϕ ϕ ϕ rgb a (t) &lt; θ l &amp; ϕ ϕ ϕ flow a (t) &lt; θ l },<label>(21)</label></formula><p>where θ h and θ l indicate high and low thresholds, respectively. Intuitively, the snippets with high/low attentions on both streams are regarded as positive/negative samples for action snippets. For context attention ϕ ϕ ϕ c , we assume context contains scenes (excluding action instances), so that the corresponding positive/negative snippet index sets are defined as</p><formula xml:id="formula_24">P c = {t | ϕ ϕ ϕ rgb a (t) &gt; θ h &amp; ϕ ϕ ϕ flow a (t) &lt; θ l }, (22) N c = P a ∪ N a .<label>(23)</label></formula><p>Subsequently, the guidance loss L g is calculated as</p><formula xml:id="formula_25">L g = L r (ϕ ϕ ϕ a , [1(|P a |), 0(|N a |)])+ L r (ϕ ϕ ϕ c , [1(|P c |), 0(|N c |)]), (24) ϕ ϕ ϕ a = ϕ ϕ ϕ a (P a ), ϕ ϕ ϕ a (N a ) , ϕ ϕ ϕ c = ϕ ϕ ϕ c (P c ), ϕ ϕ ϕ c (N c ) . (25)</formula><p>where 1(d) (or 0(d)) indicates a d-dimensional vector filled with ones (or zeros).</p><p>For L mse , in order to encourage the two latent components to focus on the foreground, we adopt the Mean Squared Error (MSE) loss between ϕ ϕ ϕ fg and ϕ ϕ ϕ, denoted as</p><formula xml:id="formula_26">L mse = MSE( ϕ ϕ ϕ fg , G(ϕ ϕ ϕ)),<label>(26)</label></formula><p>where G(·) is a Gaussian smoothing function. Finally, the AC branch is trained by minimizing the total loss L, calculated as</p><formula xml:id="formula_27">L = L cls + λ(L mse + L g ),<label>(27)</label></formula><p>where L cls is the sum of cross-entropy losses mentioned in Section 3.3. λ is the balancing weight set as 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Localization</head><p>After the inference, FB branch outputs SAPs (ϕ ϕ ϕ ∈ R 1×T ), SCPs (Ψ Ψ Ψ ∈ R (N +1)×T ) and AC branch outputs action score ( ϕ ϕ ϕ a ∈ R 1×T ), action-context offset ( Ψ Ψ Ψ ∈ R N ×T ). These outputs are leveraged for the TAL task. We first introduce the TAL baseline using only outputs of FB branch. Secondly, we present the contribution of AC branch to the TAL task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Localization Baseline</head><p>The localization baseline uses only outputs of FB branch. The temporal action proposals are generated by thresholding ϕ ϕ ϕ with 0.5. The evaluation (scoring) of temporal action proposals is based on Ψ Ψ Ψ.</p><p>After obtaining a proposal P = [t s , t e ], where t s and t e denote the starting and ending snippet indices, respectively. P is scored by leveraging the Outer-Inner-Contrastive loss <ref type="bibr" target="#b21">(Shou et al. 2018)</ref> as</p><formula xml:id="formula_28">s(P, v) = mean(v(t s : t e ))− mean( v(t s − τ : t s ), v(t e : t e + τ ) ),<label>(28)</label></formula><p>where v ∈ R 1×T is the sequence for scoring. τ = (t e −t s )/4 denotes the inflation length and mean(·) is the averaging function. Specifically, when locating the n-th action category based on Ψ Ψ Ψ, we make v = v 1 = Ψ Ψ Ψ(n, :), which is the predictions of the n-th action category of all snippets. After obtaining proposals and their scores, the TAL results are collected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Improving Localization by AC branch</head><p>The two critical steps of performing TAL are the generation and evaluation of proposals. The outputs of AC branch can improve both of them. For proposal generation, in addition to thresholding ϕ ϕ ϕ (P 1 in <ref type="table">Table 4</ref>), we also perform thresholding step on ϕ ϕ ϕ a and Ψ Ψ Ψ (P 2 and P 3 in <ref type="table">Table 4</ref>). Since ϕ ϕ ϕ a and Ψ Ψ Ψ are both action-aware and less susceptible to the influence of context, the proposals obtained by thresholding them can provide more accurate action boundaries and less context noise. For proposal evaluation, we can improve the quality of Ψ Ψ Ψ(n, :) to make the scores calculated by Eq.(28) more reliable using Ψ Ψ Ψ. Specifically, we improve Ψ Ψ Ψ(n, :) by suppressing the context and promoting the action as v 2 = Ψ Ψ Ψ(n, :) + Ψ Ψ Ψ(n, :).</p><p>(29) By replacing v with v 2 in Eq.(28), we can evaluate proposals more accurately by alleviating the influence of context.</p><p>In summery, the contribution of AC branch to the TAL is reflected in three aspects, i.e., using its outputs ( ϕ ϕ ϕ a and Ψ Ψ Ψ) to improve proposal generation (P 2 and P 3 ), and using Ψ Ψ Ψ to improve proposal scoring (v 2 ). These three aspects are validated in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate the proposed ACSNet via detailed ablation studies to explore the contribution brought by AC branch. Meanwhile, we compare our method with stateof-the-art WS-TAL methods and recent fully-supervised TAL methods on two standard benchmarks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparisons with State-of-the-Art Methods</head><p>As presented in <ref type="table" target="#tab_0">Table 1</ref>, the proposed ACSNet outperforms existing WS-TAL methods in terms of mAPs with all IoU threshold settings on THUMOS14 testing set with significant improvement. Also, the proposed ACSNet achieves state-of-the-art on ActivityNet v1.2 and v1.3, as presented in <ref type="table" target="#tab_1">Table 2</ref>. However, such performance improvement is not as significant as that on THUMOS14, possibly due to Activ-ityNet v1.2/v1.3 only has 34.6%/35.7% non-action frames </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>Is Context Really Useful for Classification? We assume that the action-context confusion is caused by both action and context can support the classification, due to the high co-occurrence of them. To validate whether the context snippets estimated by AC branch meet our assumption or not, we collect the foreground/background and action/context snippets as follows. The t-th snippet belongs to foreground if ϕ ϕ ϕ(t) &gt; 0.5 and otherwise it belongs to background. Among foreground snippets, if ϕ ϕ ϕ a (t) &gt; 0.5, the t-th snippet is assigned as action and otherwise as context. For reference, we also collect all ground truth snippets. Therefore, five snippet sets are collected, noted as S fg , S bg , S a , S c , and S gt , respectively.</p><p>Regarding the conjuncted snippets as temporal proposals among each set, these snippet sets can be evaluated in both localization and classification tasks, as summarized in <ref type="table" target="#tab_2">Table 3</ref>. For localization, we use the metrics introduced in Section 5.1 with v = v 1 = Ψ Ψ Ψ(n, :) for proposal evaluation, since Ψ Ψ Ψ(n, :) does not bias on either action or context. For classification, two metrics are adopted, i.e., the average top1 classification accuracy (A 1 ) and proportion of groundtruth actions defined as</p><formula xml:id="formula_29">R z = t∈Sz Ψ Ψ Ψ(n gt , t) N n=1 t∈Sz Ψ Ψ Ψ(n, t) , z ∈ {fg, bg, a, c, gt},<label>(30)</label></formula><p>where n gt means the groundtruth category and Ψ Ψ Ψ(n, t) is the t-th snippet's classification prediction on the n-th class. As presented in <ref type="table" target="#tab_2">Table 3</ref>, context snippets S c contain more useful information compared with S bg , indicated by the much better classification accuracy. However, in terms of localization task, both S c and S c perform poorly, which matches our assumption of context, i.e., snippets that can support classification but contain no actual actions. TAL Contribution of AC branch. The contribution of the proposed AC branch towards the TAL task is reflected in three aspects as summarized in Section 4.2. To validate these three aspects, five ablated variants are evaluated in this section. For the convenience of the discussion, we define the <ref type="table">Table 4</ref>: Ablation studies of ACSNet on THUMOS14 test. As defined in Section 5.3, the usage of P 2 /P 3 /S 2 reflect the contribution of ϕ ϕ ϕ a / Ψ Ψ Ψ/ Ψ Ψ Ψ in aspects of proposal generation/generation/evaluation. P 2 /P 3 /S 2 take up 33.3%/22.2%/44.5% of the mAP gain upon #0 (α : 0.4). following notations for experiment settings. For proposal generation settings, P 1 /P 2 /P 3 are defined as: Thresholding ϕ ϕ ϕ/ ϕ ϕ ϕ a / Ψ Ψ Ψ(n, :) with 0.5/0.5/0 to generate temporal action proposals for all/all/n-th action class. For proposal scoring settings, S 1 /S 2 are defined as: Using v 1 /v 2 as the v in Eq.(28) for proposal evaluation. Therefore, the usage of P 2 reflects the contribution of ϕ ϕ ϕ a in aspects of proposal generation. The usage of P 3 and S 2 reflect the contribution of Ψ Ψ Ψ in aspects of proposal generation and evaluation, respectively. The contribution of P 2 /P 3 /S 2 to TAL is evaluated individually below, as presented in <ref type="table">Table 4</ref>. With P 1 and S 1 , the #0 variants are the baseline methods, which depend on FB branch and are non-related to the AC branch . Noted that baselines show super sensitivity towards hyper-parameter α, we choose the best one (α = 0.4) for comparison below. In contrast, all the other ablated variants are with simple average two-stream fusion (α = 0.5). Comparison between baseline (#0) and #1 shows the contribution solely from P 2 . Similarly, the contributions solely from P 3 and S 2 can be validated by the comparisons between #2 and #4, #1 and #2, respectively. Quantitatively, P 2 /P 3 /S 2 take up 33.3%/22.2%/44.5% of the performance gain upon baseline.</p><p>Besides, compared with #4 and #5, an obvious performance drop is observed, indicating the localization result from FB branch has been burden for the final localization. Without the proposals from FB branch, and with the help of ϕ ϕ ϕ a and Ψ Ψ Ψ on proposal generation and evaluation, "#4" achieves the best localization performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>: Trained by video-level categorical label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>AC</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Foreground</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>cept of positive component (L s + ∈ R 1×T ) and negative component (L s − ∈ R 1×T ) to characterize foreground, action and context. Assuming the foreground is represented by two latent components, we define the one corresponding to the actual action as positive component, while the other one as negative component. They are obtained similarly as the SAPs in Eq.(2), by feeding features into positive module (m s + ) and negative module (m s − )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>TAL performance comparison on THUMOS14 test set, in terms of average mAP at IoU thresholds [0.3 : 0.1 : 0.7]. Recent works in both fully-supervised and weaklysupervised settings are reported. UNT and I3D represent UntrimmedNet and I3D feature backbones, respectively. ACSNet achieves state-of-the-art performance on both backbones. Compared to fully-supervised methods, our ACSNet can achieve close or even better performance.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Feature</cell><cell>mAP@IoU 0.3 0.4 0.5 0.6 0.7</cell><cell>AVG</cell></row><row><cell></cell><cell>SSN (2017)</cell><cell>UNT</cell><cell cols="2">51.9 41.0 29.8 19.6 10.7 30.6</cell></row><row><cell>Full</cell><cell>BSN (2018) MGG (2019a)</cell><cell>-I3D</cell><cell cols="2">53.5 45.0 36.9 28.4 20.0 36.8 53.9 46.8 37.4 29.5 21.3 37.8</cell></row><row><cell></cell><cell>G-TAD 2020</cell><cell>-</cell><cell cols="2">54.5 47.6 40.2 30.8 23.4 39.3</cell></row><row><cell></cell><cell>STPN (2018)</cell><cell>UNT</cell><cell cols="2">31.1 23.5 16.2 9.8 5.1 17.1</cell></row><row><cell>Weak</cell><cell>W-TALC (2018) AutoLoc (2018) CleanNet (2019b)</cell><cell>UNT UNT UNT</cell><cell cols="2">32 26.0 18.8 10.9 6.2 18.8 35.8 29.0 21.2 13.4 5.8 21.0 37.0 30.9 23.9 13.9 7.1 22.6</cell></row><row><cell></cell><cell>ACSNet (Ours)</cell><cell>UNT</cell><cell cols="2">40.3 33.8 26.7 16.8 9.2 25.4</cell></row><row><cell></cell><cell>STPN (2018)</cell><cell>I3D</cell><cell cols="2">35.5 25.8 16.9 9.9 4.3 18.5</cell></row><row><cell></cell><cell>MAAN (2019)</cell><cell>I3D</cell><cell cols="2">41.1 30.6 20.3 12.0 6.9 22.2</cell></row><row><cell></cell><cell>W-TALC (2018)</cell><cell>I3D</cell><cell cols="2">40.1 31.1 22.8 14.5 7.6 23.2</cell></row><row><cell>Weak</cell><cell>Liu(2019) BM (2019) ASSG (2019)</cell><cell>I3D I3D I3D</cell><cell cols="2">41.2 32.1 23.1 15.0 7.0 23.7 46.6 37.5 26.8 17.6 9.0 27.5 50.4 38.7 25.4 15.0 6.6 27.2</cell></row><row><cell></cell><cell>BaSNet (2020)</cell><cell>I3D</cell><cell cols="2">44.6 36.0 27.0 18.6 10.4 27.3</cell></row><row><cell></cell><cell>DGAM (2020)</cell><cell>I3D</cell><cell cols="2">46.8 38.2 28.8 19.8 11.4 29.0</cell></row><row><cell></cell><cell>ACSNet (Ours)</cell><cell>I3D</cell><cell cols="2">51.4 42.7 32.4 22.0 11.7 32.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>TAL performance comparison on ActivityNet v1.2 and v1.3 validation set, in terms of average mAP at IoU thresholds [0.5 : 0.05 : 0.95]. Our result is also comparable to fully-supervised models.</figDesc><table><row><cell></cell><cell>Method</cell><cell>1.2 /1.3</cell><cell>mAP(%)@IoU 0.5 0.75 0.95</cell><cell>Avg</cell></row><row><cell>Full</cell><cell>SSN (2017) SSN (2017)</cell><cell>v1.2 v1.3</cell><cell cols="2">41.3 27.0 6.1 26.6 39.1 23.5 5.5 24.0</cell></row><row><cell></cell><cell>AutoLoc (2018)</cell><cell>v1.2</cell><cell cols="2">27.3 15.1 3.3 16.0</cell></row><row><cell></cell><cell>TSM (2019)</cell><cell>v1.2</cell><cell cols="2">28.3 17.0 3.5 17.1</cell></row><row><cell>Weak</cell><cell>W-TALC (2018) CleanNet (2019b) Liu et al.(2019)</cell><cell>v1.2 v1.2 v1.2</cell><cell cols="2">37.0 12.7 1.5 18.0 37.1 20.3 5.0 21.6 36.8 22.0 5.6 22.4</cell></row><row><cell></cell><cell>BaSNet (2020)</cell><cell>v1.2</cell><cell cols="2">38.5 24.2 5.6 24.3</cell></row><row><cell></cell><cell>DGAM (2020)</cell><cell>v1.2</cell><cell cols="2">41.0 23.5 5.3 24.4</cell></row><row><cell></cell><cell>ACSNet (Ours)</cell><cell>v1.2</cell><cell cols="2">40.1 26.1 6.8 26.0</cell></row><row><cell></cell><cell>STPN (2018)</cell><cell>v1.3</cell><cell>29.3 16.9 2.6</cell><cell>-</cell></row><row><cell>Weak</cell><cell>TSM (2019) Liu et al.(2019) BM (2019)</cell><cell>v1.3 v1.3 v1.3</cell><cell cols="2">30.3 19.0 4.5 34.0 20.9 5.7 21.2 -36.4 19.2 2.9 -</cell></row><row><cell></cell><cell>BaSNet (2020)</cell><cell>v1.3</cell><cell cols="2">34.5 22.5 4.9 22.2</cell></row><row><cell></cell><cell>ACSNet (Ours)</cell><cell>v1.3</cell><cell cols="2">36.3 24.2 5.8 23.9</cell></row><row><cell cols="3">5.1 Experimental Setting</cell><cell></cell></row><row><cell cols="5">Evaluation Datasets. THUMOS14 dataset (Jiang et al.</cell></row><row><cell cols="5">2014) provides temporal annotations for 20 action cate-</cell></row><row><cell cols="5">gories, including 200 untrimmed videos from validation set</cell></row><row><cell cols="5">and 213 untrimmed videos from test set. On average, each</cell></row><row><cell cols="5">video contains 15.4 action instances and 71.4% frames are</cell></row><row><cell cols="5">non-action background. Following conventions, the valida-</cell></row><row><cell cols="5">tion and test sets are leveraged for training and testing, re-</cell></row><row><cell cols="5">spectively. ActivityNet v1.2 &amp; v1.3 (Fabian Caba Heilbron</cell></row><row><cell cols="5">and Niebles 2015) provide temporal annotations for 100 /</cell></row><row><cell cols="5">200 action categories, including a training set with 4, 819 /</cell></row><row><cell cols="5">10, 024 untrimmed videos and a validation set with 2, 383 /</cell></row><row><cell cols="3">4, 926 untrimmed videos 1 .</cell><cell></cell></row><row><cell cols="5">Evaluation metric. Following the standard evaluation pro-</cell></row><row><cell cols="5">tocol, we evaluate the TAL performance using mean average</cell></row><row><cell cols="5">precision (mAP) values at different levels of IoU thresholds.</cell></row><row><cell cols="5">Specifically, the IoU threshold sets are [0.3 : 0.1 : 0.7] and</cell></row><row><cell cols="5">[0.5 : 0.05 : 0.95] for THUMOS14 and ActivityNet, re-</cell></row><row><cell cols="5">spectively. Both THUMOS14 and ActivityNet benchmarks</cell></row><row><cell cols="5">provide standard evaluation implementations, which are di-</cell></row><row><cell cols="4">rectly exploited in our experiments for fair comparison.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification and localization evaluation on different snippet sets on THUMOS14 test set. Classification metric: Average top1 classification accuracy (A 1 ), and proportion of groundtruth actions (R z ) defined in Eq.(30). Localization metric: Average mAP under the IoU thresholds from 0.3 to 0.7.</figDesc><table><row><cell></cell><cell>A1 (%)</cell><cell>Rz (%)</cell><cell>0.3</cell><cell cols="3">mAP(%)@IoU 0.4 0.5 0.6</cell><cell>0.7</cell><cell>AVG</cell></row><row><cell>Sgt</cell><cell cols="2">91.4 62.4</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell cols="2">100 100</cell><cell>100</cell></row><row><cell cols="8">Sfg 88.6 59.1 38.3 30.4 21.5 14.4 7.4</cell><cell>22.4</cell></row><row><cell>Sa</cell><cell cols="7">91.0 61.5 42.4 34.6 25.0 16.7 9.4</cell><cell>25.6</cell></row><row><cell>Sc</cell><cell cols="2">81.0 53.4</cell><cell>0.7</cell><cell>0.3</cell><cell>0.2</cell><cell>0</cell><cell>0</cell><cell>0.2</cell></row><row><cell cols="3">Sbg 26.7 15.1</cell><cell>0.1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="9">per video on average, while THUMOS14 contains 71.4%</cell></row><row><cell cols="9">on average. With lower non-action ratio, the improvement</cell></row><row><cell cols="9">brought by context suppression could be less significant.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In our experiments, there are 4, 471 / 9, 937 and 2, 211 / 4, 575 videos accessible from YouTube in the training and validation set for ActivityNet v1.2 / v1.3, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">ConclusionsWe propose an ACSNet for weakly-supervised temporal action localization, which can separate action and context with only video-level categorical labels. This is achieved by characterizing foreground/action/context as combinations of positive and negative latent compositions. ACSNet significantly outperforms existing WS-TAL methods on three standard datasets, i.e., THUMOS14, ActivityNet v1.2 and v1.3. Moreover, ACSNet achieves competitive performance even compared with recent fully-supervised TAL methods. Experimental results validate the significance of action-context separation and the superiority of the proposed pipeline.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported partly by National Key R&amp;D Program of China Grant 2018AAA0101400, NSFC Grants 61629301, 61773312, and 61976171, China Postdoctoral Science Foundation Grant 2019M653642, Young Elite Scientists Sponsorship Program by CAST Grant 2018QNRC001, and Natural Science Foundation of Shaanxi Grant 2020JQ-069.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Survey on Deep Learning Based Approaches for Action and Gesture Recognition in Image Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asadiaghbolaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clapes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellantonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Poncelopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="476" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking the Faster R-CNN Architecture for Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Review of action recognition and detection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.06906</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Background Suppression Network for Weakly-Supervised Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11320" to="11327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BSN: Boundary Sensitive Network for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Completeness Modeling and Context Separation for Weakly Supervised Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly Supervised Temporal Action Localization through Contrast based Evaluation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6752" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5502" to="5511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">W-TALC: Weakly-supervised Temporal Activity Localization and Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="588" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly-Supervised Action Localization by Generative Attention Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1009" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AutoLoc: Weakly-supervised Temporal Action Localization in Untrimmed Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="371" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4597" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey on activity recognition and behavior understanding in video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwakarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="983" to="1009" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">R-C3D: region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5794" to="5803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sub-Graph Localization for Temporal Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-Tad</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="10156" to="10165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A review of Convolutional-Neural-Network-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal structure mining for weakly supervised action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5522" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Marginalized Average Attentional Network for Weakly-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial Seeded Sequence Growing for Weakly-Supervised Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="738" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal Action Detection with Structured Segment Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
