<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">All Word Embeddings from One Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
							<email>sho.takase@nlp.c.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tohoku University</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">All Word Embeddings from One Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In neural network-based models for natural language processing (NLP), the largest part of the parameters often consists of word embeddings. Conventional models prepare a large embedding matrix whose size depends on the vocabulary size. Therefore, storing these models in memory and disk storage is costly. In this study, to reduce the total number of parameters, the embeddings for all words are represented by transforming a shared embedding. The proposed method, ALONE (all word embeddings from one), constructs the embedding of a word by modifying the shared embedding with a filter vector, which is word-specific but non-trainable. Then, we input the constructed embedding into a feed-forward neural network to increase its expressiveness. Naively, the filter vectors occupy the same memory size as the conventional embedding matrix, which depends on the vocabulary size. To solve this issue, we also introduce a memory-efficient filter construction approach. We indicate our ALONE can be used as word representation sufficiently through an experiment on the reconstruction of pre-trained word embeddings. In addition, we also conduct experiments on NLP application tasks: machine translation and summarization. We combined ALONE with the current state-of-the-art encoderdecoder model, the Transformer [36], and achieved comparable scores on WMT 2014 English-to-German translation and DUC 2004 very short summarization with less parameters 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings have played a crucial role in the recent progress in the area of natural language processing (NLP) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. In particular, word embeddings are necessary to convert discrete input representations into vector representations in neural network-based NLP methods <ref type="bibr" target="#b2">[3]</ref>. To convert an input word w into a vector representation in a conventional way, we prepare a one-hot vector v w whose dimension size is equal to the vocabulary size V and an embedding matrix E whose shape is D e × V (D e represents a word embedding size). Then, we multiply E and v w to obtain a word embedding e w .</p><p>NLP researchers have used word embeddings as input in their models for several applications such as language modeling <ref type="bibr" target="#b37">[38]</ref>, machine translation <ref type="bibr" target="#b29">[30]</ref>, and summarization <ref type="bibr" target="#b25">[26]</ref>. However, in these methods, the embedding matrix forms the largest part of the total parameters, because V is much larger than the dimension sizes of other weight matrices. For example, the embedding matrix makes up one-fourth of the parameters of the Transformer, a state-of-the-art neural encoder-decoder model, on WMT English-to-German translation <ref type="bibr" target="#b35">[36]</ref>. Thus, if we can represent each word with fewer parameters without significant compromises on performance, we can reduce the model size to conserve memory space. If we save the memory space for embeddings, we can also train a bigger model to improve the performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pick an embedding</head><p>Fixed with random initialization <ref type="figure">Figure 1</ref>: Word embedding constructions by the conventional way and our proposed ALONE. ALONE represents each word with an embedding o and the filter vector m w . We fix source matrices with random initialized values, and thus it is unnecessary to prepare trainable parameters for construction of m w . To increase the expressiveness, we input the embedding into a feed-forward neural network.</p><p>To reduce the size of neural network models, some studies have proposed the pruning of unimportant connections from a trained network <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">39]</ref>. However, their approaches require twice or much computational cost, because we have to train the network before and after pruning. Another approach <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b0">1]</ref> limited the number of embeddings by utilizing the composition of shared embeddings. These methods represent an embedding by combining several primitive embeddings, whose number is less than the vocabulary. However, since they require learning the combination of assignments of embeddings to each word, they need additional parameters during the training phase. Thus, prior approaches require multiple training steps and/or additional parameters that are necessary only during the training phase.</p><p>To address the above issues, we propose a novel method: ALONE (all word embeddings from one), which can be used as a word embedding set without multiple training steps and additional trainable parameters to assign each word to a unique vector. ALONE computes an embedding for each word (as a replacement for e w ) by transforming a shared base embedding with a word-specific filter vector and a shared feed-forward network. We represent the filter vectors with combinations of primitive random vectors, whose total is significantly smaller than the vocabulary size. In addition, it is unnecessary to train the assignments, because we assign the random vectors to each word randomly. Therefore, while ALONE retains its expressiveness, its total parameter size is much smaller than the conventional embedding size, which depends on the vocabulary size due to D e × V .</p><p>Through experiments, we demonstrate ALONE can be used for NLP with comparable performances to the conventional embeddings but fewer parameters. We first indicate ALONE has enough expressiveness to represent the existing word embedding (GloVe <ref type="bibr" target="#b21">[22]</ref>) through a reconstruction experiment. In addition, on two NLP applications of machine translation and summarization, we demonstrate ALONE with the state-of-the-art encoder-decoder model trained in an end-to-end manner achieved comparable scores with fewer parameters than models with the conventional word embeddings.</p><p>2 Proposed Method: ALONE <ref type="figure">Figure 1</ref> shows an overview of the conventional and proposed word embedding construction approaches. As shown in this figure, the conventional way requires a large embedding matrix E ∈ R De×V , where each column vector is assigned to a word, and we pick a word embedding with a one-hot vector v w ∈ R 1×V . In other words, each word has a word-specific vector, whose size is D e , and the total size summed over the vocabulary is D e × V . To reduce the size, ALONE make words share some type of vector element with each other. ALONE has a base embedding o ∈ R 1×Do , which is shared by all words, and represents each word by transforming the base embedding. To concretely obtain a word representation for w, ALONE computes an element-wise product of o and a filter vector m w , and then applies a feed-forward network to increase its expressiveness. The two components are described as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Filter Construction</head><p>To make the result of the element-wise product m w o unique to each word, we have to prepare different filter vectors from each other. In the simplest way, we sample values from a distribution such as Gaussian D o × V times, and then we construct the matrix whose size is D o × V with the sampled values. However, the above method requires similar memory space to E in the conventional word embeddings. Thus, we introduce a more memory-efficient way herein.</p><p>Our filter construction approach does not prepare the filter vector for each word explicitly. Instead, we construct a filter vector by combining multiple vectors, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In the first step, we prepare M source matrices such as codebooks, each of which is D o × c. Then, we assign one column vector of each matrix to each word randomly. Thus, each word is tied to M (column) vectors. In this step, since the probability of collision between two combinations is much small (1 − exp(− V 2 2(c M ) ) based on the birthday problem), each word is probably assigned to the unique set of M vectors. Moreover, the required memory space, i.e., D o × c × M is smaller than E when we use c × M V .</p><p>To construct the filter vector m w , we pick assigned column vectors from each matrix, and compute the sum of the vectors. Then, we apply a function f (·) to the result. Formally, let m 1 w , ..., m M w be column vectors assigned to w, we compute the following equation:</p><formula xml:id="formula_0">m w = f M i m i w .<label>(1)</label></formula><p>In this paper, we use two types of filter vectors: a binary mask and a real number vector based on the following distribution and function.</p><p>Binary Mask A binary mask is a binary vector whose elements are 0 or 1, such as the dropout mask <ref type="bibr" target="#b28">[29]</ref>. Thus, we ignore some elements of o based on the binary mask for each word. For the binary mask construction, we use the Bernoulli distribution. To make the binary mask containing 0 with probability p o , we construct the source matrices with sampling from the Bernoulli</p><formula xml:id="formula_1">(1 − p 1 M o )</formula><p>. Moreover, we use the following Clip(·) <ref type="bibr" target="#b1">2</ref> as the function f to trim 1 or more to 1 in the filter vectors.</p><formula xml:id="formula_2">Clip(a) = 1 1 ≤ a 0 otherwise<label>(2)</label></formula><p>In other words, in binary mask construction, m w is computed from the element-wise logical OR operation over all m i w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real Number Vector</head><p>In addition to the binary mask, we use the filter vector, which consists of real numbers. We use the Gaussian distribution to construct source matrices, and the identity transformation as the function f . In other words, we use the sum of vectors from source matrices without any transformation as the filter vector. </p><formula xml:id="formula_3">D e × V ALONE (naive) D o + D inter × (D o + D e ) + D o × V ALONE (proposed) D o + D inter × (D o + D e ) + M × D o × c ALONE (proposed (volatile)) D o + D inter × (D o + D e )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feed-Forward Network</head><p>We obtain the unique vector to each word by computing the element-wise product of o and m w . However, in this situation, words share the same value in several elements. Thus, we increase the expressiveness by applying a feed-forward network FFN(·) to the result of m w o:</p><formula xml:id="formula_4">FFN(x) = W 2 (max(0, W 1 x)),<label>(3)</label></formula><p>where W 1 ∈ R Dinter×Do and W 2 ∈ R De×Dinter are weight matrices. In short, the feed-forward network in this paper consists of two linear transformations with a ReLU activation function. We use the output of FFN(m w o) as the word embedding for w. <ref type="table" target="#tab_0">Table 1</ref> summarizes the memory space required by each method. In this table, ALONE (naive) ignores the filter construction approach introduced in Section 2.1. As described previously, the conventional way requires a large amount of memory space because V is exceptionally large (&gt; 10 4 ) in most cases. In contrast, ALONE (proposed) drastically reduces the number of parameters due to D o , D inter , M × c V , and thus, we can reduce the memory footprint when we adopt the introduced filter construction way. Moreover, since ALONE uses random initialized vectors as filter vectors without any training, we can reconstruct them again and again if we store a random seed. Thus, we can ignore the memory space for filter vectors such as ALONE (proposed (volatile)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Discussion on the Number of Parameters and Memory Footprint</head><p>As an example, consider word embeddings on WMT 2014 English-to-German translation in the setting of the original Transformer <ref type="bibr" target="#b35">[36]</ref>. In this setting, the conventional way requires 19M as the memory footprint due to V = 37000 and D e = 512. In contrast, ALONE compresses the parameter size to 4M, which is less than a quarter of that footprint, when we set D o = 512 and D inter = 4096. These values are used in the following experiment on machine translation. For filter vectors, the naive filter construction way requires an additional 19M for the memory footprint, but our introduced approach requires only 262k more when we set c = 64 and M = 8. These values are also used in our experiments. Thus, the proposed ALONE reduces the memory footprint as compared to that of the conventional word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we investigate whether the proposed method, ALONE, can be an alternation of the conventional word embeddings. We first conduct an experiment on the reconstruction of pretrained word embeddings to investigate whether ALONE is capable of mimicking the conventional word embeddings. Then, we conduct experiments on real applications: machine translation and summarization. We train the Transformer, which is the current state-of-the-art neural encoder-decoder model, combined with the ALONE in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Embedding Reconstruction</head><p>In this experiment, we investigate whether ALONE has a similar expressiveness to the conventional word embeddings. We used the pre-trained 300 dimensional GloVe 3 <ref type="bibr" target="#b21">[22]</ref> as source word embeddings and reconstructed them with ALONE. Training details: The training objective is to mimic the GloVe embeddings with ALONE. Let e w be the conventional word embedding (GloVe in this experiment), we minimize the following objective function:</p><formula xml:id="formula_5">1 V V w=1 ||e w − FFN(m w o)|| 2 .<label>(4)</label></formula><p>We optimized the above objective function with Adam [13] whose hyper-parameters are default settings in PyTorch <ref type="bibr" target="#b20">[21]</ref>. We set mini-batch size 256 and the number of epochs 1000. We constructed each mini-batch with uniformly sampling from vocabulary and regard training on the whole vocabulary as one epoch. For c, M , and p o in the binary mask, we set 64, 8, and 0.5 respectively 4 . We used the same dimension size as GloVe (300) for D o and conducted experiments with varying D inter in {600, 1200, 1800, 2400}. In each setting, the total number of parameters is 0.4M, 0.7M, 1.1M, and 1.4M. We selected the top 5k words based on the frequency in English Wikipedia as target words <ref type="bibr" target="#b4">5</ref> .</p><p>We used five random seeds to initialize ALONE, and report the average of five scores.</p><p>Test data: We used the word similarity task, which is widely used to evaluate the quality of word embeddings <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">37]</ref>. The task investigates whether similarity based on trained word embeddings corresponds to the human-annotated similarity. In this paper, we used three test sets: SimLex-999 <ref type="bibr" target="#b8">[9]</ref>, WordSim-353 <ref type="bibr" target="#b4">[5]</ref>, and RG-65 <ref type="bibr" target="#b24">[25]</ref>. We computed Spearman's rank correlation (ρ) between the cosine similarities of word embeddings and the human annotations as in previous studies.</p><p>Results: <ref type="figure">Figure 3</ref> shows the Spearman's ρ of ALONE in both filter vectors. This figure also shows the Spearman's ρ without training o (Fix o). The dashed line indicates the Spearman's ρ of GloVe, i.e., the upper bound of word embedding reconstruction. <ref type="figure">Figure 3</ref> indicates that ALONE achieved comparable scores to GloVe on all datasets in D inter = 2400. These results indicate that ALONE has the expressive power to represent the conventional word embeddings.</p><p>In the comparison between filter vectors, we cannot find the significant difference in D inter = 2400, but the real number vectors slightly outperformed binary masks in small D inter . Thus, we should use real number vectors as the filter vectors if the number of parameters is strongly restricted. <ref type="figure">Figure 3</ref> shows that the setting without training o is defeated against training o in most cases. Therefore, it is better to train o to obtain superior representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Machine Translation</head><p>Section 3.1 indicates ALONE can mimic pre-trained word embeddings, but there remain two questions:</p><p>1. Can we train ALONE in an end-to-end manner? 2. In the realistic situation, can ALONE reduce the number of parameters related to word embeddings while maintaining performance? To answer these questions, we conduct experiments on machine translation and summarization.</p><p>Training details: In machine translation, we used ALONE as word embeddings instead of the conventional embedding matrix E in the Transformer <ref type="bibr" target="#b35">[36]</ref>. We adopted the base model setting and thus shared embeddings with the pre-softmax linear transformation matrix. We used the fairseq implementation <ref type="bibr" target="#b18">[19]</ref> and followed the training procedure described in its documentation <ref type="bibr" target="#b5">6</ref> . We set D o the same number as the dimension of each layer in the Transformer (d model , i.e., 512) and varied D inter . For other hyper-parameters, we set as follows: c = 64, M = 8, and p o = 0.5. Moreover, we applied the dropout after the ReLU activation function in Equation <ref type="formula" target="#formula_4">(3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset:</head><p>We used WMT En-De dataset since it is widely used to evaluate the performance of machine translation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b17">18]</ref>. Following previous studies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b17">18]</ref>, we used WMT 2016 training data, which contains 4.5M sentence pairs, newstest2013, newstest2014 for training, validation, and test respectively. We constructed a vocabulary with the byte pair encoding <ref type="bibr" target="#b26">[27]</ref> whose merge operation is 32K in sharing source and target. We measured case-sensitive BLEU with SacreBLEU <ref type="bibr" target="#b23">[24]</ref>.</p><p>Result: <ref type="table" target="#tab_1">Table 2</ref> shows BLEU scores of the Transformer with ALONE in the case D inter = 4096, 8192. This table also shows BLEU scores of previous studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16]</ref> and the Transformer trained in our environment with the same hyper-parameters as the original one [36] 7 . DeFINE <ref type="bibr" target="#b15">[16]</ref> uses a factorization approach, which constructs embeddings from small matrices instead of one large embedding matrix, to reduce the number of parameters <ref type="bibr" target="#b7">8</ref> . In addition, we trained the Transformer with a simple factorization approach as a baseline. In the simple factorization approach, we construct word embeddings from one small embedding matrix and weight matrix to expand a small embedding to one with the larger dimensionality D e . We modified the dimension size to make the number of parameters related to embeddings almost equal to those in ALONE. <ref type="table" target="#tab_1">Table 2</ref> shows the Transformer with ALONE in D inter = 8192 outperformed other methods even though the embedding parameter size is half that of the Transformer with the conventional embeddings. This result indicates that our ALONE can reduce the number of parameters related to embeddings without negatively affecting the performance on machine translation. In comparison with the factorized embedding approaches, Transformer with ALONE (D inter = 8192) achieved superior BLEU scores as compared to Transformer+DeFINE <ref type="bibr" target="#b15">[16]</ref>, while the total parameter size of Transformer with ALONE, 52.5M, is smaller than it (68M). In other words, Transformer+ALONE achieved better performance even though it had a disadvantage in the parameter size. Moreover, Transformer+ALONE outperformed Transformer (factorized embed) despite the number of parameters related to embeddings in both being almost equal. These results imply that ALONE is superior to a simple approach using a small embedding matrix and expansion with a weight matrix. Therefore, we also have to train o in the training ALONE with an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Summarization</head><p>Training details: We also conduct an experiment on the headline generation task, which is one of the abstractive summarization tasks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>. The purpose of this task is to generate a headline of a given document with the desired length. Thus, we introduced ALONE into the Transformer with the length control method <ref type="bibr" target="#b33">[34]</ref>. For other model details, we used the same as the experiment on machine translation. We used the publicly available implementation <ref type="bibr" target="#b8">9</ref> and followed their training procedure. As the length control method, we used the combination of LRPE and PE <ref type="bibr" target="#b33">[34]</ref>. Moreover, we re-ranked decoded candidates based on the content words following the previous study <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset:</head><p>As in previous studies <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>, we used pairs of the first sentence and headline extracted from the annotated English Gigaword with the same pre-processing script provided by Rush et al.</p><p>[26] 10 as the training data. The training set contains about 3.8M pairs. For the source-side, we used the byte pair encoding <ref type="bibr" target="#b26">[27]</ref> to construct a vocabulary. We set the hyper-parameter to fit the vocabulary size 16K. For the target-side, we used a set of characters as a vocabulary to control the number of output characters. We shared the source-side and target-side vocabulary.</p><p>We used the DUC 2004 task 1 <ref type="bibr" target="#b19">[20]</ref> as the test set. Following the evaluation protocol <ref type="bibr" target="#b19">[20]</ref>, we truncated the characters of the output headline to 75 bytes and computed the recall-based ROUGE score.</p><p>Result: <ref type="table" target="#tab_2">Table 3</ref> shows the ROUGE scores of the previous best method (Trans-former+LRPE+PE <ref type="bibr" target="#b33">[34]</ref>) with ALONE in the case D inter = 512, 1024 <ref type="bibr" target="#b10">11</ref> . This table indicates that ALONE (Real number) achieved the comparable ROUGE-2 score to the previous best in D inter = 512. In addition, Transformer+LRPE+PE+ALONE in D inter = 1024 outperformed the previous best score except for ROUGE-2 in ALONE (Binary) despite the embedding parameter size being one-eighth of that of the original Transformer+LRPE+PE.   In conclusion, based on the machine translation and summarization results, the answers for the previously mentioned two questions are as follows; 1. Yes, we can train ALONE with a neural method from scratch. 2. Yes, our ALONE can reduce the parameter size for embeddings without sacrificing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Researchers have proposed several strategies to compress neural networks such as pruning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">39]</ref>, knowledge distillation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, and quantization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>. Han et al. <ref type="bibr" target="#b7">[8]</ref> proposed iterative pruning, which consists of the following three steps: (1) train a neural network to find important connections, (2) prune the unimportant connections, and (3) re-train the neural network to tune the remaining connections. Zhang et al. <ref type="bibr" target="#b38">[39]</ref> iteratively performed steps <ref type="formula" target="#formula_2">(2)</ref> and <ref type="formula" target="#formula_4">(3)</ref> to compress a neural machine translation model.</p><p>Knowledge distillation approaches train a small network to mimic a pre-trained network by minimizing the difference between the outputs of the small network and pre-trained original network <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Kim and Rush <ref type="bibr" target="#b10">[11]</ref> applied knowledge distillation to a neural machine translation model and obtained a smaller model that achieves comparable scores to the original network.</p><p>These approaches require additional computational costs to acquire a compressed model because they need to train a base network before compression. In contrast, our proposed ALONE does not require a pre-trained network because we can train it with an end-to-end manner in the same way as the conventional word embeddings. In addition, we can also apply the above approaches to compress ALONE because the approaches are orthogonal to it.</p><p>Chen et al. <ref type="bibr" target="#b1">[2]</ref> proposed HashedNet, which constructs the weight matrix from a few parameters. HashedNet decides the assignment of trainable parameters to an element of the weight matrix based on a hash function. Some studies applied such a parameter assignment approach to word embeddings <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b0">1]</ref>. For example, Suzuki and Nagata <ref type="bibr" target="#b30">[31]</ref> constructs word embeddings with the concatenation of several sub-vectors (trainable parameters). Their method optimizes the parameters and assignments of sub-vectors through training. While those methods can represent each word with small parameters after training, they require additional parameters during the training phase. In fact, Suzuki and Nagata <ref type="bibr" target="#b30">[31]</ref> uses conventional word embeddings during training. In contrast, such additional parameters are unnecessary in ALONE.</p><p>Svenstrup et al. <ref type="bibr" target="#b32">[33]</ref> proposed the method to assign each word to several vectors using a hash function.</p><p>Their method also has no additional parameters during training, but it requires weight vectors for each word. Thus, its parameter size depends on the vocabulary size V . In contrast, since the parameters of ALONE are independent from V , ALONE can represent each word with fewer parameters.</p><p>To reduce the number of parameters related to word embeddings, some studies have reduced the vocabulary size V <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35]</ref>. In these days, it is common to construct the vocabulary with subwords <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b13">14]</ref> for generation tasks such as machine translation. Furthermore, some studies use characters (or character n-grams) as their vocabulary and construct word embeddings from character embeddings <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35]</ref>. This study does not conflict with these studies because ALONE is used to represent the word, sub-word, and character embeddings in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes ALONE, a novel method to reduce the number of parameters related to word embeddings. ALONE constructs embeddings for each word from one embedding in contrast to the conventional way that prepares a large embedding matrix whose size depends on the vocabulary size V . Through experiments, we indicated that ALONE can represent each word while maintaining the similarity of pre-trained word embeddings. In addition, we can train ALONE in an end-to-end manner on real NLP applications. We combined ALONE with the strong neural encoder-decoder method, Transformer <ref type="bibr" target="#b35">[36]</ref>, and achieved comparable scores on WMT 2014 English-to-German translation and DUC 2004 very short summarization with fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>This study addresses the reduction of trainable parameters for word embeddings. Word embeddings are fundamental component of various neural network-based NLP methods because we need them to convert a symbolic input into vector representations. Thus, the proposed method, ALONE, has potential to reduce the parameter size of existing neural network-based NLP methods. In this paper, we combined ALONE with a neural encoder-decoder model but we expect that it also has a positive effect on other methods such as large-scale neural language models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Construction of the filter vector m w . We pick several vectors corresponding to the word w from source matrices and combine them to construct m w .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>8 (Figure 3 :</head><label>83</label><figDesc>Results on word embedding reconstruction. The dashed line indicates the Spearman's rank correlation coefficient of GloVe.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>shows loss (negative log likelihood) values on validation sets of machine translation and summarization (newstest2013 and a set sampled from English Gigaword, respectively) for each word embedding method. This figure indicates that the convergence of the Transformer with ALONE is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Validation loss of each method. slower than the conventional embeddings. In particular, the convergence of Binary (512) is much slower than other methods in summarization. We consider this slow convergence harmed ROUGE scores of ALONE (Binary) in DUC 2004. Meanwhile, the Transformer+ALONE in D inter = 8192, which outperformed the original Transformer in machine translation, achieved superior validation loss values as compared to the conventional embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Memory space required by each method for word representation. ALONE (naive) represents the case where we prepare filter vectors explicitly.</figDesc><table><row><cell>Method</cell><cell>Memory usage</cell></row><row><cell>Conventional way</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of WMT En-De translation.</figDesc><table><row><cell>Method</cell><cell cols="3">D inter Embed BLEU</cell></row><row><cell>ConvS2S [6]</cell><cell></cell><cell>66.0M</cell><cell>25.2</cell></row><row><cell>Transformer [36]</cell><cell></cell><cell>16.8M</cell><cell>27.3</cell></row><row><cell>Transformer+DeFINE [16]</cell><cell></cell><cell>-</cell><cell>27.01</cell></row><row><cell cols="2">Transformer (conventional word embeddings)</cell><cell>16.8M</cell><cell>27.12</cell></row><row><cell>Transformer (factorized embed)</cell><cell></cell><cell>4.3M</cell><cell>26.43</cell></row><row><cell>Transformer (factorized embed)</cell><cell></cell><cell>8.5M</cell><cell>26.56</cell></row><row><cell>Transformer+ALONE (Binary)</cell><cell>4096</cell><cell>4.2M</cell><cell>26.97</cell></row><row><cell>Transformer+ALONE (Real number)</cell><cell>4096</cell><cell>4.2M</cell><cell>26.93</cell></row><row><cell>Transformer+ALONE (Binary)</cell><cell>8192</cell><cell>8.4M</cell><cell>27.55</cell></row><row><cell>Transformer+ALONE (Real number)</cell><cell>8192</cell><cell>8.4M</cell><cell>27.61</cell></row><row><cell>ALONE without training o</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer+ALONE (Binary)</cell><cell>4096</cell><cell>4.2M</cell><cell>26.75</cell></row><row><cell>Transformer+ALONE (Real number)</cell><cell>4096</cell><cell>4.2M</cell><cell>26.85</cell></row><row><cell>Transformer+ALONE (Binary)</cell><cell>8192</cell><cell>8.4M</cell><cell>26.90</cell></row><row><cell>Transformer+ALONE (Real number)</cell><cell>8192</cell><cell>8.4M</cell><cell>26.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on DUC 2004 task 1. The scores in bold are superior to the previous top score.</figDesc><table><row><cell>Method</cell><cell cols="2">D inter Embed</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell>ABS [26]</cell><cell></cell><cell cols="2">42.1M 28.18</cell><cell cols="2">8.49 23.81</cell></row><row><cell>LSTM EncDec+WFE [32]</cell><cell></cell><cell cols="4">37.7M 32.28 10.54 27.80</cell></row><row><cell cols="2">Transformer+LRPE+PE [34]</cell><cell cols="4">8.3M 32.29 11.49 28.03</cell></row><row><cell>+ALONE (Binary)</cell><cell>512</cell><cell cols="4">0.5M 31.60 11.12 27.25</cell></row><row><cell>+ALONE (Real number)</cell><cell>512</cell><cell cols="4">0.5M 31.96 11.50 27.74</cell></row><row><cell>+ALONE (Binary)</cell><cell>1024</cell><cell cols="4">1.0M 32.51 11.48 28.08</cell></row><row><cell>+ALONE (Real number)</cell><cell>1024</cell><cell cols="4">1.0M 32.57 11.63 28.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 also</head><label>2</label><figDesc></figDesc><table /><note>shows BLEU scores of the Transformer with ALONE in the case without training the base embedding o. The setting without training o didn't achieve BLEU scores as high as training o.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is publicly available at https://github.com/takase/alone_seq2seq 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2004.12073v3 [cs.CL] 23 Oct 2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Equation (2) represents the operation for a scalar value. If an input is a vector such as Equation(1), we apply Equation<ref type="bibr" target="#b1">(2)</ref> to all elements in the input vector.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://nlp.stanford.edu/projects/glove/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We tried several values, but we cannot observe any significant differences among the results.<ref type="bibr" target="#b4">5</ref> We can use the whole vocabulary of pre-trained GloVe embeddings but we restricted vocabulary size to shorten the training time.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt 7 The number of parameters for embeddings in the Transformer is different from that in the original one<ref type="bibr" target="#b35">[36]</ref> owing to the difference of vocabulary size.<ref type="bibr" target="#b7">8</ref> Table 2shows the reported score. We cannot demonstrate the embedding parameter size for Trans-former+DeFINE because its vocabulary size is unreported but the parameter size of Transformer+DeFINE is 68M, which is larger than that of the original Transformer (60.9M). We consider that the original Transformer (and our experiments) save the total parameter size by sharing embeddings with the pre-softmax linear transformation matrix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://github.com/takase/control-length 10 https://github.com/facebookarchive/NAMAS 11 In the abstractive summarization task, the vocabulary size is much smaller than that in the machine translation experiment because we used characters for the target-side vocabulary, and the source-side vocabulary size is also small. Thus, we set a smaller Dinter.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank Hiroshi Noji, Hitomi Yanaka, Koki Washio, and Saku Sugawara for constructive discussions. We thank anonymous reviewers for their useful suggestions. This work was supported by JSPS KAKENHI Grant Number JP18K18119. The first author is supported by Microsoft Research Asia (MSRA) Collaborative Research Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning k-way d-dimensional discrete codes for compact embedding representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web (WWW)</title>
		<meeting>the 10th international conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Compressing deep convolutional networks using vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<idno>abs/1412.6115</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28 (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SimLex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequence-level knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 2 (NIPS)</title>
		<editor>D. S. Touretzky</editor>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">De-FINE: Deep factorized input token embeddings for neural sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations (ICLR</title>
		<meeting>the 8th International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26 (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT)</title>
		<meeting>the Third Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Duc in context. Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1506" to="1520" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32 (NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT)</title>
		<meeting>the Third Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compressing word embeddings via deep compositional code learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 6th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning compact neural word embeddings by parameter space sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2046" to="2052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cutting-off redundant repeating generations for neural abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="291" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hash embeddings for efficient word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Tito</forename><surname>Svenstrup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30 (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4928" to="4936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Positional encoding to control output sequence length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3999" to="4004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Character n-gram embeddings to improve rnn language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sho Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5074" to="5082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30 (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1504" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Recurrent neural network regularization. CoRR, abs/1409.2329</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards compact and fast neural machine translation using a combined method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1475" to="1481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
