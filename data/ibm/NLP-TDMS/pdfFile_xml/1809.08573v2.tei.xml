<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning for Video Super-Resolution through HR Optical Flow Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
							<email>wanglongguang15@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Science</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
							<email>yulan.guo@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Science</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiping</forename><surname>Lin</surname></persName>
							<email>linzaiping@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Science</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpu</forename><surname>Deng</surname></persName>
							<email>dengxinpu@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Science</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
							<email>anwei@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Science</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning for Video Super-Resolution through HR Optical Flow Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video super-resolution (SR) aims to generate a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The generation of accurate correspondence plays a significant role in video SR. It is demonstrated by traditional video SR methods that simultaneous SR of both images and optical flows can provide accurate correspondences and better SR results. However, LR optical flows are used in existing deep learning based methods for correspondence generation. In this paper, we propose an endto-end trainable video SR framework to super-resolve both images and optical flows. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed according to the HR optical flows. Finally, compensated LR inputs are fed to a superresolution network (SRnet) to generate the SR results. Extensive experiments demonstrate that HR optical flows provide more accurate correspondences than their LR counterparts and improve both accuracy and consistency performance. Comparative results on the Vid4 and DAVIS-10 datasets show that our framework achieves the stateof-the-art performance. The codes will be released soon at: https://github.com/LongguangWang/SOF-VSR-Super-Resolving-Optical-Flow-for-Video-Super-Resolution-.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Super-resolution (SR) aims to generate high-resolution (HR) images or videos from their low-resolution (LR) counterparts. As a typical low-level computer vision problem, SR has been widely investigated for decades <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>. Recently, the prevalence of high-definition display further advances the development of SR. For single image SR, image details are recovered using the spatial correlation in a single frame. In contrast, inter-frame temporal correlation can further be exploited for video SR.</p><p>Since temporal correlation is crucial to video SR, the Groundtruth SOF-VSR TDVSR VSRnet <ref type="figure">Figure 1</ref>. Temporal profiles under Ã—4 configuration for VSRnet <ref type="bibr" target="#b12">[13]</ref>, TDVSR <ref type="bibr" target="#b19">[20]</ref> and our SOF-VSR on Calendar and City. Purple boxes represent corresponding temporal profiles. Our SOF-VSR produces finer details in temporal profiles, which are more consistent with the groundtruth. key to success lies in accurate correspondence generation. Numerous methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref> have demonstrated that the correspondence generation and SR problems are closely interrelated and can boost each other's accuracy. Therefore, these methods integrate the SR of both images and optical flows in a unified framework. However, current deep learning based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> mainly focus on the SR of images, and use LR optical flows to provide correspondences. Although LR optical flows can provide sub-pixel correspondences in LR images, their limited accuracy hinders the performance improvement for video SR, especially for scenarios with large upscaling factors.</p><p>In this paper, we propose an end-to-end trainable video SR framework to generate both HR images and optical flows. The SR of optical flows provides accurate correspondences, which not only improves the accuracy of each HR image, but also achieves better temporal consistency. We first introduce an optical flow reconstruction net (OFRnet) to reconstruct HR optical flows in a coarse-to-fine manner. These HR optical flows are then used to perform motion compensation on LR frames. A space-to-depth transformation is therefore used to bridge the resolution gap between HR optical flows and LR frames. Finally, the compensated LR frames are fed to a super-resolution net (SRnet) to generate each HR frame. Extensive evaluation is conducted  to test our framework. Comparison to existing video SR methods shows that our framework achieves the state-ofthe-art performance in terms of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). Moreover, our framework achieves better temporal consistency for visual perception (as shown in <ref type="figure">Fig. 1</ref>).</p><p>Our main contributions can be summarized as follows: 1) We integrate the SR of both images and optical flows into a single SOF-VSR (super-resolving optical flow for video SR) network. The SR of optical flows provides accurate correspondences and improves the overall performance; 2) We propose an OFRnet to infer HR optical flows in a coarse-tofine manner; 3) Extensive experiments have demonstrated the effectiveness of our framework. It is shown that our framework achieves the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review some major methods for single image SR and video SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single Image SR</head><p>Dong et al. <ref type="bibr" target="#b2">[3]</ref> proposed the pioneering work to use deep learning for single image SR. They used a three-layer convolutional neural network (CNN) to approximate the non-linear mapping from the LR image to the HR image. Recently, deeper and more complex network architectures have been proposed <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11]</ref>. Kim et al. <ref type="bibr" target="#b13">[14]</ref> proposed a very deep super-resolution network (VDSR) with 20 convolutional layers. Tai et al. <ref type="bibr" target="#b32">[33]</ref> developed a deep recursive residual network (DRRN) and used recursive learning to control the model parameters while increasing the depth. Hui et al. <ref type="bibr" target="#b10">[11]</ref> proposed an information distillation network to reduce computational complexity and memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video SR</head><p>Traditional video SR. To handle complex motion patterns in video sequences, Protter et al. <ref type="bibr" target="#b25">[26]</ref> generalized the non-local means framework to address the SR problem. They used patch-wise spatio-temporal similarity to perform adaptive fusion of multiple frames. Takeda et al. <ref type="bibr" target="#b33">[34]</ref> further introduced 3D kernel regression to exploit patch-wise spatiotemporal neighboring relationship. However, the resulting HR images of these two methods are over-smoothed. To exploit pixel-wise correspondences, optical flow is used in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>. Since the accuracy of correspondences provided by optical flows in LR images is usually low <ref type="bibr" target="#b16">[17]</ref>, an iterative framework is used in these methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref> to estimate both HR images and optical flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep video SR with separated motion compensation.</head><p>Recently, deep learning has been investigated for video SR. Liao et al. <ref type="bibr" target="#b17">[18]</ref> performed motion compensation under different parameter settings to generate an ensemble of SRdrafts, and then employed a CNN to recover high-frequency details from the ensemble. Kappelar et al. <ref type="bibr" target="#b12">[13]</ref> also performed image alignment through optical flow estimation, and then passed the concatenation of compensated LR inputs to a CNN to reconstruct each HR frame. In these methods, motion compensation is separated from CNN. Therefore, it is difficult for them to obtain the overall optimal solution. Deep video SR with integrated motion compensation. More recently, Caballero et al. <ref type="bibr" target="#b1">[2]</ref> proposed the first endto-end CNN framework (namely, VESPCN) for video SR. It comprises a motion compensation module and a sub-pixel convolutional layer used in <ref type="bibr" target="#b30">[31]</ref>. Since that, end-to-end framework with motion compensation dominates the research of video SR. Tao et al. <ref type="bibr" target="#b34">[35]</ref> used the motion estimation module in VESPCN, and proposed an encode-decoder network based on LSTM. This architecture facilitates the extraction of temporal context. Liu et al. <ref type="bibr" target="#b19">[20]</ref> customized ESPCN <ref type="bibr" target="#b30">[31]</ref> to simultaneously process different numbers of LR frames. They then introduced a temporal adaptive network to aggregate multiple HR estimates with learned dynamic weights. Sajjadi et al. <ref type="bibr" target="#b28">[29]</ref> proposed a framerecurrent architecture to use previously inferred HR estimates for the SR of subsequent frames. The recurrent archi-  <ref type="figure">Figure 3</ref>. Architecture of our OFRnet. Our OFRnet works in a coarse-to-fine manner. At each level, the output of its previous level is used to compute a residual optical flow. tecture can assimilate previous inferred HR frames without increase in computational demands.</p><formula xml:id="formula_0">L i I RDB 1 LD i j F ï‚® LD i I L i I L j I LD j I L j I LDU i j F ï‚® Warp L i j F ï‚® Concat L i I L j I L i j F ï‚® Warp Conv,</formula><p>It is already demonstrated by traditional video SR methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref> that simultaneous SR of images and optical flows produces better result. However, current CNNbased methods only focus on the SR of images. Different from previous works, we propose an end-to-end video SR framework to super-resolve both images and optical flows. It is demonstrated that the SR of optical flows facilitates our framework to achieve the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architecture</head><p>Our framework takes N consecutive LR frames as inputs and super-resolves the central frame. The LR inputs are first divided into pairs and fed to OFRnet to infer an HR optical flow. Then, a space-to-depth transformation <ref type="bibr" target="#b28">[29]</ref> is employed to shuffle the HR optical flow into LR grids. Afterwards, motion compensation is performed to generate an LR draft cube. Finally, the draft cube is fed to SRnet to infer the HR frame. The overview of our framework is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Optical Flow Reconstruction Net (OFRnet)</head><p>It is demonstrated that CNN has the capability to learn the non-linear mapping between LR and HR images for the SR problem <ref type="bibr" target="#b2">[3]</ref>. Recent CNN-based works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> have also shown the potential for motion estimation. In this paper, we incorporate these two tasks into a unified network to infer HR optical flows from LR images. Specifically, our OFRnet takes a pair of LR frames I L i and I L j as inputs, and reconstruct an optical flow between their corresponding HR frames I H i and I H j :</p><formula xml:id="formula_1">F H iâ†’j = Net OF R (I L i , I L j ; Î˜ OF R )<label>(1)</label></formula><p>where F H iâ†’j represents the HR optical flow and Î˜ OF R is the set of parameters.</p><p>Motivated by the pyramid optical flow estimation method in <ref type="bibr" target="#b0">[1]</ref>, we use a coarse-to-fine approach to handle complex motion patterns (especially large displacements). As illustrated in <ref type="figure">Fig. 3</ref>, a 3-level pyramid is employed in our OFRnet.</p><p>Level 1: The pair of LR images I L i and I L j are downsampled by a factor of 2 to produce I LD i and I LD j , which are further concatenated and fed to a feature extraction layer. Then, two residual dense blocks (RDB) <ref type="bibr" target="#b37">[38]</ref> with 4 layers and a growth rate of 32 are customized. Within each residual dense block, the first 3 layers are followed by a leaky ReLU using a leakage factor of 0.1, while the last layer performs feature fusion. The residual dense block works in a local residual learning manner with a local skip connection at the end. Once dense features are extracted by the residual dense blocks, they are concatenated and fed to a feature fusion layer. Then, the optical flow F LD iâ†’j at this level is inferred by the subsequent flow estimation layer.</p><p>Level 2: Once the raw optical flow F LD iâ†’j is obtained from level 1, it is upscaled by a factor of 2. The upscaled flow F LDU iâ†’j is then used to warp I L i , resulting inÃŽ L iâ†’j . Next, I L iâ†’j , I L j and F LDU iâ†’j are concatenated and fed to a network module. Note that, this module at level 2 is similar to that at level 1, except that residual learning is used.</p><p>Level 3: The module at level 2 generates an optical flow F L iâ†’j with the same size as the LR input I L j . Therefore, the module at level 3 works as an SR part to infer the HR optical flow. The architecture at level 3 is similar to level 2 except that the flow estimation layer is replaced by a subpixel convolutional layer <ref type="bibr" target="#b30">[31]</ref> for resolution enhancement.</p><p>Although numerous networks for SR <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11]</ref> and optical flow estimation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10]</ref> can be found in litera- ture, our OFRnet is, to the best of our knowledge, the first unified network to integrate these two tasks. Note that, inferring HR optical flow from LR images is quite challenging, our OFRnet has demonstrated the potential of CNN to address this challenge. Our OFRnet is compact, with only 0.6M parameters. It is further demonstrated in Sec. 4.3 that the resulting HR optical flows benefit our video SR framework in both accuracy and consistency performance.</p><formula xml:id="formula_2">Space-to-depth 2 sH sW H i j F ï‚´ ï‚´ ï‚® ïƒ© ïƒ¹ ïƒ« ïƒ» 2 2 H W s H i j F ï‚´ ï‚´ ï‚® ïƒ© ïƒ¹ ïƒ« ïƒ» HR optical flow LR flow cube</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motion Compensation</head><p>Once HR optical flows are produced by OFRnet, spaceto-depth transformation is used to bridge the resolution gap between HR optical flows and LR frames. As illustrated in <ref type="figure" target="#fig_1">Fig. 4</ref>, regular LR grids are extracted from the HR flow and placed into the channel dimension to derive a flow cube with the same resolution as LR frames:</p><formula xml:id="formula_3">F H iâ†’j sHÃ—sW Ã—2 â†’ F H iâ†’j HÃ—W Ã—2s 2<label>(2)</label></formula><p>where H and W represent the size of the LR frame, s is the upscaling factor. Note that, the magnitude of optical flow is divided by a scalar s during the transformation to match the spatial resolution of LR frames.</p><p>Then, slices are extracted from the flow cube to warp the LR frame I LR i , resulting in multiple warped drafts:</p><formula xml:id="formula_4">C L iâ†’j = W(I L i , F H iâ†’j HÃ—W Ã—2s 2 )<label>(3)</label></formula><p>where W(Â·) denotes warping operation and C L iâ†’j âˆˆ R HÃ—W Ã—s <ref type="bibr" target="#b1">2</ref> represents the warped drafts after concatenation, namely draft cube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Super-resolution Net (SRnet)</head><p>After motion compensation, all the drafts are concatenated with the central LR frame, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. Then, the draft cube is fed to SRnet to infer the HR frame:</p><formula xml:id="formula_5">I SR 0 = Net SR (C L ; Î˜ SR )<label>(4)</label></formula><p>where I SR 0 is the super-resolved result of the central LR frame, C L âˆˆ R HÃ—W Ã—(2s 2 +1) represents the draft cube and Î˜ SR is the set of parameters.  As illustrated in <ref type="figure" target="#fig_2">Fig. 5</ref>, the draft cube is first passed to a feature extraction layer with 64 kernels, and then the output features are fed to 5 residual dense blocks (which are similar to our OFRnet). Here, we increase the number of layers to 5 and the growth rate to 64 for each residual dense block. Afterwards, we concatenate all the outputs of residual dense blocks and use a feature fusion layer to distillate the dense features. Finally, a sub-pixel layer is used to generate the HR frame.</p><p>The combination of densely connected layers and residual learning in residual dense blocks has been demonstrated to have a contiguous memory mechanism <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b8">9]</ref>. Therefore, we employ residual dense blocks in our SRnet to facilitate effective feature learning from preceding and current local features. Furthermore, feature reuse in the residual dense blocks improves the model compactness and stabilizes the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>We design two loss terms L OFR and L SR for OFRnet and SRnet, respectively. For the training of OFRnet, intermediate supervision is used at each level of the pyramid: is the regularization term to constrain the smoothness of the optical flow. We empirically set Î» 2 = 0.25 and Î» 1 = 0.125 to make our OFRnet focus on the last level. We also set Î» 3 = 0.01 as the regularization coefficient.</p><formula xml:id="formula_6">L OFR = iâˆˆ[âˆ’T, T ], i =0 L level3,i +Î» 2 L level2,i +Î» 1 L level1,i 2T (5) where ï£± ï£´ ï£´ ï£² ï£´ ï£´ ï£³ L level3,i = W(I H i , F H iâ†’0 )âˆ’I H 0 2 2 +Î» 3 âˆ‡F H iâ†’0 1 L level2,i = W(I L i , F L iâ†’0 )âˆ’I L 0 2 2 +Î» 3 âˆ‡F L iâ†’0 1 L level1,i = W(I LD i , F LD iâ†’0 )âˆ’I LD 0 2 2 +Î» 3 âˆ‡F LD iâ†’0 1<label>(6)</label></formula><p>For the training of SRnet, we use the widely applied mean square error (MSE) loss:</p><formula xml:id="formula_7">L SR = I SR 0 âˆ’ I H 0 2 2<label>(7)</label></formula><p>Finally, the total loss used for joint training is L = L SR + Î» 4 L OFR , where Î» 4 is empirically set to 0.01 to balance the two loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first conduct ablation experiments to evaluate our framework. Then, we further compare our framework to several existing video SR methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We collected 152 1080P HD video clips from the CDVL Database 1 and the Ultra Video Group Database 2 . The collected videos cover diverse natural and urban scenes. We used 145 videos from the CDVL Database as the training set, and 7 videos from the Ultra Video Group Database as the validation set. Following the configuration in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35]</ref>, we downsampled the video clips to the size of 540Ã—960 as the HR groundtruth using Matlab imresize function. In this paper, we only focus on the upscaling factor of 4 since it is the most challenging case. Therefore, the HR video clips were further downsampled to produce LR inputs of size 135 Ã— 240.</p><p>For fair comparison to the state-of-the-arts, we chose the widely used Vid4 benchmark dataset. We also used another 10 video clips from the DAVIS dataset <ref type="bibr" target="#b24">[25]</ref> for further comparison, which we refer to as DAVIS-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>, we converted input LR frames into YCbCR color space and only fed the luminance channel to our network. All metrics in this section are computed in the luminance channel. During the training phase, we randomly extracted 3 consecutive frames from an LR video clip, and randomly cropped a 32 Ã— 32 patch as the input. Meanwhile, its corresponding patch in HR video clip was cropped as groundtruth. Data augmentation was performed through rotation and reflection to improve the generalization ability of our network.</p><p>We implemented our framework in PyTorch. We applied the Adam solver <ref type="bibr" target="#b14">[15]</ref> with Î² 1 = 0.9, Î² 2 = 0.999 and batch size of 16. The initial learning rate was set to 10 âˆ’4 and reduced to half after every 50K iterations. We trained our network from scratch for 300K iterations. All experiments were conducted on a PC with an Nvidia GTX 970 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we present ablation experiments on the Vid4 dataset to justify our design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Network Variants</head><p>We proposed several variants of our SOF-VSR to perform ablation study. All the variants were re-trained for 300K iterations on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOF-VSR w/o OFRnet.</head><p>To handle complex motion patterns in video sequences, optical flow is used for motion compensation in our framework. To test the effectiveness of motion compensation for video SR, we removed the whole OFRnet and fed LR frames directly to our SRnet. Note that, replicated LR frames were used to match the dimension of the draft cube C L .</p><p>SOF-VSR w/o OFRnet level3 . The SR of optical flows provides accurate correspondences for video SR and improves the overall performance. To validate the effectiveness of HR optical flows, we removed the module at level 3 in our OFRnet. Specifically, the LR optical flows at level 2 were directly used for motion compensation and subsequent processing. To match the dimension of the draft cube, compensated LR frames were also replicated before feeding to SRnet.</p><p>SOF-VSR w/o OFRnet level3 + upsampling. Superresolving the optical flow can also be simply achieved using interpolation-based methods. However, our OFRnet can recover more reliable optical flow details. To demonstrate this, we removed the module at level 3 in our OFRnet, and upsampled the LR optical flows at level 2 using bilinear interpolation. Then, we used the modules in our original framework for subsequent processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Experimental Analyses</head><p>To test the accuracy of individual output image, we used PSNR/SSIM as metrics. To further test the consistency performance, we used the temporal motion-based video integrity evaluation index (T-MOVIE) <ref type="bibr" target="#b29">[30]</ref>. Besides, we used MOVIE <ref type="bibr" target="#b29">[30]</ref> and video quality measure with variable frame delay (VQM-VFD) <ref type="bibr" target="#b36">[37]</ref> for overall evaluation. The MOVIE and VQM-VFD metrics are correlated with human perception and widely applied in video quality assessment. Evaluation results of our original framework and the 3 variants achieved on the Vid4 dataset are shown in <ref type="table" target="#tab_4">Table 1</ref>.</p><p>Motion compensation. It can be observed from Table 1 that motion compensation plays a significant role in performance improvement. If OFRnet is removed, the PSNR/SSIM values are decreased from 26.01/0.771 to 25.80/0.760. Besides, the consistency performance is also degraded, with T-MOVIE value being increased to 20.08. That is because, it is difficult for SRnet to learn the nonlinear mapping between LR and HR images under complex motion patterns.</p><p>HR optical flow. If modules at levels 1 and 2 are introduced to generate LR optical flows for motion compensation, the PSNR/SSIM values are increased to 25.88/0.764. However, the performance is still inferior to our SOF-VSR method using HR optical flows. That is because, HR optical flows provide more accurate correspondences for performance improvement. If bilinear interpolation is used to    <ref type="bibr" target="#b7">[8]</ref> to compute the groundtruth optical flow. We used the average end-point error (EPE) for quantitative comparison, and present the results in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>It can be seen from <ref type="table" target="#tab_5">Table 2</ref> that the super-resolved optical flow significantly outperforms the upsampled optical flow, with an average EPE being reduced from 1.11 to 0.45. It demonstrates that the module at level 3 effectively recovers the correspondence details. <ref type="figure" target="#fig_3">Figure 6</ref> further illustrates the qualitative comparison on City and Walk. In the upsampled optical flow, we can roughly distinguish the outlines of the building and the pedestrian. In contrast, more distinct edges can be observed in the super-resolved optical flow, with finer details being recovered. Although some checkboard artifacts generated by the sub-pixel layer can also be observed <ref type="bibr" target="#b23">[24]</ref>, the resulting HR optical flow provides highly accurate correspondences for the video SR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons to the state-of-the-art</head><p>We first compared our framework to IDNnet <ref type="bibr" target="#b10">[11]</ref> (the latest state-of-the-art single image SR method) and several video SR methods including VSRnet <ref type="bibr" target="#b12">[13]</ref>, VESCPN <ref type="bibr" target="#b1">[2]</ref>, DRVSR <ref type="bibr" target="#b34">[35]</ref>, TDVSR <ref type="bibr" target="#b19">[20]</ref> and FRVSR <ref type="bibr" target="#b28">[29]</ref> on the Vid4 dataset. Then, we conducted comparative experiments on the DAVIS-10 dataset.</p><p>For IDNnet and VSRnet, we used the codes provided by the authors to produce the results. For DRVSR and TD-VSR, we used the output images provided by the authors. For VESCPN and FRVSR, the results reported in their papers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref> are used. Here, we report the performance of FRVSR-3-64 since its network size is comparable to our <ref type="table">Table 3</ref>. Comparison of accuracy and consistency performance achieved on the Vid4 dataset under Ã—4 configuration. Note that, the first and last two frames are not used in our evaluation since VSRnet and TDVSR do not produce outputs for these frames. Results marked with * are directly copied from the corresponding papers. Best results are shown in boldface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BI degradation model</head><p>BD degradation model IDNnet <ref type="bibr" target="#b10">[11]</ref> VSRnet <ref type="bibr" target="#b12">[13]</ref> VESCPN <ref type="bibr" target="#b1">[2]</ref> TDVSR <ref type="bibr" target="#b19">[20]</ref> SOF-VSR DRVSR <ref type="bibr" target="#b34">[35]</ref> FRVSR-3-64 <ref type="bibr">[</ref> SOF-VSR. Following <ref type="bibr" target="#b35">[36]</ref>, we crop borders of 6 + s for fair comparison. Note that, DRVSR and FRVSR are trained on a degradation model different from other networks. Specifically, the degradation model used in IDNnet, VSRnet, VESCPN and TDVSR is bicubic downsampling with Matlab imresize function (denoted as BI). However, in DRVSR and FRVSR, the HR images are first blurred using Gaussian kernel and then downsampled by selecting every s th pixel (denoted as BD). Consequently, we re-trained our framework on the BD degradation model (denoted as SOF-VSR-BD) for fair comparison to DRVSR and FRVSR.</p><p>Without optimization of the implementation, our SOF-VSR network takes about 250ms to generate an HR image of size 720Ã—576 under Ã—4 configuration on an Nvidia GTX 970 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Quantitative Evaluation</head><p>Quantitative results achieved on the Vid4 dataset and the DAVIS-10 dataset are shown in <ref type="table" target="#tab_6">Tables 3 and 4</ref>.</p><p>Evaluation on the Vid4 dataset. It can be observed from <ref type="table">Table 3</ref> that our SOF-VSR achieves the best performance for the BI degradation model in terms of all metrics. Specifically, the PSNR and SSIM values achieved by our framework are better than other methods by over 0.5 dB and 0.15 dB. That is because, more accurate correspondences can be provided by HR optical flows and therefore more reliable spatial details and temporal consistency can be well recovered.</p><p>For the BD degradation model, although the FRVSR-  3-64 method achieves higher SSIM, our SOF-VSR-BD method outperforms FRVSR-3-64 in terms of PSNR. Compared to the DRVSR method, PSNR, SSIM and T-MOVIE values achieved by our SOF-VSRBD method are improved by a notable margin, while a comparable performance is achieved in terms of MOVIE and VQM-VFD.</p><p>We further show the trade-off between consistency and accuracy in <ref type="figure" target="#fig_5">Fig. 7</ref>. It can be seen that our SOF-VSR and SOF-VSR-BD methods achieve the highest PSNR values, while maintaining superior T-MOVIE performance.</p><p>Evaluation on the DAIVIS-10 dataset. It is clear in <ref type="table" target="#tab_6">Table 4</ref> that our SOF-VSR and SOF-VSR-BD methods sur- <ref type="figure">Figure 8</ref>. Visual comparison of Ã—4 SR results on Calendar and City. Zoom-in regions from left to right: IDNnet <ref type="bibr" target="#b10">[11]</ref>, VSRnet <ref type="bibr" target="#b12">[13]</ref>, TDVSR <ref type="bibr" target="#b19">[20]</ref>, our SOF-VSR, DRVSR <ref type="bibr" target="#b34">[35]</ref> and our SOF-VSR-BD. IDNnet, VSRnet, TDVSR and SOF-VSR are based on the BI degradation model, while DRVSR and SOF-VSR-BD are based on the BD degradation model. <ref type="figure">Figure 9</ref>. Visual comparison of Ã—4 SR results on Boxing and Demolition. Zoom-in regions from left to right: IDNnet <ref type="bibr" target="#b10">[11]</ref>, VSRnet <ref type="bibr" target="#b12">[13]</ref>, our SOF-VSR, DRVSR <ref type="bibr" target="#b34">[35]</ref> and our SOF-VSR-BD. IDNnet, VSRnet and SOF-VSR are based on the BI degradation model, while DRVSR and SOF-VSR-BD are based on the BD degradation model. pass the state-of-the-arts for both the BI and BD degradation models in terms of all metrics. Since the DAVIS-10 dataset comprises scenes with fast moving objects, complex motion patterns (especially large displacements) lead to deterioration of existing video SR methods. In contrast, more accurate correspondences are provided by HR optical flows in our framework. Therefore, complex motion patterns can be handled more robustly and better performance can be achieved. <ref type="figure">Figure 8</ref> illustrates the qualitative results on two scenarios of the Vid4 dataset. It can be observed from the zoom-in regions that our framework recovers finer and more reliable details, such as the word "MAREE" and the stripes of the building. The qualitative comparison on the DAVIS-10 dataset (as shown in <ref type="figure">Fig. 9</ref>) also demonstrates the superior visual quality achieved by our framework. The pattern on the shorts, the word "PEUA" and the logo "CAT" are better recovered by our SOF-VSR and SOF-VSR-BD methods. <ref type="figure">Figure 1</ref> further shows the temporal profiles achieved on Calendar and City. It can be observed that the word "MAREE" can hardly be recognized by VSRnet in both image space and temporal profile. Although finer results are achieved by TDVSR, the building is still obviously distorted. In contrast, smooth and reliable patterns with fewer artifacts can be observed in temporal profiles of our results. In summary, our framework produces temporally more consistent results and better perceptual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Qualitative Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a deep end-to-end trainable video SR framework to super-resolve both images and optical flows. Our OFRnet first super-resolves the optical flows to provide accurate correspondences. Motion compensation is then performed based on HR optical flows and SRnet is used to infer the final results. Extensive experiments have demonstrated that our OFRnet can recover reliable correspondence details for the improvement of both accuracy and consistency performance. Comparison to existing video SR methods has shown that our framework achieves the stateof-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed framework. Our framework is fully convolutional and can be trained in an end-to-end manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of space-to-depth transformation. The spaceto-depth transformation folds an HR optical flow in LR space to generate an LR flow cube.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Architecture of our SRnet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Visual comparison of optical flow estimation results achieved on City and Walk under Ã—4 configuration. The super-resolved optical flow recovers fine correspondences, which are consistent with the groundtruth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Consistency and accuracy performance achieved on the Vid4 dataset under Ã—4 configuration. Dots and squares represent performance for BI and BD degradation models, respectively. Our framework achieves the best performance in terms of both PSNR and T-MOVIE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>here T denotes the temporal window size and âˆ‡F H iâ†’0 1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Comparative results achieved by our framework and its variants on the Vid4 dataset under Ã—4 configuration. Best results are shown in boldface.</figDesc><table><row><cell></cell><cell cols="2">PSNR(â†‘) SSIM(â†‘)</cell><cell>T-MOVIE(â†“) (Ã—10 âˆ’3 )</cell><cell>MOVIE(â†“) (Ã—10 âˆ’3 )</cell><cell>VQM-VFD(â†“)</cell></row><row><cell>SOF-VSR w/o OFRnet</cell><cell>25.80</cell><cell>0.760</cell><cell>20.08</cell><cell>4.54</cell><cell>0.240</cell></row><row><cell>SOF-VSR w/o OFRnet level3</cell><cell>25.88</cell><cell>0.764</cell><cell>19.95</cell><cell>4.48</cell><cell>0.235</cell></row><row><cell cols="2">SOF-VSR w/o OFRnet level3 + upsampling 25.86</cell><cell>0.763</cell><cell>19.92</cell><cell>4.50</cell><cell>0.231</cell></row><row><cell>SOF-VSR</cell><cell>26.01</cell><cell>0.771</cell><cell>19.78</cell><cell>4.32</cell><cell>0.227</cell></row><row><cell>EPE: 0.54</cell><cell>EPE: 0.43</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame t-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame t</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EPE: 1.43</cell><cell>EPE: 0.41</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame t-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame t</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Upsampled optical flow</cell><cell cols="3">Super-resolved optical flow</cell><cell></cell><cell>Groundtruth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Average EPE results achieved on the Vid4 dataset under Ã—4 configuration. Best results are shown in boldface.</figDesc><table><row><cell></cell><cell>Upsampled</cell><cell>Super-resolved</cell></row><row><cell></cell><cell>optical flow</cell><cell>optical flow</cell></row><row><cell>Calendar</cell><cell>0.85</cell><cell>0.39</cell></row><row><cell>City</cell><cell>1.17</cell><cell>0.49</cell></row><row><cell>Foliage</cell><cell>1.18</cell><cell>0.36</cell></row><row><cell>Walk</cell><cell>1.25</cell><cell>0.55</cell></row><row><cell>Average</cell><cell>1.11</cell><cell>0.45</cell></row><row><cell cols="3">upsample LR optical flows, no consistent improvement can</cell></row><row><cell cols="3">be observed. That is because, upsampling operation can-</cell></row><row><cell cols="3">not recover reliable correspondence details as the module at</cell></row><row><cell cols="3">level 3. To demonstrate this, we further compared the super-</cell></row><row><cell cols="3">resolved optical flow (output at level 3), upsampled opti-</cell></row><row><cell cols="3">cal flow (upsampling result of the output at level 2) to the</cell></row><row><cell cols="3">groundtruth. Since no groundtruth optical flow is available</cell></row><row><cell cols="3">for the Vid4 dataset, we used the method proposed by Hu et</cell></row><row><cell>al.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Comparative results achieved on the DAVIS-10 dataset under Ã—4 configuration. Best results are shown in boldface.</figDesc><table><row><cell>29]</cell><cell>SOF-VSR-BD</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">www.cdvl.org 2 ultravideo.cs.tut.fi</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pyramidal implementation of the lucas kanade feature tracker: Description of the algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Bouguet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Intel Corporation</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2848" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image upsampling via imposed edge statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optical flow based super-resolution: A probabilistic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fransens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="115" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno>12:1-12:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust interpolation of correspondences for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4791" to="4799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and accurate single image super-resolution via information distillation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5835" to="5843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simultaneous super-resolution of depth and images using a single camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video superresolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="531" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On bayesian adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="346" to="360" />
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust video superresolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2526" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning temporal dynamics for video super-resolution: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3432" to="3445" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Handling motion blur in multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5224" to="5232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A computationally efficient superresolution image reconstruction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="583" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Distill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<title level="m">The 2017 DAVIS challenge on video object segmentation. arXiv 1704.00675</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalizing the nonlocal-means to super-resolution reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="36" to="51" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2720" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Framerecurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Motion tuned spatiotemporal quality assessment of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2790" to="2798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Superresolution without explicit subpixel motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1958" to="1975" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4482" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">NTIRE 2017 challenge on single image superresolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video quality model for variable frame delay (VQM-VFD)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinson</surname></persName>
		</author>
		<idno>TM-11- 482</idno>
	</analytic>
	<monogr>
		<title level="j">US Dept. Commer., Nat. Telecommun. Inf. Admin</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
