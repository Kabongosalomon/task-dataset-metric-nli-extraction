<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Accurate Single Image Super-Resolution via Information Distillation Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hui</surname></persName>
							<email>zhenghui@aliyun.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University Xi&apos;an</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiumei</forename><surname>Wang</surname></persName>
							<email>wangxm@xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University Xi&apos;an</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
							<email>xbgao@mail.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University Xi&apos;an</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Accurate Single Image Super-Resolution via Information Distillation Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep convolutional neural networks (CNNs)   have been demonstrated remarkable progress on single image super-resolution. However, as the depth and width of the networks increase, CNN-based super-resolution methods have been faced with the challenges of computational complexity and memory consumption in practice. In order to solve the above questions, we propose a deep but compact convolutional network to directly reconstruct the high resolution image from the original low resolution image. In general, the proposed model consists of three parts, which are feature extraction block, stacked information distillation blocks and reconstruction block respectively. By combining an enhancement unit with a compression unit into a distillation block, the local long and short-path features can be effectively extracted. Specifically, the proposed enhancement unit mixes together two different types of features and the compression unit distills more useful information for the sequential blocks. In addition, the proposed network has the advantage of fast execution due to the comparatively few numbers of filters per layer and the use of group convolution. Experimental results demonstrate that the proposed method is superior to the state-of-the-art methods, especially in terms of time performance. Code is available at https://github.com/Zheng222/IDN-Caffe. Recently, due to the strength of deep convolutional neural network (CNN), many CNN-based SR methods try to train a deep network to gain better reconstruction performance. Kim et al. propose a 20-layer CNN model known as VDSR [12], which adopts residual learning and adaptive gradient clipping to ease training difficulty. To control the model parameters, the authors construct a deeply-recursive convolutional network (DRCN) [13] by adopting recursive layer. To mitigate training difficulty, Mao et al. propose a very deep residual encoder-decoder network (RED) [17], which consists of a series of convolutional and subsequent transposed convolution layers to learn end-to-end mappings from the LR images to the ground truths. Tai et al. propose a deep recursive residual network (DRRN) [22], which em-arXiv:1803.09454v1 [cs.CV]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super-resolution (SISR) is a classical problem in low-level computer vision, which reconstructs a high-resolution (HR) image from a low-resolution (LR) image. Actually, an infinite number of HR images can get the same LR image by downsampling. Hence, the SR problem is inherently ill-posed and no unique solution exists. In order to mitigate this problem, numerous SISR methods have been proposed in the literature, including interpolation-based methods, reconstruction-based methods <ref type="bibr">Figure 1</ref>. Speed and accuracy trade-off. The average PSNR and the average inference time for upscaling 3× on Set5. The IDN is faster than other methods and achieves the best performance at the same time. and example-based methods. Since the former two kinds of methods usually suffer dramatically drop in restoration performance with larger upscaling factors, the recent SR methods fall into the example-based methods which try to learn prior knowledge from LR and HR pairs. ploys parameters sharing strategy to alleviate the requirement of enormous parameters of the very deep networks.</p><p>Although achieving prominent performance, most of deep networks still have some drawbacks. Firstly, in order to achieve better performance, deepening or widening the network has been a design trend. But the result is that these methods demand large computational cost and memory consumption, which are less applicable in practice, such as mobile and embedded vision applications. Moreover, the traditional convolutional networks usually adopt cascaded network topologies, e.g., VDSR <ref type="bibr" target="#b11">[12]</ref> and DRCN <ref type="bibr" target="#b12">[13]</ref>. In this way, the feature maps of each layer are sent to the sequential layer without distinction. However, Hu et al. <ref type="bibr" target="#b8">[9]</ref> experimentally demonstrate that adaptively recalibrating channel-wise features responses can improve the representational power of a network.</p><p>To address these drawbacks, we propose a novel information distillation network (IDN) with lightweight parameters and computational complexity as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. In the proposed IDN, a feature extraction block (FBlock) first extracts features from the LR image. Then, multiple information distillation blocks (DBlocks) are stacked to progressively distill residual information. Finally, a reconstruction Block (RBlock) aggregates the obtained HR residual representations to generate the residual image. To get a HR image, we implement an element-wise addition operation on the residual image and the upsampled LR image.</p><p>The key component of IDN is the information distillation block, which contains an enhancement unit and a compression unit. The enhancement unit mainly comprises two shallow convolutional networks as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. Each of them is a three-layer shallow module. The feature maps of the first module are extracted through a short path <ref type="bibr">(3-layer)</ref>. Thus, they can be regarded as the local short-path features. Considering that the deep networks have more expressive power, we send a portion of the local short-path features to another module. By this way, the feature maps of the second module naturally become the local long-path features. Different from the approach in <ref type="bibr" target="#b8">[9]</ref>, we divide feature maps into two parts. One part represents reserved shortpath features and another expresses the short-path features that will be enhanced. After getting long and short-path feature maps, we aggregate these two types of features for gaining more abundant and efficient information. In summary, the enhancement unit is mainly to improve the representation power of the network. As for the compression unit, we adopt a simple convolutional layer to compress the redundancy information in features of the enhancement unit.</p><p>The main contributions of this work are summarized as follows:</p><p>• The proposed IDN extracts feature maps directly from LR images and employs multiple cascaded DBlocks to generate the residual representations in HR space. In each DBlock, the enhancement unit gathers more information as much as possible and the compression unit distills more useful information. As a result, IDN achieves competitive results in spite of using less number of convolutional layer.</p><p>• Due to the concise structure of the proposed IDN, it is much faster than several CNN-based SR methods, e.g., VDSR <ref type="bibr" target="#b11">[12]</ref>, DRCN <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet <ref type="bibr" target="#b22">[23]</ref> as illustrated in <ref type="figure">Figure 1</ref>. Only the proposed method achieves real-time speed and maintains better reconstruction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single image super-resolution has been extensively studied in these years. In this section, we will focus on recent example-based and neural network based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Self-example based methods</head><p>The self-example based methods exploit the selfsimilarity property and extract example pairs merely from the LR image across different scales. This type of methods usually works well in the images containing repetitive patterns and textures but lacks the richness of image structures outside the input image and thus fails to generate satisfactory prediction for images of other classes. Huang et al. <ref type="bibr" target="#b9">[10]</ref> extend self-similarity based SR to handle the affine and perspective deformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">External-example based methods</head><p>The external-example based methods learn a mapping between LR and HR patches from external datasets. This type of approaches usually focuses on how to learn a compact dictionary or manifold space to relate LR/HR patches, such as nearest neighbor <ref type="bibr" target="#b6">[7]</ref>, manifold embedding <ref type="bibr" target="#b1">[2]</ref>, random forest <ref type="bibr" target="#b19">[20]</ref> and sparse representation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. While these approaches are effective, the extracted features and mapping functions are not adaptive, which may not be optimal for generating high-quality SR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Convolutional neural networks based methods</head><p>Recently, inspired by the achievement of many computer vision tasks tackled with deep learning, neural networks have been achieved dramatic improvement in SR. Dong et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> first exploit a three-layer convolutional neural network, named SRCNN, to jointly optimize the feature extraction, non-linear mapping and image reconstruction stages in an end-to-end manner. Afterwards Shi et al. <ref type="bibr" target="#b20">[21]</ref> propose an efficient sub-pixel convolutional neural network (ESPCN), which extracts feature maps in the LR space and replaces the bicubic upsampling operation with an efficient sub-pixel convolution. Dong et al. <ref type="bibr" target="#b4">[5]</ref> adopt deconvolution to accelerate SRCNN in combination with smaller filter sizes and more convolution layers. Kim et al. <ref type="bibr" target="#b11">[12]</ref> propose a very deep CNN model with global residual architecture to achieve superior performance, which utilizes contextual information over large image regions. Another network designed by Kim et al. <ref type="bibr" target="#b12">[13]</ref>, which has recursive convolution with skip connection to avoid introducing additional parameters when the depth is increasing. Mao et al. <ref type="bibr" target="#b16">[17]</ref> tackle the general image restoration problem with encoderdecoder networks and symmetric skip connections. Lai et al. <ref type="bibr" target="#b14">[15]</ref> propose the laplacian pyramid super-resolution network (LapSRN) to address the speed and accuracy of SR problem, which takes the original LR images as input and progressively reconstructs the sub-band residuals of HR images. Tai et al. <ref type="bibr" target="#b21">[22]</ref> propose the deep recursive residual network to effectively build a very deep network structure for SR, which weighs the model parameters against the accuracy. The authors also present a very deep end-to-end persistent memory network (MemNet) <ref type="bibr" target="#b22">[23]</ref> for image restoration task, which tackles the long-term dependency problem in the previous CNN architectures. Sajjadi et al. <ref type="bibr" target="#b18">[19]</ref> propose a novel combination of automated texture synthesis with a perceptual loss focusing on creating realistic textures at a high magnification ratio of 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we first describe the proposed model architecture and then suggest the enhancement unit and the compression unit, which are the core of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network structure</head><p>The proposed IDN, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, consists of three parts: a feature extraction block (FBlock), multiple stacked information distillation blocks (DBlocks) and a reconstruction block (RBlock). Here, we denote x and y as the input and the output of IDN. With respect to FBlock, two 3 × 3 convolutional layers are utilized to extract the feature maps from the original LR image. This procedure can be expressed as</p><formula xml:id="formula_0">B 0 = f (x) ,<label>(1)</label></formula><p>where f represents the feature extraction function and B 0 denotes the extracted features and servers as the input to the following stage. The next part is composed of multiple information distillation blocks by using chained mode. Each block contains an enhancement unit and a compression unit with stacked style. This process can be formulated as</p><formula xml:id="formula_1">B k = F k (B k−1 ) , k = 1, · · · , n,<label>(2)</label></formula><p>where F k denotes the k-th DBlock function, B k−1 and B k indicate the input and output of the k-th DBlock respectively. Finally, we take a transposed convolution without activation function as the RBlock. Hence, the IDN can be expressed as</p><formula xml:id="formula_2">y = R (F n (B n−1 )) + U (x) ,<label>(3)</label></formula><p>where R, U denote the RBlock and bicubic interpolation operation respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Loss function</head><p>We consider two loss functions that measure the difference between the predicted HR imageÎ and the corresponding ground-truth I. The first one is mean square error (MSE), which is the most widely used loss function for general image restoration as defined below:</p><formula xml:id="formula_3">l M SE = 1 N N i=1 I i −Î i 2 2 .<label>(4)</label></formula><p>However, Lim et al. <ref type="bibr" target="#b15">[16]</ref> experimentally demonstrate that training with MSE loss is not a good choice. The second loss function is mean absolute error (MAE), which is formulated as follows:</p><formula xml:id="formula_4">l M AE = 1 N N i=1 I i −Î i 1 .<label>(5)</label></formula><p>We empirically found that our model with MSE loss can improve performance of a trained network with MAE loss. Therefore, we first train the network with MAE loss and then fine-tune it by MSE loss.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Enhancement unit</head><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, enhancement unit can be roughly divided into two modules, one is the above three convolutions and another is the below three convolutions. The above module has three 3 × 3 convolutions, each of them is followed by a leaky rectified linear unit (LReLU) activation function, which is omitted here. Let's denote the feature map dimensions of the i-th layer as D i (i = 1, · · · , 6). In that way, the relationship of the convolutional layers can be expressed as</p><formula xml:id="formula_5">D 3 − D 1 = D 1 − D 2 = d,<label>(6)</label></formula><p>where d denotes the difference between the first layer and the second layer or between the first layer and the third layer. Similarly, the dimension of channels in the below module also has this relation and can be described as follows:</p><formula xml:id="formula_6">D 6 − D 4 = D 4 − D 5 = d,<label>(7)</label></formula><p>where D 4 = D 3 . The above module is composed of three cascaded convolution layers with LReLUs, and the output of the third convolution layer is sliced into two segments. Supposing the input of this module is B k−1 , we have</p><formula xml:id="formula_7">P k 1 = C a (B k−1 ) ,<label>(8)</label></formula><p>where B k−1 denotes the output of previous block and meanwhile is the input of present block, C a indicates chained convolutions operation and P k 1 is the output of the above module in the k-th enhancement unit. The feature maps with D3 s dimensions of P k 1 and the input of the first convolutional layer are concatenated in the channel dimension,</p><formula xml:id="formula_8">R k = C S P k 1 , 1/s , B k−1 ,<label>(9)</label></formula><p>where C, S represent concatenation operation and slice operation respectively. Specifically, we know the dimension of P k 1 is D 3 . Therefore, S P k 1 , 1/s denotes that D3 s dimensions features are fetched from P k 1 . Moreover, S P k 1 , 1/s concatenates features with B k−1 in channel dimension. The purpose is to combine the previous information with some current information. It can be regarded as partially retained local short-path information. We take the rest of local shortpath information as the input of the below module, which mainly further extracts long-path feature maps,</p><formula xml:id="formula_9">P k 2 = C b S P k 1 , 1 − 1/s ,<label>(10)</label></formula><p>where P k 2 , C b are the output and stacked convolution operations of the below module respectively. Finally, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the input information, the reserved local shortpath information and the local long-path information are aggregated. Therefore, the enhancement unit can be formulated as</p><formula xml:id="formula_10">P k = P k 2 + R k = C b (S (C a (B k−1 ) , 1 − 1/s)) +C (S (C a (B k−1 ) , 1/s) , B k−1 ) ,<label>(11)</label></formula><p>where P k is the output of enhancement unit. At this point, local long-path features P k 2 and the combination of local short-path features and the untreated features R k are utilized without exception by a compression unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Compression unit</head><p>We achieve compression mechanism by taking advantage of a 1 × 1 convolution layer. Concretely, the outputs of the enhancement unit are sent to a 1 × 1 convolution layer, which acts as dimensionality reduction or distilling relevant information for the later network. Thus, the compression unit can be formulated as</p><formula xml:id="formula_11">B k = f k F P k = α k F W k F P k ,<label>(12)</label></formula><p>where f k F denotes the function of the 1×1 convolution layer (α k F denotes the activation function and W k F is the weight parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Training datasets</head><p>By following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, we use 91 images from Yang et al. <ref type="bibr" target="#b25">[26]</ref> and 200 images from Berkeley Segmentation Dataset (BSD) <ref type="bibr" target="#b17">[18]</ref> as the training data. As in <ref type="bibr" target="#b21">[22]</ref>, to make full use of the training data, we apply data augmentation in three ways: (1) Rotate the images with the degree of 90 • , 180 • and 270 • . (2) Flip images horizontally. (3) Downscale the images with the factor of 0.9, 0.8, 0.7 and 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Testing datasets</head><p>The proposed method is evaluated on four widely used benchmark datasets: Set5 <ref type="bibr" target="#b0">[1]</ref>, Set14 <ref type="bibr" target="#b26">[27]</ref>, BSD100 <ref type="bibr" target="#b17">[18]</ref>, Urban100 <ref type="bibr" target="#b9">[10]</ref>. Among these datasets, Set5, Set14 and BSD100 consist of natural scenes and Urban100 contains challenging urban scenes images with details in different frequency bands. The ground truth images are downscaled by bicubic interpolation to generate LR/HR image pairs for both training and testing datasets. We convert each color image into the YCbCr color space and only process the Ychannel, while color components are simply enlarged using bicubic interpolation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>For preparing the training samples, we first downsample the original HR images with upscaling factor m (m = 2, 3, 4) by using the bicubic interpolation to generate the corresponding LR images and then crop the LR training images into a set of l sub ×l sub size sub-images. The corresponding HR training images are divided into ml sub × ml sub size sub-images. As the proposed model is trained using the Caffe package <ref type="bibr" target="#b10">[11]</ref>, its transposed convolution filters will generate the output with size (ml sub − m + 1) 2 instead of (ml sub ) 2 . So we should crop (m − 1)-pixel borders on the HR sub-images. Since the minimum size picture "t20" in the 291 dataset is a 78 × 78 size image, the maximum size of the sub-image we can crop on the LR image is 26 × 26 for maintaining data integrity when scaling factor m = 3. However, the training process will be unstable due to the larger size training samples equipped with the larger learning rate by using Caffe package. Therefore, 15 2 /43 2 training pairs are generated for training stage and 26 2 /76 2 LR/HR sub-images pairs are utilized for fine-tuning phase. The learning rate is initially set to 1e − 4 and decreases by the factor of 10 during fine-tuning phase. In this way, the sizes of training and fine-tuning samples are shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Taking into account the trade-off between the execution time and the reconstruction performance, we construct a 31-layer network that denoted as IDN. This model has 4 DBlocks, and the parameters D 3 , d and s of enhancement unit in each block are set to 64, 16 and 4 respectively. To reduce the parameters of network, we use the grouped convolution layer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref> in the second and fourth layers in each enhancement unit with 4 groups. In addition, the transposed convolution adopts 17 × 17 filters for all scaling factors and the negative scope of LReLU is set as 0.05. We initialize the weights by using the method proposed in <ref type="bibr" target="#b7">[8]</ref> and the biases are set to zero. The proposed network is optimized using Adam <ref type="bibr" target="#b13">[14]</ref>. We set the parameters of mini-batch size and weight decay to 64 and 1e − 4 respectively. In order to get better initialization parameters, we empirically pre-train the proposed model with 10 5 iterations and take these parameters as the initial values of the IDN. Training a IDN roughly takes a day with a TITAN X GPU on the 2× model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Network analysis</head><p>The proposed model with a global residual structure mainly learns a residual image. As show in <ref type="figure">Figure 4(a)</ref>, the ground truth residual image mainly contains details and texture information and its normalized pixel value ranges from -0.4 to 0.5. From <ref type="figure">Figure 4(b)</ref>, we find that there are positive and negative values in the residual image, and the number of positive pixels is intuitively similar to that of negative ones. Obviously, the number of zero value and its neighbors is the  <ref type="table">Table 3</ref>. Average IFCs for scale 2×, 3× and 4×. Red color indicates the best and blue color indicates the second best performance. most, which suggests that smooth region in residual image is almost eliminated. Therefore, the task of our network is to gradually subtract the smooth area of the original input image. In order to verify our intuition, we need inspect the outputs of enhancement and compression units. For better visualizing the intermediary of the proposed model, we consider an operation T that can transform a 3D tensor A to a flattened 2D tensor defined over the spatial dimensions, which can be formulated as follows:</p><formula xml:id="formula_12">T : R c×h×w → R h×w .<label>(13)</label></formula><p>Specifically, in this work, we will consider the mean of the feature maps in channel dimension, which can be described by</p><formula xml:id="formula_13">T mean (A) = 1 c c i=1 A i ,<label>(14)</label></formula><p>where A i = A (i, :, :) (using Matlab notation). The average feature map can roughly represent the situations of the whole feature maps. To explore the functions of enhancement unit and compression unit, we visualize the outputs of each enhancement unit and compression unit by utilizing above-mentioned method. As illustrated in <ref type="figure">Figure 5(a)</ref>, from the first subpicture to the third subpicture, average feature maps gradually reduce the pixel values, especially in smooth areas. According to <ref type="figure">Figure 5</ref>(a), we can easily see that the first subfigure holds larger pixel values but has rough outline of the butterfly. The second and the third subfigures show that the later enhancement units continue decreasing the pixel values to obtain the features with a relatively clear contour profile. In addition, the last subfigure obviously surpasses the former figures, which brings the better inputs for the sequential compression unit that directly connects to RBlock. In summary, the function of the enhancement unit mainly enhances the outline areas of input LR image. As for the effect of compression unit, comparing <ref type="figure">Figure 5</ref>  <ref type="figure">Figure 5</ref>(a), we can see some regions of the average feature map of compression unit are enhanced by the following enhancement unit. This indicates that the process of the first three stacked blocks is to reduce the pixel value as a whole, while the last block greatly enhances the contrast between the contour and the  <ref type="figure">Figure 6</ref>. The "barbara" image from the Set14 dataset with an upscaling factor 4. smooth areas.</p><p>The RBlock, a transposed convolution layer, assembles the output of the final DBlock to generate the residual image. The bias term of this transposed convolution can automatically adjust the central value of the residual image data distribution to approach the ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with state-of-the-arts</head><p>We compare the proposed method with other SR methods, including bicubic, SRCNN <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, VDSR <ref type="bibr" target="#b11">[12]</ref>, DRCN <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b14">[15]</ref>, DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet <ref type="bibr" target="#b22">[23]</ref>. <ref type="table" target="#tab_2">Table 2</ref> shows the average peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) values on four benchmark datasets. The proposed method performs favorably against state-of-the-art results on most datasets. In addition, we measure all methods with information fidelity criterion (IFC) metric, which assesses the image quality based on natural scene statistics and correlates well with human perception of image super-resolution. <ref type="table">Table 3</ref> shows the proposed method achieves the best performance and outperforms MemNet <ref type="bibr" target="#b22">[23]</ref> by a considerable margin. <ref type="figure" target="#fig_5">Figure 6, 7 and 8</ref> show visual comparisions. The "barbara" image has serious artifacts in the read box due to the loss of high frequency information, which can be seen from the result of bicubic interpolation. Only the proposed method recovers roughly the outline of several stacked books as shown in <ref type="figure">Figure 6</ref>. From <ref type="figure" target="#fig_5">Figure 7</ref>, we can obviously see that the proposed method gains clearer contour without serious artifacts while other methods have different degrees of the fake information. In <ref type="figure" target="#fig_6">Figure 8</ref>, the building structure on image "img085" of Urban100 dataset is relatively clear in the proposed method.</p><p>From <ref type="table" target="#tab_2">Table 2</ref>, the performance of the proposed IDN is lower than that of MemNet in Urban100 dataset and 3×,  4× scale factors, while our IDN can achieve slightly better performance in other benchmark datasets. The main reason is that MemNet takes an interpolated LR image as its input so that more information is fed into the network and the process of the SR only needs to correct the interpolated image. The algorithms that take the original LR image as input demand predicting more pixels from scratch, especially in larger images and larger magnification factors.</p><p>As for inference time, we use the public codes of the compared algorithms to evaluate the runtime on the machine with 4.2GHz Intel i7 CPU (32G RAM) and Nvidia TITAN X (Pascal) GPU (12G memory). Since we note that official implementations of MemNet and DRRN have the condition of out of the GPU memory when testing the images in BSD100 and Urban100 datasets, we divide 100 images into several parts and evaluate on these parts and then collect them for these two datasets. <ref type="table">Table 4</ref> shows the aver-age execution time on four benchmark datasets. It is noteworthy that the proposed IDN is approximately 500 times faster than MemNet <ref type="bibr" target="#b22">[23]</ref> with 2× magnification on the Ur-ban100 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a novel network that employs distillation blocks to gradually extract abundant and efficient features for the reconstruction of HR images. The proposed approach achieves competitive results on four benchmark datasets in terms of PSNR, SSIM and IFC. Meanwhile the inference time substantially exceeds the state-of-the-art methods such as DRRN <ref type="bibr" target="#b21">[22]</ref> and MemNet <ref type="bibr" target="#b22">[23]</ref>. This compact network will be more widely applicable in practice. In the future, this approach of image super-resolution will be explored to facilitate other image restoration problems such as denosing and compression artifacts reduction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of the proposed network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The architecture of enhancement unit in the proposed model. Orange circle represents slice operation and purple circle indicates concatenation operation in channel dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>The residual image and its data distribution of the "butterfly" image from Set5 dataset.(a) the average feature maps of enhancement units (b) the average feature maps of compression units Visualization of the average feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) with Figure 5(b), we find that the pixel values of features are mapped into a smaller range through the compression unit. From the second subfigure in Figure 5(b) and the third subfigure in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>(a) Original (PSNR/SSIM/IFC) (b) Bicubic (28.50/0.8285/2.638) (c) VDSR (29.54/0.8651/3.388) (d) DRCN (30.28/0.8653/3.323) (e) LapSRN (30.10/0.8710/3.432) (f) DRRN (29.74/0.8671/3.509) (g) MemNet (30.14/0.8697/3.563) (h) IDN (30.40/0.8715/3.703) The "8023" image from the BSD100 dataset with an upscaling factor 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>(a) Original (PSNR/SSIM/IFC) (b) Bicubic (25.90/0.8365/1.976) (c) VDSR (27.14/0.8771/2.480) (d) DRCN (27.15/0.8761/2.442) (e) LapSRN (27.11/0.8809/2.593) (f) DRRN (26.65/0.8739/2.399) (g) MemNet (26.83/0.8750/2.444) (h) IDN (27.26/0.8824/2.705) The "img085" image from the Urban100 dataset with an upscaling factor 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The sizes of training and fine-tuning sub-images for different scaling factors.</figDesc><table><row><cell cols="3">Scale Training Fine-tuning</cell></row><row><cell>2</cell><cell>29 2 /57 2</cell><cell>39 2 /77 2</cell></row><row><cell>3</cell><cell>15 2 /43 2</cell><cell>26 2 /76 2</cell></row><row><cell>4</cell><cell>11 2 /41 2</cell><cell>19 2 /73 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>.9299 37.53/0.9587 37.63/0.9588 37.52/0.9591 37.74/0.9591 37.78/0.9597 37.83/0.9600 ×3 30.39/0.8682 33.66/0.9213 33.82/0.9226 33.81/0.9220 34.03/0.9244 34.09/0.9248 34.11/0.9253 ×4 28.42/0.8104 31.35/0.8838 31.53/0.8854 31.54/0.8852 31.68/0.8888 31.74/0.8893 31.82/0.8903 Set14 ×2 30.24/0.8688 33.03/0.9124 33.04/0.9118 32.99/0.9124 33.23/0.9136 33.28/0.9142 33.30/0.9148 Average PSNR/SSIMs for scale 2×, 3× and 4×. Red color indicates the best and blue color indicates the second best performance.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Scale</cell><cell>Bicubic</cell><cell>VDSR [12]</cell><cell>DRCN [13]</cell><cell>LapSRN [15]</cell><cell>DRRN [22]</cell><cell cols="2">MemNet [23]</cell><cell>IDN (Ours)</cell></row><row><cell>Set5</cell><cell cols="9">×2 33.66/0×3 27.55/0.7742 29.77/0.8314 29.76/0.8311 29.79/0.8325 29.96/0.8349 30.00/0.8350</cell><cell>29.99/0.8354</cell></row><row><cell></cell><cell>×4</cell><cell cols="8">26.00/0.7027 28.01/0.7674 28.02/0.7670 28.09/0.7700 28.21/0.7721 28.26/0.7723</cell><cell>28.25/0.7730</cell></row><row><cell></cell><cell>×2</cell><cell cols="8">29.56/0.8431 31.90/0.8960 31.85/0.8942 31.80/0.8952 32.05/0.8973 32.08/0.8978</cell><cell>32.08/0.8985</cell></row><row><cell>BSD100</cell><cell>×3</cell><cell cols="8">27.21/0.7385 28.82/0.7976 28.80/0.7963 28.82/0.7980 28.95/0.8004 28.96/0.8001</cell><cell>28.95/0.8013</cell></row><row><cell></cell><cell>×4</cell><cell cols="8">25.96/0.6675 27.29/0.7251 27.23/0.7233 27.32/0.7275 27.38/0.7284 27.40/0.7281</cell><cell>27.41/0.7297</cell></row><row><cell></cell><cell>×2</cell><cell cols="8">26.88/0.8403 30.76/0.9140 30.75/0.9133 30.41/0.9103 31.23/0.9188 31.31/0.9195</cell><cell>31.27/0.9196</cell></row><row><cell>Urban100</cell><cell>×3</cell><cell cols="8">24.46/0.7349 27.14/0.8279 27.15/0.8276 27.07/0.8275 27.53/0.8378 27.56/0.8376</cell><cell>27.42/0.8359</cell></row><row><cell></cell><cell>×4</cell><cell cols="8">23.14/0.6577 25.18/0.7524 25.14/0.7510 25.21/0.7562 25.44/0.7638 25.50/0.7630</cell><cell>25.41/0.7632</cell></row><row><cell cols="2">Dataset</cell><cell cols="7">Scale Bicubic VDSR [12] DRCN [13] LapSRN [15] DRRN [22] MemNet [23]</cell><cell>IDN (Ours)</cell></row><row><cell></cell><cell></cell><cell>×2</cell><cell>6.083</cell><cell>8.580</cell><cell>8.783</cell><cell>9.010</cell><cell>8.670</cell><cell>8.850</cell><cell>9.252</cell></row><row><cell>Set5</cell><cell></cell><cell>×3</cell><cell>3.580</cell><cell>5.203</cell><cell>5.336</cell><cell>5.194</cell><cell>5.394</cell><cell>5.503</cell><cell>5.620</cell></row><row><cell></cell><cell></cell><cell>×4</cell><cell>2.329</cell><cell>3.542</cell><cell>3.543</cell><cell>3.559</cell><cell>3.700</cell><cell>3.787</cell><cell>3.826</cell></row><row><cell></cell><cell></cell><cell>×2</cell><cell>6.105</cell><cell>8.159</cell><cell>8.370</cell><cell>8.501</cell><cell>8.280</cell><cell>8.469</cell><cell>8.839</cell></row><row><cell>Set14</cell><cell></cell><cell>×3</cell><cell>3.473</cell><cell>4.691</cell><cell>4.782</cell><cell>4.662</cell><cell>4.870</cell><cell>4.958</cell><cell>5.062</cell></row><row><cell></cell><cell></cell><cell>×4</cell><cell>2.237</cell><cell>3.106</cell><cell>3.098</cell><cell>3.145</cell><cell>3.249</cell><cell>3.309</cell><cell>3.354</cell></row><row><cell></cell><cell></cell><cell>×2</cell><cell>5.619</cell><cell>7.494</cell><cell>7.577</cell><cell>7.715</cell><cell>7.513</cell><cell>7.665</cell><cell>7.931</cell></row><row><cell cols="2">BSD100</cell><cell>×3</cell><cell>3.138</cell><cell>4.151</cell><cell>4.184</cell><cell>4.057</cell><cell>4.235</cell><cell>4.300</cell><cell>4.398</cell></row><row><cell></cell><cell></cell><cell>×4</cell><cell>1.978</cell><cell>2.679</cell><cell>2.633</cell><cell>2.677</cell><cell>2.746</cell><cell>2.778</cell><cell>2.837</cell></row><row><cell></cell><cell></cell><cell>×2</cell><cell>6.245</cell><cell>8.629</cell><cell>8.959</cell><cell>8.907</cell><cell>8.889</cell><cell>9.122</cell><cell>9.594</cell></row><row><cell cols="2">Urban100</cell><cell>×3</cell><cell>3.620</cell><cell>5.159</cell><cell>5.314</cell><cell>5.156</cell><cell>5.440</cell><cell>5.560</cell><cell>5.676</cell></row><row><cell></cell><cell></cell><cell>×4</cell><cell>2.361</cell><cell>3.462</cell><cell>3.465</cell><cell>3.530</cell><cell>3.669</cell><cell>3.786</cell><cell>3.789</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>.</head><label></label><figDesc>Comparison the running time (sec) on the 4 benchmark datasets with scale factors 2×, 3× and 4×. Red color indicates the fastest algorithm and blue color indicates the second fastest method. Our IDN achieves the best time performance.</figDesc><table><row><cell>Dataset</cell><cell cols="6">Scale VDSR [12] DRCN [13] LapSRN [15] DRRN [22] MemNet [23]</cell><cell>IDN (Ours)</cell></row><row><cell></cell><cell>×2</cell><cell>0.054</cell><cell>0.735</cell><cell>0.032</cell><cell>4.343</cell><cell>5.715</cell><cell>0.016</cell></row><row><cell>Set5</cell><cell>×3</cell><cell>0.062</cell><cell>0.748</cell><cell>0.049</cell><cell>4.380</cell><cell>5.761</cell><cell>0.011</cell></row><row><cell></cell><cell>×4</cell><cell>0.054</cell><cell>0.735</cell><cell>0.040</cell><cell>4.450</cell><cell>5.728</cell><cell>0.009</cell></row><row><cell></cell><cell>×2</cell><cell>0.113</cell><cell>1.579</cell><cell>0.035</cell><cell>8.540</cell><cell>12.031</cell><cell>0.025</cell></row><row><cell>Set14</cell><cell>×3</cell><cell>0.122</cell><cell>1.569</cell><cell>0.061</cell><cell>8.298</cell><cell>11.543</cell><cell>0.014</cell></row><row><cell></cell><cell>×4</cell><cell>0.112</cell><cell>1.526</cell><cell>0.040</cell><cell>8.540</cell><cell>11.956</cell><cell>0.010</cell></row><row><cell></cell><cell>×2</cell><cell>0.071</cell><cell>0.983</cell><cell>0.018</cell><cell>4.430</cell><cell>5.875</cell><cell>0.015</cell></row><row><cell>BSD100</cell><cell>×3</cell><cell>0.071</cell><cell>0.996</cell><cell>0.037</cell><cell>4.430</cell><cell>5.897</cell><cell>0.009</cell></row><row><cell></cell><cell>×4</cell><cell>0.071</cell><cell>0.984</cell><cell>0.023</cell><cell>4.373</cell><cell>5.887</cell><cell>0.007</cell></row><row><cell></cell><cell>×2</cell><cell>0.451</cell><cell>5.010</cell><cell>0.082</cell><cell>26.699</cell><cell>35.871</cell><cell>0.062</cell></row><row><cell>Urban100</cell><cell>×3</cell><cell>0.514</cell><cell>5.054</cell><cell>0.122</cell><cell>26.693</cell><cell>35.803</cell><cell>0.034</cell></row><row><cell></cell><cell>×4</cell><cell>0.448</cell><cell>5.048</cell><cell>0.100</cell><cell>26.702</cell><cell>37.404</cell><cell>0.022</cell></row><row><cell cols="8">Table 4(a) Original (PSNR/SSIM/IFC) (b) Bicubic (25.15/0.6863/2.553) (c) VDSR (25.79/0.7403/3.473) (d) DRCN (25.82/0.7339/3.466)</cell></row><row><cell cols="8">(e) LapSRN (25.77/0.7430/3.546) (f) DRRN (25.74/0.7403/3.589) (g) MemNet (25.61/0.7399/3.639) (h) IDN (25.84/0.7442/3.749)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Francois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Examplebased super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Caffe: convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3791" to="3799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3147" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3147" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image superresolution as sparse representation of raw image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
