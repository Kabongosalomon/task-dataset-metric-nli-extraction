<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DenseRaC: Joint 3D Pose and Shape Estimation by Dense Render-and-Compare</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook Reality Labs</orgName>
								<address>
									<settlement>Sausalito</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
							<email>sczhu@stat.ucla.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
							<email>tony.tung@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook Reality Labs</orgName>
								<address>
									<settlement>Sausalito</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DenseRaC: Joint 3D Pose and Shape Estimation by Dense Render-and-Compare</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present DenseRaC, a novel end-to-end framework for jointly estimating 3D human pose and body shape from a monocular RGB image. Our two-step framework takes the body pixel-to-surface correspondence map (i.e., IUV map) as proxy representation and then performs estimation of parameterized human pose and shape. Specifically, given an estimated IUV map, we develop a deep neural network optimizing 3D body reconstruction losses and further integrating a render-and-compare scheme to minimize differences between the input and the rendered output, i.e., dense body landmarks, body part masks, and adversarial priors. To boost learning, we further construct a large-scale synthetic dataset (MOCA) utilizing web-crawled Mocap sequences, 3D scans and animations. The generated data covers diversified camera views, human actions and body shapes, and is paired with full ground truth. Our model jointly learns to represent the 3D human body from hybrid datasets, mitigating the problem of unpaired training data. Our experiments show that DenseRaC obtains superior performance against state of the art on public benchmarks of various humanrelated tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Though much progress has been made in human pose estimation, body segmentation and action recognition, it remains underexplored to leverage such estimations into the 3D world, due to the difficulty in data acquisition, ambiguities from monocular inputs and nuisances in natural images (e.g., illumination, occlusion, texture). Existing learningbased methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">55]</ref> heavily rely on sparse 2D/3D landmarks (i.e., skeleton joints), body part masks or silhouettes. However, it is ambiguous to recover 3D human pose and body shape from such limited information.</p><p>In this paper we propose DenseRaC, a new framework for 3D human pose and body shape estimation from monocular RGB image, as illustrated in <ref type="figure">Fig. 2:</ref> • The task is solved in a two-step framework, first by estimating pixel-to-surface correspondences (i.e., IUV images) from the RGB inputs, and then by leveraging the estimated IUV images into 3D human pose and body shape. <ref type="bibr">Figure 1</ref>. DenseRaC estimates 3D human poses and body shapes given people-in-the-wild images. The proposed framework handles scenarios with multiple people, all genders, and various clothing in real time. Here, we show results on Internet images <ref type="bibr" target="#b0">[1]</ref>.</p><p>• A parametric human pose and body shape representation is integrated into the forward pass and backward propagation, inspired by recent work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39]</ref>. • An IUV image based dense render-and-compare scheme is incorporated into the framework. We minimize 3D reconstruction errors as well as discrepancies between inputs and rendered images from estimated outputs.</p><p>We learn the proposed model with both unpaired and paired data, compatible with different levels of supervisions. The end-to-end training minimizes multiple losses defined upon human pose and body shape jointly, including parameter regression, 3D reconstruction, landmark reprojection, body part segmentation, as well as adversarial loss on impossible configurations (see Sec. 3.3).</p><p>To boost learning, we further construct a large scale synthetic dataset covering diversified human poses and body shapes. The synthetic data is generated using web-crawled 3D animations and scanned all-gender body shapes for human studies, and rendered from various camera views (see <ref type="bibr">Sec. 4)</ref>. Learning from synthetic data mitigates the problem of unpaired, partial paired, or inaccurately annotated training data in popular public people-in-the-wild and Mocap benchmarks, as well as improves the model robustness against varied camera views and occlusions.</p><p>In our experiments, we evaluate DenseRaC on three tasks: 3D pose estimation, semantic body segmentation and 3D body reconstruction. Qualitative and quantitative exper- <ref type="bibr">Figure 2</ref>. Illustration of DenseRaC. Our two-step framework uses pixel-to-surface correspondences of human body as the intermediate representation, fed with data sources either from estimations on realistic images through DensePose-RCNN or rendered images on synthetic 3D humans. Given IUV images, we develop a deep neural network conducting parametric pose and shape regression and a differentiable renderer performing render-and-compare. The proposed framework optimizes losses of 3D reconstruction and discrepancies between inputs and rendered outputs by end-to-end learning.</p><p>imental results show DenseRaC outperforms existing methods on both public benchmarks and the newly proposed synthetic dataset (see Sec. 5).</p><p>To the best of our knowledge, this is the first end-to-end framework introducing a pixel-to-surface correspondence map as the intermediate representation and a corresponding dense render-and-compare scheme for learning 3D human pose and body shapes. We believe DenseRaC shows a great potential for numerous real-world applications in surveillance, entertainment, AR/VR, etc. Some featured results are shown in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The proposed method is mainly related to researches in three fields.</p><p>Monocular 3D pose estimation is a longstanding problem in computer vision. Current approaches train deep networks from large-scale training sets to regress 3D human joint transformations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref>. Deep neural net architectures enable direct body location with pose prediction, which is an advantage compared to traditional modelbased methods that require good initialization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>. Several methods predict 3D pose directly given monocular data <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b46">47]</ref>. On the other hand, many approaches lift 2D human poses <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5]</ref>, used as intermediate representation, and learn a model for 2D-3D pose space mapping <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b8">9]</ref>. State of the art in this track obtains fascinating performance on popular benchmarks limited to laboratory instrumented environments, and yet shows unsatisfactory results on in-the-wild images. Another common issue is that most existing methods do not in-corporate a physically plausible human skeleton model and lack constraints on the estimated results, which results in extra post-processings for graphics related applications.</p><p>3D human body reconstruction aims at recovering full 3D meshes of the human body from single RGB images or video sequences, rather than major 3D skeleton joints. For example, Zuffi et al. <ref type="bibr" target="#b63">[64]</ref> integrated both realistic body model and part-based graphical models <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59]</ref> for jointly emphasizing graphics-like models of human body shape and part-based human pose inference. In <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b52">53]</ref>, a skinned body model (SMPL) is used to formulate body shape as a linear function of deformation basis (i.e., with blend shapes). In <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39]</ref>, SMPL is considered as the parametric representation of 3D human body and DNNs are developed to estimate such parameters endto-end. Guler et al. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref> build a FCN for human shape estimation by learning dense image-to-template correspondences. Other work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b19">20]</ref> focuses on reconstructing 3D body shapes using RGB or RGBD images and not directly estimates 3D human pose and body shapes. These approaches are also suitable for multiple-view video capture setup <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b53">54]</ref>. In this paper, we use a SMPL variant as the parametric representation of 3D human body and further develop a pixel-to-surface dense correspondence based render-and-compare framework. Learning from synthetic humans. Modeling 3D humans in arbitrary scenes requires representative training sets. A number of previous work has considered automatically generating data for assisting 3D models, e.g., upper body <ref type="bibr" target="#b39">[40]</ref>, full-body silhouettes <ref type="bibr" target="#b1">[2]</ref>. <ref type="bibr" target="#b13">[14]</ref> artificially renders pedestrians in a scene while leveraging camera parameters and geometrical layout, and further trains a scene-specific pedes-IUV map 3D body model RGB image IUV image <ref type="figure">Figure 3</ref>. Illustration of mapping from pixel to 3D surface. Our framework estimates an IUV image and dense 3D landmarks from an RGB input, whose pixels refer to 3D points on the body model. trian detector. In <ref type="bibr" target="#b43">[44]</ref>, real 2D pose samples are reshaped by adding small perturbations, and augmented with different backgrounds. Rogez et al. <ref type="bibr" target="#b48">[49]</ref>, for a given 3D pose, combines local image patches from several images with kinematic constraints to create a new synthetic image. Rahmani et al. <ref type="bibr" target="#b45">[46]</ref> fits synthetic 3D human models to Mocap skeletons and renders human poses from numerous virtual viewpoints. Varol et al. <ref type="bibr" target="#b55">[56]</ref> also generate a synthetic human body dataset with random factors (e.g., pose, shape, texture, background, etc.). These datasets cannot solely serve to train models generalized to real data, due the gap between synthesized and realistic images. In this paper, we propose to use pixel-to-surface correspondence maps to bridge the gap. The joint training on hybrid datasets is proved to be effective in improving performance on realistic data. To our best knowledge, we are the first to address joint human pose and body shape estimation using such training modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DenseRaC Framework</head><p>As illustrated in <ref type="figure">Fig. 2</ref>, the proposed framework estimates 3D human poses and body shapes in two steps: first obtaining pixel-to-surface correspondences (i.e., IUV images) and then leveraging the intermediate results IUV images into 3D surfaces. There are two sources of IUV inputs: i) estimations from RGB inputs using a pre-trained DensePose-RCNN <ref type="bibr" target="#b11">[12]</ref>, and ii) rendered IUV images from synthetic data.</p><p>Our framework employs a compact and expressive 3D human body model, which is parameterized by 3D human pose θ ∈ R 58×3 , body shape β ∈ R 50 , instead of directly estimating 3D point clouds, voxels or depth maps. The 3D human pose is represented as a tree structure, with 58 relative 3D rotations between parent and child joints while the body shape is represented by 50 shape coefficients, as elaborated in Sec. 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>Given IUV inputs, we design a network architecture consisting of three modules: • A generator with a back-boned base network (i.e., ResNet-50 <ref type="bibr" target="#b14">[15]</ref>) to extract expressive feature maps and a regressor which takes the stretched feature maps (i.e., 2048D feature vector) from the base network as inputs and estimates 3D human body parameters [θ, β] and camera parameters α ∈ R 3 (i.e., 227D concatenated vector). The camera model is assumed to be an orthographic projection, parameterized by scale factor f and camera axis (x, y). The regressor is composed of 3 fully connected layers with 1024 nodes each. Inspired by <ref type="bibr" target="#b21">[22]</ref>, we consider the regressor to model an iterative update ∆ θ,β,α to the final output, starting from the parameter mean [θ,β,ᾱ]. The weights are shared across all three layers, simulating the recursive tree structure within 3D human pose. • A differentiable renderer creates 2D projections of the reconstructed 3D human body mesh, using the estimated camera parameters (see Sec. 3.3). We implement a differential rasterizer which creates an IUV image suitable for gradient flow. Following a render-and-compare scheme, we define three losses to measure and minimize the differences between the input IUV image and the rendered IUV image from our model output.</p><p>• A discriminator to constrain impossible configurations for unpaired data. We design two shallow networks with two fully connected layers as a discriminator. One is used for discriminating 3D human poses and the other one for body shapes. The number of nodes for pose and shape in sub-networks are set to 512 and 64, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">IUV as Proxy Representation</head><p>As illustrated in <ref type="figure">Fig. 3</ref>, we utilize the IUV image as a proxy representation. An IUV map, similarly to UV map in graphics, defines pixel-to-surface correspondences (oneto-one), from 2D image to 3D surface mesh. Each pixel of an IUV image refers to a body part index I, and (U, V ) coordinates that map to a unique point on the body model surface (see Sec. 3.5).</p><p>As also discussed in <ref type="bibr" target="#b38">[39]</ref>, RGB input contains much more information of the human target than 2D joints, silhouettes, or body part masks that are traditionally used as proxy representation. However information such as appearance, illumination or clothing may not be relevant for inferring the 3D geometry, and even overfits the network to nuisance factors. Similar to <ref type="bibr" target="#b38">[39]</ref>, we also observe that explicit body part representations are more useful for the task of 3D human pose and body shape estimation, compared to RGB images and plain silhouettes. Better part segmentation produces better 3D reconstruction accuracy, while providing full spatial coverage of the person (compared to joint heatmaps). While further increasing the number of segmentation parts above a certain threshold (12) only incrementally leverage 3D pose prediction accuracy, it nevertheless greatly improves body shape estimation (see Sec. 5). We argue that prior work only estimates average body shape.</p><p>Note we further use two sources of IUV images as inputs, i.e., IUV images from realistic images estimated from <ref type="bibr" target="#b11">[12]</ref> and IUV images from virtual humans synthesized by our renderer (see Sec. 3.3). The IUV estimation could be obtained by other off-the-shelf models or two-stage/end-toend training. Both inputs go through our neural network model and are used to estimate 3D human pose and body shape parameters. Thus, there are several benefits for using IUV image representation: i) improving robustness against nuisances of light and texture in natural images, ii) providing richer geometry information on 3D human body (by including body part masks and dense landmarks), iii) unifying realistic and synthetic data for joint learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dense Render-and-Compare</head><p>In this paper, 3D human pose and body shape are represented compactly by a parametric model (see Sec. 3.5). Parametrized 3D human body is inferred and fit to the input image, given also camera parameters. Human body surface is represented as a 3D triangular mesh, and body posing is obtained by standard linear blend skinning. To fully compare a reconstructed 3D human body to a 2D observation of it, we integrate a differentiable renderer, i.e., a computer graphics technique that creates a 2D image from a 3D object using differentiable operations <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23]</ref>, and develop an end-to-end weakly-supervised training scheme.</p><p>Rendering consists of projecting 3D vertices of a mesh onto a 2D image plane and rasterizing it (i.e., sampling the faces). 3D-to-2D projection is obtained by a combination of differentiable transformations <ref type="bibr" target="#b32">[33]</ref>. Rasterization is a discrete operation that requires gradient definition to allow back-propagation in a neural network. In <ref type="bibr" target="#b30">[31]</ref>, the authors approximate derivatives at occlusion boundaries which are discontinuous, while colors are interpolated between vertices (i.e., there is no differentiation with respect to texture). In <ref type="bibr" target="#b22">[23]</ref>, the authors obtain approximate gradients by blurring image to avoid sudden pixel color change. This produces non-zero gradients and enables gradient-flow between pixel (color) value to vertex position. However, lighting and material properties in natural images are complex to model and integrate into neural networks.</p><p>On the contrary, our IUV representation is invariant to background, lighting conditions and surface texture like clothing (see Sec. 3.2). In addition, UV values on each body part I are continuous with respect to neighbor pixels (see <ref type="figure">Fig. 3</ref>). This actually allows to naturally compute gradients on mesh surface and at boundaries and back-propagate them through network layers.</p><p>Our renderer creates IUV image comparable to the generated output of <ref type="bibr" target="#b11">[12]</ref> (see <ref type="figure" target="#fig_0">Fig. 4</ref>). Self-occlusion is handled by depth buffering. Our rasterizer draws only the surface faces closest to the camera (and facing it) at each pixel. During back propagation, we only pass gradient flows of pixels corresponding to visible regions.</p><p>Different from <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b23">24]</ref> where render-and-compare losses are computed upon silhouettes and 2D depth maps, we compute dense render-and-compare losses L rac using IUV values between ground-truth IUV images and rendered ones (see Sec. 3.4). The differentiable renderer (including IUV rasterizer) and losses are implemented with differentiable operations using a neural net framework with automatic differentiation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b23">24</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Terms</head><p>Our model integrates a dense render-and-compare module with corresponding loss computations in the backward propagation, hence leveraging previous methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">55]</ref>. The loss function is defined as</p><formula xml:id="formula_0">L = Lrac + 1Lrec + 1Lrgr,<label>(1)</label></formula><p>where 1 indicates the existence of such annotation, L rac , L rec and L rgr denote render-and-compare loss, 3D reconstruction loss and parameter regression loss, respectively.</p><p>• Render-and-Compare Loss L rac is evaluated under three measurements, that is,</p><formula xml:id="formula_1">Lrac = Lrpj + L msk + L adv ,<label>(2)</label></formula><p>where L rpj , L msk and L adv denote landmark reprojection loss, part mask loss and adversarial loss, respectively. Landmark Reprojection Loss L rpj measures displacement between ground truth and estimated dense 2D landmarks:</p><formula xml:id="formula_2">Lrpj = N i 1i pi −pi 1,<label>(3)</label></formula><p>where 1 i indicates the visibility (1 if visible, 0 otherwise) for i-th 2D landmark (N in total), p i ∈ R 2 andp i ∈ R 2 represent i-th 2D landmark from ground truth and 3D mesh reprojection, respectively. To correctly localize the landmarks from ground truth (i.e., IUV image estimated from DensePose <ref type="bibr" target="#b11">[12]</ref>), we formulate this problem as a point-topoint greedy match and solve the correspondence problem by k-Nearest Neighbor (k-NN) search. Specifically, we first create a k-D tree for IUV values of 3D body mesh vertices. For any input IUV image, we search for 1-NN of each visible pixel and obtain a matched pair with the closest 3D body mesh vertex within a distance threshold τ . Empirically, τ ∈ [0.01, 0.1] yields 100-300 matching pairs considered as near-optimal one-to-one 2D/3D dense landmarks correspondences. This serves as a weakly-supervised scaffold to densely fit 3D human body to the re-projected 2D image. Note the matching is computed offline and serves as a pre-processing step on IUV inputs, as shown in <ref type="figure" target="#fig_1">Fig. 5</ref>. Part Mask Loss L msk provides semantic information for the location of body part:</p><formula xml:id="formula_3">L msk = k (1 − IoU(I k ,Î k )), IoU(I k ,Î k ) = |I k ∩Î k | |I k ∪Î k | ,<label>(4)</label></formula><p>where k is body part index and IoU(·, ·) represents intersection over union of two masks. We keep the same body segments (12 parts) I and (U, V ) mapping as specified in <ref type="bibr" target="#b11">[12]</ref>. Adversarial Loss L adv constrains configuration plausibility. Unlike <ref type="bibr" target="#b21">[22]</ref> using unpaired or Mosh-based <ref type="bibr" target="#b28">[29]</ref> weakly-supervised SMPL annotations, we use ground-truth 3D human poses and body shapes from our synthetic dataset, which contains much larger action variations than most Mocap sequences (see Sec. 4). We believe such long- tail poses are crucial for the adversarial loss in finding the decision boundary. Hereby, we account for millions of synthetic samples as both paired ground truth and unpaired adversarial prior for realistic datasets. We follow the GAN loss definitions in <ref type="bibr" target="#b9">[10]</ref> and train our generator and discriminator jointly. • 3D Reconstruction Loss L rec measures the deformation of reconstructed 3D human body, compared with ground truth:</p><formula xml:id="formula_4">Lrec = K i Pi −Pi 2,<label>(5)</label></formula><p>where P i andP i represent 3D keypoint positions from input and generated 3D mesh, respectively. • Parameter Regression Loss L rgr measures mean square errors between estimated parameters [θ, β, α] and ground truth [θ,β,α]:</p><formula xml:id="formula_5">Lrgr = [R θ , β, α] − [Rθ,β,α] 2,<label>(6)</label></formula><p>where R θ denotes the rotation matrix of θ. Notably, pose parameters are first transformed in rotation matrices. Losses are computed upon such matrices and gradients are automatically back-propagated. This helps avoid the singularity problem of XYZ-Euler based 3D rotation and requires no extra constraints on the rotation matrix, that is,</p><formula xml:id="formula_6">RR T = diag(1, . . . , 1), det(R) = 1,<label>(7)</label></formula><p>where det(·) denotes the matrix determinant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Human Body Model</head><p>We use a body shape model similar to SMPL <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4]</ref>. The statistical body model is obtained by PCA on posenormalized 3D models of real humans, obtained by nonrigid registration of a body template to 3D scans of the CAESAR dataset 1 , which represents anthropometric variability of 4,400 men and women. The body template mesh 1 http://store.sae.org/caesar/ has 7,324 vertices, 14,644 triangular faces and a skeletal rig with body and hand joints.</p><p>Our model is trained with all 3D scans in the dataset, resulting in a statistical model that can describe bodies from unseen in-the-wild images regardless of gender. An arbitrary body shape can then be described by a set of shape coefficients (i.e., shape parameters or shape blend shapes) using a linear representation. Truncating shape coefficients to 50 principal components enables reconstruction of allgender body shapes without noticeable distortions: e.g., the SMPL-Male with 10 coefficients does not reconstruct well female shape (RMSE=9.9mm), while an all-gender model does (RMSE=6.3/3.4mm with 10/50 coeffs respectively).</p><p>Considering potential applications in AR/VR, 3D animations and better utilization of annotations, we enrich the standard SMPL 24-joint skeleton with 28 joints for modeling fingers and 5 more joints on spine and head for better flexibility. We further add a root node for global translation and rotation, leading to a skeleton with 58 joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MOCA Synthetic Dataset</head><p>The literature has provided several datasets to evaluate human 3D pose (e.g., H3.6M <ref type="bibr" target="#b17">[18]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b35">[36]</ref>), but only few for joint 3D pose and body shape (e.g., SUR-REAL <ref type="bibr" target="#b55">[56]</ref> and UP-3D <ref type="bibr" target="#b25">[26]</ref>). However, SURREAL is dedicated to body segmentation and depth estimation and only has a rough skeleton (24 major body joints), while UP-3D has weakly-supervised shapes (from SMPL fitted to LSP and MPII), arguably imprecise <ref type="bibr" target="#b54">[55]</ref>.</p><p>Hence, we propose MOCA , a large-scale synthetic dataset with 2,089,104 images containing ground-truth body shapes and 3D poses, as shown in <ref type="figure" target="#fig_0">Fig. 4</ref>. For various human poses and actions, we seek to a popular collection center of 3D human animations (i.e., Mixamo 2 ), whose sources mainly come from Mocap systems and artist designs. We implement a web crawler to fetch high fidelity animations. Notably, Mixamo supports tuning parameters (e.g., limb length, energy, overdrive) for each action sequence to generate variants. As we observe certain parameter settings may introduce artifacts, we thus keep the default setting for all sequences. We collect a set of 2,446 3D animation sequences with 261,138 frames at 30 fps, covering wide action categories of sports, combat, daily and social activities. We extract a finer 3D skeleton with fingers and facial bones using Maya and re-map those joints onto our body model.</p><p>We then generate 2,781 bodies using the 3D scans from CAESAR dataset and compute corresponding (PCA) shape coefficients. By combining 3D pose θ and shape β, we pose body models to specific pose&amp;shape configurations by standard linear blend skinning.</p><p>The complete combination of all 3D poses and body shapes produces an enormous amount of 3D human body samples. Currently, we randomly select 8 body shapes for each action sequence. We further add a random camera view for each sequence, and render them as IUV image sequences using our IUV rasterizer (see Sec. 3.3), obtaining a dataset with 2,089,104 frames in total and fully paired ground truth of body shape, 3D pose and the camera view. For training/testing set partition, we set the ratio as 90%/10%. We synthesize the training set with the first 2,201 Mixamo action sequences and 2,502 CAESAR body shapes and leave the rest 246 action sequences and 279 body shapes only visible to the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate DenseRaC on several public large-scale benchmarks for three tasks: 3D pose estimation, body shape estimation and body semantic segmentation. We further assess human 3D reconstruction results (i.e., mesh-level reconstruction, joint&amp;shape parameter estimation) on the proposed large synthetic dataset MOCA that contains groundtruth 3D pose and body shape. Our experiments compare favorably to the state of the art. Estimated 3D poses and body shapes are stable on videos (see additional materials). Our qualitative results also show natural hand poses (e.g., opened, clenched).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We use five public human benchmarks plus our synthesized MOCA for model training and evaluation, i.e., LSP <ref type="bibr" target="#b20">[21]</ref>, MPII <ref type="bibr" target="#b2">[3]</ref>, COCO <ref type="bibr" target="#b27">[28]</ref>, H3.6M <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b35">[36]</ref>. We adopt standard training/validation/testing partitions on all datasets and calibrate loss terms using cross-validation. When a certain dataset is used for evaluation, all data from other datasets will be used in training.</p><p>For all training and testing samples, we crop out each person in the whole image using ground-truth bounding boxes. All samples are resized to ∼150-180 pixel height with preserved aspect ratio, and further adjusted to 224 × 224 with padding/cropping respectively. We then run IUV image estimation <ref type="bibr" target="#b11">[12]</ref> on all samples. Considering a sample I may contain multiple people and false alarms, we compute a saliency score s = |m| mc−Ic 2 for each detected person mask m, where m c and I c represent the center of the person mask and the image, respectively. We then pick the person mask with the largest saliency score and suppress the other detection responses.</p><p>For the training set, we further run pixel-to-surface matching (as described in Sec. 3.3) to create dense correspondences. We discard samples with less than 200 corresponding pairs, as IUV image estimation usually failed under such situation. As illustrated in <ref type="figure" target="#fig_1">Fig. 5</ref>, pre-processing suppresses nuisances in the training samples quite well. During training, all training samples will further be augmented with a random jittering of translation, scaling and reflection to improve the model robustness. We also randomly black out a rectangle image region for the synthetic samples to simulate occlusion in realistic scenarios.</p><p>To unite the skeleton structure across all datasets, we use the same 14 joints as in LSP for joint related computation while maintaining our 58-joint skeleton in the backend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>In these experiments, the whole framework is implemented with TensorFlow and runs on a DGX workstation with 2 Intel E5 CPUs, 512GB memory and 8 Titan V100 GPUs. Data synthesis and pre-processing (i.e., IUV image estimation) are implemented with multi-gpu data parallelism. The multi-gpu renderer processes around 300 fps and takes 2 days to generate 2 million MOCA samples (total size 2.7TB). Data pre-processing on realistic datasets takes 12 hours to prepare 0.8 million samples.</p><p>For learning, only a single GPU is used due to difficulty in gradient transfer and a potential performance drop. We use batch size 128, learning rate 10 −5 for the generator and 10 −4 for the discriminator, and Adam as the optimizer. Our full model is jointly trained on all datasets for 30 epochs. Empirically, for one batch, the forward pass takes around 15ms and the backward propagation takes (∼130ms) with IUV image render-and-compare (∼55ms) as the overhead. The total training process takes around a week to complete. For inference, IUV images are first estimated at around 15 fps and then the forward pass of our model is called, taking 120 fps and thus achieves real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">3D Pose Estimation</head><p>We first evaluate our method for the task of 3D pose estimation on H3.6M <ref type="bibr" target="#b17">[18]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b35">[36]</ref> datasets.</p><p>For H3.6M, we use three evaluation protocols used to measure the performance: i) Protocol #1 uses 5 subjects (S1, S5, S6, S7 and S8) for training and 2 subjects (S9 and S11) for testing. Sequences are down-sampled to 10 fps and all 4 cameras and trials are used for evaluation. MSE is measured between estimated and ground-truth 3D joints. ii) Protocol #2 selects the same subjects for training and test-  sequences captured from the frontal camera (i.e., "cam 3") from trial 1 on all frames. Predictions are post-processed via rigid transformations (i.e., per-frame Procrustes analysis) before comparison. iii) Protocol #3 uses the same subjects, frame rates and trials for training and testing in Protocol #1 except that camera views are further partitioned. The first three cameras (i.e., "cam 0, 1, 2") are used for training and the last camera (i.e., "cam 3") for testing.</p><p>For MPI-INF-3DHP, we use all sequences from S1-S7 as training set and sequences from S8 as testing set. We regard Protocol #1 as the default comparison and Protocol #2 as applying rigid transformations before comparison.</p><p>We compare our method with both task-oriented 3D pose state of the art <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b15">16]</ref> and four parametric body model based estimators <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b38">39]</ref>.  We set up two baselines to validate the effectiveness of two key components in the proposed framework: renderand-compare and joint learning with synthetic data. In "DenseRaC baseline", we use SMPL model and the same losses as <ref type="bibr" target="#b21">[22]</ref>, only switch input sources from RGB images to IUV images. Variant "+ render-and-compare" denotes adding the proposed dense render-and-compare scheme losses into the framework and part masks. Variant "+ synthetic data" switches to our human body model and further uses augmented synthetic data for joint learning. As reported in <ref type="table" target="#tab_0">Table 1</ref>, we can observe each component in DenseRaC contributes to the final performance and leads DenseRaC to outperform state-of-the-art parametric body model estimators by a large margin. Also notice DenseRaC is comparable with latest task-oriented 3D pose estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Human Body Segmentation</head><p>Given rendered images from outputs, we further employ semantic segmentation as another task to measure how similar the reconstructed 3D human body looks to the person in the input image. We evaluate the tasks of human body segmentation and test our approach on the LSP subset of UP-3D <ref type="bibr" target="#b25">[26]</ref> and MOCA datasets. For UP-3D, we post-process our 24 body part masks by merging into the annotated 6 body part masks (i.e., head, torso, left and right leg, and left and right arm) and evaluate on body part and foreground segmentation, while we evaluate both body part segmentation (ignoring 4 subtle body parts, i.e., hands and feet) and foreground segmentation on MOCA. We measure segmentation accuracy and mean F1 score of the results and report metrics and comparisons in <ref type="table">Table 5</ref>.3. It can be observed that our method achieves comparable or better performance with state of the art <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b54">55]</ref> on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">3D Human Body Reconstruction</head><p>Notice 3D pose estimation and body semantic segmentation are tasks focusing on evaluating partial knowledge of the reconstructed 3D human body, We further evaluate the reconstructed 3D human body using two metrics: Mean Per Mesh Vertex Position Error (MPVPE) and regression error on MOCA dataset. These two metrics consider the 3D human body as a whole and provide more guidance about how well the reconstructed 3D human body is. For comparison, we re-train HMR which takes IUV images as input and uses 2D/3D joint supervisions (i.e., only 14 2D/3D joints in LSP format) and their original unpaired data (Mosh <ref type="bibr" target="#b28">[29]</ref> on H3.6M and external Mocap) for the adversarial prior. As reported in <ref type="table" target="#tab_3">Table 3</ref>, DenseRaC still significantly outperforms the competitive method.</p><p>Ablative Studies. We set up variants of DenseRaC to validate effectiveness of each loss terms. We also define two loss variants L J rpj and L J rec representing 14-jointonly keypoint reprojection and 3D reconstruction losses, respectively. From the results, we could reach the following conclusions: i) All loss terms contribute to the final performance; ii) Losses used for dense render-andcompare provide richer information than those from sparse joints, greatly reduce impossible 3D body configurations; iii) When task oriented loss terms are given (i.e., L rec and L rgr ), the contribution from the dense render-and-compare scheme seems to be suppressed, yet such finer supervisions help DenseRaC reach a much better local optimum.</p><p>Empirical Studies. We present qualitative results and comparisons to have a better understanding of merits of our method. As shown in <ref type="figure">Fig. 6</ref>, DenseRaC outperforms other competitive methods and reconstructs more plausible and natural 3D human bodies. Notably, HMR, which relies on sparse landmarks, sometimes reconstructs plausible 3D human body appearance, but confuses body front and back. Both NBF and BodyNet are sensitive to occlusions and heavy clothing. When fitting SMPL to such erroneously reconstructed volumes, BodyNet tends to produce highly non-human body shapes 3 . For all three methods, the estimated human bodies are arguably in an average body shape and insensitive to genders. We also search failure cases on validation set, as shown in <ref type="figure">Fig. 7</ref>. DenseRaC suffers from errors in IUV estimations (e.g., occlusions, long-tail data), and is limited by the orthographic projection assumption and SMPL-based human body representation.</p><p>We also explored virtual dressing, namely draping virtual clothing on 3D human body, using our beneath-clothing estimation. As shown in <ref type="figure">Fig. 1 (top right)</ref> and <ref type="figure">Fig. 8</ref>, a cascaded framework for adding physical simulations of clothing is possible <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref> and more visually acceptable than end-to-end volumetric reconstruction of BodyNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose DenseRaC, a new end-to-end framework for reconstructing 3D human body from monocular RGB images in the wild. DenseRaC utilizes the pixel-to-surface correspondence map as proxy representation and incorporates a dense render-and-compare scheme to minimize the gap between rendered outputs and inputs. We further boost the model training with large scale synthetic data (MOCA), mitigating the problem of unpaired training data. The proposed framework obtains superior performance and we will explore handling occlusion and interaction (e.g., by multiview fusion <ref type="bibr" target="#b44">[45]</ref>, temporal smoothing <ref type="bibr" target="#b42">[43]</ref>) next.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>IUV images from MOCA generated by rasterizing 3D bodies obtained with 3D poses from Mixamo and body shapes from CAESAR. MOCA contains 2M+ images with fully paired ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Pre-processed training samples from public benchmarks. Left: original image, right: estimated IUV image, ground-truth keypoint annotations (yellow) and dense landmarks (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Current limitations: heavy occlusions (first row), incorrect IUV estimations (second row) and under-represented body shapes like children (third row). Each triplet shows the original image, IUV from [12] (our model input), and our model output. Comparisons for cascaded and end-to-end frameworks on the application of virtual dressing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Qualitative comparisons of results estimated from DenseRaC versus state of the art [22, 39, 55]. DenseRaC estimates 3D human poses and body shapes closest to the reality. Note that all examples come from the test set. Best viewed in color. Quantitative comparisons of mean per joint position error (MPJPE), PCK and AUC between the estimated 3D pose and ground truth on H3.6M under Protocol #1, #2, #3 and MPI-INF-3DHP under Protocol #1, #2. -indicates results not reported. Lower MPJPE, higher PCK and AUC indicate better performance. Best scores are marked in bold.</figDesc><table><row><cell>Input</cell><cell>DenseRaC</cell><cell>HMR</cell><cell>NBF</cell><cell>BodyNet</cell><cell>Input</cell><cell>DenseRaC</cell><cell>HMR</cell><cell>NBF</cell><cell>BodyNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparisons of foreground and part segmentation on UP-3D and MOCA datasets. Accuracy unit is in %.indicates results not reported. Best scores are marked in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>L rpj + L adv + L msk + L rec L rpj + L msk + L rec + L rgr</figDesc><table><row><cell>Methods</cell><cell>MPJPE</cell><cell>MPVPE</cell><cell>MSE θ,β</cell></row><row><cell>HMR (CVPR'18) [22] unpaired</cell><cell>110.2</cell><cell>-</cell><cell>-</cell></row><row><cell>HMR (CVPR'18) [22] paired</cell><cell>91.9</cell><cell>-</cell><cell>-</cell></row><row><cell>DenseRaC, L J rpj</cell><cell>133.0</cell><cell>174.5</cell><cell>18.227</cell></row><row><cell>DenseRaC, L J rpj + L adv</cell><cell>131.5</cell><cell>173.6</cell><cell>17.820</cell></row><row><cell>DenseRaC, L J rpj + L adv + L msk</cell><cell>122.8</cell><cell>161.5</cell><cell>16.305</cell></row><row><cell>DenseRaC, L rpj + L adv + L msk</cell><cell>107.9</cell><cell>142.3</cell><cell>13.608</cell></row><row><cell>DenseRaC, L J rpj + L adv + L J rec</cell><cell>88.6</cell><cell>121.1</cell><cell>11.901</cell></row><row><cell>DenseRaC, L J rpj + L adv + L msk + L J rec</cell><cell>86.5</cell><cell>119.8</cell><cell>10.496</cell></row><row><cell>DenseRaC, L rpj + L adv + L rec</cell><cell>82.9</cell><cell>111.0</cell><cell>8.943</cell></row><row><cell cols="2">DenseRaC, 82.4</cell><cell>110.7</cell><cell>8.722</cell></row><row><cell cols="2">DenseRaC, 80.4</cell><cell>105.4</cell><cell>8.164</cell></row><row><cell>DenseRaC, full</cell><cell>80.3</cell><cell>105.2</cell><cell>8.151</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparisons of MPJPE, MPVPE, Pose&amp;Shape Parameter Mean Square Error MSE θ,β on MOCA dataset. Lower values are better. See text for detailed explanations.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.mixamo.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We uses results from 3D skeleton fitting for BodyNet, as volume fitting usually performs much worse.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank Tengyu Liu and Elan Markowitz for helping with data collection, Tuur Jan M Stuyck and Aaron Ferguson for cloth simulation, Natalia Neverova and colleagues at FRL, FAIR and UCLA for their support and advice.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Images and videos available at youtube.com, onlinedoctor.superdrug.com, shutterstock</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recovering 3D human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The helmholtz machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radford</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hs-nets: Estimating human body shape from silhouettes with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Endri</forename><surname>Dibra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remo</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Drape: Dressing any person</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loretta</forename><surname>Peng Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hirshberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Riza Alp Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Epameinondas Antonakos, Patrick Snape, Stefanos Zafeiriou, and Iasonas Kokkinos. Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Riza Alp Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trigeorgis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning scene-specific pedestrian detectors without real data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hironori</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hand pose estimation via latent 2.5d heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Shape-from-mask: A deep learning based human body shape reconstruction from binary mask images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongping</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08485</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instancelevel 3d object reconstruction via render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepwrinkles: Accurate and realistic clothing modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zorah</forename><surname>Laehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maximummargin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mosh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Opendr: An approximate differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fundamentals of computer graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Marschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shirley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">3d video and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Matsuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Takai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Bruce Xiaohan Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast pose estimation with parameter-sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory Shakhnarovich</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Thormahlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene-centric joint parsing of cross-view videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d action recognition from novel viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Sporri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3d human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Structured prediction of 3D human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Complete multi-view reconstruction of dynamic scenes from probabilistic fusion of narrow and wide baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Matsuyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attentive fashion grammar network for fashion landmark detection and clothing category classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Human re-identification by matching compositional template with cluster sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A causal and-or graph model for visibility fluent reasoning in tracking interacting objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Konstantinos G Derpanis, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The stitched puppet: A graphical model of 3d human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
