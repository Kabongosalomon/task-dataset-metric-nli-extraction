<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-based Graph Neural Network for Semi-supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-03-13">March 13, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><forename type="middle">K</forename><surname>Thekumparampil</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Attention-based Graph Neural Network for Semi-supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-03-13">March 13, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fullyconnected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the major bottlenecks in applying machine learning in practice is collecting sizable and reliable labeled data, essential for accurate predictions. One way to overcome the problem of limited labeled data is semi-supervised learning, using additional unlabeled data that might be freely available. In this paper, we are interested in a scenario when this additional unlabeled data is available in a form of a graph. The graph provides underlying pairwise relations among the data points, both labeled and unlabeled.</p><p>Of particular interest are those applications where the presence or absence of an edge between two data points is determined by nature, for instance as a result of human activities or natural relations. As a concrete example, consider a citation network. Each node in the graph is a published research paper, associated with a bag-of-words feature vector. An (directed) edge indicates a citation link. Presence of an edge indicates that the authors of a paper have consciously determined to refer to the other paper, and hence captures some underlying relation that might not be inferred from the bag-of-words feature vectors alone. Such external graph data are available in several applications of interest, such as classifying users connected via a social network, items and customers connected by purchase history, users and movies connected by viewing history, and entities in a knowledge graph connected by relationships. In this paper, we are interested in the setting where the graph is explicitly given and represents additional information not present in the feature vectors.</p><p>The goal of such graph-based semi-supervised learning problems is to classify the nodes in a graph using a small subset of labeled nodes and all the node features. There is a long line of literature on this topic since <ref type="bibr" target="#b6">[7]</ref> which seeks graph cuts that preserve the known labels and <ref type="bibr" target="#b49">[50]</ref> which uses graph Laplacian to regularize the nearby nodes to have similar labels. However, <ref type="bibr" target="#b26">[27]</ref> recently demonstrated that the existing approaches can be significantly improved upon on a number of standard benchmark datasets, using an innovative neural network architecture on graph-based data known collectively as graph neural networks.</p><p>Inspired by this success, we seek to understand the reason behind the power of graph neural networks, to guide our design of a novel architecture for semi-supervised learning on graphs. To this end, we first found that a linear classifier of multinomial logistic regression achieves the accuracy comparable to the best known graph neural network. This linear classifier removes all intermediate non-linear activation layers, and only keeps the linear propagation function from neighbors in graph neural networks. This suggests the importance of aggregation information form the neighbors in the graph. This further motivates us to design a new way of aggregating neighborhood information through attention mechanism since, intuitively, neighbors might not be equally important. This proposed attention-based graph neural network captures this intuition and (a) greatly reduces the model complexity, with only a single scalar parameter at each intermediate layer; (b) discovers dynamically and adaptively which nodes are relevant to the target node for classification; and (c) improves upon state-of-the-art methods in terms of accuracy on standard benchmark datasets. Further, the learned attention strengths provide some form of interpretability. They provide insights on why a particular prediction is made on a target node and which neighbors are more relevant in making that decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Given a graph G(V, E) with a set of n nodes V and a set of edges E, we let X i ∈ R dx denote the feature vector at node i and let Y i denote the true label. We use Y L to denote the labels that are revealed to us for a subset L ⊂ V . We let X = [X 1 , . . . , X n ] denote all features, labeled and unlabeled.</p><p>Traditionally, semi-supervised learning using both labeled and un-labled data has been solved using two different approaches -Graph Laplacian based algorithms solving for locally consistent solutions <ref type="bibr" target="#b46">[47]</ref> and Expectation Maximization based algorithms <ref type="bibr" target="#b32">[33]</ref> where true-labels of the unlabeled data points are considered as the latent variables of a generative model. Graph Laplacian regularization. Based on the assumption that nearby nodes in a graph are more likely to have the same labels, the graph information has been used as explicit regularization:</p><formula xml:id="formula_0">L(X, Y L ) = L label (X L , Y L ) + λ L G (X) , where L label = i∈L l(Y i , f (X i ))</formula><p>is the standard supervised loss for some loss functions l and L G is the graph-based regularization, for example L G = (i,j)∈E f (X i ) − f (X j ) 2 , which is called the graph Laplacian regularization. Earlier approaches are non-parametric and searches over all f considering it as a look-up table. Most popular one is the Label Propagation <ref type="bibr" target="#b48">[49]</ref> that forces the estimated labels to agree in the labeled instances and uses weighted graph Laplacian. This innovative formulation admits a closed form solution which makes it practically attractive with very low computationally cost. ManiReg <ref type="bibr" target="#b4">[5]</ref> replaces supervised loss with that of a support vector machine. ICA <ref type="bibr" target="#b29">[30]</ref> generalizes LP by allowing more general local updates. A more thorough survey on using non-neural network methods for semi-supervised learning can be found in <ref type="bibr" target="#b11">[12]</ref>.</p><p>More recent approaches are parametric, using deep neural networks. SemiEmb <ref type="bibr" target="#b41">[42]</ref> was the first to use a deep neural network to model f (x) and minimize the above loss. Planetoid <ref type="bibr" target="#b45">[46]</ref> significantly improves upon the existing graph regularization approaches by replacing the regularization by another loss based on skip-grams (defined below). In a slightly different context, Buchnik and Cohen <ref type="bibr" target="#b10">[11]</ref> show that the accuracy of these approaches can be further improved by bootstrapping these models sequentially. Unsupervised node embedding for semi-supervised learning. Several approaches have been proposed to embed the nodes in some latent Euclidean space using only the connectivity in graph G. Once the embedding is learned, standard supervised learning is applied on those embedded features to train a model. Inspired by the success of word2vec <ref type="bibr" target="#b27">[28]</ref>, several approaches define "skip-grams" on graphs as the neighborhood (context) of a node on the graph and tries to maximize the posterior probability of observing those skip-grams. DeepWalk <ref type="bibr" target="#b34">[35]</ref> and node2vec <ref type="bibr" target="#b21">[22]</ref> use random walks as skip-grams, LINE <ref type="bibr" target="#b39">[40]</ref> uses local proximities, LASAGNE <ref type="bibr" target="#b17">[18]</ref> uses the Personalized PageRank random walk. Graph2Gauss <ref type="bibr" target="#b0">[1]</ref> represents a node as a Gaussian distribution, and minimizes the divergence between connected pairs. Yang et al. <ref type="bibr" target="#b44">[45]</ref> provide a post-processing scheme that takes any node embedding and attempts to improve it by by taking the weighted sum of the given embeddings with Personalized PageRank weights. The strength of these approaches is universality, as the node embedding does not depend on the particular task at hand (and in particular the features or the labels). However, as they do not use the node features and the training only happens after embedding, they cannot meet the performance of the state-of-the-art approaches (see DeepWalk in <ref type="table">Table 2</ref>).</p><p>Graph Neural Network (GNN). Graph neural networks are extensions of neural networks to structured data encoded as a graph. Originally introduced as extensions of recurrent neural networks, GNNs apply recurrent layers to each node with additional local averaging layer <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>. However, as the weights are shared across all nodes, GNNs can also be interpreted as extensions of convolutional neural networks on a 2D grid to general graphs. Typically, a message aggregation step followed by some neural network architecture is iteratively applied. The model parameters are trained on (semi-)supervised examples with labels. We give a typical example of a GNN in Section 3, but several diverse variations have been proposed in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. GNNs have been successfully applied in diverse applications such as molecular activation prediction <ref type="bibr" target="#b18">[19]</ref>, community detection <ref type="bibr" target="#b8">[9]</ref>, matrix completion <ref type="bibr" target="#b5">[6]</ref>, combinatorial optimization <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>, and detecting similar binary codes <ref type="bibr" target="#b43">[44]</ref>.</p><p>In particular, for the benchmark datasets that we consider in this paper, Kipf and Welling <ref type="bibr" target="#b26">[27]</ref> proposed a simple but powerful architecture called Graph Convolutional Network (GCN) that achieves the state-of-the-art accuracy. In the following section, (a) we show that the performance of GCN can be met by a linear classifier; and (b) use this insight to introduce novel graph neural networks that compare favourably against the state-of-the-art approaches on benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dissection of Graph Neural Network</head><p>In this section, we propose a novel Graph Neural Network (GNN) model which we call Attention-based Graph Neural Network (AGNN), and compare its performance to state-of-the-art models on benchmark citation networks in Section 5. We seek a model Z = f (X, A) ∈ R n×dy that predicts at each node one of the d y classes. Z ic is the estimated probability that the label at node i ∈ [n] is c ∈ [d y ] given the features X and the graph A. The data features X ∈ R n×dx has at each row d x features for each node, and A ∈ {0, 1} n×n is the adjacency matrix of G.</p><p>The forward pass in a typical GNN alternates between a propagation layer and a single layer perceptron. Let t be the layer index. We use H (t) ∈ R n×d h to denote the current (hidden) states, with the i-th row H (t) i as the d h dimensional hidden state of node i. A propagation layer with respect to a propagation matrix P ∈ R n×n is defined as</p><formula xml:id="formula_1">H (t) = P H (t) .</formula><p>For example, the natural random walk <ref type="bibr" target="#b0">1</ref> </p><formula xml:id="formula_2">P = D −1 A givesH (t) i = (1/|N (i)|) j∈N (i) H (t)</formula><p>j . The neighborhood of node i is denoted by N (i), and D = diag(A1). This is a simple local averaging common in consensus or random walk based approaches. Typical propagation layer respects the adjacency pattern in A, performing a variation of such local averaging. GNNs encode the graph structure of A into the model via this propagation layer, which can be also interpreted as performing a graph convolution operation as discussed in <ref type="bibr" target="#b26">[27]</ref>. Next, a single layer perceptron is applied on each node separately and the weights W (t) are shared across all the nodes:</p><formula xml:id="formula_3">H (t+1) = σ(H (t) W (t) ) ,</formula><p>where W (t) ∈ R d h t+1 ×d h t is the weight matrix and σ(·) is an entry-wise activation function. This weight sharing reduces significantly the number of parameters to be trained, and encodes the invariance property of graph data, i.e. two nodes that are far apart but have the similar neighboring features and structures should be classified similarly. There are several extensions to this model as discussed in the previous section, but this standard graph neural network has proved powerful in several problems over graphs, e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>. Graph Convolutional Network (GCN). Kipf and Welling <ref type="bibr" target="#b26">[27]</ref> introduced a simple but powerful architecture, and achieved the state-of-the-art performance in benchmark citation networks (see <ref type="table">Table 2</ref>). GCN is a special case of GNN which stacks two layers of specific propagation and perceptron:</p><formula xml:id="formula_4">H (1) = ReLU (P X) W (0) , Z = f (X, A) = softmax ( P H (1) ) W (1) ,<label>(1)</label></formula><p>with a choice of P =D −1/2ÃD−1/2 , whereÃ = A + I, I is the identity matrix,D = diag(Ã1) and 1 is the all-ones vector. ReLU(a) = max{0, a} is an entry-wise rectified linear activation function, and softmax([a 1 , . . . , a k ]) =</p><p>(1/Z)[exp(a 1 ), . . . , exp(a k )] with Z = i exp(a i ) is applied row-wise. Hence, the output is the predicted likelihoods on the d y dimensional probability simplex. The weights W (0) and W <ref type="bibr" target="#b0">(1)</ref> are trained to minimize the crossentropy loss over all labeled examples L:</p><formula xml:id="formula_5">L = − i∈L dy c=1 Y ic ln Z ic .<label>(2)</label></formula><p>Graph Linear Network (GLN). To better understand GCN, we remove the intermediate nonlinear activation units from GCN, which gives Graph Linear Network defined as</p><formula xml:id="formula_6">Z = f (X, A) = softmax (P 2 X) W (0) W (1) ,<label>(3)</label></formula><p>with the same choice of P =D −1/2ÃD−1/2 as in GCN. The weights W (0) and W <ref type="bibr" target="#b0">(1)</ref> have the same dimensions as GCN and are trained on a cross entropy loss in <ref type="bibr" target="#b1">(2)</ref>. The two propagation layers simply take (linear) local average of the raw features weighted by their degrees, and at the output layer a simple linear classifier (multinomial logistic regression) is applied. This allows us to separate the gain in the linear propagation layer and the non-linear perceptron layer.</p><p>Comparing the differences in performances in <ref type="table">Table 2</ref>, we show that, perhaps surprisingly, GLN achieves an accuracy comparable to the that of the best GNN, and sometimes better. This suggests that, for citation networks, the strength of the general GNN architectures is in the propagation layer and not in the perceptron layer. On the other hand, the propagation layers are critical in achieving the desired performance, as is suggested in <ref type="table">Table 2</ref>. There are significant gaps in accuracy for those approaches not using the graph, i.e. T-SVM, and also those that use the graph differently, such as Label Propagation (LP) and Planetoid. Based on this observation, we propose replacing the propagation layer of GLN with an attention mechanism and test it on the benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Attention-based Graph Neural Network (AGNN).</head><p>The original propagation layer in GCN and several other graph neural networks such as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref> use a static (does not change over the layers) and non-adaptive (does not take into account the states of the nodes) propagation, e.g. P ij = 1/ |N (i)| |N (j)|. Such propagations are not able to capture which neighbor is more relevant to classifying a target node, which is critical in real data where not all edges imply the same types or strengths of relations.</p><p>We need novel dynamic and adaptive propagation layers, capable of capturing the relevance of different edges, which leads to more complex graph neural networks with more parameters. However, training such complex models is challenging in the semi-supervised setting, as the typical number of samples we have for each class is small; it is 20 in the standard benchmark dataset. This is evidenced in <ref type="table">Table 2</ref> where more complex graph neural network models by Verma, Boyer, and Verbeek <ref type="bibr" target="#b40">[41]</ref>, Monti et al. <ref type="bibr" target="#b30">[31]</ref>, and <ref type="bibr" target="#b37">[38]</ref> do not improve upon the simple GCN.</p><p>On the other hand, our experiments with GLN suggests that we can remove all the perceptron layers and focus only on improving the propagation layers. To this end, we introduce a novel Attention-based Graph Neural Network (AGNN). AGNN is simple; it only has a single scalar parameter β (t) at each intermediate layer. AGNN captures relevance; the proposed attention mechanism over neighbors in <ref type="bibr" target="#b4">(5)</ref> learns which neighbors are more relevant and weighs their contributions accordingly. This builds on the long line of successes of attention mechanisms in summarizing long sentences or large images, by capturing which word or part-of-image is most relevant <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4]</ref>. Particularly, we use the attention formulation similar to the one used in <ref type="bibr" target="#b20">[21]</ref>. <ref type="bibr" target="#b1">2</ref> It only has one parameter and we found this is important for successfully training the model when the number of labels is small as in our semi-supervised learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">AGNN model</head><p>We start with a word-embedding layer that maps a bag-of-words representation of a document into an averaged word embedding, and the word embedding W (0) ∈ R dx×d h is to be trained as a part of the model:</p><formula xml:id="formula_7">H (1) = ReLU(XW (0) ) .<label>(4)</label></formula><p>This is followed by layers of attention-guided propagation layers parameterized by β (t) ∈ R at each layer,</p><formula xml:id="formula_8">H (t+1) = P (t) H (t) ,<label>(5)</label></formula><p>where the propagation matrix P (t) ∈ R n×n is also a function of the input states H (t) and is zero for absent edges such that the output row-vector of node i is</p><formula xml:id="formula_9">H (t+1) i = j∈N (i)∪{i} P (t) ij H (t) j , with P (t) i = softmax [ β (t) cos(H (t) i , H (t)</formula><p>j ) ] j∈N (i)∪{i} and cos(x, y) = x T y/ x y with the L 2 norm x , for t ∈ {1, . . . , } and an integer . Here is the number of propagation layers. Note that the new propagation above is dynamic; propagation changes over the layers with differing β (t) and also the hidden states. It is also adaptive; it learns to weight more relevant neighbors higher. We add the self-loop in the propagation to ensure that the features and the hidden states of the node itself are not lost in the propagation process. The output layer has a weight W (1) ∈ R d h ×dy :</p><formula xml:id="formula_10">Z = f (X, A) = softmax H ( +1) W (1) .<label>(6)</label></formula><p>The weights W (0) , W <ref type="bibr" target="#b0">(1)</ref> , and β (t) 's are trained on a cross entropy loss in <ref type="bibr" target="#b1">(2)</ref>. To ease the notations, we have assumed that the input feature vectors to the first and last layers are augmented with a scalar constant of one, so that the standard bias term can be included in the parameters W (0) and W <ref type="bibr" target="#b0">(1)</ref> . The softmax function at attention ensures that the propagation layer P (t) row-sums to one. The attention from node j to node i is</p><formula xml:id="formula_11">P (t) ij = (1/C)e β (t) cos(H (t) i ,H (t) j ) ,<label>(7)</label></formula><p>with</p><formula xml:id="formula_12">C = j∈N (i)∪{i} e β (t) cos(H (t) i ,H (t)</formula><p>j ) which captures how relevant j is to i, as measured by the cosine of the angle between the corresponding hidden states. We show how we can interpret the attentions in Section 5.2 and show that the attention selects neighbors with the same class to be more relevant. On the standard benchmark datasets on citation networks, we show in Section 5 that this architecture achieves the best performance in <ref type="table">Table 2</ref>.</p><p>Here we note that independently from this work attention over sets has been proposed as "neighborhood attention" <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> for a different application. The main difference of AGNN with respect to these work is the fact that in AGNN attention is computed over a neighborhood of a node on a graph, whereas in these work attention over set of all entities is used to construct a "soft neighborhood".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on Benchmark Citation Networks</head><p>On standard benchmark datasets of three citation networks, we test our proposed AGNN model on semi-supervised learning tasks. We test on a fixed split of labeled/validation/test sets from <ref type="bibr" target="#b45">[46]</ref> and compare against baseline methods in <ref type="table">Table 2</ref>. We also test it on random splits of the same sizes in <ref type="table" target="#tab_2">Table 3</ref>, and random splits with larger number of labeled nodes in <ref type="table">Table 4</ref>. Benchmark Datasets. A citation network dataset consists of documents as nodes and citation links as directed edges. Each node has a human annotated topic from a finite set of classes and a feature vector. We consider three datasets <ref type="bibr" target="#b2">3</ref> . For CiteSeer and Cora datasets, the feature vector has binary entries indicating the presence/absence of the corresponding word from a dictionary. For PubMed dataset, the feature vector has real-values entries indicating Term Frequency-Inverse Document Frequency (TF-IDF) of the corresponding word from a dictionary. Although the networks are directed, we use undirected versions of the graphs for all experiments, as is common in all baseline approaches. Experimental setup. The accuracy of the baseline methods are all taken from existing literature. If a baseline result is not reported in the existing literature, we intentionally left those cells empty in the table for fairness, as opposed to running those experiments ourselves on untuned hyperparameters. We train and test only the two models we propose: GLN for comparisons and our proposed AGNN model. We do not use the validation set labels in training, but use them for optimizing hyper-parameters like dropout rate, learning rate, and L 2 -regularization factor. For AGNN, we use a Labeled nodes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Nodes Edges Classes Features <ref type="table" target="#tab_2">Table 2 Table 3  Table 4  CiteSeer  3,327  4,732  6  3,703  120  120  2,218  2,994  Cora  2,708  5,429  7  1,433  140  140  1,805  2,437  PubMed 19,717 44,328  3  500  60  60 13,145 17,745   Table 1</ref>: Citation Network Dataset fixed number of d h = 16 units in the hidden layers and use 4 propagation layers ( = 4) for CiteSeer and Pubmed and 2 propagation layers ( = 2) for Cora as defined in <ref type="bibr" target="#b6">(7)</ref>. For GLN, we use 2 propagation layers as defined in <ref type="bibr" target="#b0">(1)</ref>. We row-normalize the input feature vectors, as is standard in the literature. The tables below show the average accuracy with the standard error over 100 training instances with random weight initializations. We implement our model on TensorFlow <ref type="bibr" target="#b1">[2]</ref>, and the computational complexity of evaluating AGNN is O( d h |E| + d x d h n). Detailed desription of the experiments is provided in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative results</head><p>Fixed data splits. In this first experiment, we use the fixed data splits from <ref type="bibr" target="#b45">[46]</ref> as they are the standard benchmark data splits in literature. All experiments are run on the same fixed split of 20 labeled nodes for each class, 500 nodes for validation, 1,000 nodes for test, and the rest of nodes as unlabeled data. Perhaps surprisingly, the linear classifier GLN we proposed in (3) achieves performance comparable to or exceeding the state-of-the-art performance of GCN. This leads to our novel attention-based model AGNN defined in <ref type="bibr" target="#b5">(6)</ref>, which achieves the best accuracy on all datasets with a gap larger than the standard error. The classification accuracy of all the baseline methods are collected from <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b40">41]</ref>.  <ref type="table">Table 2</ref>: Classification accuracy with a fixed split of data from <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>In semi-supervised learning on graphs, it is critical to utilize both the structure of the graph and the node features. Methods not using all the given data achieve performance far from the state-of-the-art. Supervised methods-Single and Multi-layer Perceptrons-only use the labeled examples (Y L , X L ). Semi-supervised methods, e.g. T-SVM, only use the labeled and unlabeled examples Y L , and X. Skip-gram based approaches, such as DeepWalk, ignores the node features X and only use the labels Y L and the graph G.</p><p>A breakthrough result of Planetoid by Yang, Cohen, and Salakhutdinov <ref type="bibr" target="#b45">[46]</ref> significantly improved upon the existing skip-gram based method of DeepWalk and node2vec and the Laplacian regularized methods of ManiReg and SemiEmb. Kipf and Welling <ref type="bibr" target="#b26">[27]</ref> was the first to apply a graph neural network to citation datasets, and achieved the state-of-the-art performance with GCN. Other variations of graph neural networks immediately followed, achieving comparable performance with MoNet, Graph-CNN, and DynamicFilter. Bootstrap uses a Laplacian regularized approach of <ref type="bibr" target="#b47">[48]</ref> as a sub-routine with bootstrapping to feed high-margin predictions as seeds.</p><p>Random splits. Next, following the setting of Buchnik and Cohen <ref type="bibr" target="#b10">[11]</ref>, we run experiments keeping the same size in labeled, validation, and test sets as in <ref type="table">Table 2</ref>, but now selecting those nodes uniformly at random. This, along with the fact that different topics have different number of nodes in it, means that the labels might not be spread evenly across the topics. For 20 such randomly drawn dataset splits, average accuracy is shown in <ref type="table" target="#tab_2">Table 3</ref> with the standard error. As we do not force equal number of labeled data for each class, we observe that the performance degrades for all methods compared to <ref type="table">Table 2</ref>, except for DeepWalk. AGNN achieves the best performance consistently. Here, we note that Kipf and Welling <ref type="bibr" target="#b26">[27]</ref> does a similar but different experiment using GCN, where random labeled nodes are evenly spread across topics so that each topic has exactly 20 labeled examples. As this difference in sampling might affect the accuracy, we do not report those results in this table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CiteSeer  Larger training set. Following the setting of <ref type="bibr" target="#b37">[38]</ref>, we run experiments with larger number of labeled data on Cora dataset. We perform k-fold cross validation experiments for k = 3 and 10, by uniformly and randomly dividing the nodes into k equal sized partitions and then performing k runs of training by masking the labels of each of the k partitions followed by validation on the masked nodes. Finally the average validation accuracy across k runs is reported. We run 10 trials of this experiment and reports the mean and standard error of the average k-fold validation accuracy. Compared to <ref type="table">Table 2</ref>, the performance increases with the size of the training set, and AGNN consistently outperforms the current state-of-the-art architecture for this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>3-fold Split 10-fold split Graph-CNN <ref type="bibr" target="#b37">[38]</ref> 87.55±1.38 89.18±1.96 GLN 87.98±0.08 88.24±0.07 AGNN (this paper) 89.07±0.08 89.60±0.09 <ref type="table">Table 4</ref>: Classification accuracy with larger sets of labelled nodes.</p><p>Performance versus Number of Propagation Layers. In this section we provide experimental results justifying the choice of number of propagation layers for each dataset. We use the same data split and hyper-parameters (except number of propagation layer) as in the fixed data splits setting in Section 5.1. Similar to other settings the first propagation layer for Cora dataset is non-trainable with β (t=1) = 0. Tables 5 gives the average (over 10 trials) testing accuracy respectively for various choices of number of propagation layers. We note that different datasets require different number of propagation layers for best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative analysis</head><p>Inter-class relevance score. One useful aspect of incorporating attention into a model is that it provides some form of  interpretation capability <ref type="bibr" target="#b3">[4]</ref>. The learned P</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(t)</head><p>ij 's in Eq. <ref type="formula" target="#formula_11">(7)</ref> represent the attention from node j to node i, and provide insights on how relevant node j is in classifying node i. In <ref type="figure" target="#fig_0">Figure 1</ref>, we provide statistics of this attention over all adjacent pairs of nodes for Cora and CiteSeer datasets. We refer to <ref type="figure">Figure 3</ref> for similar statistics on PubMed. In <ref type="figure" target="#fig_0">Figure  1</ref>, we show average attention from a node in topic c 2 (column) to a node in topic c 1 (row), which we call the relevance from c 2 to c 1 and is defined as</p><formula xml:id="formula_13">Relevance(c 2 → c 1 ) = 1 |S c1,c2 | (i,j)∈Sc 1 ,c 2 R(j → i) ,<label>(8)</label></formula><p>for edge-wise relevance score defined as</p><formula xml:id="formula_14">R(j → i) = P (t) ij − 1 |N (i)| + 1 1 |N (i)| + 1 ,<label>(9)</label></formula><p>where |N (i)| is the degree of node i, and S c1,</p><formula xml:id="formula_15">c2 = {(i, j) ∈ E s and Y i = c 1 , Y j = c 2 } where E s = E ∪ {(i, i) for i ∈ V }</formula><p>is the edge set augmented with self-loops to include all the attentions learned. If we are not using any attention, then the typical propagation will be uniform P ij = 1/(|N (i)| + 1), in which case the above normalized attention is zero. We are measuring for each edge the variation of attention P ij from uniform 1/(|N (i)| + 1) as a multiplicative error, normalized by 1/(|N (i)| + 1). We believe this is the right normalization, as attention should be measure in relative strength to others in the same neighborhood, and not in absolute additive differences. We are measuring this multiplicative variation of the attention, averaged over the ordered pairs of classes. <ref type="figure" target="#fig_0">Figure 1</ref> shows the relevance score for CiteSeer and Cora datasets. (PubMed is shown in Appendix A.) For both datasets, the diagonal entries are dominant indicating that the attention is learning to put more weight to those in the same class. A higher value of Relevance(c 2 → c 1 ) indicates that, on average, a node in topic c 1 pays more attention to a neighbor in topic c 2 than neighbors from other topics. For CiteSeer dataset <ref type="figure" target="#fig_0">(Figure 1 left)</ref>, we are showing the average attention at the first propagation layer, P (t=1) , for illustration. In the off-diagonals, the most influential relations are HCI→Agents, Agents→ML, Agents→HCI, and ML→Agents, and the least influential relations are AI→IR and DB→ML. Note that these are papers in computer science from late 90s to early 2000s. For Cora dataset <ref type="figure" target="#fig_0">(Figure 1</ref> right), we are showing the relevance score of the second propagation layer, P (t=2) , for illustration. In the off-diagonals, the most influential relations are CB→PM, PM→CB, Rule→PM, and PM→Rule, and the least influential relations are GA→PM and PM→RL. This dataset has papers in computer science from the 90s. We note that these relations are estimated solely based on the available datasets for that period of time and might not accurately reflect the relations for the entire academic fields. We also consider these relations as a static property in this analysis. If we have a larger corpus over longer period of time, it is possible to learn the influence conditioned on the period and visualize how these relations change.</p><p>CiteSeer Cora PubMed Top 100 0.69 0.64 0.69 Bottom 100 0.34 0.09 0.13 <ref type="table">Table 6</ref>: Fraction of edges from top 100 most relevant edges and bottom 100 least relevant edges which are connecting two distinct nodes from the same class.</p><p>Attention quality. Next, we analyze the edges with high and low relevance scores. We remove the self-loops and then sort the edges according to the relevance score defined in Eq. <ref type="bibr" target="#b8">(9)</ref>. We take the top 100 and bottom 100 edges and with respect to their relevance scores, and report the fraction of the edges which are connecting nodes from the same class. <ref type="table">Table 6</ref> shows the result on the benchmark datasets for the relevance scores calculated using the last propagation layer. This suggests that our architecture learns to put higher attention between nodes of the same class.</p><p>PubMed 8434</p><p>PubMed 1580</p><p>CiteSeer 1512 <ref type="figure">Figure 2</ref>: We show three selected target nodes in the test set that are mistaken by GCN but correctly classified by AGNN. We denote this target node by the node with a thick outline (node 8434 from PubMed on the left, node 1580 from PubMed in the middle, and node 1512 from CiteSeer on the right). We show the strength of attention from a node in the 2-hop neighborhood to the target node by the size of the corresponding node. Colors represent the hidden true classes (nodes with the same color belong to the same topic). None of the nodes in the figure was in the training set, hence none of the colors were revealed. Still, we observe that AGNN has managed to put more attention to those nodes in the same (hidden) classes, allowing the trained model to find the correct labels.</p><p>Neighborhood attention. Finally, we analyze those nodes in the test sets that were mistaken by GCN but correctly classified by AGNN, and show how our attention mechanism weighted the contribution of its local neighborhood, and show three illustrative examples in <ref type="figure">Figure 2</ref>. More examples of this local attention network (including the legends for the color coding of the topics) are provided in Appendix A. We show a entire 2-hop neighborhood of a target node (marked by a thick outline) from the test set of the fixed data splits of Citeseer, Cora, or Pubmed. The colors denote the true classes of the nodes (including the target) in the target's neighborhood, some of which are unknown to the models at the training time. The radius of a node j is proportional to the attention to the target node i aggregated over all the layers, i.e. (P (t=4) P (t=3) P (t=2) P (t=1) ) ij for CiteSeer. The size of the target node reflects its self-attention defined in a similar way. The first example on the left is node 8434 from PubMed. AGNN correctly classifies the target node as light blue, whereas GCN mistakes it for yellow, possibly because it is connected to more yellow nodes. Not only has the attention mechanism learned to put more weight to its light blue 1-hop neighbor, but put equally heavy weights to a path of light blue neighbors some of which are not immediately connected to the target node. The second example in the middle is node 1580 from PubMed. AGNN correctly classifies it as yellow, whereas GCN mistakes it for a red, possibly because it only has two neighbors. Not only has the attention mechanism learned to put more weight to the yellow neighbor, but it has weighted the yellow neighbor (who is connected to many yellow nodes and perhaps has more reliable hidden states representing the true yellow class) even more than itself. The last example on the right is node 1512 from CiteSeer. AGNN correctly classifies it as light blue, whereas GCN mistakes it for a white. This is a special example as those two nodes are completely isolated. Due to the static and non-adaptive propagation of GCN, it ends up giving the same prediction for such isolated pairs. If the pair has two different true classes, then it always fails on at least on one of them (in this case the light blue node). However, AGNN is more flexible in adapting to such graph topology and puts more weight to the target node itself, correctly classifying both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we present an attention-based graph neural network model for semi-supervised classification on a graph. We demonstrate that our method consistently outperforms competing methods on the standard benchmark citation network datasets. We also show that the learned attention also provides interesting insights on how neighbors influence each other. In training, we have tried more complex attention models. However, due to the increased model complexity the training was not stable and does not give higher accuracy. We believe that for semi-supervised setting with such a limited number of labeled examples, reducing model complexity is important. Note that we are able to train deeper (4layers) models compared to a shallower (2-layers) model of GCN, in part due to the fact that we remove the non-linear layers and reduce the model complexity significantly. In comparison, deeper GCN models are known to be unstable and do not give the performance of shallower GCNs <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional experiments on interpretability of attention</head><p>PubMed dataset has 3 classes, and the relevance score is shown in the <ref type="figure">Figure 3</ref>. We also show examples of 2-hop local neighborhood of nodes that are mistaken by GCN but correctly classified by AGNN in <ref type="figure" target="#fig_2">Figures 4, 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Performance of GCN on other dataset splits.</head><p>Here we provide the performance of GCN on random splits and larger training set dataset splits from Section 5.1. We relegate these tables to the appendix since they were not present in <ref type="bibr" target="#b26">[27]</ref>. We conducted the experiments with the   <ref type="table">Table 7</ref>: Classification accuracy with random splits of the data. same hyper-parameters as chosen by <ref type="bibr" target="#b26">[27]</ref> for the fixed split. In <ref type="table" target="#tab_7">Tables 7 and 8</ref> we provide average testing accuracy and standard error over 20 and 10 runs on random splits and larger training set respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiment and Architect Details</head><p>In this section we will list all the choices made in training and tuning of hyper-parameters. The parameters are chosen as to maximize the validation. All the models use Adam optimization algorithm with full-batchs, as standard in other works on GNNs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref>. We also a weight decay term to the objective function for all the learnable weights. We add dropout to the first and last layers of all models. In <ref type="table">Table 9</ref>. we show the hyper-parameters used in training the AGNN models for various settings and datasets. For Cora dataset the architecture consist of = 2 propagation layers, but the first propagation layer P (t=1) is non-trainable and the variable β (t=1) value is fixed at zero. While training these AGNN models we maintain the validation accuracy for each iteration and finally choose the trained model parameters from the iteration where average validation accuracy of previous 4 epochs is maximized. For the k-fold cross validation setting we take the epoch with maximum validation accuracy. For the Graph Linear Network (GLN) as define in <ref type="formula" target="#formula_6">(3)</ref>, we use the same hyper-parameters as GCN <ref type="bibr" target="#b26">[27]</ref> for all the experimental settings: hidden dimension of 16, learning rate of 0.01, weight decay of 5 × 10 −4 , dropout of 0.5, 200 epochs and early stopping criteria with a window size of 10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>3-fold Split 10-fold split Graph-CNN <ref type="bibr" target="#b37">[38]</ref> 87.55±1.38 89.18±1.96 GCN 87.23±0.21 87.78±0.04 GLN 87.98±0.08 88.24±0.07 AGNN (this paper) 89.07±0.08 89.60±0.09  <ref type="table">Table 9</ref>: Hyper-parameters for AGNN model</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>-0.267 -0.356 -0.26 -0.14 -0.101 -0.096 -0.319 0.077 -0.271 -0.449 -0.54 -0.426 -0.126 -0.48 -0.248 0.028 -0.415 -0.273 -0.279 -0.316 -0.252 -0.339 -0.335 0.034 -0.145 -0.083 -0.166 -0.191 -0.513 -0.619 -0.129 0.026 0.017 -0.047 -0.167 -0.428 -0.315 -0.165 -0.011 0.043 -0.116 -0.221 -0.524 -0.372 -0.348 -0.077 -0.18 0Relevance score in Eq. (8) from a neighbor node with column-class to a center node in row-class. For example the average normalized attention to Agents from HCI is = −0.141 , largest off-diagonal entry in CiteSeer. The average attention to Probabilistic Methods (PM) from Case Based (CB) is 0.017, largest off-diagonal entry in Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Average attention in Eq. (8) from a column class to a row class Examples from CiteSeer dataset of attention strength in the local neighborhood of a target node (in thick outline) from the test set that is mistaken by GCN but correctly classified by AGNN. Colors are true classes and node sizes are proportional to the attention strength from a neighbor to the target node. Labeled nodes from training set are marked with '*'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Examples from Cora dataset of attention strength in the local neighborhood of a target node (in thick outline) from the test set that is mistaken by GCN but correctly classified by AGNN. Colors are true classes and node sizes are proportional to the attention strength from a neighbor to the target node. Labeled nodes from training set are marked with '*'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Examples from Pubmed dataset of attention strength in the local neighborhood of a target node (in thick outline) from the test set that is mistaken by GCN but correctly classified by AGNN. Colors are true classes and node sizes are proportional to the attention strength from a neighbor to the target node. None of the nodes are labeled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>4±0.45 80.0±0.43 77.7±0.63 AGNN (this paper) 69.8±0.35 81.0±0.34 78.0±0.46</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell>PubMed</cell></row><row><cell>DeepWalk [35]</cell><cell>47.2</cell><cell>70.2</cell><cell>72.0</cell></row><row><cell>node2vec [22]</cell><cell>47.3</cell><cell>72.9</cell><cell>72.4</cell></row><row><cell>Bootstrap [11]</cell><cell>50.3</cell><cell>78.2</cell><cell>75.6</cell></row><row><cell>GLN</cell><cell>68.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy with random splits of the data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Average testing error of AGNN for different number of Propagation Layers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>9±0.50 79.2±0.46 77.5±0.61 GLN 68.4±0.45 80.0±0.43 77.7±0.63 AGNN (this paper) 69.8±0.35 81.0±0.34 78.0±0.46</figDesc><table><row><cell>Method</cell><cell>CiteSeer</cell><cell>Cora</cell><cell>PubMed</cell></row><row><cell>DeepWalk [35]</cell><cell>47.2</cell><cell>70.2</cell><cell>72.0</cell></row><row><cell>node2vec [22]</cell><cell>47.3</cell><cell>72.9</cell><cell>72.4</cell></row><row><cell>Bootstrap [11]</cell><cell>50.3</cell><cell>78.2</cell><cell>75.6</cell></row><row><cell>GCN</cell><cell>66.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Classification accuracy with larger sets of labelled nodes.</figDesc><table><row><cell>Setting</cell><cell>Dataset</cell><cell>Propagation layers ( )</cell><cell>Hidden state dimension (d h )</cell><cell>Learning Rate</cell><cell>Weight Decay</cell><cell cols="2">Dropout Epochs</cell></row><row><cell cols="2">Fixed Split CiteSeer</cell><cell>4</cell><cell>16</cell><cell cols="2">0.005 0.0005</cell><cell>0.5</cell><cell>1000</cell></row><row><cell>Fixed Split</cell><cell>Cora</cell><cell>2</cell><cell>16</cell><cell cols="2">0.01 0.0005</cell><cell>0.5</cell><cell>1000</cell></row><row><cell cols="2">Fixed Split PubMed</cell><cell>4</cell><cell>16</cell><cell>0.008</cell><cell>0.001</cell><cell>0.5</cell><cell>400</cell></row><row><cell cols="2">Rand. Split CiteSeer</cell><cell>4</cell><cell>16</cell><cell cols="2">0.01 0.0005</cell><cell>0.5</cell><cell>1000</cell></row><row><cell>Rand. Split</cell><cell>Cora</cell><cell>3</cell><cell>16</cell><cell cols="2">0.01 0.0005</cell><cell>0.5</cell><cell>1000</cell></row><row><cell cols="2">Rand. Split PubMed</cell><cell>4</cell><cell>16</cell><cell>0.008</cell><cell>0.001</cell><cell>0.5</cell><cell>1000</cell></row><row><cell>Cross Val.</cell><cell>Cora</cell><cell>3</cell><cell>16</cell><cell cols="2">0.04 0.0005</cell><cell>0.25</cell><cell>500</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Random walk which moves from a node to one of its neighbors selected uniformly at random.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We have experimented other types of more complex attention formulations but we found the training becomes much less stable especially when the number of labeled nodes is small.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://linqs.soe.ucsc.edu/node/236</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Yu Wang and Mason Ng for helping us run the experiments efficiently on the Google servers. We would also like to thank Fei-Fei Li, Wei Wei, Zhe Li, Xinlei Chen and Achal Dave for insightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep Gaussian Embedding of Attributed Graphs: Unsupervised Inductive Learning via Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnemann</forename><forename type="middle">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03815</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning from labeled and unlabeled data using graph mincuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchi</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;01 Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael M Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08097</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Community Detection with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08415</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bootstrapped Graph Diffusions: Exposing the Power of Nonlinearity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliav</forename><surname>Buchnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edith</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02618</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning (chapelle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<editor>o. et al.</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning Combinatorial Optimization Algorithms over Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01665</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">One-Shot Imitation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07326</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David K Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">LASAGNE: Locality And Structure Aware Graph Node Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Faerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06520</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, 2005. IJCNN&apos;05. Proceedings. 2005 IEEE International Joint Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep convolutional networks on graph-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">VAIN: Attentional Multi-agent Predictive Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2698" to="2708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning (ICML-03</title>
		<meeting>the 20th International Conference on Machine Learning (ICML-03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08402</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual international conference on machine learning</title>
		<meeting>the 33rd annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised text classification using EM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semi-Supervised Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="33" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A Note on Learning Algorithms for Quadratic Assignment with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nowak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07450</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Robust Spatial Filtering with Graph Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe Petroski</forename><surname>Such</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00792</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning Multiagent Communication with Backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dynamic Filters in Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05206</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06525</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast network embedding enhancement via high order proximity approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03</title>
		<meeting>the 20th International conference on Machine learning (ICML-03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
