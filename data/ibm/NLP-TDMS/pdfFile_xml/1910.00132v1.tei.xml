<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule Routing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duarte</surname></persName>
							<email>kevinduarte@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer</orgName>
								<orgName type="institution">Vision University of Central Florida Orlando</orgName>
								<address>
									<postCode>32816</postCode>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
							<email>yogesh@crcv.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer</orgName>
								<orgName type="institution">Vision University of Central Florida Orlando</orgName>
								<address>
									<postCode>32816</postCode>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>shah@crcv.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer</orgName>
								<orgName type="institution">Vision University of Central Florida Orlando</orgName>
								<address>
									<postCode>32816</postCode>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule Routing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we propose a capsule-based approach for semi-supervised video object segmentation. Current video object segmentation methods are frame-based and often require optical flow to capture temporal consistency across frames which can be difficult to compute. To this end, we propose a video based capsule network, CapsuleVOS, which can segment several frames at once conditioned on a reference frame and segmentation mask. This conditioning is performed through a novel routing algorithm for attention-based efficient capsule selection. We address two challenging issues in video object segmentation: 1) segmentation of small objects and 2) occlusion of objects across time. The issue of segmenting small objects is addressed with a zooming module which allows the network to process small spatial regions of the video. Apart from this, the framework utilizes a novel memory module based on recurrent networks which helps in tracking objects when they move out of frame or are occluded. The network is trained end-to-end and we demonstrate its effectiveness on two benchmark video object segmentation datasets; it outperforms current offline approaches on the Youtube-VOS dataset while having a run-time that is almost twice as fast as competing methods. The code is publicly available at https://github.com/KevinDuarte/CapsuleVOS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semi-supervised video object segmentation aims to segment objects in a video, given their segmentation masks for the first frame. This is a challenging problem because of issues like occlusion, changes in object appearance over time, motion blur, fast motions, and scale variations of different objects. Deep learning approaches have achieved impressive results and the recent release of the Youtube-VOS dataset <ref type="bibr" target="#b36">[37]</ref> has allowed for the training and evaluation of new methods on a wider variety of videos and objects.</p><p>The majority of current approaches can be divided into two categories. The first are detection-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref> that learn representations of the object segmented in the first frame and attempt to perform the pixel-wise detection of this object in future frames; the second is propagationbased methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> that formulate the task as a tracking problem and attempt to propagate the mask to fit the object over time. The first set of methods tends to segment single frames independently and rarely employ temporal information, while the later set segments single frames sequentially and makes use of temporal information, usually in the form of optical flow or RNNs. There has been some work on hybrid methods, that attempt to unify both approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>We propose a hybrid method that makes use of a video capsule network to segment a video conditioned on the segmented object in the first frame. A capsule is a group of neurons that represents an object, or part of an object. Layers in capsule networks undergo a routing-by-agreement algorithm that finds similarities between these capsules, and allow for the modeling of part-to-whole relationships. Capsule networks have performed well in image classification <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b10">11]</ref>, and have shown outstanding results in various segmentation tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8]</ref>. In this paper, we leverage the segmentation ability of capsule networks and the ability of the routing algorithm to find similarity between capsules for the task of semi-supervised video object segmentation.</p><p>Our video capsule network, CapsuleVOS, contains two branches: a video branch and a frame branch. The video branch processes several frames at once and produces a set of video capsules. This allows the network learn temporal/motion information without the reliance of optical flow. The frame branch processes the first frame and object segmentation and generates a set of frame capsules, which model the object of interest. The frame branch makes use of a recurrent memory module that allows the network to overcome issues like occlusion or objects exiting the scene.</p><p>Both sets of capsules are then passed through our novel attention-routing procedure which allows the frame capsules to condition the video capsules. Through this routing algorithm, our network learns where the object of interest is within the video clip, allowing the network to segment multiple frames simultaneously.</p><p>Moreover, our method makes use of a parametrized zooming module which allows the network to focus on regions of the frame which are relevant to the object of interest. This module allows for the segmentation of smaller objects, which can easily be lost when resizing frames to lower spatial dimensions.</p><p>We make the following contributions in this work,</p><p>• We present a novel capsule network for the task of video object segmentation that achieves state-of-theart results on the largest video segmentation dataset.</p><p>• We propose a novel attention based EM routing algorithm to condition capsules based on an input segmentation.</p><p>• The proposed network contains integrated zooming module and memory module, which we show through experimental results to be effective for segmenting small and occluded objects in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semi-supervised video object segmentation: Earlier works in video object segmentation used hand-crafted features based on appearance, boundary and optical flow <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23]</ref>. The availability of large-scale video object segmentation datasets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37]</ref> enabled us to explore deep learning methods for this problem. Most of the early works are mainly motivated by the image segmentation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b19">20]</ref>. These works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref> lack the integration of sequential modelling which is important from video perspective. In some of these works, the temporal consistency is achieved by taking a guidance from the predicted mask of the previous frame <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref>. The majority of recent works also utilize online learning <ref type="bibr" target="#b1">[2]</ref> in which the segmentation networks are fine-tune on the first frame of each test video -this greatly improves segmentation results at the expense of inference speed. Several recent works have utilized recurrent units to learn the evolution of objects over time. The authors in <ref type="bibr" target="#b27">[28]</ref> use a ConvGRU to combine the outputs of pretrained appearance and a motion networks and generate a final segmentation. Similarly, the authors in <ref type="bibr" target="#b35">[36]</ref> propose a Con-vLSTM sequence-to-sequence model that learns to generate segmentations from sequences of frames. Ventura et al. <ref type="bibr" target="#b30">[31]</ref> also use a ConvLSTM for recurrence in both the temporal domain (between frames) and the spatial domain (between object instances within each frame). Our use of a recurrent memory unit differs from these methods in that we do not generate segmentations directly from the features generated by the ConvLSTM, but rather condition a segmentation network based on these features.</p><p>Segmentation of small objects is challenging and zooming in on regions of the frame has been explored to overcome this problem. The authors in <ref type="bibr" target="#b6">[7]</ref> demonstrated the effectiveness of processing only a tight region around the foreground object. Although this allows for improved segmentations, it assumes the object moves smoothly within the video -in cases of large motions, this may fail. Our approach can handle this issue, since our network learns the extent to which it must zoom in on the object of interest, allowing the network to learn these cases where large motions occur. The work in <ref type="bibr" target="#b4">[5]</ref> performs segmentation by tracking parts -their network zooms in on and processes each part of the object separately. This requires multiple passes through their segmentation model, instead of having a single segmentation of the whole object.</p><p>Capsule networks: The idea of capsules was first introduced in <ref type="bibr" target="#b9">[10]</ref>, and they were popularized in <ref type="bibr" target="#b25">[26]</ref>, where dynamic routing for capsules was proposed. This was further extended in <ref type="bibr" target="#b10">[11]</ref>, where a more effective EM routing algorithm was introduced. Recently, capsule networks have shown state-of-the-art results for human action localization in video <ref type="bibr" target="#b7">[8]</ref>, object segmentation in medical images <ref type="bibr" target="#b17">[18]</ref>, and text classification <ref type="bibr" target="#b38">[39]</ref>. In this work, we propose a capsule based network for video object segmentation where we introduce a novel attention based EM routing which can be used as a conditioning mechanism for capsules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>We propose an end-to-end trained network that segments an object throughout an entire video clip when given the object's segmentation mask for the first frame. This network contains two modules, depicted in <ref type="figure">Figures 1 and 2</ref>: a frame-conditioned video capsule network, CapsuleVOS, which segments a short video clip (8 frames) based on the object segmentation in the first frame, and a zooming module, which refines the spatial area processed by the capsule network. Section 3.1 explains how we leverage capsules for the task of video object segmentation, with our attentionbased routing algorithm. We then describe the CapsuleVOS architecture and the zooming module in sections 3.2 and 3.3 respectively. This is followed by the objective function used to train this network in section 3.4. <ref type="figure">Figure 1</ref>. CapsuleVOS Architecture. The network is given the low resolution video clip and the segmented object in the first frame, and generates the foreground segmentations for all frames of the clip. The memory module consists of a ConvLSTM and allows the network to overcome issues like occlusion and objects leaving the frame. The previous and new memory states are the hidden and cell states of the ConvLSTM for time steps t and t − 1 respectively. The new memory state is passed to the memory module for the following video clip. capsules described in <ref type="bibr" target="#b10">[11]</ref>, which have a logistic unit (an activation denoted by a) representing the presence of the entity and a 4 × 4 pose matrix (denoted by M ) which contains the properties of the entity. Capsules in one layer vote for the pose matrices of many capsules in the following layer and an iterative EM routing algorithm finds the agreement between the votes to create the set of capsules in the next layer. For a more comprehensive understanding of capsules, and the intuition behind them, we suggest reading <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>We view capsule networks' ability to model entities and find agreement between entities as an ideal mechanism to accomplish the semi-supervised video object segmentation task. A given video may contain several objects and the reference segmentation mask specifies the object which must be segmented. If we extract a set of capsules from both the video and the reference frame with a segmentation mask, then the former set (video capsules) models all objects within the video, while the latter set (frame capsules) represents the object of interest. Then, to obtain the object of interest throughout the video, one only needs to filter out all video capsules that are dissimilar to the frame capsules; in other words, an agreement, or similarity, between the video capsules and frame capsules would result in the set of video capsules that represent the object that must be segmented. Although the original EM routing algorithm works well for finding agreement within a set of capsules, it can not explicitly find agreement between two sets of capsules. For this reason, we propose an attention-based routing algorithm which finds the agreement between two sets of capsules.</p><p>Here, we use the query, key, value terminology found in <ref type="bibr" target="#b29">[30]</ref>, as our conditioning algorithm takes inspiration from this attention mechanism. From a video clip we extract a set of the video capsules M V i , a V i , indexed by i; from a reference frame and segmentation mask, we extract a set of frame capsules M F k , a F k , indexed by k. The key-value pairs are votes from the video capsules for the following layers' capsules while the query is the set of votes from the frame capsules. These votes are calculated as follows:</p><formula xml:id="formula_0">V k ij = M V i W k ij V v ij = M V i W v ij V q kj = M F k W q kj<label>(1)</label></formula><p>where W k ij , W v ij , and W q kj are learned weight matrices. The superscripts k, v, and q correspond to the key, value, and query respectively.</p><p>Once these votes are obtained, the EM routing operation is performed for the frame capsule (query) votes. This results in a set of higher-level capsules M q j , a q j , which represents the object, or parts of the object, in the reference segmentation mask. To find the similarity, or agreement, between the video capsules and the frame capsules, we measure the Euclidean distance between the key votes (V k ij ) and their corresponding higher-level query capsule:</p><formula xml:id="formula_1">D ij = h M q j − V k ij 2 h ,<label>(2)</label></formula><p>where h denotes the dimensions of the vote and pose matrices. This distance is used to compute an assignment coefficient</p><formula xml:id="formula_2">R v ij = e −Dij j e −Dij .<label>(3)</label></formula><p>The assignment coefficient, R v ij , determines the amount of information the i th video capsule sends to the j th higher- <ref type="figure">Figure 2</ref>. Zooming Module. Given the high-resolution first frame and segmentation mask, the zooming module outputs a bounding box around the object of interest. This bounding box is used to zoom in on the object in the video clip along with the first frame and segmentation mask, which are resized and passed into the CapsuleVOS network. level capsule. If the distance, D ij , is large, then the i th video capsule does not contain information pertaining the the object represented by the j th higher-level capsule, so its corresponding assignment coefficient is close to 0, and it sends less information to that higher-level capsule; conversely, a small distance leads to a large assignment coefficient, resulting in more information being sent.</p><p>We obtain the conditioned set of video capsules by performing the M-step of the EM routing algorithm using the value votes (V v ij ) and the video capsules' assignment coefficients. The result is a set of higher-level video capsules, M v j , a v j , that receive information from lower-level video capsules which agree with the frame capsules. This procedure of conditioning with capsules is described in Algorithm 1.</p><p>Algorithm 1 This routing algorithm returns the activations and pose matrices of the capsules in layer L + 1 when given the activations and poses of layer L (the video capsules and frame capsules). The indices i and j refer to the capsule types in layer L and L + 1 respectively. The index h refers to the dimensions of the vote or pose matrices. The EM ROUTING and M-STEP functions referenced are those defined in <ref type="bibr" target="#b10">[11]</ref>.</p><formula xml:id="formula_3">1: procedure ATTROUTING(M V , a V , M F , a F ) 2: V v ← M V W v 3: V k ← M V W k 4: V q ← M F W q 5: a q , M q ← EM ROUTING(a F , V q ) 6: D ij ← h M q i − V k ij 2 h</formula><p>For each i and j 7:</p><formula xml:id="formula_4">R v ij ← e −D ij j e −D ij</formula><p>For each i 8:</p><formula xml:id="formula_5">a v j , M v j ← M-STEP(a V , R v , V v , j) For each j 9:</formula><p>return a v , M v</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CapsuleVOS Architecture</head><p>The CapsuleVOS network segments 8 frames based on the segmentation mask of the first frame. It contains two branches -the video branch and the frame branch -and each creates sets of capsules. The video capsules are conditioned on the frame capsules, to produce a new set of conditioned capsules. These are followed by a convolutional capsule layer and a series of transposed convolutions to generate a segmentation map for all 8 frames.</p><p>The video branch passes the 8 RGB frames of size 128 × 224 through 6 (2+1)D convolutions <ref type="bibr" target="#b28">[29]</ref> to obtain feature maps of size 8 × 32 × 56 × 512. The video capsules are composed of 12 capsule types, which are obtained by passing the feature maps to strided 3 × 3 × 3 convolution operations.</p><p>The frame branch concatenates the first frame and the segmentation mask (each of size 128 × 224) and passes them through 4 2D convolutions. This is followed by the memory module, which consists of a ConvLSTM <ref type="bibr" target="#b33">[34]</ref> layer that allows the frame branch to maintain information which might be lost in cases of occlusion or objects leaving the frame. The ConvLSTM produces a set of features of shape 32×56×128 which are transformed into the frame capsules through a strided 3 × 3 convolution operation. The frame capsules, which are composed of 8 capsule types, are then tiled 8 times to match the temporal dimension of the video capsules.</p><p>Once the video and frame capsules have been formed, we perform capsule conditioning as described in Section 3.1, which results in a set of 16 capsule types. This is followed by a convolutional capsule layer that has 16 capsule types. All routing operations make use of capsule pooling <ref type="bibr" target="#b7">[8]</ref> to reduce network's memory consumption.</p><p>To obtain a foreground segmentation mask from this capsule representations we flatten the capsules' pose matrices and pass them to a decoder composed of strided transposed convolutions. Skip connections from the video capsules and conditioned capsules are used to maintain spatiotemporal information which is lost from striding. The result of this decoder is 8 frames of binary segmentations corresponding to the object of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Zooming Module</head><p>The zooming module is given the high-resolution first frame and the object of interest segmentation mask, and it outputs the bounding box containing the spatial region which our segmentation network will process. Since our segmentation network processes 8 frames at a time, the predicted bounding box must be large enough to contain the object of interest in all 8 frames, but not too large as to contain extraneous information not necessary for segmentation.</p><p>The input for the zooming module is a high-resolution frame (512 × 896) and the high-resolution binary object segmentation mask. These are passed through a series of strided 2D convolutional layers, a LSTM layer, and a fullyconnected layer which outputs two values,b h andb w , rep-resenting the height and the width of the bounding box centered on the object of interest. The LSTM layer allows the network to learn from motion information from previous time steps, resulting in larger bounding boxes for objects with more motion, and tighter bounding boxes for objects with relatively little motion. Once the bounding box is obtained, the network extracts this region from the highresolution segmentation mask and the next 8 frames of the high-resolution video; these are then resized to 128 × 224 and passed to CapsuleVOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective Function</head><p>For each pixel i in the video, we have ground-truth segmentations y i ∈ {0, 1} and our network predictsŷ i ∈ [0, 1]. We use both binary cross-entropy</p><formula xml:id="formula_6">L s = − 1 N N i=1 y i log (ŷ i ) + (1 − y i ) log (1 −ŷ i ) ,<label>(4)</label></formula><p>and the dice loss [21]</p><formula xml:id="formula_7">L D = 1− N i=1ŷ i y i + N i=1ŷ i + y i + − N i=1 (1 −ŷ i ) (1 − y i ) + N i=1 2 −ŷ i − y i + ,<label>(5)</label></formula><p>to train the network for segmentation. The term is a small value to ensure stability of the loss. We use this second segmentation loss because video object segmentation methods are evaluated using region similarity, or intersection-overunion (IoU), and the dice loss directly maximizes this metric.</p><p>We train the zooming module by computing the L2 loss between the ground-truth bounding box height and width (b h and b w ) and the predicted height and width (b h andb w ).</p><formula xml:id="formula_8">L r = b h −b h 2 + b w −b w 2 .<label>(6)</label></formula><p>During training, we define the ground-truth height and width as the bounding box centered at the object in the first frame that contains the object in the following 7 frames (the other frames in the clip to be processed). This ensures that the object of interest will be present in all frames being processed, even if there is a large amount of motion.</p><p>In and end-to-end fashion, we train our network with an objective function which is the sum of these three losses:</p><formula xml:id="formula_9">L = L s + L D + L r .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets We evaluate our method on two video object segmentation datasets: Youtube-VOS <ref type="bibr" target="#b36">[37]</ref> and DAVIS-2017 <ref type="bibr" target="#b24">[25]</ref>. Training The network is trained using the objective function described in 3.4. Since our segmentation loss requires segmentations for all 8 frames given to the network and the Youtube-VOS training set contains segmentations every 5th frame, we use the method found in <ref type="bibr" target="#b21">[22]</ref> to interpolate the segmentation frames that are unavailable. Training is done using the Adam optimizer <ref type="bibr" target="#b16">[17]</ref>, starting with a learning rate of 0.0001. When training on Youtube-VOS, the method converges in about 400 epochs. For our experiments on DAVIS-2017, we fine-tune the network for an extra 200 epochs on the DAVIS-2017 training videos.</p><p>Inference During inference, longer videos are processed one clip (8 frames) at a time; the segmentation generated from one clip is used as the input segmentation for the subsequent clip. We find that having frame overlaps between these clips results in improved segmentations at test time, with only a minor decrease of inference speed. All reported results (both accuracy and speed) use an overlap of 3 frames.</p><p>Evaluation Metrics For both datasets, we evaluate the segmentation results using the region similarity J and the contour accuracy F as described in <ref type="bibr" target="#b23">[24]</ref>. For Youtube-VOS, results are averaged over the "seen" categories -those objects found in training videos -and "unseen" categoriesthe objects present in the validation and testing sets but not present in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with State-of-the-art</head><p>Since our method does not use online learning, we compare with only offline approaches. The exception to this is OSVOS <ref type="bibr" target="#b1">[2]</ref>, which is a standard benchmark video object segmentation approach.</p><p>Youtube-VOS The performance of our network on Youtube-VOS are shown in <ref type="table" target="#tab_1">Table 1</ref>. Overall, our model performs at least 4% better than all offline methods and 3.5% better than OSVOS. OSVOS slightly outperforms us on unseen categories, but our network has a substantial 8% improvement in both of the "seen" metrics. Some qualitative results on Youtube-VOS videos are shown in <ref type="figure" target="#fig_0">Figure 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>OL J seen J unseen F seen F unseen Overall Speed (frames/s) OSVOS <ref type="bibr" target="#b1">[2]</ref> 59 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS-2017</head><p>Our performance on the DAVIS-2017 testdev set are shown in <ref type="table">Table 2</ref>. We find that our offline network is unable to achieve better results than many contemporary methods because many of the objects found in DAVIS-2017 do not appear in the Youtube-VOS training set. DyeNet <ref type="bibr" target="#b18">[19]</ref> is able to outperform our network by a wide margin; we attribute this to the fact that the method is image based, which allows their region-proposal network <ref type="figure">Figure 5</ref>. A qualitative comparison between networks with and without the zooming module. Rows 1,3  <ref type="bibr" target="#b18">[19]</ref> and feature extraction network to be pretrained on larger image datasets.</p><p>Speed Analysis Running on a Titan X Pascal GPU, our network segments an average of 13.5 frames per second. We compare our network's inference speed with other approaches in <ref type="figure">Figure 6</ref>. Our network is able to segment frames at a much faster rate than previous methods, because we simultaneously segment 8 frames at once as opposed to one frame at a time. <ref type="figure">Figure 6</ref>. Comparison of quality and speed of previous video object segmentation methods on the Youtube-VOS dataset. We graph the overall performance percentage vs the frames-per-second. The x-axis (fps) is in the log scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>All ablation experiments are performed on the Youtube-VOS dataset. The quantitative results for the ablations are shown in <ref type="table" target="#tab_3">Table 3</ref>. Zooming Module To test the effectiveness of our zooming module, we first evaluate our method without any zooming. In this experiment, we resize all frames to 128 × 224 and segment them with CapsuleVOS. Without the zooming module, the network's performance decreased by about 8%. The zooming module improves the segmentations in two ways: (1) the network is able to keep track of smaller objects, and (2) the network can generate finer segmentation masks for medium sized objects. <ref type="figure">Figure 5</ref> shows examples of our method with and without the zooming module; there is a noticeable decrease in segmentation accuracy for smaller objects without the zooming module. We also test if if a simple, hand-crafted zooming method would perform as well as our zooming module. In this experiment, we use a hand-crafted bounding-box around the foreground object in lieu of the zooming module. We find that the hand-crafted bounding-box results in improved segmentations when compared to no zooming, but the zooming module's learned bounding-boxes perform best.</p><p>Attention Routing We run two ablations to test the effectiveness of our proposed capsule routing algorithm. The first is performing conventional EM-routing by simply concatenating the video and frame capsules; the second is removing capsules entirely, and having a fully convolutional network with a similar number of parameters. We find that our proposed routing algorithm does improve segmentations when compared to simple capsule concatenation; this is because the proposed routing algorithm conditions the video capsules based on their agreement with the frame capsules, whereas concatenation does not differentiate between frame and video capsules and attempts to find agreement between all capsules. We also find that the network without capsules performs similar to the network with capsule concatenation; this suggests that the standard EM routing algorithm cannot effectively perform the conditioning operation which this tasks requires and that our proposed routing procedure successfully conditions the video capsules based on the frame capsules.</p><p>Memory Module In this final ablation, we test the importance of the memory module in the frame network. We find that this ConvLSTM improves results by 4%, because it allows the network to handle issues like occlusion or when the object of interest leaves the frame. <ref type="figure" target="#fig_1">Figure 4</ref> contains some qualitative results depicting the two issues that the memory module solves: occlusion and objects leaving the frame. Once the occlusion ends or the object re-enters the frame, the ConvLSTM allows the network to remember the object which it must segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a video capsule network, Cap-suleVOS, for semi-supervised video object segmentation. The use of capsules provides an effective modeling of entities present in the video and the attention-based routing helps in the tracking and segmentation of objects. The network contains two additional novel components: a zooming module and a memory module. The zooming module ensures the capture of small objects present in the video and the memory module tracks objects in scenarios when they are occluded or when they move out of the scene. The experimental evaluation demonstrates the effectiveness of our proposed network in video object segmentation and its ability to segment small and occluded objects. Moreover, our ablations show the effectiveness of our proposed routing procedure when compared to the exists EM routing algorithm. The network segments multiple frames at once which allows it to perform segmentation at a much faster rate when compared with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results showing object segmentations on videos from the Youtube-VOS validation set. The first three rows contain examples in which multiple instances of objects are present within the video; the later two show how our network is able to finely segment larger objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>A qualitative comparison between networks with and without the memory module. Rows 1,3: with memory module. Rows 2,4: without memory module. The first example contains a bear which is completely occluded for over 40 frames, but the memory module allows the network to segment the bear when it reappears. The second video shows that the memory module can handle cases where an object leaves and reenters the scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Youtube-VOS contains 4,453 videos -3,471 for training, 474 for validation, and 508 for testing. The training and validation videos have pixel-level ground truth annotations for every 5th frame (6 fps). The DAVIS-2017 dataset contains a total of 150 videos -60 for training, 30 for validation, 60 for testing. These testing videos are split into a test-dev and test-challenge set, each with 30 videos; we evaluate our method on the test-dev set. The videos in DAVIS-2017 have annotations for all frames. Both datasets contain a wide variety of objects and both contain videos with multiple object instances.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Our results on the Youtube-VOS validation set. "OL" denotes online learning. We compare with OSVOS<ref type="bibr" target="#b1">[2]</ref> and methods which do not perform online learning.</figDesc><table><row><cell></cell><cell>.8</cell><cell>54.2</cell><cell>60.5</cell><cell>60.7</cell><cell>58.8</cell><cell>0.10</cell></row><row><cell>OSMN [38]</cell><cell>60.0</cell><cell>40.6</cell><cell>60.1</cell><cell>44.0</cell><cell>51.2</cell><cell>7.14</cell></row><row><cell>S2S (offline) [36]</cell><cell>66.7</cell><cell>48.2</cell><cell>65.5</cell><cell>50.3</cell><cell>57.6</cell><cell>6.25</cell></row><row><cell>Our Method</cell><cell>67.3</cell><cell>53.7</cell><cell>68.1</cell><cell>59.9</cell><cell>62.3</cell><cell>13.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>,5: with zooming module. Rows 2,4,6: without zooming module. The first example demonstrates the network's ability to generate fine-grained segmentations on small objects when the zooming module is used. Very small objects that move rapidly, like those in examples 2 and 3, are lost rather quickly when the zooming module is not present.</figDesc><table><row><cell></cell><cell cols="3">OSVOS [2] DyeNet [19] Ours</cell></row><row><cell>Online Learning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>J Mean ↑</cell><cell>47.2</cell><cell>60.2</cell><cell>47.4</cell></row><row><cell>J Recall ↑</cell><cell>50.8</cell><cell>-</cell><cell>54.1</cell></row><row><cell>F Mean ↑</cell><cell>53.7</cell><cell>64.8</cell><cell>55.2</cell></row><row><cell>F Recall ↑</cell><cell>57.8</cell><cell>-</cell><cell>64.6</cell></row><row><cell>Global Mean</cell><cell>50.5</cell><cell>62.5</cell><cell>51.3</cell></row><row><cell cols="4">Table 2. Our results on the DAVIS-2017 test-dev set. We compare</cell></row><row><cell cols="3">with OSVOS [2] and the offline version of DyeNet</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation J seen J unseen F seen F unseen Overall Our ablation experiment results on the Youtube-VOS validation set. Each row corresponds to a different ablation. The final row contains the results of our method without any changes.</figDesc><table><row><cell>No Zooming</cell><cell>62.1</cell><cell>45.8</cell><cell>61.3</cell><cell>48.1</cell><cell>54.3</cell></row><row><cell>HC Zooming</cell><cell>65.8</cell><cell>51.7</cell><cell>66.5</cell><cell>57.5</cell><cell>60.4</cell></row><row><cell>Concat Routing</cell><cell>65.2</cell><cell>51.0</cell><cell>65.6</cell><cell>56.9</cell><cell>59.7</cell></row><row><cell>Fully Conv</cell><cell>64.5</cell><cell>51.5</cell><cell>64.8</cell><cell>57.0</cell><cell>59.4</cell></row><row><cell>No Memory</cell><cell>64.9</cell><cell>49.6</cell><cell>65.3</cell><cell>53.9</cell><cell>58.4</cell></row><row><cell>Full Method</cell><cell>67.3</cell><cell>53.7</cell><cell>68.1</cell><cell>59.9</cell><cell>62.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. D17PC00345. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="282" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Oneshot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7415" to="7424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="686" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video object segmentation by learning location-sensitive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="501" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Videocapsulenet: A simplified network for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="7621" to="7630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video segmentation by nonlocal consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida D</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Matrix capsules with em routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frosst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Motion-guided cascaded refinement network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yap-Peng</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="325" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervoxelconsistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Suyog Dutt Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="656" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Suyog Dutt Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Capsules for object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulas</forename><surname>Bagci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04241</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="90" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anestis</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Shankar Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4481" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rvos: Endto-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5277" to="5286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by referenceguided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maojun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1140" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="373" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="585" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6499" to="6507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Investigating capsule networks with dynamic routing for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00538</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
