<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Extension Module for Skeleton-Based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Obinata</surname></persName>
							<email>obinata.yuya@fujitsu.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Digital Innovation Core Unit FUJITSU LABORATORIES LTD. Kanagawa</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Yamamoto</surname></persName>
							<email>takuma.yamamoto@fujitsu.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Digital Innovation Core Unit FUJITSU LABORATORIES LTD. Kanagawa</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Extension Module for Skeleton-Based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action recognition</term>
					<term>Skeleton-based</term>
					<term>Graph convo- lution networks</term>
					<term>Temporal</term>
					<term>NTU RGB+D</term>
					<term>Kinetics-Skeleton</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a module that extends the temporal graph of a graph convolutional network (GCN) for action recognition with a sequence of skeletons. Existing methods attempt to represent a more appropriate spatial graph on an intra-frame, but disregard optimization of the temporal graph on the interframe. Concretely, these methods connect between vertices corresponding only to the same joint on the inter-frame. In this work, we focus on adding connections to neighboring multiple vertices on the inter-frame and extracting additional features based on the extended temporal graph. Our module is a simple yet effective method to extract correlated features of multiple joints in human movement. Moreover, our module aids in further performance improvements, along with other GCN methods that optimize only the spatial graph. We conduct extensive experiments on two large datasets, NTU RGB+D and Kinetics-Skeleton, and demonstrate that our module is effective for several existing models and our final model achieves state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Action recognition is a challenging task in computer vision and is on the cusp of applications toward understanding human activity <ref type="bibr" target="#b0">[1]</ref> and human social behavior <ref type="bibr" target="#b1">[2]</ref>. There are two mainstream methods using RGB images or a sequence of skeletons for action recognition. Action recognition with RGB images may achieve high performance. However, the method needs high-cost computing resources for real-time processing because the method handles several hundred pixels per image to extract features. The method is also affected by noise from various illumination conditions and the background. In contrast, action recognition with a sequence of skeletons, which represent a sequence of 2D or 3D coordinates as human body joints and trajectories, requires lower-cost computing resources than action recognition with RGB images because the method handles only a few dozen joints per skeleton. Moreover, skeletons are robust for the noise described above. Therefore, action recognition with a sequence of skeletons is suitable for practical applications. High-accuracy pose estimation methods such as OpenPose <ref type="bibr" target="#b4">[5]</ref> and CPN <ref type="bibr" target="#b5">[6]</ref> also support the practicality of the method. Thus, we focus on action recognition with a se-quence of skeletons (skeleton-based action recognition) in this work.</p><p>In skeleton-based action recognition, conventional deeplearning-methods use RNNs <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> to feed a sequence of skeletons as a sequence of vectors, or use CNNs <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> to extract features from a 2D pseudoimage that represents a sequence of skeletons. Recently, a graph convolutional network (GCN)-based method was proposed and drew attention owing to its achievement of high performance. GCN represents joints as vertices and their natural connections in the human body as edges and then calculates convolution based on vertices connected by edges. Therefore, GCN more naturally models the human body than a sequence vector and 2D pseudo-image. Thus, we focus on those methods using GCNs. Spatial temporal graph convolutional networks (ST-GCN) <ref type="bibr" target="#b6">[7]</ref> is the first method to use GCN for action recognition with a sequence of skeletons. ST-GCN includes a spatial graph and temporal graph to input a sequence of skeletons directly and extract features from joints on both the intra-and the inter-frames. Subsequently, many GCN methods representing a <ref type="figure">Fig. 1</ref>. Our concept. We draw vertices corresponding to human body joints with dots and edges corresponding to the joint connection with the black solid line as the spatial graph. The green solid line represents the edge for the temporal graph. In the conventional temporal graph, vertices corresponding only to the same joint on the inter-frame are connected (left). In this work, we focus on extending the temporal graph and connect neighboring multiple vertices as well as the same vertex on the inter-frame (right). more appropriate spatial graph on intra-frames have been proposed <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> and the performance has improved dramatically. However, these methods disregard optimization of the temporal graph on the inter-frame. We show an example in <ref type="figure">Fig.1</ref>. Conventional GCN methods connect between vertices corresponding only to the same joint for the temporal dimension ( <ref type="figure">Fig.1, left)</ref>. This methodology is certain to be effective in extracting feature of the trajectory of the same joints. However, it is too simple to extract the feature of the correlative movement between each joint on the inter-frame.</p><p>Our research goal is to optimize both spatial and temporal graphs for a further performance improvement. In this paper, we propose the temporal extension module (TEM) to extend the temporal graph on the inter-frame for skeleton-based action recognition. Our module directly adds edges to not only the same vertex but also neighboring multiple vertices and calculates convolution based on the same multiple vertices on the inter-frame ( <ref type="figure">Fig.1, right)</ref>. In <ref type="figure">Fig.1</ref>, right, the vertex corresponding to elbow at time connects joints corresponding to elbow, wrist, and shoulder at time − 1 on the inter-frame as an example. The elbow position at time and the connected joint position at time − 1 are correlated because these adjacent joints often move together in cases of action movement such as a "throw." Therefore, it is useful for action recognition to extract the feature from multiple adjacent joints at time − 1 as well as the same joint. Our module is simple yet effective in extracting correlated features of multiple adjacent joints that are connected in human body movement. Moreover, our module aids in further performance improvements, combining with other GCN-methods that optimize only the spatial graph. Implementation is easy because this module is independent of graph convolutions on the spatial graph. It has the benefit of reuse of the existing method. We experiment on two large-scale skeleton datasets to evaluate the performance of our module and show that our module is effective and the model with our module achieves state-of-the-art performance compared with previous state-of-the-art methods.</p><p>Our contributions of the paper are the following:</p><p>• We propose a temporal extension module for extending the temporal graph on the inter-frame. The module is simple yet effective in extracting correlated features of multiple adjacent joints that are connected in human body movement.</p><p>• We show the effectiveness of our module in ablation studies. We implement our module to some state-of-theart models and conduct extensive experiments to evaluate performance. We show that the model with our module outperforms models without our module on two large-scale datasets.</p><p>• We conduct several experiments to evaluate performance and show that the model with our module achieves state-of-the-art performance compared with previous state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Extensive research has been conducted on more appropriate representations of a sequence of skeletons for skeleton-based action recognition. There are two methods: handcraft-based and deep-learning based methods. In handcraft-based methods, for example, 3D skeletons are represented as a Lie group <ref type="bibr" target="#b13">[14]</ref> or vector-valued function <ref type="bibr" target="#b14">[15]</ref>. However, the performance of methods using handcrafted features is limited. Currently, datadriven methods using deep-learning are proposed owing to their high representation capacity. In deep-learning based action recognition, there are three methods: RNN-based <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, CNN-based <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, and GCN-based <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p><p>The RNN-based methods regard a sequence of skeletons as a sequence of vectors and feed a sequence of skeletons directly into several recurrent neural networks. Song et al. <ref type="bibr" target="#b15">[16]</ref> introduce an attention module <ref type="bibr" target="#b16">[17]</ref> and long short-term memory <ref type="bibr" target="#b17">[18]</ref> for the selection of important joints in temporal and spatial dimensions. The CNN-based methods represent a sequence of skeletons as some 2D pseudo-image and extract features through several convolutional neural networks. Li et al. <ref type="bibr" target="#b23">[24]</ref> introduce a skeleton transformer module to select important skeleton joints. Liu et al. <ref type="bibr" target="#b25">[26]</ref> introduce a method to transform a sequence of skeletons into the 2D pseudo-image, which is robust for view variations. However, these improvements are limited because it disregards the kinematic structure of the human body.</p><p>Recently, the GCN-based method has been proposed and has received attention due to its achievement of high performance. The GCN-based method represents a sequence of skeletons as a graph. In the graph, joints correspond to vertices and natural connections of the human body correspond to an edge. This representation has enabled extraction of features based on vertices connected by an edge. Therefore, the GCNbased method can represent the kinematic structure of the human body more naturally than the RNN-based and CNN-based methods. There are two approaches to extract feature from a graph: spectral-based approaches <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> and spatial-based approaches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Spectralbased approaches perform a graph convolution of input signals based on the spectral graph theory. Spatial-based approaches perform convolution directly on the vertices and their neighboring vertices. Since spatial-based approaches do not require high computational complexity, we follow spatial-based approaches in this work. Spatial temporal graph convolutional networks (ST-GCN) <ref type="bibr" target="#b6">[7]</ref> is the first method to use a graph convolutional neural network for action recognition with a sequence of skeletons. The spatial graph in ST-GCN is fixed, and the edge is connected between joints in a human natural body only. Therefore, Shi et al. propose a two-stream adaptive graph convolutional network (2s-AGCN) <ref type="bibr" target="#b8">[9]</ref> that learns optimal edges in the spatial graph. They also propose a directed graph neural network (DGNN) <ref type="bibr" target="#b9">[10]</ref> and a multi-stream adaptive graph convolutional network (MS-AAGCN) <ref type="bibr" target="#b11">[12]</ref>. DGNN <ref type="bibr" target="#b9">[10]</ref> intro-duces two-stage training processes to learn the optimal edges more flexibly and MS-AAGCN <ref type="bibr" target="#b11">[12]</ref> introduces attention modules and multi-stream networks to 2s-AGCN <ref type="bibr" target="#b8">[9]</ref>. These GCNbased methods mainly modify the structure of the spatial graph; however, the structure of the temporal graph remains the same as <ref type="bibr" target="#b6">[7]</ref>. Gao et al. <ref type="bibr" target="#b32">[33]</ref> introduce a graph regression based GCN (GR-GCN) that enables extending a temporal graph and learning of edge weights on the extended graph from training data. However, since graph regression module and GCN are not trained end-to-end learning, the edge weights are not optimum. Gao et al. <ref type="bibr" target="#b21">[22]</ref> introduce a latent node to capture temporal contextual information. However, the information on the correlation between each joint on the inter-frame is lost because the features of all joints on the intra-frame are aggregated on only one latent node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GRAPH CONVOLUTIONAL NETWORKS</head><p>In this section, we show a basic graph construction and the operation of graph convolution on a sequence of skeletons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph Construction on a Sequence of Skeletons</head><p>A sequence of skeletons in one sample is represented as a sequence of vectors. Each vector corresponds to spatially and temporally ordered joints of the 2D or 3D position. Yan et al. <ref type="bibr" target="#b6">[7]</ref> introduces a spatial-temporal graph to model such a sequence. Therefore, we review the spatial-temporal graph in <ref type="bibr" target="#b6">[7]</ref>. As shown in <ref type="figure">Fig.1</ref>, left, the graph represents joints as vertices and natural connections of the human body as edges on the intra-frame. Each vertex is also connected to the vertex corresponding to the same joint for the temporal dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatial Graph Convolution</head><p>Here, we consider graph convolution above the graph in the single frame case for simplicity. The output of graph convolution out ( ) for -th vertex on the intra-frame can be defined as <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_0">out ( ) = ∑ 1 ( ) ( ) • ( ( )) ∈ S ( ) ,<label>(1)</label></formula><p>where in ( ) is feature map for and is the weighting function that calculates a weight vector on given input. S ( ) denotes sampling area for and can be written as <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_1">S ( ) = { | ( , ) ≤ S },<label>(2)</label></formula><p>where ( , ) denotes the minimum length of any path from to . S is the maximum length for sampling on the intraframe. In this work, S is set to 1 as with <ref type="bibr" target="#b6">[7]</ref>.</p><p>In <ref type="formula" target="#formula_0">(1)</ref>, ( ) denotes the label map on at and empirically divides S ( ) into three kinds of subsets, namely vertex itself, neighboring vertices whose lengths to center of gravity are shorter than , and the remainder of the neighboring vertices. <ref type="figure" target="#fig_1">Fig.2</ref> show the example of subsets. The normalizing term ( ) denotes the cardinality of the corresponding subset.</p><p>In implementation, (1) is transformed as <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref>:</p><formula xml:id="formula_2">out = ∑ (( S ⊙ S ) in ) S S ,<label>(3)</label></formula><p>where out ∈ ℝ × out denotes output on input feature map in ∈ ℝ × in , is number of joints, out is number of output channels and in is number of input channels in one frame. In practice, for spatial temporal cases, we can represent the input feature map as the tensor ( × × in ), where is the number of frames in one sample. S ∈ ℝ × is the normalized adjacency matrix, i.e., </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Temporal Graph Convolution</head><p>Yan et al. <ref type="bibr" target="#b6">[7]</ref> also define graph convolution for temporal dimension. They performed simple × 1 convolution to out  in <ref type="formula" target="#formula_2">(3)</ref>, where denotes kernel size for the temporal dimension. Therefore, sampling area T ( ) for the temporal dimension on can be written as <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_3">T ( ) = { || − | ≤ ⌊ 2 ⌋} .<label>(4)</label></formula><p>In this work, is set to 9 as with <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. TEMPORAL EXTENSION MODULE</head><p>In this section, we introduce the temporal extension module (TEM) in detail and the temporal convolution added for the module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Temporal Extension Module</head><p>The conventional temporal graph convolution written in Sec. III calculates based on vertices corresponding to the same joints for temporal dimension. This way does not extract features of the neighboring vertices directly except at the same time. Therefore, it is too simple to extract the feature of the correlative movement between each joint on the inter-frame as explained in Sec. I.</p><p>To solve this limitation, we propose the temporal extension module. Our module adds edges to the vertices corresponding not only to the same joint but also to multiple adjacent joints on the inter-frame ( <ref type="figure">Fig.1, right)</ref>. In general, GCN-based method first extracts the spatial features at a current time and then extracts the temporal features for the long-term <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Our module extracts the features on inter-frame. To expand the sampling area for the temporal dimension gradually, we attach our module between conventional spatial graph and temporal graph convolution (i.e., conventional spatial graph convolution is calculated first, then temporal convolution of our module is calculated, and finally, conventional temporal convolution is calculated). We do not change the structure of conventional spatial graph convolution and temporal convolution at all. Therefore, our module can readily apply to many existing methods.</p><p>The output of the temporal convolution of our module out ( ) for -th vertex at time can be written as</p><formula xml:id="formula_4">out ( ) = ∑ 1 ( ( −1) ) ( ( −1) ) • ( ( −1) ( ( −1) )) ( −1) ∈ T ( ) (5) ,</formula><p>where ( −1) ( ( −1) ) denotes a label map on <ref type="bibr">( −1)</ref> . <ref type="formula">(5)</ref> is similar to <ref type="bibr" target="#b0">(1)</ref>. The difference between <ref type="formula">(5)</ref> and <ref type="formula" target="#formula_0">(1)</ref> is that the convolution on (5) is calculated from vertices at time − 1, while the convolution on (1) is calculated from vertices at time . In other words, our module extract features from multiple joints on the inter-frame. T ( ) denotes sampling area on the interframe for and can be written as</p><formula xml:id="formula_5">T ( ) = { ( −1) | ( ( −1) , ( −1) ) ≤ },<label>(6)</label></formula><p>where is maximum length for sampling on the inter-frame. In this work, is set to 1. T ( ) is divided into three kinds of subsets such as on conventional spatial convolution. <ref type="figure" target="#fig_2">Fig.3</ref> shows the example of subsets on the elbow at time . Comparing the sampling area with the sampling area of the conventional temporal convolution in (4), the temporal convolution of our modules chooses multiple neighboring joints on the interframe, while the conventional temporal convolution chooses a certain trajectory of the same joints for the temporal dimension. Therefore, our module helps to extract correlated features of multiple adjacent joints connected in human body movement.</p><p>In implementation, (5) is transformed into</p><formula xml:id="formula_6">out = ∑ (( T ⊙ T ) in −1 ) T T ,<label>(7)</label></formula><p>where T is the number of subsets on the inter-frame described above. In this work, we set T to 3 . in −1 ∈ ℝ × in equals output of spatial graph convolution out at time − 1 in ( 3 ). T ∈ ℝ × is a normalized adjacency matrix such as S . The difference is that T refers to connection of joints on <ref type="figure">Fig. 4</ref>. Implementation of our temporal extension module (TEM) in ST-GCN <ref type="bibr" target="#b6">[7]</ref>. ST-GCN <ref type="bibr" target="#b6">[7]</ref> consists of multiple layers containing a spatial graph convolution and a temporal graph convolution. We therefore attach one of our modules between these convolutions per a layer.</p><p>the inter-frame while S refers to connection of joints on the intra-frame and is the same as T and T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Detail</head><p>We show the implementation of our module in ST-GCN <ref type="bibr" target="#b6">[7]</ref> as the example in <ref type="figure">Fig.4</ref>. ST-GCN <ref type="bibr" target="#b6">[7]</ref> consists of multiple layers for the operation of spatial and temporal graph convolution as described above, including a global average pooling layer, a fully connected network and a softmax layer for action recognition. This one layer contains both spatial graph and temporal graph convolution. Therefore, per this one layer, we place one of our modules between these convolutions. Many graph convolutional networks for skeleton-based action recognition contain such multiple layers that include both spatial graph and temporal graph convolution separately <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>. We therefore implement our module readily to such models in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>To evaluate the performance of our module, we experiment on two large-scale skeleton datasets: NTU RGB+D and Kinetics-skeleton for an action recognition task. We perform ablation studies on these datasets to validate the effectiveness of our module. Next, we compare some models that added our modules with other state-of-the-art methods.</p><p>A. Datasets 1) NTU RGB+D NTU RGB+D <ref type="bibr" target="#b28">[29]</ref> is a large-scale and multi-modality dataset for skeleton-based action recognition. It consists of 56,880 video clips in 60 action classes (40 daily actions, nine health-related actions, and 11 mutual actions). These actions are performed by 40 subjects whose ages range from 10 to 35. The action clips are captured by three Microsoft Kinect v2 sensors (sensor 1, sensor 2, and sensor 3). These sensors are located at the same height and three horizontal angles. They set values at -45 degrees for sensor 1, 0 degrees for sensor 2, and 45 degrees for sensor 3. The dataset includes sequences of 3d skeleton that consist of 25 joints, RGB frames, depth maps and IR sequence. We only use sequences of 3d skeleton in this work. Two benchmarks are defined as Cross-Subject (CS) and Cross-View (CV) <ref type="bibr" target="#b28">[29]</ref>. In CS, the training sets contain 40,320 samples from 20 subjects and the test sets contain 16,560 samples from the remaining 20 subjects. In CV, the training sets contain 37,920 samples captured by sensor 2 and sensor 3 and the test sets contain 18,960 samples captured by sensor 1. Shahroudy et al. <ref type="bibr" target="#b28">[29]</ref> report top-1 accuracy on both benchmarks. Therefore, we follow these protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Kinetics-Skeleton</head><p>Kinetics-Skeleton <ref type="bibr" target="#b6">[7]</ref> is a large-scale dataset for skeletonbased action recognition. Original Kinetics <ref type="bibr" target="#b30">[31]</ref> contains about 300,000 video clips in 400 classes collected from YouTube. The original Kinetics does not contain joint information, so Yan. et al. <ref type="bibr" target="#b6">[7]</ref> estimate the 2D 18 joint coordinates and confidence per person using the OpenPose <ref type="bibr" target="#b4">[5]</ref> toolbox. The Open-Pose toolbox is publicly available. The released dataset is divided into training sets (240,000 clips) and test sets (20,000 clips). The clips have 300 frames and each frame contains top-2 persons for high confidence. Yan et al. <ref type="bibr" target="#b6">[7]</ref> reports top-1 and top-5 accuracy on the Kinetics-Skeleton. Therefore, we follow the protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model using Our Module for Experiments</head><p>We compare the performance of some models with and without our module. We select the current three state-of-the-art models, ST-GCN <ref type="bibr" target="#b6">[7]</ref>, 2s-AGCN <ref type="bibr" target="#b8">[9]</ref>, and MS-AAGCN <ref type="bibr" target="#b11">[12]</ref>, that are available for official implementation. Next, we implement our module in these models. We call these implemented models ST-GCN+TEM, 2s-AGCN+TEM, and MS-AAGCN+TEM. These models contain multiple layers, including spatial graph and temporal graph convolution for each layer. Therefore, we attach our module between these spatial graph and temporal graph convolution as described in Sec. IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Detail</head><p>All experiments are conducted on the PyTorch deep learning framework <ref type="bibr" target="#b28">[29]</ref>. We apply a stochastic gradient descent with Nesterov momentum for optimization strategy. We set Nesterov momentum, weight decay, and initial learning rate to 0.9, 0.0001, 0.1, respectively. We follow same preprocessing and hyper parameter as in ST-GCN <ref type="bibr" target="#b6">[7]</ref>, 2s-AGCN <ref type="bibr" target="#b8">[9]</ref>, and MS-AAGCN <ref type="bibr" target="#b11">[12]</ref> for fair comparison except batch size. We set batch size of all models to 32 on NTU RGB+D. On Kinetics-Skeleton, we set batch size to 128 on ST-GCN, ST-GCN+TEM, 2s-AGCN, and 2s-AGCN+TEM, and to 64 on MS-AAGCN and MS-AAGCN+TEM. These numbers are maximum values on our GPUs. We report the best result after performing the weighted-score-level fusion on a two-stream (joints and bones) for 2s-AGCN+TEM and a four-stream (joints, bones, joint motion, and bone motion) for MS-AAGCN+TEM. We search weighted parameters for the fusion by a hyperparameter optimization framework, Optuna <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>To validate the effectiveness of our module, we compare the performance of ST-GCN and ST-GCN+TEM, 2s-AGCN and 2s-AGCN+TEM, MS-AAGCN and MS-AAGCN+TEM.</p><p>We show the evaluation results in <ref type="table" target="#tab_0">Table I</ref>. For ST-GCN and ST-GCN+TEM, TEM brings improvements of 2.6% and 1.5% on CS and CV benchmarks on NTU RGB+D, respectively. TEM also brings improvements of 2.0% and 1.8% on Top-1 and Top-5 benchmarks on the Kinetics-skeleton, respectively. For 2s-AGCN and 2s-AGCN+TEM, TEM brings improvements of 0.1% and 0.6% on CS and CV benchmarks, respectively, on NTU RGB+D. TEM also brings improvements of 1.9% and 1.8% on Top-1 and Top-5 benchmarks, respectively on the Kinetics-skeleton. For MS-AAGCN and MS-AAGCN+TEM, TEM brings improvements of 0.7% and 0.4% on CS and CV benchmarks, respectively, on NTU RGB+D. TEM also brings improvements of 0.6% and 0.8% on Top-1 and Top-5 benchmarks, respectively, on the Kinetics-skeleton. These results show the effectiveness of our module. The results also show that the best performance model is MS-AAGCN+TEM on NTU RGB+D and 2s-AGCN+TEM on the Kinetics-Skeleton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with State-of-the-arts Methods</head><p>We compare the best performance model in ablation study with several state-of-the-art methods on NTU RGB+D and Kinetics-skeleton datasets respectively. We select MS-AAGCN+TEM from the NTU RGB+D dataset and 2s-AGCN+TEM from Kinetics-Skeleton for the comparison. The state-of-the-arts methods for comparisons is including RNNbased <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, CNN-based <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, GCN-based <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. We show the result on <ref type="table" target="#tab_0">Table II and Table III</ref> on NTU RGB+D and Kinetics-skeleton datasets respectively. For NTU RGB+D, MS-AAGCN+TEM achieves state-of-the-art performance on both CV and CS. For Kinetics-skeleton, 2s-AGCN+TEM achieves the state-of-the-art performance on Top-1 and Top-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose a TEM for skeleton-based action recognition. The module is simple yet effective for extracting the feature of neighboring multiple joints connected in human body movement by extending the temporal graph on the interframe. We conducted extensive experiments on two very large datasets, NTU RGB+D and Kinetics-Skeleton. As a result, we showed the effectiveness of our module and showed that the model in which our module is implemented achieves state-ofthe-art performance compared with previous state-of-the-art methods on both of them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTU-RGB+D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CS (%) CV (%)</head><p>ST-LSTM (Tree Traversal) + Trust Gate <ref type="bibr" target="#b18">[19]</ref> 69.2 77.7 TSRJI (Late Fusion) <ref type="bibr" target="#b26">[27]</ref> 73.3 80.3 STA-LSTM <ref type="bibr" target="#b15">[16]</ref> 73.4 81.2 ESV (Synthesized+Pre-trained) <ref type="bibr" target="#b25">[26]</ref> 80.0 87.2 VA-LSTM <ref type="bibr" target="#b19">[20]</ref> 79.6 87.6 ST-GCN <ref type="bibr" target="#b6">[7]</ref> 81.5 88.3 Si-GCN <ref type="bibr" target="#b33">[34]</ref> 84.2 89.1 CNN-based <ref type="bibr" target="#b23">[24]</ref> 83.2 89.3 ARRN-LSTM <ref type="bibr" target="#b22">[23]</ref> 81.8 89.6 DPRL+GCNN <ref type="bibr" target="#b7">[8]</ref> 83.5 89.8 multi-scale network (ResNet152 + 3scale) <ref type="bibr" target="#b24">[25]</ref> 85.0 92.3 Complete GR-GCN model <ref type="bibr" target="#b32">[33]</ref> 87.5 94,3 2s-AGCN <ref type="bibr" target="#b8">[9]</ref> 88.5 95.1 GCN-NAS <ref type="bibr" target="#b10">[11]</ref> 89.4 95.7 DGNN <ref type="bibr" target="#b9">[10]</ref> 89.9 96.1 MS-AAGCN <ref type="bibr" target="#b11">[12]</ref> 90.0 96.2 BAGCN <ref type="bibr" target="#b21">[22]</ref> 90.3 96.3 Sym-GNN <ref type="bibr" target="#b12">[13]</ref> 90.1 96.4 MS-AAGCN+TEM(Ours) 91.0 96.5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kinetics-Skeleton</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-1 (%) Top-5 (%)</head><p>ST-GCN [7] 30.7 52.8 2s-AGCN <ref type="bibr" target="#b8">[9]</ref> 36.1 58.7 DGNN <ref type="bibr" target="#b9">[10]</ref> 36.9 59.6 GCN-NAS <ref type="bibr" target="#b10">[11]</ref> 37.1 60.1 Sym-GNN <ref type="bibr" target="#b12">[13]</ref> 37.2 58.1 BAGCN <ref type="bibr" target="#b21">[22]</ref> 37.3 60.2 MS-AAGCN <ref type="bibr" target="#b11">[12]</ref> 37.8 61.0 2s-AGCN+TEM (Ours) 38.6 61. <ref type="bibr" target="#b5">6</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>denotes the degree matrix for normalization. S ̅̅̅̅ ∈ ℝ × denotes a summation of adjacency matrix , S ∈ {0,1} × and identity matrix for selfloops. On , S , ( , )-th A , S = 1 when the -th and the -th joints are connected with bone; otherwise, A , S = 0. ⊙ is the element-wise product. S ∈ ℝ × is the learnable matrix for multiplying weight to each vertex. S ∈ ℝ in × out denotes a weight matrix for 1 × 1 convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Subsets for spatial convolution. We draw body joints with dots and joint connection with a solid line. The cross marks the center of gravity. The numbers in dots represent indices of subsets on elbow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Subsets for temporal convolution of our module. The numbers in dots represent indices of subsets on elbow at time . We set multiple subsets to represent kinematics correlation as with<ref type="bibr" target="#b6">[7]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>COMPARISONS OF THE RECOGNITION ACCURACY THE MODELS WITH TEM AND WITHOUT TEM TABLE II. COMPARISONS OF THE RECOGNITION ACCURACY WITH MS-AAGCN+TEM AND CURRENT STATE-OF-THE-ART METHODS ON NTU RGB+D DATASET</figDesc><table><row><cell>Methods</cell><cell cols="2">NTU-RGB+D CS (%) CV (%)</cell><cell cols="2">Kinetics-Skeleton Top-1 (%) Top-5 (%)</cell></row><row><cell>ST-GCN</cell><cell>82.6</cell><cell>88.7</cell><cell>32.5</cell><cell>54.9</cell></row><row><cell>ST-GCN+TEM</cell><cell>85.2</cell><cell>90.2</cell><cell>34.5</cell><cell>56.7</cell></row><row><cell>2s-AGCN</cell><cell>88.6</cell><cell>95.2</cell><cell>36.7</cell><cell>59.8</cell></row><row><cell>2s-AGCN+TEM</cell><cell>88.7</cell><cell>95.8</cell><cell>38.6</cell><cell>61.6</cell></row><row><cell>MS-AAGCN</cell><cell>90.3</cell><cell>96.1</cell><cell>37.4</cell><cell>60.6</cell></row><row><cell>MS-AAGCN+TEM</cell><cell>91.0</cell><cell>96.5</cell><cell>38.0</cell><cell>61.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III .</head><label>III</label><figDesc>COMPARISONS OF THE RECOGNITION ACCURACY WITH 2S-AGCN+TEM AND CURRENT STATE-OF-THE-ART METHODS ON THE KINETICS-SKELETON</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2104</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Social scene understanding: end-to-end multi-person action localization and collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning graph convolutional network for skeleton-based human action recognition by neural searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition with multi-stream adaptive graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Symbiotic graph neural networks for 3d skeleton-based human action recognition and motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Human action recognition by representing 3d skeletons as points in a lie Group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Spatio-temporal LSTM with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Action recognition from 3d skeleton sequences using deep networks on lie group features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rhif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>Farah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Focusing and diffusion: Bidirectional attentive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Relational network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Skeleton image representation for 3d action recognition based on tree structure and reference joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIBGRAPI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">NTU RGB+D: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Optuna: A next-generation hyperparameter optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Optimized skeleton-based action recognition via sparsified graph regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in MM&apos;19</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Si-GCN: structure-induced graph convolution network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
