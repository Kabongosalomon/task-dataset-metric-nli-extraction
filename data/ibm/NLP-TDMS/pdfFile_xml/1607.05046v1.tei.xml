<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Cascaded Bi-Network for Face Hallucination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Cascaded Bi-Network for Face Hallucination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel framework for hallucinating faces of unconstrained poses and with very low resolution (face size as small as 5pxIOD 1 ). In contrast to existing studies that mostly ignore or assume pre-aligned face spatial configuration (e.g. facial landmarks localization or dense correspondence field), we alternatingly optimize two complementary tasks, namely face hallucination and dense correspondence field estimation, in a unified framework. In addition, we propose a new gated deep bi-network that contains two functionality-specialized branches to recover different levels of texture details. Extensive experiments demonstrate that such formulation allows exceptional hallucination quality on in-the-wild low-res faces with significant pose and illumination variations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Increasing attention is devoted to detection of small faces with an image resolution as low as 10 pixels of height <ref type="bibr" target="#b0">[1]</ref>. Meanwhile, facial analysis techniques, such as face alignment <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and verification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, have seen rapid progress. However, the performance of most existing techniques would degrade when given a low resolution facial image, because the input naturally carries less information, and images corrupted with down-sampling and blur would interfere the facial analysis procedure. Face hallucination <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, a task that super-resolves facial images, provides a viable means for improving low-res face processing and analysis, e.g. person identification in surveillance videos and facial image enhancement.</p><p>Prior on face structure, or face spatial configuration, is pivotal for face hallucination <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6]</ref>. The availability of such prior distinguishes the face hallucination task from the general image super-resolution problem <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, where the latter lacks of such global prior to facilitate the inference. In this study, we extend the notion of prior to pixel-wise dense face correspondence field. We observe that an informative prior provides a strong semantic guidance that enables face hallucination even from a very low resolution. Here the dense I 1 =↑I 0 +g 1 (↑I 0 ; W 1 (z)) p 2 =p 1 +f 2 (I 1 ; p 1 ) I 2 =↑I 1 +g 2 (↑I 1 ; W 2 (z)) p <ref type="bibr" target="#b2">3</ref>  The dense correspondence field prediction step The face hallucination step correspondence field is necessary for describing the spatial configuration for its pixel-wise (not by facial landmarks) and correspondence (not by face parsing) properties. The importance of dense field will be reflected in Sec. 3.2. An example is shown in <ref type="figure" target="#fig_0">Fig. 1</ref> -even an eye is only visible from a few pixels in a low-res image, one can still recover its qualitative details through inferring from the global face structure.</p><p>Nevertheless, obtaining an accurate high-res pixel-wise correspondence field is non-trivial given only the low-res input. First, the definition of the high-res dense field is by itself ill-posed because the gray-scale of each pixel is distributed to adjacent pixels on the interpolated image ( <ref type="figure" target="#fig_0">Fig. 1(c)</ref>). Second, the blur causes difficulties for many existing face alignment or parsing algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> because most of them rely on sharp edge information. Consequently, we face a chicken-and-egg problem -face hallucination is better guided by face spatial configuration, while the latter requires a high resolution face. This issue, however, has been mostly ignored or bypassed in previous works (Sec. 2).</p><p>In this study, we propose to address the aforementioned problem with a novel task-alternating cascaded framework, as shown as <ref type="figure" target="#fig_0">Fig. 1(d)</ref>. The two tasks at hand -the high-level face correspondence estimation and low-level face hallucination are complementary and can be alternatingly refined with the guidance from each other. Specifically, motivated by the fact that both tasks are performed in a cascaded manner <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref>, they can be naturally and seamlessly integrated into an alternating refinement process. During the cascade iteration, the dense correspondence field is progressively refined with the increasing face resolution, while the image resolution is adaptively upscaled guided by the finer dense correspondence field.</p><p>To better recover different levels of texture details on faces, we propose a new gated deep bi-network architecture in the face hallucination step in each cascade.  Deep convolutional neural networks have demonstrated state-of-the-art results for image super resolution <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. In contrast to aforementioned studies, the proposed network consists two functionality-specialized branches, which are trained end-to-end. The first branch, referred as common branch, conservatively recovers texture details that are only detectable from the low-res input, similar to general super resolution. The other branch, referred as high-frequency branch, super-resolves faces with the additional high-frequency prior warped by the estimated face correspondence field in the current cascade. Thanks to the guidance of prior, this branch is capable of recovering and synthesizing un-revealed texture details in the overly low-res input image. A pixel-wise gate network is learned to fuse the results from the two branches. <ref type="figure" target="#fig_3">Figure 2</ref> demonstrates the properties of the gated deep bi-network. As can be observed, the two branches are complementary. Although the high-frequency branch synthesizes the facial parts that are occluded (the eyes with sun-glasses), the gate network automatically favours the results from the common branch during fusion. We refer the proposed framework as Cascaded Bi-Networks (CBN) hereafter. We summarize our contribution as follows:</p><p>1. While conducting face hallucination or dense face correspondence field is hard on low-res images, we circumvent this problem through a novel taskalternating cascade framework. In comparison to existing approaches, this framework has an appealing property of not assuming pre-aligned inputs or any availability of spatial information (e.g. landmark, parsing map). 2. We propose a gated deep bi-network that can effectively exploit face spatial prior to recover and synthesize texture details that even are not explicitly presented in the low-resolution input. 3. We explore the lower bound of the input face resolution for recovering reasonable and high quality details, and provide extensive results and discussion.</p><p>We perform extensive experiments against general super-resolution and face hallucination approaches on various benchmarks. Our method not only achieves high Peak Signal to Noise Ratio (PSNR), but also superior quality perceptually. Demo codes will be available in our project page http://mmlab.ie.cuhk.edu. hk/projects/CBN.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Face hallucination and spatial cues. There is a rich literature in face hallucination <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Spatial cues are proven essential in most of previous works, and are utilized in various forms. For example, Liu et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9]</ref> and Jin et al. <ref type="bibr" target="#b5">[6]</ref> devised a warping function to connect the face local reconstruction with the high-res faces in the training set. However, a low-res correspondence field 2 may not be sufficient for aiding the high-res face reconstruction process, while obtaining the high-res correspondence field is ill-posed with only a low-res face given. Yang et al. <ref type="bibr" target="#b7">[8]</ref> assumed that facial landmarks can be accurately estimated from the low-res face image. This is not correct if the low-res face is rather small (e.g. 5pxIOD), since the gray-scale is severely distributed to the adjacent pixels ( <ref type="figure" target="#fig_0">Fig. 1(c)</ref>). Wang et al. <ref type="bibr" target="#b9">[10]</ref> and Kolouri et al. <ref type="bibr" target="#b6">[7]</ref> only aligned the input low-res faces with an identical similarity transform (e.g. the same scaling and rotation). Hence these approaches can only handle canonical-view low-res faces. Zhou et al. <ref type="bibr" target="#b25">[26]</ref> pointed out the difficulties of predicting the spatial configuration over a low-res input, and did not take any spatial cues into account for hallucination. In contrast to all aforementioned approaches, we adaptively and alternatingly estimate the dense correspondence field as well as hallucinate the faces in a cascaded framework. The two mutual tasks aid each other and hence our estimation of the spatial cues and hallucination can be better refined with each other. Cascaded prediction The cascaded framework is privileged both for image super-resolution (SR) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref> and facial landmark detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. For image SR, Wang et al. <ref type="bibr" target="#b14">[15]</ref> showed that two rounds of 2× upscaling is better than a single round of 4× upscaling in their framework. For facial landmark detection, the cascaded regression framework has revolutionized the accuracy and has been extended to other areas <ref type="bibr" target="#b30">[31]</ref>. The key success of the cascaded regression comes from its coarse-to-fine nature of the residual prediction. As pointed out by Zhang et al. <ref type="bibr" target="#b27">[28]</ref>, the coarse-to-fine nature can be better achieved by the increasing facial resolution among the cascades. To our knowledge, no existing work has integrated these two closely related tasks into a unified framework. The bi-network architecture The bi-network architecture <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> has been explored in various form, such as bilinear networks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> and two-stream convolutional network <ref type="bibr" target="#b36">[37]</ref>. In <ref type="bibr" target="#b34">[35]</ref>, the two factors, namely object identification and localization, are modeled by the two branches respectively. This is different from our model, where the two factors, the low-res face and the prior, are jointly modeled in one branch (the high-frequency branch), while the other branch (the common branch) only models the low-res face. In addition, the two branches are joined via the gate network in our model, different from the outer-production in <ref type="bibr" target="#b34">[35]</ref>. In <ref type="bibr" target="#b36">[37]</ref>, both spatial and temporal information are modeled by the network, which is different from our model, where no temporal information is incorporated. Our architecture also differs from <ref type="bibr" target="#b25">[26]</ref>. In <ref type="bibr" target="#b25">[26]</ref>, the output is the average weighted by a scalar between the result of one branch and the low-res input. Moreover, neither of the two branches utilizes any spatial cues or prior in <ref type="bibr" target="#b25">[26]</ref>. 3 Cascaded Bi-Network (CBN)</p><formula xml:id="formula_0">(a) Mean Face M (b) Face image I (c) High-Frequency Prior E (d) Warped Prior E W z 1 z 2 z 1 x 1 =W(z 1 ) z 2 x 2 =W(z 2 ) E(z 1 ) E(z 2 ) E W (x 1 )=E(z 1 ) E W (x 2 )=E(z 2 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Problem and notation. Given a low-resolution input facial image, our goal is to predict its high-resolution image. We introduce the two main entities involved in our framework:</p><p>The facial image is denoted as a matrix I. We use x ∈ R 2 to denote the (x, y) coordinates of a pixel on I.</p><p>The dense face correspondence field defines a pixel-wise correspondence mapping from M ⊂ R 2 (the 2D face region in the mean face template) to the face region in image I. We represent the dense field with a warping function <ref type="bibr" target="#b37">[38]</ref>, x = W (z) : M → R 2 , which maps the coordinates z ∈ M from the mean shape template domain to the target coordinates x ∈ R 2 . See <ref type="figure" target="#fig_4">Fig. 3</ref>(a,b) for a clear illustration. Following <ref type="bibr" target="#b38">[39]</ref>, we model the warping residual W (z) − z as a linear combination of the dense facial deformation bases, i.e.</p><formula xml:id="formula_1">W (z) = z + B(z)p (1) where p = [p 1 . . . p N ] ∈ R N ×1 denotes the deformation coefficients and B(z) = [b 1 (z) . . . b N (z)] ∈ R 2×N</formula><p>denotes the deformation bases. The N bases are chosen in the AAMs manner <ref type="bibr" target="#b39">[40]</ref>, that 4 out of N correspond to the similarity transform and the remaining for non-rigid deformations. Note that the bases are pre-defined and shared by all samples. Hence the dense field is actually controlled by the deformation coefficients p for each sample. When p = 0, the dense field equals to the mean face template. We use the hat notation (ˆ) to represent ground-truth in the learning step. For example, we denote the high-resolution training image asÎ.</p><p>Framework overview. We propose a principled framework to alternatively refine the face resolution and the dense correspondence field. Our framework <ref type="figure">Fig. 4</ref>. Architecture of the proposed deep bi-network (for the k-th cascade). It consists of a common branch (blue), a high-frequency branch (red) and the gate network (cyan).</p><formula xml:id="formula_2">E Wk ↑I k-1 Common Branch High-Frequency Branch G A G B {↑I k-1 ; E Wk } Gate G 1−G G +↑I k-1 I k</formula><p>consists of K iterations ( <ref type="figure" target="#fig_0">Fig. 1(d)</ref>). Each iteration updates the prediction via</p><formula xml:id="formula_3">p k = p k−1 + f k (I k−1 ; p k−1 ); W k (z) = z + B k (z)p k ;</formula><p>(2)</p><formula xml:id="formula_4">I k = ↑I k−1 + g k (↑I k−1 ; W k (z)); (∀z ∈ M k ),<label>(3)</label></formula><p>where k iterates from 1 to K. Here, Eq. 2 represents the dense field updating step while Eq. 3 stands for the spatially guided face hallucination step in each cascade. '↑' denotes the upscaling process (2× upscaling with bicubic interpolation in our implementation). All the notations are now appended with the index k to indicate the iteration. A larger k in the notation of</p><formula xml:id="formula_5">I k , W k , B k and M k 3</formula><p>indicates the larger resolution and the same k indicates the same resolution. The framework starts from I 0 and p 0 . I 0 denotes the input low-res facial image. p 0 is a zero vector representing the deformation coefficients of the mean face template. The final hallucinated facial image output is I K .</p><p>Model, inference and learning. Our model is composed of functions f k (dense field estimation) and g k (face hallucination with spatial cues). The deformation bases B k are pre-defined for each cascade and fixed during the whole training and testing procedures. During testing, we repeatedly update the image I k and the dense correspondence field W k (z) (basically the coefficients p k ) with Eq. 2, 3. The learning procedure works similarly to the inference but incorporating the learning process of the two functions -g k for hallucination and f k for predicting the dense field coefficients. We present their learning procedures in Sec. 3.2 and Sec. 3.3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">g k -Gated deep bi-network: Face hallucination with spatial cues</head><p>We propose a gated deep bi-network architecture for face hallucination with the guidance from spatial cues. We train one gated bi-network for each cascade. For the k-th iteration, we take in the input image ↑I k−1 and the current estimated dense correspondence field W k (z), to predict the image residual G = I k − ↑I k−1 .</p><p>As the name indicates, our gated bi-network contains two branches. In contrast to <ref type="bibr" target="#b34">[35]</ref> where two branches are joined with outer production, we combine the two branches with a gate network. More precisely, if we denote the output from the common branch (A) and the high-frequency branch (B) as G A and G B respectively, we combine them with</p><formula xml:id="formula_6">g k (↑I k−1 ; W k (z)) = G = (1 − G λ ) ⊗ G A + G λ ⊗ G B ,<label>(4)</label></formula><p>where G denotes our predicted image residual I k − ↑I k−1 (i.e. the result of g k ), and G λ denotes the pixel-wise soft gate map that controls the combination of the two outputs G A and G B . We use ⊗ to denote element-wise multiplication. <ref type="figure">Figure 4</ref> provides an overview of the gated bi-network architecture. Three convolutional sub-networks are designed to predict G A , G B and G λ respectively. The common branch sub-network (blue in <ref type="figure">Fig. 4</ref>) takes in only the interpolated low-res image ↑I k−1 to predict G A while the high-frequency branch sub-network (red in <ref type="figure">Fig. 4</ref>) takes in both ↑I k−1 and the warped high-frequency prior E W k (warped according to the estimated dense correspondence field). All the inputs (↑I k−1 and E W k ) as well as G A and G B are fed into the gate sub-network (cyan in <ref type="figure">Fig. 4</ref>) for predicting G λ and the final high-res output G.</p><p>We now introduce the high-frequency prior and the training procedure of the proposed gated bi-network.</p><p>High-frequency prior. We define high-frequency prior as the indication for location with high-frequency details. In this work, we generate high-frequency prior maps to enforce spatial guidance for hallucination. The prior maps are obtained from the mean face template domain. More precisely, for each training image, we compute the residual image between the original imageÎ and the bicubic interpolation of I 0 , and then warp the residual map into the mean face template domain. We average the magnitude of the warped residual maps over all training images and form the preliminary high-frequency map. To suppress the noise and provide a semantically meaningful prior, we cluster the preliminary high-frequency map into C continuous contours (10 in our implementation). We form a C-channel maps, with each channel carrying one contour. We refer this C-channel maps as our high-frequency prior, and denote it as E k (z) : M k → R C . We use E k to represent E k (z) for all z ∈ M k . An illustration of the prior is shown in <ref type="figure" target="#fig_4">Fig. 3(c)</ref>.</p><p>Learning the gated bi-network. We train the three parts of convolutional neural networks to predict G A , G B and G λ in our unified bi-network architecture. Each part of the network has a distinct training loss. For training the common branch, we use the following loss over all training samples</p><formula xml:id="formula_7">L A = Î k − ↑I k−1 − G A 2 F .<label>(5)</label></formula><p>The high-frequency branch has two inputs: ↑I k−1 and the warped high-frequency prior E W k (see <ref type="figure" target="#fig_4">Fig. 3(d)</ref> for illustration) to predict the output G B . The two inputs are fused in the channel dimension to form a (1 + C)-channel input. We use the following loss over all training samples</p><formula xml:id="formula_8">L B = C c=1 (E W k ) c ⊗ (Î k − ↑I k−1 − G B ) 2 F ,<label>(6)</label></formula><p>where (E W k ) c denotes the c-th channel of the warped high-frequency prior maps. Compared to the common branch, we additionally utilize the prior knowledge as input and only penalize over the high-frequency area. Learning to predict the gate map G λ is supervised by the final loss</p><formula xml:id="formula_9">L = Î k − ↑I k−1 − G 2 F .<label>(7)</label></formula><p>We train the proposed gated bi-network with three steps.</p><p>Step i : We only enable the supervision from L A (Eq. 5) to pre-train the common branch;</p><p>Step ii : We only enable L B (Eq. 6) to pre-train the high-frequency branch;</p><p>Step iii : We finally fine-tune the whole gated bi-network with the supervision from L (Eq. 7). In the last step, we set the learning rate of the parameters related to the gate map to be 10 times as the parameters in the two branches. Note that we can still use back-propagation to learn the whole bi-network in our last step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">f k -Dense field deformation coefficients prediction</head><p>We apply a simple yet effective strategy to update the correspondence field coefficients estimation (f k ). Observing that predicting a sparse set of facial landmarks is more robust and accurate under low resolution, we transfer the facial landmarks deformation coefficients to the dense correspondence field. More precisely, we simultaneously obtain two sets of N deformation bases: B k (z) ∈ R 2×N for the dense field, and S k (l) ∈ R 2×N for the landmarks, where l is the landmark index. The bases for the dense field and landmarks are one-to-one related, i.e. both B k (z) and S k (l) share the same deformation coefficients p k ∈ R N :</p><formula xml:id="formula_10">W k (z) = z + B k (z)p k ; x k (l) =x k (l) + S k (l)p k ,<label>(8)</label></formula><p>where x k (l) ∈ R 2 denotes the coordinates of the l-th landmark, andx k (l) denotes its mean location.</p><p>To predict the deformation coefficients p k in each cascade k, we utilize the powerful cascaded regression approach <ref type="bibr" target="#b22">[23]</ref> for estimation. A Gauss-Newton steepest descent regression matrix R k is learned in each iteration k to map the observed appearance to the deformation coefficients update:</p><formula xml:id="formula_11">p k = p k−1 + f k (I k−1 ; p k−1 ) = p k−1 + R k (φ(I k−1 ; x k−1 (l)| l=1,...,L ) −φ),<label>(9)</label></formula><p>where φ is the shape-indexed feature <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2]</ref> that concatenates the local appearance from all L landmarks, andφ is its average over all the training samples.</p><p>To learn the Gauss-Newton steepest descent regression matrix R k , we follow <ref type="bibr" target="#b22">[23]</ref> to learn the Jacobian J k and then obtain R k via constructing the project-out Hessian: R k = (J k J k ) −1 J k . We refer readers to <ref type="bibr" target="#b22">[23]</ref> for more details.</p><p>It is worth mentioning that the face flow method <ref type="bibr" target="#b38">[39]</ref> that applies a landmarkregularized Lucas-Kanade variational minimization <ref type="bibr" target="#b37">[38]</ref> is also a good alternative to our problem. Since we have obtained satisfying results with our previously introduced deformation coefficients transfer strategy, which is purely discriminative and much faster than face flow (8ms per cascade in our approach v.s. 1.4s for face flow), we use the coefficients transfer approach in our experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>We train a deep convolutional bi-network for each cascade. The bi-network is divided into three sub-nets: common sub-net, high-frequency sub-net and the gate sub-net. Due to the flexibility of the proposed cascaded framework, the depth for each cascade can be different. As a trade-off between accuracy and speed, we employ a deeper structure (24 layers for the common and high-frequency sub-nets) in the first cascade, while 12 layers for the subsequent cascades. Please refer to Tab. 1 (first cascade) and Tab. 2 (subsequent cascades) for the detailed network structure. Note that no pooling layer is used in our architecture. When predicting the dense correspondence field, the appearance vector for each sample is obtained by concatenating the local SIFT descriptor of all facial key points in order to keep consistent with <ref type="bibr" target="#b22">[23]</ref>. We do not claim that it is the optimal appearance representation. Given the limited amount of training data (only 2811 with facial points annotations), we empirically observed that such representation is powerful enough for the discriminative prediction. Before the dense field prediction in the first cascade, the face is transformed to a reference frame based on the predicted eyes location (by a common 6-layer CNN). We used eye locations since their predictions are more robust and stable under lowres condition. For the face hallucination step in each cascade, we add a backprojection regularization to suppress error accumulation among cascades. The effect brought by such regularization is specifically significant in the subsequent cascades.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. Following <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref>, we choose the following datasets that contain both in-the-wild and lab-constrained faces with various poses and illuminations.</p><p>1. MultiPIE <ref type="bibr" target="#b40">[41]</ref> was originally proposed for face recognition. A total of more than 750,000 faces from 337 identities are collected under lab-constrained environment. We use the same 351 images as used in <ref type="bibr" target="#b7">[8]</ref> for evaluation. 2. BioID <ref type="bibr" target="#b41">[42]</ref> contains 1521 faces also collected in the constrained settings. We use the same 100 faces as used in <ref type="bibr" target="#b5">[6]</ref> for evaluation. 3. PubFig <ref type="bibr" target="#b42">[43]</ref> contains 42461 faces (the evaluation subset) from 140 identities originally for evaluating face verification and later used for evaluating face hallucination <ref type="bibr" target="#b7">[8]</ref>. The faces are collected from the web and hence in-thewild. Due to the existence of invalid URLs, we use a total of 20991 faces for evaluation. Further, following <ref type="bibr" target="#b5">[6]</ref>, we use PubFig83 <ref type="bibr" target="#b43">[44]</ref>, a subset of PubFig with 13838 images, to experiment with input blurred by unknown Gaussian kernel. Similar to <ref type="bibr" target="#b5">[6]</ref>, we test with the same 100-image-subset of PubFig83. 4. Helen <ref type="bibr" target="#b44">[45]</ref> contains 2330 in-the-wild faces with high resolution. The mean face size is as large as 275pxIOD. We evaluate with the 330-image test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric.</head><p>We follow existing studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> to adopt PSNR (dB) and only evaluate on the luminance channel of the facial region. The definition of the facial region is the same as used in <ref type="bibr" target="#b5">[6]</ref>. Similar to <ref type="bibr" target="#b5">[6]</ref>, SSIM is not reported for in-the-wild faces due to irregular facial shape. Implementation details. Our framework consists of K = 4 cascades, and each cascade has its specific learned network parameters and Gauss-Newton steepest descent regression matrix. During training, our model requires two parts of training data, one for training the cascaded dense face correspondence field, and the other for training the cascaded gated bi-networks for hallucination. The model is trained by iterating between these two parts of the training data. For the former part, we use the training set from 300W <ref type="bibr" target="#b45">[46]</ref> (the same 2811 images used in <ref type="bibr" target="#b22">[23]</ref>) for estimating deformation coefficient and BU4D <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> dataset for obtaining dense face correspondence basis (following <ref type="bibr" target="#b38">[39]</ref>). For the latter part, as no manual labeling is required, we leverage the existing large face database CelebA <ref type="bibr" target="#b48">[49]</ref> for training the gated bi-network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with state-of-the-art methods</head><p>We compare our approach with two types of methods: (I) general super resolution (SR) approaches and (II) face hallucination approaches. For SR methods, we compare with the recent state-of-the-art approaches <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref> based on the original released codes. For face hallucination methods, we report the result of <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b11">12]</ref> by directly referring to the literature <ref type="bibr" target="#b5">[6]</ref>. We compare with <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52]</ref> by following the implementation of <ref type="bibr" target="#b7">[8]</ref>. We re-transform the input face to canonicalview if the method assumes the input must be aligned. Hence, such method would enjoy extra advantages in the comparison. If the method requires exemplars, we feed in the same in-the-wild samples in our training set. We observe that such in-the-wild exemplars improve the exemplar-based baseline methods compared to their original implementation. Codes for <ref type="bibr" target="#b8">[9]</ref> is not publicly available. Similar to <ref type="bibr" target="#b5">[6]</ref>, we provide the qualitative comparison with <ref type="bibr" target="#b8">[9]</ref>. We conduct the comparison in two folds: 1. The input is the down-sampled version of the original high-res image as many of the previous SR methods are evaluated on <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref> (referred as the conventional SR setting, Sec. 4.1.1); 2. The input is additionally blurred with unknown Gaussian kernel before downsampling as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref> (referred as the Gaussian-blurred setting, Sec. 4.1.2). 4.1.1 The conventional SR evaluation setting. We experiment with two scenarios based on two different types of input face size configuration:</p><p>1. Fixed up-scaling factors -The input image is generated by resizing the original image with a fixed factor. For MultiPIE, following <ref type="bibr" target="#b7">[8]</ref> we choose the fixed factor to be 4. For the in-the-wild datasets (PubFig and Helen), we evaluate for scaling factors of 2, 3 and 4 as in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b49">50]</ref> (denoted as 2×, 3×, 4× respectively in Tab. 3). In this case, different inputs might have different face sizes. The proposed CBN is flexible to handle such scenario. Other existing face hallucination approaches <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b7">8]</ref> cannot handle different input face sizes and their results in this scenario are omitted. 2. Fixed input face sizes -Similar to the face hallucination setting, the input image is generated by resizing the original image to ensure the input face size to be fixed (e.g. 5 or 8 pxIOD, denoted as 5/8px in Tab. 3). Hence, the required up-scaling factor is different for each input. For baseline approaches, <ref type="bibr" target="#b14">[15]</ref> can naturally handle any up-scaling requirement. For other approaches, we train a set of models for different up-scaling factors. During testing, we pick up the most suitable model based on the specified up-scaling factor. We need to point out that the latter scenario is more challenging and appropriate for evaluating a face hallucination algorithm, because recovering the details of the face with the size of 5/8pxIOD is more applicable for low-res face processing applications. In the former scenario, the input face is not small enough (as revealed in the bicubic PSNR in Tab. 3), such that it is more like a facial image enhancement problem rather than the challenging face hallucination task.</p><p>We report the results in Tab. 3, and provide qualitative results in <ref type="figure">Fig. 5</ref>. As can be seen from the results, our proposed CBN outperforms all general SR and face hallucination methods in both scenarios. The improvement is especially significant in the latter scenario because our incorporated face prior is more critical when hallucinating face from very low resolution. We observe that the general SR algorithms did not obtain satisfying results because they take full efforts to recover only the detectable high-frequency details, which obviously contain noise. In contrast, our approach recovers the details according to the high-frequency prior as well as the estimated dense correspondence field, thus achieving better performance. The existing face hallucination approaches did not perform well either. In comparison to the evaluation under the constrained or canonical-view condition (e.g. <ref type="bibr" target="#b7">[8]</ref>), we found that these algorithms are more likely to fail under in-the-wild setting with substantial shape deformation and appearance variation. <ref type="table">Table 3</ref>. Results under the conventional SR setting (for Sec 4.1.1). Numbers in the parentheses indicate SSIM and the remaining represent PSNR (dB). The first part of the results are from Scenario 1 where each method super-resolves for a fixed factor (2×, 3× or 4×), while the latter part are from Scenario 2 that each method begins from the same face size (5 or 8 pxIOD, i.e. the inter-ocular distance is 5 or 8 pixels). The omitted results (-) are due to their incapability of handling varying input face size.   <ref type="bibr" target="#b5">[6]</ref>. For a fair comparison, we feed in the same number of in-the-wild exemplars from CelebA when evaluating <ref type="bibr" target="#b7">[8]</ref>, instead of the originally used MultiPIE in the released codes.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.2</head><p>The Gaussian-blur evaluation setting. It is also important to explore the capability of handling blurred input images <ref type="bibr" target="#b52">[53]</ref>. Our method demonstrates certain degrees of robustness toward unknown Gaussian blur. Specifically, in this section, we still adopt the same model as in Sec. 4.1.1, with no extra efforts spent in the training to specifically cope with blurring. To compare with <ref type="bibr" target="#b5">[6]</ref>, we add Gaussian blur to the input facial image in the same way as <ref type="bibr" target="#b5">[6]</ref>. The experimental settings are precisely the same as in <ref type="bibr" target="#b5">[6]</ref> -the input faces have the same size (around 8pxIOD); the up-scaling factor is set to be 4; and σ for Gaussian blur kernel is set to be 1.6 for PubFig83 and 2.4 for BioID. Additional Gaussian noise with η = 2 is added in BioID. We note that our approach only uses single frame for inference, unlike multiple frames in <ref type="bibr" target="#b5">[6]</ref>.</p><p>We summarize the results in Tab. 4. Qualitative results are shown in <ref type="figure">Fig. 6</ref>. From the results it is observed that again CBN significantly outperforms all the compared approaches. We attribute the robustness toward the unknown Gaussian blur on the spatial guidance provided by the face high-frequency prior.</p><p>Taking advantages of such robustness of our approach, we further test the proposed algorithm over the faces from real surveillance videos. In <ref type="figure">Fig. 7</ref>, we compare our result with <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6]</ref>. Note that the presented test cases are directly imported from <ref type="bibr" target="#b5">[6]</ref>. Again, our result demonstrates the most appealing visual quality compared to existing state-of-the-art approaches, suggesting the potential of our proposed framework in real-world applications. 4.1.3 Run time. The major time cost of our approach is consumed on the forwarding process of the gated deep bi-networks. On a single core i7-4790 CPU, the face hallucination steps for the four cascades (from 5pxIOD to 80pxIOD) require 0.13s, 0.17s, 0.70s, 2.76s, respectively. The time cost of the dense field prediction steps is negligible compared to the hallucination step. Our framework totally consumes 3.84s, which is significantly faster than existing face hallucination approaches, for examples, 15-20min for <ref type="bibr" target="#b5">[6]</ref>, 1min for <ref type="bibr" target="#b7">[8]</ref>, 8min for <ref type="bibr" target="#b11">[12]</ref>, thanks to CBN's purely discriminative inference procedure and the non-exemplar and parametric model structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">An ablation study</head><p>We investigate the effects of three important components in our framework:</p><p>1. Effects of the gated bi-network (a) We explore the results if we replace the cascaded gated bi-network with the vanilla cascaded CNN, in which only the common branch (the blue branch in <ref type="figure">Fig. 4</ref>) is remained. In this case, the spatial information, i.e. the dense face correspondence field is not considered or optimized at all. (b) We also explore the case where only the high-frequency branch (the red branch in <ref type="figure">Fig. 4</ref>) is remained. 2. Effects of the progressively updated dense correspondence field In our framework, the pixel-level correspondence field is refined progressively to better facilitate the subsequent hallucination process. We explore the results if we only use the correspondence estimated from the input low-res image <ref type="bibr" target="#b3">4</ref> . In this case, the spatial configuration estimation is not updated with the growth of the resolution. 3. Effects of the cascade The cascaded alternating framework is the core for our framework. We explore the results if we train one network and directly super resolve the input to the required size. High-frequency prior is still used in this baseline. We observe an even worse result without this prior.</p><p>We present the results in Tab. 5. The experimental setting follows the same setting in Sec. 4.1 -The PubFig and HELEN datasets super-resolve from 5pxIOD while the PubFig83 dataset up-scales 4 times with unknown Gaussian blur. The results suggest that all components are important to our proposed approach. Original 3pxIOD 5pxIOD 8pxIOD 10pxIOD <ref type="figure">Fig. 9</ref>. Visualization of the result of various input resolution: 3, 5, 8, 10 pxIOD (for Sec. 5). For each resolution, the left is the bicubic result while the right is the output from the proposed CBN framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Despite the effectiveness of our method, we still observe a small set of failure cases. <ref type="figure" target="#fig_5">Figure 8</ref> illustrates three typical types of failure: (1) Over-synthesis of occluded facial parts, e.g., the eyes in <ref type="figure" target="#fig_5">Fig. 8(a)</ref>. In this case, the gate network might have been misled by the light-colored sun-glasses and therefore favours the results from the high-frequency branch. (2) Ghosting effect, which is caused by inaccurate spatial prediction under low-res. It is rather challenging to localize facial parts with very large head pose in the low-res image. (3) Incorrect details such as gaze direction. We found that there is almost no reliable gaze direction information presented in the input. Our method only synthesizes the eyes with the most probable gaze direction. We leave it as future works to address the aforementioned drawbacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Input resolution lower bound</head><p>It is of research interest to explore how small an input face we can recover for the in-the-wild settings. To achieve this goal, we train a set of models that hallucinates the face from varying input sizes. We select four representative face sizes: 3, 5, 8, 10 pxIOD as the input and observe their corresponding results. According to the PSNR results in Tab. 6 and the qualitative results in <ref type="figure">Fig. 9</ref>, we observe that results originated from 3pxIOD are mostly unrealistic and visually dissimilar to the full resolution image. The dense correspondence field is likely to be incorrectly predicted, and very few information is provided in the low-res input. On the other hand, if the input face size is no smaller than 5pxIOD, the PSNR would enjoy a significant increase (Tab. 5). Based on such observations, we believe that the input size of 3pxIOD might be below the lower bound where it is very difficult to recover faces from such resolution. This constitutes the reason why we choose 5/8pxIOD faces as low-res input in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a novel framework for hallucinating faces under substantial shape deformation and appearance variation. Owing to the specific capability to adaptively refine the dense correspondence field and hallucinate faces in an alternating manner, we obtain state-of-the-art performance and visually appealing qualitative results. Guided by the high-frequency prior, our framework can leverage spatial cues in the hallucination process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>p 1</head><label>1</label><figDesc>=p 0 +f 1 (I 0 ; p 0 ) W 1 (z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>(a) The original high-res image. (b) The low-res input with a size of 5pxIOD. (c) The result of bicubic interpolation. (d) An overview of the proposed face hallucination framework. The solid arrows indicate the hallucination step that hallucinates the face with spatial cues, i.e. the dense correspondence field. The dashed arrows indicate the spatial prediction step that estimates the dense correspondence field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Examples for visualizing the effects of the proposed gated deep bi-network. (a) The bicubic interpolation of the input. (b) Results where only common branches are enabled. (c) Results where only high-frequency branches are enabled. (d) Results of the proposed CBN when both branches are enabled. (e) The original high-res image. Best viewed by zooming in the electronic version.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>(a,b) Illustration of the mean face template M and the facial image I. The grid denotes the dense correspondence field W (z). The warping from z to x is determined by this warping function W (z). (c,d) Illustration of the high-frequency prior E and the prior after warping E W for the sample image in (b). Note that both E and E W have C channels. Each channel only contains one 'contour line'. For the purpose of visualization, in this figure, we reduce their channel dimension to one channel with max operation. We leave out all indices k for clarity. Best viewed in the electronic version.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(Fig. 8 .</head><label>8</label><figDesc>a)Bicubic (a)CBN (a)Original (b)Bicubic (b)CBN (b)Original (c)Bicubic (c)CBN (c)Original Three types of representative failure cases of our approach (for Sec. 4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>=p 2 +f 3 (I 2 ; p 2 ) W 2 (z) I 3 =↑I 2 +g 3 (↑I 2 ; W 3 (z)) p 4 =p 3 +f 4 (I 3 ; p 3 ) I 4 =↑I 3 +g 4 (↑I 3 ; W 4 (z))</figDesc><table><row><cell>W 4 (z)</cell></row><row><cell>W 3 (z)</cell></row><row><cell>CBN output</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The architecture of the bi-network in the first cascade.</figDesc><table><row><cell>Network</cell><cell>Layer Index (Depth)</cell><cell>Kernel Size</cell><cell cols="2">Stride Pad</cell><cell>Output Channels</cell><cell>Rectifier</cell><cell>Learining Rate (Pre-train)</cell><cell>Learning Rate (End-to-end)</cell></row><row><cell>Common Sub-net (24 layers)</cell><cell>1-4 5-20 21-23 24</cell><cell>3 × 3 3 × 3 3 × 3 3 × 3</cell><cell>1 1 1 1</cell><cell>1 1 1 1</cell><cell>64 128 32 1</cell><cell>ReLU ReLU ReLU /</cell><cell>10 −4 10 −4 10 −4 10 −5</cell><cell>10 −5 10 −5 10 −5 10 −6</cell></row><row><cell>High-frequency Sub-net (24 layers)</cell><cell>1-4 5-20 21-23 24</cell><cell>3 × 3 3 × 3 3 × 3 3 × 3</cell><cell>1 1 1 1</cell><cell>1 1 1 1</cell><cell>64 128 32 1</cell><cell>ReLU ReLU ReLU /</cell><cell>10 −4 10 −4 10 −4 10 −5</cell><cell>10 −5 10 −5 10 −5 10 −6</cell></row><row><cell>Gate Network</cell><cell>1-5</cell><cell>3 × 3</cell><cell>1</cell><cell>1</cell><cell>64</cell><cell>ReLU</cell><cell>/</cell><cell>10 −4</cell></row><row><cell>(6 layers)</cell><cell>6</cell><cell>3 × 3</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>/</cell><cell>/</cell><cell>10 −5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The architecture of the bi-network in the subsequent cascades.</figDesc><table><row><cell>Network</cell><cell>Layer Index (Depth)</cell><cell>Kernel Size</cell><cell cols="2">Stride Pad</cell><cell>Output Channels</cell><cell>Rectifier</cell><cell>Learining Rate (Pre-train)</cell><cell>Learning Rate (End-to-end)</cell></row><row><cell>Common Sub-net (12 layers)</cell><cell>1-4 5-8 9-11 12</cell><cell>3 × 3 3 × 3 3 × 3 3 × 3</cell><cell>1 1 1 1</cell><cell>1 1 1 1</cell><cell>64 128 32 1</cell><cell>ReLU ReLU ReLU /</cell><cell>10 −5 10 −5 10 −5 10 −6</cell><cell>10 −6 10 −6 10 −6 10 −7</cell></row><row><cell>High-frequency Sub-net (12 layers)</cell><cell>1-4 5-8 9-11 12</cell><cell>3 × 3 3 × 3 3 × 3 3 × 3</cell><cell>1 1 1 1</cell><cell>1 1 1 1</cell><cell>64 128 32 1</cell><cell>ReLU ReLU ReLU /</cell><cell>10 −5 10 −5 10 −5 10 −6</cell><cell>10 −6 10 −6 10 −6 10 −7</cell></row><row><cell>Gate Network</cell><cell>1-5</cell><cell>3 × 3</cell><cell>1</cell><cell>1</cell><cell>64</cell><cell>ReLU</cell><cell>/</cell><cell>10 −5</cell></row><row><cell>(6 layers)</cell><cell>6</cell><cell>3 × 3</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>/</cell><cell>/</cell><cell>10 −6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Results under the Gaussian-blur setting (for Sec. 4.1.2). Numbers in parentheses indicate SSIM and the remaining represent PSNR (dB). Settings adhere to</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>PSNR results (dB) of in-house comparison of the proposed CBN (for Sec. 4.2).</figDesc><table><row><cell>Dataset</cell><cell>1a. Only Common Branch i.e. Vanilla Cascaded CNN</cell><cell>1b. Only High-Freq. Branch</cell><cell>2. Fixed Correspondence</cell><cell>3. Single Cascade</cell><cell>Full Model</cell></row><row><cell>PubFig</cell><cell>23.76</cell><cell>24.66</cell><cell>23.85</cell><cell>22.09</cell><cell>25.31</cell></row><row><cell>HELEN</cell><cell>23.57</cell><cell>24.53</cell><cell>23.77</cell><cell>21.83</cell><cell>25.09</cell></row><row><cell>PubFig83</cell><cell>28.06</cell><cell>29.31</cell><cell>28.34</cell><cell>26.70</cell><cell>29.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>PSNR results (dB) of varying input face size: 3, 5, 8, 10 pxIOD (for Sec. 5).</figDesc><table><row><cell>Dataset</cell><cell cols="2">3xpIOD Bicubic CBN</cell><cell cols="2">5pxIOD Bicubic CBN</cell><cell cols="2">8pxIOD Bicubic CBN</cell><cell cols="2">10pxIOD Bicubic CBN</cell></row><row><cell>PubFig</cell><cell>18.10</cell><cell>20.01</cell><cell>20.63</cell><cell>25.31</cell><cell>22.32</cell><cell>26.83</cell><cell>23.69</cell><cell>27.92</cell></row><row><cell>HELEN</cell><cell>17.82</cell><cell>19.78</cell><cell>20.28</cell><cell>25.09</cell><cell>21.86</cell><cell>26.36</cell><cell>23.29</cell><cell>27.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Qualitative results from PubFig/HELEN with input size 5pxIOD (for Sec. 4.1.1, detailed results refer Tab. 3). Best viewed by zooming in the electronic version. Qualitative results from the PubFig83 dataset (for Sec. 4.1.2, detailed results refer Tab. 4). The six test samples presented are chosen by strictly following [6]. Qualitative results for real surveillance videos (for Sec. 4.1.2). The test samples are directly imported from<ref type="bibr" target="#b5">[6]</ref>. Best viewed by zooming in the electronic version.</figDesc><table><row><cell>Bicubic</cell><cell>[12]</cell><cell>CSCN</cell><cell></cell><cell>Bicubic</cell><cell>[12]</cell><cell cols="2">CSCN</cell><cell cols="2">Bicubic</cell><cell>[12]</cell><cell>CSCN</cell></row><row><cell>Original</cell><cell>[8]</cell><cell>CBN</cell><cell></cell><cell>Original</cell><cell>[8]</cell><cell>CBN</cell><cell></cell><cell>Original</cell><cell>[8]</cell><cell>CBN</cell></row><row><cell cols="4">Fig. 5. Original Bicubic SRCNN</cell><cell>NBF</cell><cell>CSCN</cell><cell>[8]</cell><cell></cell><cell>[9]</cell><cell>[6]</cell><cell>CBN</cell></row><row><cell cols="2">Fig. 6. Bicubic</cell><cell>CSCN</cell><cell>[6]</cell><cell>CBN</cell><cell cols="2">Bicubic</cell><cell cols="2">CSCN</cell><cell>[6]</cell><cell>CBN</cell></row><row><cell>Fig. 7.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">This paper is to appear in Proceedings of ECCV 2016.<ref type="bibr" target="#b0">1</ref> Throughout this paper, we use the inter-ocular distance measured in pixels (denoted as pxIOD), to concisely and unambiguously represent the face size.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We assume that we only correspond from pixel to pixel.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We also append the subscript k for M because the mean face template domain M k do not have the same size in different iteration k.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">As the correspondence estimation is by itself a cascaded process, in this case, we re-order the face corresponding cascades before the super resolution cascades.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06523</idno>
		<title level="m">Wider face: A face detection benchmark</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust multi-image based blind face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Bouganis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transport-based single frame super resolution of very low resolution face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Structured face hallucination. In: CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A bayesian approach to alignment-based image hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hallucinating face by eigentransformation. Systems, Man, and Cybernetics, Part C: Applications and Reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Super-resolution of face images using kernel pca-based prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">Face hallucination: Theory and practice. IJCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hallucinating faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>AFGR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep networks for image superresolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Convolutional sparse coding for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Super-resolution with deep convolutional sufficient statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Naive bayes super-resolution forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez-Pellitero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Depth map super resolution by deep multi-scale guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Project-out cascaded regression with an application to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exemplar-based face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3484" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep network cascade for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning face hallucination in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1078" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Coarse-to-fine auto-encoder networks (CFAN) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tric-track: tracking by regression with incrementally learned cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pridmore</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Bilinear classifiers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Recognize complex events from static images by fusing deep channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unifying holistic and parts-based deformable model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alabort-I</forename><surname>Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<title level="m">Face flow. In: ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<title level="m">Multi-pie. Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Robust face detection using the hausdorff distance. In: Audio-and video-based biometric person authentication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jesorsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Kirchberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Frischholz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Scaling up biologically-inspired computer vision: A case study in unconstrained face recognition on facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="692" to="706" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference and Workshops on</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Super-resolution from multiple views using learnt image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Capel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hallucinating face by position-patch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2224" to="2236" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Accurate blur models vs. image priors in single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Apartsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
