<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Re-thinking Co-Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengpeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Huazhu</forename><forename type="middle">Fu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Re-thinking Co-Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Co-saliency Detection</term>
					<term>Co-attention Projection</term>
					<term>CoSOD Dataset</term>
					<term>Benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we conduct a comprehensive study on the co-salient object detection (CoSOD) problem for images. CoSOD is an emerging and rapidly growing extension of salient object detection (SOD), which aims to detect the co-occurring salient objects in a group of images. However, existing CoSOD datasets often have a serious data bias, assuming that each group of images contains salient objects of similar visual appearances. This bias can lead to the ideal settings and effectiveness of models trained on existing datasets, being impaired in real-life situations, where similarities are usually semantic or conceptual. To tackle this issue, we first introduce a new benchmark, called CoSOD3k in the wild, which requires a large amount of semantic context, making it more challenging than existing CoSOD datasets. Our CoSOD3k consists of 3,316 high-quality, elaborately selected images divided into 160 groups with hierarchical annotations. The images span a wide range of categories, shapes, object sizes, and backgrounds. Second, we integrate the existing SOD techniques to build a unified, trainable CoSOD framework, which is long overdue in this field. Specifically, we propose a novel CoEG-Net that augments our prior model EGNet with a co-attention projection strategy to enable fast common information learning. CoEG-Net fully leverages previous large-scale SOD datasets and significantly improves the model scalability and stability. Third, we comprehensively summarize 40 cutting-edge algorithms, benchmarking 18 of them over three challenging CoSOD datasets (iCoSeg, CoSal2015, and our CoSOD3k), and reporting more detailed (i.e., group-level) performance analysis. Finally, we discuss the challenges and future works of CoSOD. We hope that our study will give a strong boost to growth in the CoSOD community. The benchmark toolbox and results are available on our project page at http://dpfan.net/CoSOD3K/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S ALIENT object detection (SOD) in color images <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b5">[6]</ref>, RGB-D images <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b10">[11]</ref>, and videos <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> has been an active field of research in the computer vision community over the past <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b21">[22]</ref>. SOD mimics the human vision system to detect the most attention-grabbing object(s) in a single image, as shown in <ref type="figure">Fig. 1 (a)</ref>. As a extension of this, co-salient object detection (CoSOD) emerged recently to employ a set of images. The goal of CoSOD is to extract the salient object(s) that are common within a single image (e.g., red-clothed football players in <ref type="figure">Fig. 1 (b)</ref>) or across multiple images (e.g., the blue-clothed gymnast in <ref type="figure">Fig. 1 (c)</ref>). Two important characteristics of co-salient objects are local saliency and global similarity. Due to its useful potential, CoSOD has been attracting growing attention in many applications, including collection-aware crops <ref type="bibr" target="#b22">[23]</ref>, co-segmentation <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, weakly supervised learning <ref type="bibr" target="#b25">[26]</ref>, image retrieval <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, and video foreground detection <ref type="bibr" target="#b28">[29]</ref>.</p><p>As such, the CoSOD task has been rapidly growing in recent few years <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b33">[34]</ref>, with hundreds of related publications since 2010 <ref type="bibr" target="#b0">1</ref> . Most CoSOD datasets tend to focus on the appearance-similarity between objects to identify the co-salient object across multiple images. However, this leads to data selection bias <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b34">[35]</ref> and is not always appropriate, since, in real-world applications, the salient objects in a group of images often vary in terms of texture, scene, and background (see our CoSOD3k dataset in <ref type="figure">Fig. 1 (d)</ref>), even if they belong to the same category. In addition to the data selection bias, CoSOD methods also suffer from two main limitations:</p><p>(A) Completeness. (Mean Absolute Error) <ref type="bibr" target="#b35">[36]</ref> and Fmeasure <ref type="bibr" target="#b36">[37]</ref> are two widely used metrics in CoSOD/SOD model evaluation. As discussed in <ref type="bibr" target="#b37">[38]</ref>, these metrics have their inherent limitations. To provide thorough and reliable conclusions, we need introduce more accurate metrics e.g., structural based evaluation metric or perceptual based evaluation metric.</p><p>(B) Fairness. To evaluate the F-measure, the first step is to binarize a saliency map into a set of foreground maps using different threshold values. There are many binarization strategies <ref type="bibr" target="#b38">[39]</ref>, such as adaptive threshold, fixed threshold and so on. However, different strategies will result in different F-measure performances. Further, few previous works provide details on their binarization strategy, leading to inconsistent F-measures for different researchers.</p><p>To address the aforementioned limitations, we argue that integrating various publicly available CoSOD algorithms, datasets, and metrics, and then providing a complete, unified benchmark, is highly desired. As such, we make four distinct contributions in this work: Image GT (a) (b) (c) (d) <ref type="figure">Fig. 1</ref>. Different salient object detection (SOD) tasks. (a) Traditional SOD <ref type="bibr" target="#b29">[30]</ref>. (b) Within-image co-salient object detection (CoSOD) <ref type="bibr" target="#b30">[31]</ref>, where common salient objects are detected from a single image. (c) Existing CoSOD, where salient objects are detected across a pair <ref type="bibr" target="#b31">[32]</ref> or a group <ref type="bibr" target="#b32">[33]</ref> of images with similar appearances. (d) The proposed CoSOD in the wild, which requires a large amount of semantic context, making it more challenging than existing CoSOD.</p><p>• First, we construct a challenging CoSOD3k dataset, with more realistic settings. Our CoSOD3k 2 is the largest CoSOD dataset to date, with two aspects: 1) it contains 13 super-classes, 160 groups, and 3,316 images in total, where each super-class is carefully selected to cover diverse scenes; e.g., Vehicle, Food, Tool, etc.;</p><p>2) each image is accompanied by hierarchical annotations, including category, bounding box, object, and instance, which could greatly benefit various vision tasks (e.g., object proposal, co-location, co-segmentation, coinstance detection, etc.), as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. • Second, we present the first large-scale co-salient object detection study, reviewing 40 state-of-theart (SOTA) models, and evaluating 18 of them on three challenging, large-scale CoSOD datasets (iCoSeg, CoSal2015, and the proposed CoSOD3k). A convenient benchmark toolbox is also provided to integrate various publicly available CoSOD datasets and multiple metrics for better performance evaluation. The benchmark toolbox and results have been made publicly available at https://dpfan.net/CoSOD3K/. • Third, we propose a simple but effective CoEG-Net baseline for CoSOD, which uniformly and simultaneously embeds the appearance and semantic features through a co-attention projection and a basic SOD network. Comprehensive benchmarking results show that CoEG-Net outperforms the 18 SOTA models. Moreover, it also yields competitive visual results, making it an efficient solution for the CoSOD task.</p><p>• Finally, we make several interesting observations, discuss the important issues arising from the benchmark results, and suggest some future directions. Our study serves as a potential catalyst for promoting large-scale model comparison for future CoSOD research.</p><p>2. Collecting the CoSOD dataset is more difficult than the SOD dataset, that is why the previous largest CoSOD dataset, i.e., <ref type="bibr" target="#b39">[40]</ref>, in the past 15 years has only 2K images. Even for our 3K dataset, we have spent 1 year to collect such high-quality dataset. Moreover, we also pay more attention to provide high-quality hierarchical annotations (e.g., image-level and object-/instance-level) to promote related vision tasks rather than the size of the dataset. This paper is based on and extends our previous CVPR2020 version <ref type="bibr" target="#b0">[1]</ref> in the following aspects. 1) We have implemented a simple but effective framework of CoSOD, which uniformly and simultaneously embeds the appearance and semantic features through a sparse convolution and a basic SOD network. Importantly, we also designed a common feature detector, which solved with Plug-and-Play. 2) We have made a lot of efforts to improve the presentations (e.g., dataset, framework, key results) and organizations of our paper. We have added several new sections to describe our new framework about the method formulation, corresponding technical components, and further experiments (e.g., comparison with baselines, running time). Besides, several sections have been re-written to improve the readability and provide more detailed explanations about the introduction, CoSOD models, quantitative/qualitative comparisons, and discussions. 3) We build the first standard Benchmark and model zoo of CoSOD, which integrates various publicly available CoSOD datasets with uniform input/output formats (i.e., JPEG for image; PNG for GT). The gathered code of traditional or learning-based will be released soon as well.  <ref type="bibr" target="#b41">[42]</ref>. CR = Coseg-Rep <ref type="bibr" target="#b42">[43]</ref>. DO = DUT-OMRON <ref type="bibr" target="#b43">[44]</ref>. COS = COCO-subset. Main Component: IMC = Intra-Image Contrast. IGS: Intra-Group Separability. IGC: Intra-Group Consistency. SPL: Self-Paced Learning. CH: Color Histogram. GMR: Graph-based Manifold Ranking. CAE: Convolutional Auto Encoder. HSR: High-spatial Resolution. FSM: five saliency models including CBCS <ref type="bibr" target="#b28">[29]</ref>, RC <ref type="bibr" target="#b44">[45]</ref>, DCL <ref type="bibr" target="#b15">[16]</ref>, RFCN <ref type="bibr" target="#b45">[46]</ref>, DWSI <ref type="bibr" target="#b30">[31]</ref>. SL. = Supervision Level. W = Weakly-supervised. S = Supervised. U = Unsupervised. Sp.: Whether or not superpixel techniques are used. Po.: Whether or not proposal algorithms are utilized. Ed.: Whether or not edge features are explicitly used. Post.: Whether or not post-processing methods, such as, CRF <ref type="bibr" target="#b46">[47]</ref>, GraphCut (GCut), or adaptive/constant threshold (THR), are introduced. ‡ denotes deep models. More details about these models can be found in recent survey papers <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b33">[34]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CoSOD Datasets</head><p>Currently, only a few CoSOD datasets have been proposed <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, as shown in <ref type="table" target="#tab_0">Table 1</ref> Although the aforementioned datasets have advanced the CoSOD task to various degrees, they are severely limited in variety, with only dozens of groups. On such smallscale datasets, the scalability of methods cannot be fully evaluated. Moreover, these datasets only provide objectlevel labels. None of them provide rich annotations such as bounding boxes, instances, etc., which are important for progressing many vision tasks and multi-task modeling. Especially in the current deep learning era, where models are often data-hungry. In this work, thus, we will focus on the two relatively large-scale datasets (i.e., iCoSeg <ref type="bibr" target="#b40">[41]</ref> and CoSal2015 <ref type="bibr" target="#b39">[40]</ref>) together with the proposed challenging dataset to provide more in-depth analysis. and instance-level masks. Our CoSOD3k will provide a solid foundation for the CoSOD task and can benefit a wide range of related fields, e.g., co-segmentation, weakly supervised localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CoSOD Methods.</head><p>Previous CoSOD studies <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b79">[80]</ref> have found that the inter-image correspondence can be effectively modeled by segmenting the input image into several computational units (e.g., superpixel regions <ref type="bibr" target="#b104">[105]</ref>, or pixel clusters <ref type="bibr" target="#b28">[29]</ref>). A similar observation can be found in recent reviews <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b33">[34]</ref>. In these approaches, heuristic characteristics (e.g., contour <ref type="bibr" target="#b50">[51]</ref>, color, luminance) are extracted from images, and the high-level features are captured to express the semantic attributes in different ways, such as through metric learning <ref type="bibr" target="#b71">[72]</ref> or self-adaptive weighting <ref type="bibr" target="#b56">[57]</ref>. Several studies have also investigated how to capture inter-image constraints through various computational mechanisms, such as translational alignment <ref type="bibr" target="#b22">[23]</ref>, efficient manifold ranking <ref type="bibr" target="#b52">[53]</ref>, and global correspondence <ref type="bibr" target="#b55">[56]</ref>. Some methods (e.g., PCSD <ref type="bibr" target="#b47">[48]</ref>, which only uses a filter bank technique) do not even need to perform the correspondence matching between the two input images, and are able to achieve CoSOD before the coattention occurs.</p><p>Recently, deep learning based CoSOD models have achieved good performance by learning co-salient object representations jointly. For instance, Zhang et al. <ref type="bibr" target="#b57">[58]</ref> introduced a domain adaption model to transfer prior knowledge for CoSOD. Wei et al. <ref type="bibr" target="#b66">[67]</ref> used a group input and output to discover the collaborative and interactive relationships between group-wise and single-image feature representations, in a collaborative learning framework. Along another line, the MVSRCC [70] model employs typical features, such as SIFT, LBP, and color histograms, as multiview features. In addition, several other methods <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b96">[97]</ref> are based on more powerful CNN models (e.g., ResNet <ref type="bibr" target="#b75">[76]</ref>, Res2Net <ref type="bibr" target="#b105">[106]</ref>, GoogLeNet <ref type="bibr" target="#b83">[84]</ref>, and VGGNet <ref type="bibr" target="#b68">[69]</ref>), achieving SOTA performances. These deep models generally achieve better performance through either weakly-supervised (e.g., CODW <ref type="bibr" target="#b58">[59]</ref>, SP-MIL <ref type="bibr" target="#b62">[63]</ref>, GONet <ref type="bibr" target="#b74">[75]</ref>, and FASS <ref type="bibr" target="#b77">[78]</ref>) or fully supervised learning (e.g., DIM <ref type="bibr" target="#b57">[58]</ref> and GD <ref type="bibr" target="#b66">[67]</ref>, and DML <ref type="bibr" target="#b72">[73]</ref>). There are also some concurrent works <ref type="bibr" target="#b99">[100]</ref>- <ref type="bibr" target="#b103">[104]</ref> that are proposed after this submission. A summary of the existing CoSOD models is provided in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COSOD3K DATASET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Collection</head><p>We build a high-quality dataset, CoSOD3k, images of which are collected from the large-scale object recognition dataset ILSVRC <ref type="bibr" target="#b106">[107]</ref>. There are several benefits of using ILSVRC to generate our dataset. First, ILSVRC is gathered from Flickr using scene-level queries and thus it includes various object categories, diverse realistic-scenes, and different object appearances, and covers a large span of the major challenges in CoSOD, providing us a solid basis for building a representative benchmark dataset for CoSOD. More importantly, though, the accompanying axis-aligned bounding boxes for each target object category allow us to identify unambiguous instance-level annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Annotation</head><p>Similar to <ref type="bibr" target="#b107">[108]</ref>, <ref type="bibr" target="#b108">[109]</ref>, the data annotation is performed in a hierarchical (coarse to fine) manner (see <ref type="figure" target="#fig_1">Fig. 2</ref>).</p><p>• Category Labeling.</p><p>We establish a hierarchical (threelevel) taxonomic system for the CoSOD3k dataset. 160 common categories (see <ref type="figure">Fig. 3</ref>) are selected to generate subclasses (e.g., Ant, <ref type="figure">Fig, Violin</ref>, Train, etc.), which are consistent with the original categories in ILSVRC. Then, an upper-level class (middle-level) is assigned for each sub-class. Finally, we integrate the upper-level classes into 13 super-classes. The taxonomic structure of our CoSOD3k is given in <ref type="figure">Fig. 4</ref>.</p><p>• Bounding Box Labeling. The second level of annotation is bounding box labeling, which is widely used in object detection and localization. Although the ILSVRC dataset provides bounding box annotations, the labeled objects are not necessarily salient. Following many famous SOD datasets <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b109">[110]</ref>- <ref type="bibr" target="#b115">[116]</ref>, we ask three viewers to redraw the bounding boxes around the object(s) in each image that dominate their attention. Then, we merge the bounding boxes labeled by the three viewers and have two additional senior researchers in the CoSOD field double-check the annotations. After that, as done in <ref type="bibr" target="#b116">[117]</ref>, we discard the images that contain more than six objects. Finally, we collect 3,316 images within 160 categories. Examples can be found in <ref type="figure" target="#fig_1">Fig. 2</ref>. • Object-/Instance-level Annotation. High-quality pixellevel masks are necessary for CoSOD datasets. We hire twenty professional annotators and train them with 100 image examples. They are then instructed to annotate the images with object-and instance-level labels according to the previous bounding boxes. The average annotation time per image is about 8 and 15 minutes for object-level and instance-level labeling, respectively. Moreover, we also have three volunteers cross-check the whole process (more than three-fold), to ensure high-quality annotation (see <ref type="figure" target="#fig_3">Fig. 5</ref>). In this way, we obtain an accurate and challenging dataset with a total of 3,316 object-level and 4,915 instance-level annotations. Note that our final bounding box labels are refined further based on the instance-level annotations to tighten the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Features and Statistics</head><p>To provide deeper insight into our CoSOD3k, we present several important characteristics below.</p><p>• Mixture-specific Category Masks. <ref type="figure" target="#fig_5">Fig. 7</ref> shows the average ground-truth masks for individual categories and the overall dataset. As can be observed, some categories with unique shapes (e.g., airplane, zebra, and bicycle) present shape-biased maps, while categories with non-rigid or convex shapes (e.g., goldfish, bird, and bus) do not have clear shape-bias. The overall dataset mask (the right of <ref type="figure" target="#fig_5">Fig. 7</ref>) tends to appear as a center-biased map without shape bias.</p><p>As is well-known, humans are usually inclined to pay more attention to the center of a scene when taking a photo. Thus, it is easy for a SOD model to achieve a high score when employing a Gaussian function in its algorithm. Due to the limitation of space, we present all 160 mixture-specific category masks in the supplementary materials.   • Sufficient Object Diversity. As shown in <ref type="table" target="#tab_7">Table 5</ref> (2 nd row) and <ref type="figure">Fig. 3</ref>, our CoSOD3k covers a large variety of super-classes including Vegetables, Food, Fruit, Tool, Necessary, Traffic, Cosmetic, Ball, Instrument, Kitchenware, Animal, and Others, enabling a comprehensive understanding of real-world scenes.</p><p>• Number of Instances.</p><p>Being able to parse objects into instances is critical for humans to understand, categorize, and interact with the world. To enable learning methods to gain instance-level understanding, annotations with instance labels are in high demand. With this in mind, in contrast to existing CoSOD datasets, our CoSOD3k contains the multi-instance scenes with instance-level annotations. As illustrated in <ref type="figure" target="#fig_6">Fig. 8 (a)</ref>, the number of instances (1, 2, ≥3) is subject to a ratio of 7:2:1.</p><p>• Size of Instances.</p><p>The instance size is defined as the ratio of foreground instance pixels to the total image pixels. <ref type="figure" target="#fig_6">Fig. 8 (b)</ref> shown the instance sizes of our CoSOD3k in terms of small, middle, and large instance/object. The distributions of instance sizes are 0.02% ∼ 86.5% (avg.: 13.8%), yielding a broad range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED METHOD</head><p>In this work, we also propose a simple but effective CoEG-Net baseline for CoSOD, which extend state-of-the-art SOD model EGNet <ref type="bibr" target="#b117">[118]</ref> by introducing co-attention information in an unsupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Method Formulation</head><p>For a group of N associated images {I n } N n=1 , the co-saliency detection task aims at segmenting out the common attentive foreground objects and generating optimized co-saliency maps, which indicate common salient objects among the input images. To predict the co-saliency masks, we present a two-branch detection framework to respectively capture the concurrent dependencies and salient foregrounds in a multiply independent fashion. <ref type="figure">Fig. 9</ref> illustrates the framework of the proposed method, which independently outputs coattention maps {A n } N n=1 in the top branch and saliency prior maps {S n } N n=1 in the bottom branch. The co-attention map A n and saliency prior map S n are then integrated via element-wise multiply to produce the final co-saliency prediction A n ⊗ S n .</p><p>To obtain the saliency prior map S n for an input image I n , we simply use the edge guided salient object detection method EGNet <ref type="bibr" target="#b117">[118]</ref> to collect multi-scale saliency priors. The EGNet is trained on large scale single image SOD dataset DUTS <ref type="bibr" target="#b29">[30]</ref>, which helps to identify the salient object regions in images without cross image information. The real challenge then becomes how to discover co-attention map A n in an unsupervised manner, which we present in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Co-attention Projection for Co-saliency Learning</head><p>The design of co-attention learning (see <ref type="figure" target="#fig_4">Fig. 6</ref>) is motivated by the class activation mapping (CAM) technique proposed by Zhou et al. <ref type="bibr" target="#b118">[119]</ref>. Given an input image I n , the corresponding feature activations X n in the last convolution </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge-Loss</head><p>Saliency-Loss Element-wise Multiplication <ref type="figure">Fig. 9</ref>. Pipeline of the proposed architecture which contains two separate branches. For a group of images {I n } N n=1 as inputs, in the top branch, the extracted high-level image features are fed into the co-attention projection module to produce a co-attention map A n for each input image I n .</p><p>In the bottom branch, each image I n is sent into the edge-guided saliency detection network (EGNet) <ref type="bibr" target="#b117">[118]</ref> to generate the saliency prior map S n . Finally, A n and S n are simply integrated using element-wise multiply to produce the optimized outputs A n ⊗ S n . See Section 4 for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol Dimensions Indices Meaning</head><formula xml:id="formula_0">A n H × W (i, j) co-attention map of I n S n H × W (i, j) saliency prior map of I n X n H × W × K (i, j, k) activations of the last conv layer X n k H × W (i, j) feature map of a channel in X n H 1 × 1 scalar spatial height W 1 × 1 scalar spatial width K 1 × 1 scalar number of feature channels x n (i, j) K × 1 k descriptor of X n at location (i, j) M n c H × W (i, j) attention map for class c ω c K × 1 k channel-wise weights for class c x K × 1 k average value of all x n (i, j) x n (i, j) K × 1 kx n (i, j) = x n (i, j) −x with zero mean Cov(x) K × K - covariance matrix for {x n (i, j)} ξ * K × 1 k first eigenvector of Cov(x)</formula><p>layer can be easily obtained using a standard classification network (e.g. VGGNet <ref type="bibr" target="#b68">[69]</ref>). See <ref type="table" target="#tab_4">Table 3</ref> for more details.</p><p>Utilizing images with only keywords labeling, the CAM technique aims at producing a class specific attention map M n c for each class c using the feature maps {X n k }:</p><formula xml:id="formula_1">M n c = K k=1 ω c k X n k ,<label>(1)</label></formula><p>where the weights ω c could be trained using keyword level weak supervision <ref type="bibr" target="#b118">[119]</ref>. Notice that each spatial element of the class activation map M n c can be independently estimated using the weights ω c and the channel-wise descriptor in X n at spatial location (i, j) as</p><formula xml:id="formula_2">M n c (i, j) = (ω c ) · x n (i, j).<label>(2)</label></formula><p>Thus the CAM <ref type="bibr" target="#b118">[119]</ref> technique essentially plays a linear transformation that transforms the image features x n (i, j) into class specific activation scores M n c (i, j) using the learned class specific weights ω c .</p><p>Unfortunately, in the co-saliency detection problem settings, the keywords level supervision is not available. Thus, we have to discover the weighting ω for the common objects in an unsupervised fashion, by revealing the internal structure of the image features. Ideally, the unknown common object category among a group of associated images {I n } N n=1 should corresponds to a linear projection that results in high class activation scores in the common object regions, while having low class activation scores in other image regions. From another point of view, the common object category should correspond to the linear transformation that generates the highest variance (most informative) in the resulting class activation maps. Follow the idea in coarse localization task <ref type="bibr" target="#b122">[123]</ref>, we achieve this gold by exploring the classical principle component analysis (PCA) <ref type="bibr" target="#b123">[124]</ref>, which is the simplest way of revealing the internal structure of the data in a way that best explains the variance in the data.</p><p>Specifically, given the associated images {I n } N n=1 , with corresponding feature activations X n for each image I n , we aims at finding the linear transformation of X n that results in the co-attention maps {A n } with the highest variance. This can be achieved by analyzing the covariance matrix of the feature descriptors {x n (i, j)}. Let</p><formula xml:id="formula_3">x = 1 Z n i,j x n (i, j), where Z = N × H × W .</formula><p>We have the zero mean version of the descriptors asx n (i, j) = x n (i, j) −x. The covariance matrix can be denoted as</p><formula xml:id="formula_4">Cov(x) = 1 Z n i,j (x n (i, j) −x)(x n (i, j) −x) T .<label>(3)</label></formula><p>Then the expected linear projection can be established by using the eigenvector ξ * , that corresponds to the largest eigenvalue of Cov(x). Thus, the co-attention projection can be designed as a projection that presents the features in its most informative viewpoint  <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, and our CoSOD3k. The symbol "•" means that the code or results are not available. Note that the UMLF adopts half of the images from both MSRC and CoSal2015 to train their model. Underline indicates the scores generated by models (e.g., SP-MIL and UMLF) that have been trained on corresponding dataset. See <ref type="table" target="#tab_1">Table 2</ref> for more training details. <ref type="bibr">[</ref>  <ref type="figure">Fig. 10</ref>. Visualization of the common activation maps (second and third row), using largest and second eigenvalue, and their corresponding post-processed (i.e., manifold ranking and DenseCRF) co-attention map A n (fourth row) selected from the "banana" group of CoSal2015 <ref type="bibr" target="#b39">[40]</ref>. <ref type="figure">Fig. 10</ref> shows some visual templates of common activation maps (second and third row) resulting from Eq. 4. The given images contains multiple objects of diverse categories including banana, apple, bottle and pineapple, increasing the difficulty of differentiate correct regions, while using the largest eigenvalue of Cov(x) (second row) can sufficiently localize the common objects and mask them out (last row) initially.</p><formula xml:id="formula_5">A n (i, j) = ξ * ·x n (i, j).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric CBCS ESMG RFPR CSHS SACS CODR UMLF DIM CODW MIL IML GONet SP-MIL CSMG CPD GSPA AGC EGNet CoEG-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation</head><p>The VGGNet16 network <ref type="bibr" target="#b68">[69]</ref> after removing the top layer is selected as our backbone for a fair comparison. The training process is finished in 30 epochs and the learning rate is divided by 10 after 15 epochs. For the edge-guided contextual saliency network, the setting is the same with <ref type="bibr" target="#b117">[118]</ref>. Note that in the training stage, the loss function is the same with EGNet. Similar to the post-processing in <ref type="bibr" target="#b74">[75]</ref>, we utilize the DenseCRF <ref type="bibr" target="#b46">[47]</ref> and manifold ranking <ref type="bibr" target="#b124">[125]</ref> to further refine co-attention map A n before integrating them with the saliency prior map S n . The examples are shown in the third row of <ref type="figure">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">BENCHMARK EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Evaluation Metrics.</head><p>To provide a comprehensive evaluation, four widely used metrics are employed for evaluating CoSOD performance, including maximum F-measure F β <ref type="bibr" target="#b36">[37]</ref>, mean absolute error (MAE) <ref type="bibr" target="#b35">[36]</ref>, S-measure S α <ref type="bibr" target="#b125">[126]</ref>, and maximum E-measure E φ <ref type="bibr" target="#b126">[127]</ref>. The complete evaluation toolbox can be found at https://github.com/ DengPingFan/CoSODToolbox. F-measure F β <ref type="bibr" target="#b36">[37]</ref> evaluate the weighted harmonic mean of precision and recall. The saliency maps have to be binarized using different threshold, where each threshold corresponds to a binary saliency prediction. The predicted and ground-truth binary maps are compared to get precision and recall values. F β is typically chosen as the Fmeasure score that corresponds to the best fixed threshold for the whole dataset.</p><p>MAE <ref type="bibr" target="#b35">[36]</ref> is a much simple evaluation metric that directly measures the absolute difference between the groundtruth value and the predicted value, without any binarization requirements. Both F-measure and MAE evaluate the prediction in a pixel by pixel manner.</p><p>S-measure S α <ref type="bibr" target="#b125">[126]</ref> is designed to evaluate the structural similarity between a saliency map and the corresponding ground-truth. It can directly evaluate the continuous saliency prediction without binarization and consider the large scale structure similarity at the same time.</p><p>E-measure E φ <ref type="bibr" target="#b126">[127]</ref> is a perceptual metric that evaluates both local and global similarity between the predicted map and ground-truth simultaneously.</p><p>• Competitors.</p><p>In the CoSOD experiments, we evaluate/compare sixteen SOTA CoSOD models, including seven traditional methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b120">[121]</ref> and nine deep learning models <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b117">[118]</ref>, <ref type="bibr" target="#b121">[122]</ref>. The methods were chosen based on two criteria: (1) representative, and (2) released code or results.</p><p>• Benchmark Protocols.</p><p>We evaluate on two existing  CoSOD datasets, i.e., iCoSeg <ref type="bibr" target="#b40">[41]</ref>, and CoSal2015 <ref type="bibr" target="#b39">[40]</ref>, and our CoSOD3k. To the best of our knowledge, ours is the largestscale and most comprehensive benchmark. For comparison, we run the available codes directly, either under default settings (e.g., CBCS <ref type="bibr" target="#b28">[29]</ref>, ESMG <ref type="bibr" target="#b52">[53]</ref>, RFPR <ref type="bibr" target="#b119">[120]</ref>, CSHS <ref type="bibr" target="#b50">[51]</ref>, SACS <ref type="bibr" target="#b56">[57]</ref>, CODR <ref type="bibr" target="#b120">[121]</ref>, UMLF <ref type="bibr" target="#b71">[72]</ref>, DIM <ref type="bibr" target="#b57">[58]</ref>, CPD <ref type="bibr" target="#b121">[122]</ref>, and EGNet <ref type="bibr" target="#b117">[118]</ref>) or using the CoSOD maps provided by the authors (e.g., IML <ref type="bibr" target="#b84">[85]</ref>, CODW <ref type="bibr" target="#b58">[59]</ref>, GONet <ref type="bibr" target="#b74">[75]</ref>, SP-MIL <ref type="bibr" target="#b62">[63]</ref>, and CSMG <ref type="bibr" target="#b94">[95]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantitative Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Performance on iCoSeg.</head><p>The iCoSeg dataset <ref type="bibr" target="#b40">[41]</ref> was originally designed for image co-segmentation but is widely used for the CoSOD task. Interestingly, as can be seen in <ref type="table" target="#tab_5">Table 4</ref>, the two SOD models (i.e., EGNet <ref type="bibr" target="#b117">[118]</ref> and CPD <ref type="bibr" target="#b121">[122]</ref>) achieve the state-of-theart performances. The CoSOD methods (e.g., CODR <ref type="bibr" target="#b120">[121]</ref>, IML <ref type="bibr" target="#b84">[85]</ref>, and CSMG <ref type="bibr" target="#b94">[95]</ref>) also obtain very close performances to the top SOD models (i.e., EGNet <ref type="bibr" target="#b117">[118]</ref> and CPD <ref type="bibr" target="#b121">[122]</ref>). Our CoEG-Net obtains the best performance in E φ , S α , and F β , but the results are very close to those of the backbone, i.e., EGNet <ref type="bibr" target="#b117">[118]</ref>. One possible reason is that the iCoSeg dataset contains a lot of images with single objects, which can easily be detected by SOD models. The co-salient feature is not an importance role in iCoSeg dataset. This also suggests that the iCoSeg dataset may not be suitable for evaluating CoSOD methods in the deep learning era. Some examples can be found in <ref type="figure" target="#fig_1">Fig. 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Performance on CoSal2015</head><p>. <ref type="table" target="#tab_5">Table 4</ref> shows the evaluation results on the CoSal2015 dataset <ref type="bibr" target="#b39">[40]</ref>. One interesting observation is that the existing salient object detection methods, e.g., EGNet <ref type="bibr" target="#b117">[118]</ref> and CPD <ref type="bibr" target="#b121">[122]</ref>, obtain higher performances than most CoSOD methods. This implies that some top-performing salient object detection frameworks may be better-suited for extension to CoSOD tasks. The CoSOD method CSMG <ref type="bibr" target="#b94">[95]</ref> achieves comparable performance in E φ (0.842) and F β (0.784), but worse scores in S α (0.774) and (0.130). This demonstrates that existing CoSOD methods cannot solve the task well. Our CoEG-Net obtains the best results, significantly outperforming both SOD and Co-SOD baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Performance on CoSOD3k.</head><p>The overall results on our CoSOD3k are presented in <ref type="table" target="#tab_5">Table 4</ref>.</p><p>As expect, our model still achieve the best performance. To provide deeper insight into each group, we report the performances of models on 13 super-classes in <ref type="table" target="#tab_7">Table 5</ref>. We observe that lower average scores are achieved on classes such as Other (e.g., baby bed and pencil box), Instrument (e.g., piano, guitar, cello, etc.), Necessary (e.g., pitcher), Tool (e.g., axe, nail, chain saw, etc.), and Ball (e.g., soccer, tennis, etc.), which contain complex structures in real scenes. Note that almost all of the deep-based models (e.g., EGNet <ref type="bibr" target="#b117">[118]</ref>, CPD <ref type="bibr" target="#b121">[122]</ref>, IML <ref type="bibr" target="#b84">[85]</ref>, and CSMG <ref type="bibr" target="#b94">[95]</ref>) perform better than the traditional approaches (CODR <ref type="bibr" target="#b120">[121]</ref>, CSHS <ref type="bibr" target="#b50">[51]</ref>, CBCS <ref type="bibr" target="#b28">[29]</ref>, and ESMG <ref type="bibr" target="#b52">[53]</ref>), demonstrating the potential advantages in utilizing deep learning techniques to address the CoSOD problem. Another interesting finding is that edge features can help provide good boundaries for the results. For instance, the best methods from both traditional (CSHS <ref type="bibr" target="#b50">[51]</ref>) and deep learning models (e.g., EGNet <ref type="bibr" target="#b117">[118]</ref>) introduce edge information to aid detection. Finally, our method CoEG-Net obtains the best performance on average, with an E φ of 0.825 which is much higher than the second-best method, i.e., CSMG <ref type="bibr" target="#b94">[95]</ref> with 0.804. Moreover, the performances <ref type="table" target="#tab_5">(Table 4</ref>) of all methods are worse than on the other two datasets (e.g., iCoSeg and CoSal2015), which clearly shows <ref type="figure" target="#fig_1">Fig. 12</ref>. Qualitative examples of 10 representative models evaluated on iCoSeg <ref type="bibr" target="#b40">[41]</ref>, CoSal2015 <ref type="bibr" target="#b39">[40]</ref>, and our CoSOD3k.</p><formula xml:id="formula_6">iCoSeg Gymnastics CoSal2015 Banana CoSOD3k Dog Input GT CoEG-Net DIM CSMG ESMG CBCS EGNet CPD UMLF CSHS CODR</formula><p>that the proposed CoSOD3k dataset is challenging and leaves abundant room for further research. <ref type="figure" target="#fig_1">Fig. 12</ref> shows some qualitative examples on iCoSeg, CoSal2015, and our CoSOD3k. As can be seen, the SOD models, e.g., EGNet <ref type="bibr" target="#b117">[118]</ref> and CPD <ref type="bibr" target="#b121">[122]</ref>, detect all salient objects and obtain sharp boundaries, performing better than other baselines. However, these SOD models ignore the context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Comparisons</head><p>For example, the "banana" group in the CoSal2015 dataset contain several other irrelevant objects, e.g., oranges, pineapples, and apples. The SOD models cannot distinguish these as being irrelevant. Another similar situation also occurs in the images in the dog group of our CoSOD3k, where the humans (the third and fifth images) are detected together with the dogs. On the other hand, the CoSOD methods, e.g., CSMG <ref type="bibr" target="#b94">[95]</ref> and DIM <ref type="bibr" target="#b57">[58]</ref>, can identify the common salient objects and remove the other objects (e.g., human). However, these CoSOD methods cannot produce accurate predicted maps, especially around object boundaries. By contrast, our CoEG-Net preserves the advantages of SOD and CoSOD methods, and obtains the best visual results in all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with Baselines</head><p>Our baseline CoEG-Net consists of a co-attention projection and a basic SOD model. In order to explore the efficiency <ref type="bibr">TABLE 6</ref> Ablative studies of our model on three benchmark datasets, where Ours-A, Ours-P, Ours-E represent the co-salient results of Amulet, PiCANet, EGNet on our baseline, respectively.  of the co-attention projection, we (1) adopt the same training dataset (i.e., DUTS <ref type="bibr" target="#b29">[30]</ref>) and test datasets (i.e., iCoSeg, CoSal2015, and CoSOD3k) for three SOTA SOD models (i.e., Amulet <ref type="bibr" target="#b16">[17]</ref>, PiCANet <ref type="bibr" target="#b91">[92]</ref>, and EGNet <ref type="bibr" target="#b117">[118]</ref>); and (2) apply the same co-attention projection strategy for these models, as presented in Section 4, to conduct this experiment. <ref type="table">Table 6</ref> shows the performances of three baselines in terms of E φ , S α , F β , and metrics. Based on the results, we observe that: (i) On the relatively simple iCoSeg dataset, our baselines (i.e., Ours-A/-P/-E) slightly improve upon the backbone models (i.e., Amulet, PiCANet, and EGNet). We note that because this dataset contains a large number of single objects with similar appearances <ref type="figure" target="#fig_1">(Fig. 12</ref>) in each group, only using a SOD model can achieve very high performance. This conclusion is consistent with the analysis in Section 5.2.1; (ii) On the classical CoSal2015 dataset, our baselines are consistently better than the backbones in terms of all four metrics. It is worth noting that, for this more complex dataset, we still obtain a 2.5%, 1.4%, and 1.8% S α score improvement; (iii) For the proposed and most challenging dataset CoSOD3k, we find that the improvement is still significant (e.g., 7.1% F β score for Amulet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets Metric Amulet Ours-A PiCANet Ours-P EGNet</head><p>To further analyze the improvement, we also provide the 160 sub-class performances in the supplementary materials. We observe that, for objects in the common super-class (i.e., 'Ball') such as "rugby ball" and "soccer ball", we achieve 23.5% and 23.9% F β improvements. We attribute this to the co-attention projection operation being able to automatically learn mutual-features, which are crucial for overcoming challenging ambiguities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Running Time</head><p>Our CoEG-Net is implemented in PyTorch and Caffe with an RTX 2080Ti GPU for acceleration. For traditional algorithms (CBCS <ref type="bibr" target="#b28">[29]</ref>, ESMG <ref type="bibr" target="#b52">[53]</ref>, CSHS <ref type="bibr" target="#b50">[51]</ref>, CODR <ref type="bibr" target="#b120">[121]</ref>, and UMLF <ref type="bibr" target="#b71">[72]</ref>), the comparison experiments are executed on a laptop with Inter(R) Core(TM) i7-2600 CPU @3.4GHz. The remaining deep learning models (DIM <ref type="bibr" target="#b57">[58]</ref>, CSMG <ref type="bibr" target="#b94">[95]</ref>, CPD <ref type="bibr" target="#b121">[122]</ref>, and EGNet <ref type="bibr" target="#b117">[118]</ref>) are tested on a workstation with Intel(R) Core(TM) i7-8700K CPU @3.70GHz and an RTX 2080Ti GPU. As shown in <ref type="table" target="#tab_9">Table 7</ref>, among the top-3 CoSOD models, i.e., the proposed CoEG-Net, CSMG <ref type="bibr" target="#b94">[95]</ref>, and UMLF <ref type="bibr" target="#b71">[72]</ref>, evaluated in terms of E φ measure on the proposed CoSOD3k, our model achieves the fastest inference time. In addition, compared with the top-2 fastest CoSOD models (i.e., CBCS <ref type="bibr" target="#b28">[29]</ref> and ESMG <ref type="bibr" target="#b94">[95]</ref>), although the proposed model has a longer test time, it obtains a significantly improved S α measure. This partially suggests that our framework is not only efficient but also effective for the CoSOD task. However, compared to two recently released state-of-the-art models, CPD <ref type="bibr" target="#b121">[122]</ref> and EGNet <ref type="bibr" target="#b117">[118]</ref>, there is still large room for improvement in running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND FUTURE DIRECTIONS</head><p>From the evaluation, we observe that, in most cases, the current SOD methods (e.g., EGNet <ref type="bibr" target="#b117">[118]</ref> and CPD <ref type="bibr" target="#b121">[122]</ref>) can obtain very competitive or even better performances than the CoSOD methods (e.g., CSMG <ref type="bibr" target="#b94">[95]</ref> and SP-MIL <ref type="bibr" target="#b62">[63]</ref>). However, this does not necessarily mean that the current datasets are not complex enough or using the SOD methods directly can obtain the good performances-the performances of the SOD methods on the CoSOD datasets are actually lower than those on the SOD datasets. For example, EGNet achieves 0.937 and 0.943 F β scores on the HKU-IS dataset <ref type="bibr" target="#b112">[113]</ref> and ECSSD dataset <ref type="bibr" target="#b115">[116]</ref>, respectively. However, it only obtains 0.786 and 0.702 F β scores on the CoSal2015 and CoSOD3k datasets, respectively. Consequently, the evaluation results reveal that many problems in CoSOD are still under-studied and this makes the existing CoSOD models less effective. In this section, we discuss four important issues (i.e., scalability, stability, compatibility, and metrics) that have not been fully addressed by the existing co-salient object detection methods and should be studied in the future. Finally, we discuss the weakness of the the proposed CoEG-Net framework.</p><p>• Scalability.</p><p>The scalability is one of the most important issues that needs to be considered when designing CoSOD algorithms. Specifically, it indicates the capability of a CoSOD model of handling large-scale image scenes. As we know, one key property of CoSOD is that the model needs to consider multiple images from each group. However, in reality, an image group may contain numerous related images. Under this circumstance, methods that do not consider scalability would have huge computational costs and take a very long time to run, making them unacceptable in practice (e.g., CSHS-102s and UMLF-87s). Thus, how to address the scalability issue, or how to reduce the computational complexity caused by the number of images contained in an image group, becomes a key problem in this field, especially when applying CoSOD methods for real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Stability.</head><p>Another important issue is the stability of model. When dealing with image groups containing multiple images, some existing methods (e.g., HCNco <ref type="bibr" target="#b127">[128]</ref>, PCSD <ref type="bibr" target="#b47">[48]</ref>, and IPCS <ref type="bibr" target="#b31">[32]</ref>) divide the image group into image pairs or image sub-groups (e.g., GD <ref type="bibr" target="#b66">[67]</ref>). Another school of methods adopt the RNN-based model (e.g., GWD <ref type="bibr" target="#b98">[99]</ref>), which involves assigning an order to the input images. These strategies all make the overall training process unstable as there is no principle way of dividing image groups or assigning input order to related images. In other words, when generating image sub-groups or assigning the input orders following different strategies, the learning procedure produces different co-saliency detectors, and the test results are also unstable. Consequently, this not only brings difficulty for evaluating the performance of the learned cosaliency detectors but also influences the application of the co-salient object detection.</p><p>• Compatibility.</p><p>Introducing SOD in CoSOD is a direct yet effective strategy for building CoSOD framework as the single image saliency can conduce to the co-saliency pattern identification. However, most existing CoSOD works only utilize the results or features of the SOD models as useful information cues. The proposed CoEG-Net baseline still follows this two-stage framework that spends more inference time than the single SOD model. Although as a preliminary attempt, we have also achieved the best performance among the existing CoSOD models. From this point of view, one further direction for leveraging the SOD technique is to deeply combine a CNN-based SOD network with a CoSOD model to build an end-to-end trainable framework for detecting CoSOD directly. To achieve this goal, one needs to consider the compatibility of the CoSOD framework, making it convenient for integrating the existing SOD techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Metrics.</head><p>Current evaluation metrics for CoSOD are designed in terms of SOD, i.e., they calculate the mean of the SOD scores on each group directly. In contrast to SOD, CoSOD involves relationship information between cosalient objects of different images, which is more important for CoSOD evaluation. For example, current CoSOD metrics assume taht the target objects have similar sizes in all images. As the objects actually have different sizes in different images, these metrics (S α , E φ , F β , in Sec. 5) would likely be inclined to detecting large objects. Moreover, the current CoSOD metrics are based towards detecting objects in a single image, rather than identifying co-occurring objects across multiple images. Thus, how to design suitable metrics for CoSOD is an open issue.</p><p>• Weakness.</p><p>Compared with the end-to-end CoSOD detection frameworks that output binary predictions with smoothed fine-structures, the prediction results of CoEG-Net suffers from coarse boundaries, indicating that CoEG-Net cannot finely preserve detailed shape information for the co-salient objects. Some failed detection cases are shown in <ref type="figure" target="#fig_9">Fig. 13</ref>.</p><p>• Potential Applications.</p><p>In this part, we discuss two potential new applications that could benefit from the high-  quality CoSOD models. For more CoSOD applications, please refer to the related survey in <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p><p>Collection-Aware Crops. This application is derived from Jacob et al. 's work <ref type="bibr" target="#b22">[23]</ref>. It studies where people look when comparing images and triggers the seminal works on the CoSOD task. Sharing the same spirit, we show a more general potential application which is not limited to image pairs. As an example, when dealing with the automatic thumb-nailing task as in <ref type="figure" target="#fig_10">Fig. 14,</ref> we first obtain the yellow bounding box from the saliency maps generated by our CoEG-Net. After that, an enlarged (∼60 pixels) red box 3 is used to identify the crop regions automatically. To obtain high-quality crops from the first row, we can also introduce existing SOTA super-resolution techniques <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b129">[130]</ref> to further improve the visualization results.</p><p>Object Co-Localization. As shown by DeepCO 3 <ref type="bibr" target="#b96">[97]</ref>, the co-saliency detection results will provide the class-agnostic attention cues for the object co-location task. Introducing our CoEG-Net to existing commerce application will be a possible solution to improve the performance in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we have presented a comprehensive investigation on the co-salient object detection (CoSOD) task. After identifying the serious data bias in current dataset, which assume that each image group contains salient object(s) of similar visual appearance, we built a new high-quality dataset, named CoSOD3k, containing co-salient object(s) that are similar at a semantic or conceptual level. Notably, CoSOD3k is the most challenging CoSOD dataset so far, containing 160 groups and total of 3,316 images labeled with category, bounding box, object-level, and instance-level annotations. Our CoSOD3k dataset makes a significant leap in terms of diversity, difficulty and scalability, benefiting several related vision tasks, e.g., co-segmentation, weakly supervised localization, and instance-level detection, and their future development.</p><p>To creat an effective co-salient object detector, we integrated existing SOD techniques to build a unified, trainable CoSOD framework called CoEG-Net. Specifically, we augmented our prior model EGNet with a co-attention projection strategy to enable efficient common information learning, improving the scalability and stability of the cosalient object detection framework.</p><p>Besides, this paper has also provided a comprehensive study by summarizing 40 cutting-edge algorithms, benchmarking 18 of them over two classical datasets, as well as the proposed CoSOD3k. By evaluating recent SOD and CoSOD methods, this paper demonstrated that the SOD methods are surprisingly better. This is an interesting finding that can guide further investigation into better CoSOD algorithms. We hope the studies presented in this work will give a strong boost to the growth of the CoSOD community. In the future, we plan to increase the dataset scale to spark more novel ideas.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>D.-P. Fan, Z. Lin and M.-M. Cheng are with the College of Computer Science, Nankai University, Tianjin, China. (Email: dengpfan@gmail.com, frazer.linzheng@gmail.com, cmm@nankai.edu.cn) • T. Li is with the B-DAT and CICAEET, Nanjing University of Information Science and Technology, Nanjing, China. (E-mail: ltp-for1225@gmail.com) • G.-P. Ji is with the School of Computer Science, Wuhan University, Hubei, China. (E-mail: gepengai.ji@gmail.com) • D. Zhang is with the Brain and Artificial Intelligence Laboratory, School of Automation, Northwestern Polytechnical University, Xi'an 710072, China. (E-mail: zhangdingwen2006yyy@gmail.com) • H. Fu and J. Shen are with the Inception Institute of Artificial Intelligence, Abu Dhabi, UAE. (E-mail: {huazhu.fu, jianbing.shen}@inceptioniai.org) • A preliminary version of this work has appeared in CVPR 2020 [1]. • Corresponding author: M.-M. Cheng.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Sample images from our CoSOD3k dataset. It has rich annotations, i.e., image-level categories (top), bounding boxes, object-level masks,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Number of images in the 160 sub-classes of our dataset. Best viewed on screen and zoomed-in for details. apple cello CoSOD3k Taxonomic structure of our dataset, which contains 13 superclasses with 160 sub-classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Some passed and rejected cases (e.g., occlusion, precision) in our CoSOD3k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Illustration of our co-attention projection operation. Given the original feature representation which covers common objects (circle), noisy foregrounds (triangle) and background clutter (square), the co-attention projection identifies the principle components of common objects, helping to preserve the common objects while removing interference. By adopting our co-attention projection operation, we finally project the principle component and obtain the new feature representation. Please refer to Section 4.2 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization of overlap masks for mixture-specific category and overall dataset masks of our CoSOD3k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>The number of images for the MSRC, iCoSeg, Image Pair, CoSal2015, WICOS, and our CoSOD3k dataset in terms of the number of instances (a) and the instance/object size (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Examples of our CoSOD3k. We visualize segmentation examples for representative object categories from 13 super-classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Some challenge cases for our CoEG-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Collection-Aware Crops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 Statistics</head><label>1</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell cols="4">Year #Gp #Img #Avg IL Ceg BBx HQ</cell><cell>Input</cell></row><row><cell cols="2">MSRC [33] 2005 8</cell><cell cols="2">240 30</cell><cell>Group images</cell></row><row><cell cols="4">iCoSeg [41] 2010 38 643 17</cell><cell>Group images</cell></row><row><cell cols="3">Image Pair [32] 2011 105 210</cell><cell>2</cell><cell>*</cell><cell>Two images</cell></row><row><cell cols="4">CoSal2015 [40] 2015 50 2,015 40</cell><cell>*</cell><cell>Group images</cell></row><row><cell cols="3">WICOS [31] 2018 364 364</cell><cell>1</cell><cell>Single image</cell></row><row><cell cols="4">CoSOD3k 2020 160 3,316 21</cell><cell>Group images</cell></row></table><note>of existing CoSOD datasets and the proposed CoSOD3k, showing that CoSOD3k provides higher-quality and much richer annotations. #Gp: number of image groups. #Img: number of images. #Avg: average number of images per group. IL: whether or not instance-level annotations are provided. Ceg: whether or not category labels are provided for each group. BBx: whether or not bounding box labels are provided for each image. HQ: high-quality annotation.* denotes coarse category rather than explicitly accurate category.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Summary</head><label>2</label><figDesc></figDesc><table /><note>of 40 classic and cutting-edge CoSOD approaches. Training set: PV = PASCAL VOC07</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 Table</head><label>3</label><figDesc></figDesc><table /><note>of symbols, their dimensions, indices, and meaning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Benchmarking results of 18 leading CoSOD approaches on two classical</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>29] [53] [120] [51] [57] [121] [72] [58] ‡ [59] ‡ [64] ‡ [85] ‡ [75] ‡ [63] ‡ [95] ‡ [122] ‡ [101] ‡ [103] ‡ [118] ‡ Ours ‡ iCoSeg E φ ↑ .797 .784 .841 .841 .817 .889 .827 .864 .832 .799 .895 .864 .843 .889 .900 .818 .897 .911 .912 Sα ↑ .658 .728 .744 .750 .752 .815 .703 .758 .750 .727 .832 .820 .771 .821 .861 .784 .821 .875 .875 F β ↑ .705 .685 .771 .765 .770 .823 .761 .797 .782 .741 .846 .832 .794 .850 .855 .718 .837 .875 .876 ↓ .172 .157 .170 .179 .154 .114 .226 .179 .184 .186 .104 .122</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.174</cell><cell>.106 .057 .098 .079 .060</cell><cell>.060</cell></row><row><cell>CoSal2015</cell><cell>E φ ↑ .656 .640 Sα ↑ .544 .552 F β ↑ .532 .476 ↓ .233 .247</cell><cell>• • • •</cell><cell cols="5">.685 .749 .749 .769 .695 .752 .720 .592 .694 .689 .662 .592 .648 .673 .564 .650 .634 .690 .580 .667 .620 .313 .194 .204 .271 .312 .274 .210</cell><cell>----</cell><cell>.805 .751 .740 .160</cell><cell>• • • •</cell><cell>.842 .841 .855 .890 .843 .774 .814 .797 .823 .818 .784 .782 .779 .831 .786 .130 .098 .099 .090 .099</cell><cell>.882 .836 .832 .077</cell></row><row><cell>CoSOD3k</cell><cell>E φ ↑ .637 .635 Sα ↑ .528 .532 F β ↑ .466 .418 ↓ .228 .239</cell><cell>• • • •</cell><cell>.656 .563 .484 .309</cell><cell>• • • •</cell><cell>.700 .758 .662 .630 .632 .559 .530 .639 .495 .229 .285 .327</cell><cell>• • • •</cell><cell>• • • •</cell><cell>.773 .720 .652 .164</cell><cell>• • • •</cell><cell>• • • •</cell><cell>.804 .791 .800 .823 .793 .711 .757 .736 .759 .762 .709 .699 .682 .729 .702 .157 .120 .124 .094 .119</cell><cell>.825 .762 .736 .092</cell></row><row><cell cols="2">Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Activation</cell><cell>Map-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Activation</cell><cell>Map-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Co-attention</cell><cell>Map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Per super-class average E-measure performance E φ on our CoSOD3k. Vege. = Vegetables, Nece. = Necessary, Traf. = Traffic, Cosm.= Cosmetic, Inst. = Instrument, Kitch. = Kitchenware, Elec. = Electronic, Anim. = Animal, Oth. = Others. "All" means the score on the whole dataset. We only evaluate the 10 state-of-the-art models with released codes. Note that CPD and EGNet are the top-2 SOD models on the socbenchmark (http://dpfan.net/socbenchmark).</figDesc><table><row><cell></cell><cell>Vege.</cell><cell>Food</cell><cell>Fruit</cell><cell>Tool</cell><cell>Nece.</cell><cell>Traf.</cell><cell>Cosm.</cell><cell>Ball</cell><cell>Inst.</cell><cell>Kitch.</cell><cell>Elec.</cell><cell>Anim.</cell><cell>Oth.</cell><cell>All</cell></row><row><cell>#Sub-class</cell><cell>4</cell><cell>5</cell><cell>9</cell><cell>11</cell><cell>12</cell><cell>10</cell><cell>4</cell><cell>7</cell><cell>14</cell><cell>9</cell><cell>9</cell><cell>49</cell><cell>17</cell><cell>160</cell></row><row><cell>ESMG [53]</cell><cell>.577</cell><cell>.635</cell><cell>.735</cell><cell>.625</cell><cell>.546</cell><cell>.673</cell><cell>.633</cell><cell>.559</cell><cell>.655</cell><cell>.631</cell><cell>.629</cell><cell>.687</cell><cell>.592</cell><cell>.635</cell></row><row><cell>CBCS [29]</cell><cell>.680</cell><cell>.621</cell><cell>.739</cell><cell>.617</cell><cell>.603</cell><cell>.666</cell><cell>.664</cell><cell>.619</cell><cell>.627</cell><cell>.625</cell><cell>.640</cell><cell>.672</cell><cell>.594</cell><cell>.637</cell></row><row><cell>CSHS [51]</cell><cell>.613</cell><cell>.591</cell><cell>.733</cell><cell>.677</cell><cell>.585</cell><cell>.691</cell><cell>.677</cell><cell>.563</cell><cell>.637</cell><cell>.651</cell><cell>.665</cell><cell>.715</cell><cell>.624</cell><cell>.656</cell></row><row><cell>CODR [121]</cell><cell>.682</cell><cell>.682</cell><cell>.774</cell><cell>.679</cell><cell>.634</cell><cell>.756</cell><cell>.678</cell><cell>.580</cell><cell>.671</cell><cell>.686</cell><cell>.695</cell><cell>.771</cell><cell>.638</cell><cell>.700</cell></row><row><cell>DIM  ‡ [58]</cell><cell>.622</cell><cell>.687</cell><cell>.773</cell><cell>.650</cell><cell>.604</cell><cell>.708</cell><cell>.633</cell><cell>.577</cell><cell>.665</cell><cell>.612</cell><cell>.641</cell><cell>.709</cell><cell>.623</cell><cell>.662</cell></row><row><cell>UMLF [72]</cell><cell>.781</cell><cell>.777</cell><cell>.781</cell><cell>.694</cell><cell>.779</cell><cell>.836</cell><cell>.714</cell><cell>.668</cell><cell>.711</cell><cell>.763</cell><cell>.748</cell><cell>.810</cell><cell>.690</cell><cell>.758</cell></row><row><cell>IML  ‡ [85]</cell><cell>.802</cell><cell>.725</cell><cell>.808</cell><cell>.740</cell><cell>.714</cell><cell>.867</cell><cell>.753</cell><cell>.653</cell><cell>.734</cell><cell>.795</cell><cell>.729</cell><cell>.855</cell><cell>.663</cell><cell>.773</cell></row><row><cell>CPD  ‡ [122]</cell><cell>.805</cell><cell>.763</cell><cell>.818</cell><cell>.734</cell><cell>.758</cell><cell>.894</cell><cell>.763</cell><cell>.629</cell><cell>.638</cell><cell>.848</cell><cell>.784</cell><cell>.892</cell><cell>.693</cell><cell>.791</cell></row><row><cell>EGNet  ‡ [118]</cell><cell>.833</cell><cell>.761</cell><cell>.815</cell><cell>.746</cell><cell>.767</cell><cell>.890</cell><cell>.769</cell><cell>.632</cell><cell>.654</cell><cell>.841</cell><cell>.771</cell><cell>.893</cell><cell>.697</cell><cell>.793</cell></row><row><cell>CSMG  ‡ [95]</cell><cell>.755</cell><cell>.872</cell><cell>.854</cell><cell>.722</cell><cell>.744</cell><cell>.908</cell><cell>.766</cell><cell>.778</cell><cell>.690</cell><cell>.849</cell><cell>.840</cell><cell>.885</cell><cell>.690</cell><cell>.804</cell></row><row><cell>CoEG-Net (Ours)  ‡</cell><cell>.802</cell><cell>.842</cell><cell>.840</cell><cell>.811</cell><cell>.790</cell><cell>.897</cell><cell>.795</cell><cell>.780</cell><cell>.746</cell><cell>.844</cell><cell>.842</cell><cell>.881</cell><cell>.739</cell><cell>.825</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7</head><label>7</label><figDesc>Average running time of ten SOTA models.</figDesc><table><row><cell>Models</cell><cell cols="4">CBCS [29] ESMG [53] CSHS [51] CODR [121]</cell></row><row><cell>Time (seconds)</cell><cell>0.3</cell><cell>1.2</cell><cell>102</cell><cell>35</cell></row><row><cell>Language</cell><cell>Matlab</cell><cell>Matlab</cell><cell>Matlab</cell><cell>Matlab</cell></row><row><cell>Models</cell><cell cols="4">UMLF [72] DIM  ‡ [58] CSMG  ‡ [95] CPD  ‡ [122]</cell></row><row><cell>Time (seconds)</cell><cell>87</cell><cell>25</cell><cell>3.2</cell><cell>0.016</cell></row><row><cell>Language</cell><cell>Matlab</cell><cell>Matlab</cell><cell>Caffe</cell><cell>PyTorch</cell></row><row><cell>Models</cell><cell>EGNet  ‡ [118]</cell><cell>Ours  ‡</cell><cell></cell><cell></cell></row><row><cell>Time (seconds)</cell><cell>0.034</cell><cell>2.3</cell><cell></cell><cell></cell></row><row><cell>Language</cell><cell>PyTorch</cell><cell>PyTorch</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. Note that we keep the original width of the yellow box when the enlarged red box touched the boundary of the image.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We also thank professor Kaihua Zhang from Nanjing University of Information Science &amp; Technology for insightful feedback. This research was supported by NSFC (61922046), S&amp;T innovation project from Chinese Ministry of Education, and Tianjin Natural Science Foundation (18ZXZNGX00110).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Taking a Deeper Look at the Co-salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="186" to="202" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards highresolution salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7234" to="7243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nested network with two-stream pyramid for salient object detection in optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Geosci. Remote. Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="9156" to="9166" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="150" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Boundaryaware segmentation network for mobile and web applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diagne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sant&amp;apos;anna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suàrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04704</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y.</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking RGB-D Salient Object Detection: Models, Datasets, and Large-Scale Benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Uncertainty inspired rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03075</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Siamese network for rgb-d salient object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12134</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rgb-d salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Media</title>
		<imprint>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video Saliency Detection via Sparsity-Based Reconstruction and Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4819" to="4831" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revisiting video saliency prediction in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Review of visual saliency detection with comprehensive information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Circuit Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2941" to="2959" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09146</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selectivity or invariance: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3799" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">C2net: a complementary co-saliency detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Vis. Comput</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Co-saliency detection using collaborative feature extraction and high-to-low feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cosaliency: Where people look when comparing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23nd annual ACM symposium on User interface software and technology</title>
		<meeting>the 23nd annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Higher-order image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1011" to="1021" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object-Based Multiple Foreground Video Co-Segmentation via Multi-State Selection Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3415" to="3424" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Capsal: Leveraging captioning to boost semantics for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6024" to="6033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A model of visual attention for natural image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Inf. Sci. Cloud Comput. Companion</title>
		<imprint>
			<biblScope unit="page" from="728" to="733" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Salientshape: group saliency in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cluster-based co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3766" to="3778" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cosaliency detection within a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Art. Intell</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7509" to="7516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A co-saliency model of image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3365" to="3375" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A review of cosaliency detection algorithms: Fundamentals, applications, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Intell Syst Technol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient salient region detection with soft image abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1529" to="1536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Frequencytuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Co-saliency detection via looking deep and wide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2994" to="3002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">icoseg: Interactive co-segmentation with intelligent scribble guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cosegmentation and cosketch by unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1305" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Global Contrast based Salient Region Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Preattentive co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dynamic visual attention: Searching for coding length increments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="681" to="688" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Co-salient object detection from multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1896" to="1909" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Co-saliency detection based on hierarchical segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="92" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient saliency-model-guided visual co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="588" to="592" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient manifold ranking for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ACM Spec. Interest Group Inf. Ret</title>
		<imprint>
			<biblScope unit="page" from="525" to="534" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSMC</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Co-saliency detection via base reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="997" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Self-adaptively weighted co-saliency detection via rank constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4175" to="4186" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cosaliency detection based on intrasaliency prior transfer and deep intersaliency mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1163" to="1176" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Detection of cosalient objects by looking deep and wide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FTML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a selfpaced multiple-instance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A self-paced multiple-instance learning framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Looking beyond the image: Unsupervised learning for object saliency and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3238" to="3245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Group-wise deep co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">E F</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Jt. Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3041" to="3047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Revisiting co-saliency detection: A novel approach based on two-stage multi-view spectral rotation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3196" to="3209" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A unified metric learningbased framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Circuit Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2473" to="2483" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep learning intra-image and inter-images features for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brit. Mach. Vis. Conf</title>
		<imprint>
			<biblScope unit="page">291</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised CNN-based co-saliency detection with graphical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="485" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Co-attention cnns for unsupervised object co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Jt. Conf. Artif. Intell</title>
		<imprint>
			<biblScope unit="page" from="748" to="756" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A feature-adaptive semisupervised framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="959" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Image co-saliency detection and co-segmentation via progressive joint optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="71" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Co-salient object detection based on deep saliency networks and seed propagation over an integrated graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5866" to="5879" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Quality-guided fusionbased co-saliency estimation for image co-segmentation and colocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Jerripothula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2466" to="2477" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">An easy-to-hard learning strategy for within-image co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="page" from="166" to="176" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Co-saliency Detection via Integration of Multi-layer Convolutional Features and Interimage Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">371</biblScope>
			<biblScope unit="page" from="137" to="146" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Deep group-wise fully convolutional network for co-saliency detection with graph propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">E F</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5052" to="5063" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Detecting Robust Co-Saliency with Recurrent Co-Attention Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<editor>Int. Jt. Conf</editor>
		<imprint>
			<biblScope unit="page" from="818" to="825" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Robust deep co-saliency detection with group semantic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Art. Intell</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8917" to="8924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Multiple Graph Convolutional Networks for Co-Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="332" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A Unified Multiple Graph Learning and Convolutional Network Model for Co-saliency Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1375" to="1382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">PiCANet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Co-saliency Detection Based on Hierarchical Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia, 2019</title>
		<imprint>
			<biblScope unit="page" from="1392" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Co-saliency detection via mask-guided fully convolutional networks with multi-scale label smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3095" to="3104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Learning to Detect A Salient Object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">DeepCO3: Deep Instance Co-Segmentation by Co-Peak Search and Co-Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8846" to="8855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Supervision by fusion: Towards unsupervised learning of deep salient object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4048" to="4056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Group-Wise Deep Object Co-Segmentation With Co-Attention Recurrent Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8519" to="8528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Co-saliency detection with co-attention fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Circuit Syst. Video Technol</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Robust deep cosaliency detection with group semantic and pyramid attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Co-saliency detection via a general optimization model and adaptive graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE T. Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional network with attention graph clustering for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9050" to="9059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Gradient-induced co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Flic: Fast linear iterative clustering with active search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Media</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="348" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Camouflaged Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Image segmentation by probabilistic bottom-up aggregation and cue integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Joint Salient Object Detection and Existence Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Comput. Sci</title>
		<imprint>
			<biblScope unit="page" from="778" to="788" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Instance-level salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">What is and what is not a salient object? learning salient object detector by ensembling linear exemplar regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4142" to="4150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">The discrimination of visual number</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Reese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Volkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Psychol</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="525" />
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">EGNet: Edge Guidance Network for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Co-saliency detection based on region-level fusion and pixel-level refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Co-saliency detection via co-salient object discovery and recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2073" to="2077" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and co-localization by deep descriptor transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="113" to="126" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">On lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos Mag (Abingdon)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901" />
		</imprint>
	</monogr>
	<note>LIII</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="321" to="328" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Structuremeasure: A New Way to Evaluate Foreground Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Enhanced-alignment Measure for Binary Foreground Map Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Jt. Conf. Artif. Intell</title>
		<imprint>
			<biblScope unit="page" from="698" to="704" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Hierarchical co-salient object detection via color names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Asian Conf. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="718" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Explorable super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bahat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2716" to="2725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Srflow: Learning the super-resolution space with normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="715" to="732" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
