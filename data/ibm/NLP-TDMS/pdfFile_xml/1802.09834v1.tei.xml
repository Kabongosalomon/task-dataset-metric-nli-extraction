<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatio-Temporal Graph Convolution for Skeleton Based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaolong</forename><surname>Li</surname></persName>
							<email>lichaolong@seu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
							<email>zhen.cui@njust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
							<email>wenmingzheng@seu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">School of Biological Science &amp; Medical Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatio-Temporal Graph Convolution for Skeleton Based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Variations of human body skeletons may be considered as dynamic graphs, which are generic data representation for numerous real-world applications. In this paper, we propose a spatio-temporal graph convolution (STGC) approach for assembling the successes of local convolutional filtering and sequence learning ability of autoregressive moving average. To encode dynamic graphs, the constructed multi-scale local graph convolution filters, consisting of matrices of local receptive fields and signal mappings, are recursively performed on structured graph data of temporal and spatial domain. The proposed model is generic and principled as it can be generalized into other dynamic models. We theoretically prove the stability of STGC and provide an upper-bound of the signal transformation to be learnt. Further, the proposed recursive model can be stacked into a multi-layer architecture. To evaluate our model, we conduct extensive experiments on four benchmark skeleton-based action datasets, including the large-scale challenging NTU RGB+D. The experimental results demonstrate the effectiveness of our proposed model and the improvement over the state-of-the-art. * Chaolong Li and Zhen Cui have equal contributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action recognition is one of the most active research topics due to its wide applications to video surveillance, robot vision, human computer interaction, etc. Since human body itself can be viewed as an articulated system of rigid bones connected by hinged joints, the actions of human body are essentially embodied in skeletal motions in the 3D space <ref type="bibr" target="#b45">(Ye et al. 2013)</ref>. Thereby, various skeleton based action recognition methods <ref type="bibr" target="#b30">Shahroudy et al. 2016;</ref>) are springing up in recent years, accompanying with the progress of more accessible deep sensors.</p><p>Different from grid-shaped structures of images/videos, human skeleton, consisting of a series of joints and bones, has an irregular geometric structure. Human action may be regarded as a consecutive dynamic sequence of such irregular structures. Considering the motion complexity of the entire body skeleton, the sophisticated strategy is to separately model the trajectory of each joint or clique of joints (body part). The statistic model <ref type="bibr" target="#b42">(Xia, Chen, and Aggarwal 2012)</ref>, shape model <ref type="bibr" target="#b2">(Chaudhry et al. 2013</ref>), and geometric model <ref type="bibr" target="#b35">(Vemulapalli, Arrate, and Chellappa 2014)</ref> are often used to characterize the motion trajectories. With the thriving of representation learning, the learning based methods, such as Hidden Markov Models (HMMs) <ref type="bibr" target="#b42">(Xia, Chen, and Aggarwal 2012)</ref> and recursive models (long-short term memory, LSTM) <ref type="bibr" target="#b32">Song et al. 2017)</ref>, become predominant in the dynamic representation of skeletal joints, because they dedicate more promising results on those public skeleton datasets.</p><p>Recently graph as a tool is used to represent skeletons <ref type="bibr" target="#b40">(Wang et al. 2016a</ref>), although graph is popular to model various structured objects. Nevertheless, the graph based method <ref type="bibr" target="#b40">(Wang et al. 2016a</ref>) still takes the conventional technique line of graph kernel matching. In fact, in the research fields of graph classification/matching, the graph representation (e.g., the statistic on graphlet <ref type="bibr" target="#b26">(Pr≈æulj 2007)</ref>) and graph metric (e.g., graph kernel <ref type="bibr" target="#b36">(Vishwanathan et al. 2010)</ref>) have been historically well-studied. With recent successes of deep learning on various problems, deep representation of graphs has aroused more attention <ref type="bibr" target="#b43">(Yanardag and Vishwanathan 2015;</ref><ref type="bibr" target="#b29">Seo et al. 2016;</ref><ref type="bibr" target="#b16">Li et al. 2017)</ref>. But the most crucial problem is the definition/identification of homogeneous graphs because the same responses should be produced from those homogeneous graphs. To this end, Niepert et al. <ref type="bibr" target="#b24">(Niepert, Ahmed, and Kutzkov 2016)</ref> used a greedy strategy of sorting those nodes within a local neighbor region and then performed convolution-like filtering on the sorted nodes. Instead of this explicit spatial definition way, Defferrard et al.  introduced a deep graph method based on spectral filtering, inspired by the recent signal processing theory on graph <ref type="bibr" target="#b31">(Shuman et al. 2013)</ref>. <ref type="bibr" target="#b16">Li et al. (Li et al. 2017</ref>) introduced attention mechanism into graph convolution model. But those methods intrinsically belong to still-graph deep learning.</p><p>For dynamic graphs, some variants of recurrent neural network (RNN) are developed recently based on the traverse way of spatio-temporal graph. Jain et al. <ref type="bibr" target="#b13">(Jain et al. 2016)</ref> proposed a structural-RNN by casting spatio-temporal graph as a RNN mixture for the task of action prediction. <ref type="bibr" target="#b16">Li et al. (Li et al. 2015)</ref> proposed gated graph sequential neural network for the basic logical reasoning task. <ref type="bibr" target="#b29">Seo et al. (Seo et al. 2016)</ref> fed the spatial filtered graph signals into LSTM for image generation. However, these methods do not yet absorb the essential successes of convolutional neural networks (CNN), which have changed AI landscape with breakthrough results on numerous applications.</p><p>In this paper, we propose a spatio-temporal graph convolution (STGC) approach to represent dynamic skeletal graph sequences. To encode the graph structure data, we design multi-scale convolutional filters, each of which is composed of receptive field computation and signal mapping. The receptive field is computed from the polynomial of adjacency matrix, which makes sure the same responses for homogeneous graphs. We simultaneously perform local convolutional filtering on temporal motions and spatial structures. The temporal convolutional filtering recursively encodes motion variations while the spatial filtering extracts more robust feature of spatial structures. In theory, the frequency responses of multi-scale convolutional filtering are equivalent to an approximate graph Flourier transform following by one linear function of feature mapping. Taking the philosophy of multi-scale convolutional filtering, we develop a recursive graph convolution model inspired by autoregressive moving average (ARMA). We theoretically analyze the stability of the proposed model and provide a theoretical upper-bound. Finally, we stack the spatio-temporal graph convolution into a deep architecture. To verify our proposed method, we conduct extensive experiments on four benchmark skeleton-based action datasets including the largest NTU RGB+D dataset . The experimental results demonstrate the effectiveness of our proposed model and a more promising direction for skeleton based action recognition.</p><p>In summary, our contributions are three folds: ‚Ä¢ Propose a spatio-temporal graph convolution approach, which assembles the successes of local convolutional filtering and sequence learning ability of recursive learning. The generic model is further extended to a deep architecture. ‚Ä¢ Theoretically prove the stability of the proposed model and provide a theoretical upper-bound. ‚Ä¢ Achieve the state-of-the-art performances on the four benchmark datasets including the large-scale challenging dataset NTU RGB+D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Graph Preliminary</head><p>Consider an undirected graph</p><formula xml:id="formula_0">G = (V, A) of N nodes, where V = {v i } N i=1</formula><p>is the set of nodes, A is a (weighted) adjacency matrix. The adjacency matrix A ‚àà R N √óN records the connections between nodes, where if v i , v j are not connected, then A ij = 0, otherwise A ij = 0.</p><p>The graph Laplacian matrix L is defined as L = D ‚àí A, where D ‚àà R N √óN is the diagonal degree matrix with D ii = j A ij . A popular option is the normalized version, i.e., each weight A ij is multiplied by a factor 1 ‚àö DiiDjj , for-</p><formula xml:id="formula_1">mally, L norm = D ‚àí 1 2 LD ‚àí 1 2 = I ‚àí D 1 2 AD 1 2</formula><p>, where I is the identity matrix. Unless otherwise specified, we use the normalized Laplacian matrix below.</p><p>As a symmetric semi-positive definite (SPD) matrix, the graph Laplacian L has a complete set of orthonormal eigenvectors {œÜ 1 , ¬∑ ¬∑ ¬∑ , œÜ N } satisfying LœÜ i = Œª i œÜ i , where {Œª i } are nonnegative real eigenvalues. We assume all eigenvalues are ordered as 0 = Œª 1 ‚â§ Œª 2 ¬∑ ¬∑ ¬∑ ‚â§ Œª N = Œª max . For the normalized Laplacian matrix, we have a bound of Œª max = 2. In matrix expression, the Laplacian matrix can be written</p><formula xml:id="formula_2">as L = Œ¶ŒõŒ¶ , where Œõ = diag([Œª 1 , Œª 2 , ¬∑ ¬∑ ¬∑ , Œª N ]) and Œ¶ = [œÜ 1 ; œÜ 2 ; ¬∑ ¬∑ ¬∑ ; œÜ N ] .</formula><p>According to graph theory <ref type="bibr" target="#b31">(Shuman et al. 2013)</ref>, the graph Fourier transform (GFT) of a signal x in spatial domain can be defined as x = Œ¶ x, where x is the produced graph frequency signal. The corresponding inverse GFT is x = Œ¶ x. A graph filtering H is an operator that acts upon a graph signal x by amplifying or attenuating its graph Fourier coefficients as Hx = N n=1 H(Œª n ) x n œÜ n . The graph frequency response H : [Œª min , Œª max ] ‚Üí C controls how much H amplifies the signal spectra H(Œª n ) = (œÜ n Hx)/ x n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Model</head><p>Human body skeleton is represented with a group of 3D spatial coordinates of joints. Hence, one can use a graph to depict spatial relation of skeletal joints. To keep the original coordinate information, we add attributes of nodes into the graph, i.e.,</p><formula xml:id="formula_3">G = (V, A, X), where V = {v i } N i=1</formula><p>is the set of nodes w.r.t. skeletal joints, A is a (weighted) adjacency matrix and X is a matrix of graph signals/attributes. According to the body bones between joints, we define those connected edges, and simply assign them to 1. In addition, other strategies (e.g., Gaussian kernel) may be used to produce the adjacency matrix. The signal matrix X = [x 1 , x 2 , ¬∑ ¬∑ ¬∑ , x d ] ‚àà R N √ód is supported on the node set V, whose i-th component (or channel) x i represents a signal of all nodes. That means, the i-th node v i ‚àà V is assigned with a signal vector of d dimensions. For skeletal data, we define the signal of each joint with its 3D spatial coordinates, i.e., x i¬∑ : V ‚Üí (x i , y i , z i ). Thus, for a dynamic graph sequence of length T , we may formulate it as a stream of graphs (G 1 , G 2 , ¬∑ ¬∑ ¬∑ , G T ), where G k = (V, A k , X k ) denotes the skeleton at the k-th time slice.</p><p>Multi-scale Graphical Convolutional Kernels In the standard CNN running on images, the receptive field may be conveniently defined as a local square spatial region, due to grid-shaped structure. So convolutional filtering on regular structures is accessible. On the contrary, the construction of convolutional kernels on graphs is intractable because the same filtering responses are required for homogeneous graph structures. Inspired by the graph theory <ref type="bibr" target="#b31">(Shuman et al. 2013)</ref>, we resort to the adjacency matrix A, which expresses the connections between nodes. As A k exactly records the k-path reachable nodes, so we can construct a k-neighbor receptive field by defining a k-order polynomial of A, denoted as œà k (A). Taking the simplest case, let œà k (A) = A k , which actually describes the k-hop neighbor nodes. In practice, we may replace A with the Laplacian matrix L to avoid the scale effect of matrix norm during the recursive inference (see the following model). Thus, for receptive fields of K scales, we define multi-scale convolutional filtering as</p><formula xml:id="formula_4">Z = G * f = K‚àí1 k=0 œà k (L)XV k ,<label>(1)</label></formula><p>where œà k (L) expresses the receptive field of the k-th scale and V k ‚àà R d√ód is the corresponding signal transformation. The computation of œà k (L)X weightedly summarizes the information of all nodes within the k-scale receptive field, which thus is homogenous-invariant for graphs.</p><p>Spatio-Temporal Graph Convolution Inspired by the design philosophy of autoregressive moving average (ARMA) (Hannan and Deistler 2012), we construct the spatio-temporal graphical convolutional model as follows,</p><formula xml:id="formula_5">Y t+1 = K1‚àí1 k=0 œà k (L)Y t W k + X t V 0 ,<label>(2)</label></formula><formula xml:id="formula_6">O t+1 = Y t+1 + K2‚àí1 k=1 œà k (L)X t V k ,<label>(3)</label></formula><p>where œà k (¬∑) is a receptive field function on the k-th scale,</p><formula xml:id="formula_7">{W k ‚àà R d √ód , V k ‚àà R d√ód }</formula><p>are the signal transformation matrices with regard to the k-scale, K 1 and K 2 are the number of kernels respectively in the temporal and spatial domain. In the above model,</p><formula xml:id="formula_8">Y = [y 1 , ¬∑ ¬∑ ¬∑ , y d ], O = [o 1 , ¬∑ ¬∑ ¬∑ , o d ]</formula><p>can be respectively viewed as the hidden state and the output state. Along time slices, signals are recursively regressed with local convolutional kernels in Eqn. <ref type="formula" target="#formula_5">(2)</ref>, thus motion variations can be sequentially encoded. The output signals in Eqn.</p><p>(3) combine spatially convolutional graph signals as well as dynamic temporal signals. Moreover, each output signal o i is dependent on all input signals {x 1 , ¬∑ ¬∑ ¬∑ , x d }. Specifically, when signals are independent on each other, and the spatio-temporal convolutional filters are separately operated on a channel of signals, then the dynamic graph convolution model can be written as</p><formula xml:id="formula_9">Y t+1 = K1‚àí1 k=0 œà k (L)Y t diag(w k ) + X t diag(v 0 ), (4) O t+1 = Y t+1 + K2‚àí1 k=1 œà k (L)X t diag(v k ),<label>(5)</label></formula><p>where</p><formula xml:id="formula_10">w k = [w k1 , ¬∑ ¬∑ ¬∑ , w kd ] , v k = [v k1 , ¬∑ ¬∑ ¬∑ , v kd ]</formula><p>are mapping parameters, and w ki , v ki are associated to the k-th scale of the i-th signal. Note here the dimension of output is assumed to be the input dimension. It is easy to extend into d = d in the case of signal independency.</p><p>Frequency Domain Analysis As the receptive field function œà k (¬∑) is a polynomial expression, we can derive</p><formula xml:id="formula_11">œà k (L) = œà k (œÜ diag(Œª)œÜ ) = œÜœà k (diag(Œª))œÜ .</formula><p>Thus the frequency response of convolution filtering in Eqn.</p><p>(1) can be written as</p><formula xml:id="formula_12">Z = K‚àí1 k=0 diag(œà k (Œª)) XV k .<label>(6)</label></formula><p>In particular, if signals are independent as expressed in Eqn. <ref type="formula">(4)</ref> and <ref type="formula" target="#formula_9">(5)</ref>, then we can obtain the frequency responses of graphical signals as Z =</p><formula xml:id="formula_13">K‚àí1 k=0 (œà k (Œª)v k ) X, each frequency response of which is formally Z ij = K‚àí1 k=0 (v kj œà k (Œª i )) X ij .</formula><p>Therefore, when signals are independent, the multi-scale convolutional filtering on graphs may be regarded as a (K-1)-order polynomial approximation of graph Flourier transform, if let H(Œª</p><formula xml:id="formula_14">i ) = K‚àí1 k=0 v ki œà k (Œª i ).</formula><p>In the signal dependency case, we can decompose V k into a diagonal matrix multiplied by a general matrix, i.e., V k = diag(Œ± k ) V k . Then the frequency response of convolution filtering in Eqn. (6) can be written as Z =</p><formula xml:id="formula_15">K‚àí1 k=0 ((œà k (Œª)Œ± k ) X) V k .</formula><p>Therefore, in the general case of signal dependency, the calculation of frequency responses may be understood as two steps: (i) perform a polynomial approximation of GFT on each input signal, and (ii) transform multi-channel signals by the new mappings { V k }.</p><p>Stability Analysis For any sequence of graph realizations {G 1 , G 2 , ¬∑ ¬∑ ¬∑ }, we can prove the stability of the recursive model (Eqn. <ref type="formula" target="#formula_5">(2)‚àº(3)</ref>), which is summarized in the following theory. Theorem 1. Suppose the Laplacian matrix L has the eigenvalue decomposition L = Œ¶ŒõŒ¶ , and œà k (¬∑) is a k-order polynomial function satifying œà k (L) 2 ‚â§ 1. For the signal</p><formula xml:id="formula_16">mappings {W 0 , ¬∑ ¬∑ ¬∑ , W K1‚àí1 }, if their diagonal elements are non-negative, i.e., W k,ii ‚â• 0, and K1‚àí1 k=0 W k ‚àû &lt; 1, the frequency response vec( O) in Eqn. (3) will converge to lim t‚Üí‚àû vec( O t ) = T ¬∑ vec( X t‚àí1 ),<label>(7)</label></formula><formula xml:id="formula_17">T = (I‚àíŒì K1 0 (W, Œõ)) ‚àí1 Œì 1 0 (V, Œõ)+Œì K2 1 (V, Œõ), (8) where Œì b a (W, Œõ) = b‚àí1 k=a (W k ‚äó œà k (Œõ)) (similar for Œì K2 0 )</formula><p>. Moreover, the transformation function T has an upper-bound:</p><formula xml:id="formula_18">T 2 &lt; V 0 ‚àû 1 ‚àí K1‚àí1 k=0 W k ‚àû + K2 k=1 V k ‚àû . (9)</formula><p>Proof. As œà k (¬∑) is a polynomial function, we can transform the recursive model (Eqn.</p><p>(2)‚àº(3)) from spatial domain into frequency domain by using Y = Œ¶ Y and X = Œ¶ X,</p><formula xml:id="formula_19">Y t+1 = K1‚àí1 k=0 œà k (Œõ) Y t W k + X t V 0 ,<label>(10)</label></formula><formula xml:id="formula_20">O t+1 = Y t+1 + K2‚àí1 k=1 œà k (Œõ) X t V k .<label>(11)</label></formula><p>By using the abbreviated notation Œì b a (¬∑, ¬∑) and œà 0 (¬∑) = I, we can derive a vertorized style of Eqn. (10) as vec(</p><formula xml:id="formula_21">Y t+1 ) = Œì K1 0 (W, Œõ)vec( Y t ) + Œì 1 0 (V, Œõ)vec( X t ) = t œÑ =0 ((Œì K1 0 (W, Œõ)) œÑ )Œì 1 0 (V, Œõ)vec( X t ) + (Œì K1 0 (W, Œõ)) t+1 vec( Y 0 ).<label>(12)</label></formula><p>Next we need to prove Œì K1</p><formula xml:id="formula_22">0 (W, Œõ) 2 &lt; 1. The matrix Œì K1 0 (W, Œõ) is a block matrix of d √ó d blocks, each of which is a diagonal matrix. Let A (ij) = K1‚àí1 k=0 W k,ij œà k (Œõ) denote the (i, j)-th block diagonal matrix, where W k,ij denotes the (i, j)-th element of the matrix W k . As œà k (Œõ) is a diagonal matrix, we denote œà k (Œõ) = diag([Œª k1 ,Œª k2 , ¬∑ ¬∑ ¬∑ ,Œª kn ]) for simplification. Since œà k (L) 2 ‚â§ 1, we haveŒª ki ‚àà [‚àí1, 1]. And, since K1‚àí1 k=0 W k ‚àû &lt; 1, we have Œì K1 0 (W, Œõ) 2 ‚â§max i,r j |A (ij) rr |=max i,r j | K1‚àí1 k=0 W k,ijŒªkr | ‚â§ max i,r j K1‚àí1 k=0 |W k,ij | &lt; 1.<label>(13)</label></formula><p>Therefore, when t ‚Üí ‚àû, we can derive vec( Y t+1 ) as</p><formula xml:id="formula_23">vec( Y t+1 ) = (I ‚àí Œì K1 0 (W, Œõ)) ‚àí1 Œì 1 0 (V, Œõ)vec( X t ).</formula><p>(14) After a simple algebra calculation on Eqn. (11), we can reach the conclusion of Eqn. (7)‚àº(8).</p><p>Now we prove the bound of the transformation function T . First, we can derive the following two inequations,</p><formula xml:id="formula_24">(I ‚àí A ii ) ‚àí1 ‚àí1 ‚àû = (I ‚àí K1‚àí1 k=0 W k,ii œà k (Œõ)) ‚àí1 ‚àí1 ‚àû = min l |1 ‚àí K1‚àí1 k=0 W k,iiŒªkl | &gt;= 1 ‚àí | K1‚àí1 k=0 W k,ii |, (15) j =i A ij ‚àû = j =i K1‚àí1 k=0 W k,ij œà k (Œõ) ‚àû = j =i max l | K1‚àí1 k=0 W k,ijŒªkl | &lt; j =i K1‚àí1 k=0 |W k,ij |.<label>(16)</label></formula><p>Note the above derivation uses the conditions W k,ii ‚â• 0 andŒª kl ‚àà [‚àí1, 1]. When</p><formula xml:id="formula_25">K‚àí1 k=0 W k ‚àû &lt; 1, we can further have (I ‚àí A ii ) ‚àí1 ‚àí1 ‚àû &gt; j =i A ij ‚àû for ‚àÄi = 1, ¬∑ ¬∑ ¬∑ , d . So I‚àíŒì K1 0 (W, Œõ)</formula><p>is strictly block diagonal dominant (SBDD). According to Ahlberg-Nilson-Varah bound of SSDD matrix <ref type="bibr" target="#b21">(Moraƒça 2007)</ref>, after a series of algebra derivations, we can reach the bound:</p><formula xml:id="formula_26">(I ‚àí Œì(W, Œõ)) ‚àí1 ‚àû &lt; 1 1 ‚àí K1 k=0 W k ‚àû .<label>(17)</label></formula><p>Similarly,</p><formula xml:id="formula_27">Œì 1 0 (V, Œõ) ‚àû ‚â§ V 0 ‚àû , Œì K2 1 (V, Œõ) ‚àû ‚â§ K2 k=1 V k ‚àû .</formula><p>Finally we obtain an upper bound of T . When signals are independent, i.e., the recursive model takes Eqn. (4)‚àº(5), we can have the following corollary based on the above theory. Proposition 1. Suppose the Laplacian matrix L has the eigenvalue decomposition L = Œ¶ diag(Œª)Œ¶ = Œ¶ diag([Œª 1 , ¬∑ ¬∑ ¬∑ , Œª n ])Œ¶ , and œà k (L) is a k-order polynomial. If signals are independent, i.e., taking the recursive model of Eqn. (4)‚àº(5), for ‚àÄi = 1, ¬∑ ¬∑ ¬∑ , d, j = 1, ¬∑ ¬∑ ¬∑ , n, and w ki ‚â• 0 and | K1‚àí1 k=0 w ki œà k (Œª j )| &lt; 1, then the frequency response O in Eqn. (5) will converge to</p><formula xml:id="formula_28">lim t‚Üí‚àû O t+1 = T X t ,<label>(18)</label></formula><formula xml:id="formula_29">T = 1v 0 1 ‚àí K1‚àí1 k=0 œà k (Œª)w k + K2‚àí1 k=1 œà k (Œª)v k ,<label>(19)</label></formula><p>where ‚àí, respectively denote the elementwise division and multiplication. Further, if |œà k (Œª j )| &lt; 1, and considering T is the elementwise multiplication, then for ‚àÄ(i, j) term of T , we have an upper-bound:</p><formula xml:id="formula_30">T ij 2 &lt; v 0 ‚àû 1 ‚àí K1‚àí1 k=0 w k ‚àû + K2 k=1 v k ‚àû .<label>(20)</label></formula><p>Proof. According to Eqn. (12) in Theory 1, let</p><formula xml:id="formula_31">Œ• b a (w, Œª) = b‚àí1 k=a œà k (Œª)w k , we can have Y t+1 = t œÑ =0 ((Œ• K1 0 (w, Œª)) œÑ ) Œ• 1 0 (v, Œª) X t +(Œ• K1 0 (w, Œª)) t+1 Y 0 .<label>(21)</label></formula><p>Since</p><formula xml:id="formula_32">| K1‚àí1 k=0 w ki œà k (Œª j )| &lt; 1, when t ‚Üí ‚àû, we can have Y t+1 = Œ• 1 0 (v, Œª) 1 ‚àí Œ• K1 0 (w, Œª) X t .<label>(22)</label></formula><p>Further, Eqn. (18) can be obtained after simple derivations.</p><p>The upper-bound of T can also be easily obtained..</p><p>In practice we can take some normalization strategies on {L, W} to make them satisfy these preconditions, in order to guarantee model stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Stacking</head><p>The above recursive convolutional model can be easily extended into a deep architecture. Taking the recursive model as one basic layer, we may stack it into a multi-layer network architecture, in which the output signal O at the bottom layer is used as the input of the top layer. With the increase of layers, the receptive field size of convolutional kernels can become larger, thus the topper layer can abstract more global information. As observed from our experiments, this deep architecture (named deep STGC) can improve the performance of skeleton based action recognition. Besides, we may insert the recursive model into other networks as one basic unit to form a mixture network, according to the requirement of solved problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on four public skeleton based action datasets: Florence 3D <ref type="bibr" target="#b28">(Seidenari et al. 2013</ref>), HDM05 <ref type="bibr" target="#b22">(M√ºller et al. 2007</ref>), Large Scale Combined dataset <ref type="bibr" target="#b46">(Zhang et al. 2016</ref>) and NTU RGB+D . To investigate the effectiveness of our model, we conduct extensive experiments with different configurations listed as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Settings</head><p>Florence 3D (Florence) This dataset was collected from a stationary Kinect, and each body skeleton was recorded with only 15 joints. It contains 215 action sequences of 10 subjects with 9 actions: wave, drink from a bottle, answer phone, clap, tight lace, sit down, stand up, read watch, and bow. Due to a few skeletal joints, some types of actions are difficult to distinguish, such as drink from a bottle, answer phone and read watch. We follow the standard experimental settings to perform leave-one-subject-out crossvalidation <ref type="bibr" target="#b40">(Wang et al. 2016a</ref>).</p><p>HDM05 This dataset was captured by using an optical marker-based Vicon system, and contains 2337 action sequences of 130 motion classes, which are acted by 5 nonprofessional actors named "bd", "bk", "dg", "mm" and "tr". Each skeleton data is represented with 31 joints. Until now, this dataset should involve the most skeleton-based action categories to the best of our knowledge. Due to the intraclass variations and large number of motion classes, this dataset is challenging in action recognition. To compare with those previous literatures, we conduct two types of experiments by following two widely-used protocols. Firstly, we use two subjects "bd" and "mm" for training and the remaining three for testing <ref type="bibr" target="#b38">(Wang et al. 2015a)</ref>. Secondly, to fairly compare the current deep learning methods, we conduct 10 random evaluations, each of which randomly selects half of the sequences for training and the rest for testing <ref type="bibr" target="#b11">(Huang and Van Gool 2017)</ref>.</p><p>Large Scale Combined (LSC) This dataset combines nine publicly available datasets <ref type="bibr" target="#b17">(Li, Zhang, and Liu 2010;</ref><ref type="bibr" target="#b39">Wang et al. 2015b;</ref><ref type="bibr" target="#b41">2016b</ref>; Xia, Chen, and Aggarwal 2012;  <ref type="bibr" target="#b37">Wang et al. 2012;</ref><ref type="bibr" target="#b25">Oreifej and Liu 2013;</ref><ref type="bibr" target="#b15">Koppula, Gupta, and Saxena 2013;</ref><ref type="bibr" target="#b33">Sung et al. 2012;</ref><ref type="bibr" target="#b0">Bloom, Argyriou, and Makris 2013;</ref><ref type="bibr" target="#b1">Bloom, Makris, and Argyriou 2012;</ref><ref type="bibr" target="#b23">Ni, Wang, and Moulin 2011;</ref><ref type="bibr" target="#b3">Chen, Jafari, and Kehtarnavaz 2015)</ref>, and form a complex action dataset with 88 actions. As each individual dataset has its own characteristics in action execution manners, backgrounds, acting positions, view angles, resolutions, and sensor types, the combination of a large number of action classes makes the dataset more challenging in suffering large intra-class variation compared to each individual dataset. Following Zhang et al. <ref type="bibr" target="#b46">(Zhang et al. 2016</ref>), we conduct experiments using two standard settings, i.e., random cross subject evaluation and random cross sample evaluation. For each action, half of the subjects/samples are randomly selected for training while the rest for testing.</p><p>NTU RGB+D (NTU) This dataset is collected by Kinect v2 cameras from different views. It consists of 56880 sequences for 60 distinct actions, including various of daily actions and pair actions performed by 40 subjects. The skeleton data is represented by 25 joints. As far as we know, this dataset is currently the largest skeleton-based action recognition dataset. The large intra-class and view point variations make this dataset great challenging. Meanwhile, a large amount of samples will bring a new challenge to the current skeleton-based action recognition methods. We follow the two types of standard evaluation protocols , i.e., cross-view evaluation, cross-subject evaluation, to perform experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Data Processing As skeletal data is usually captured from multi-view points and human actions are independent on the user coordinate system, we modify the origin of the coor-  <ref type="bibr" target="#b28">(Seidenari et al. 2013)</ref> 82.00% Riemannian Manifold <ref type="bibr" target="#b6">(Devanne et al. 2015)</ref> 87.04% Lie Group <ref type="bibr" target="#b35">(Vemulapalli, Arrate, and Chellappa 2014)</ref> 90.88% Graph-Based <ref type="bibr" target="#b40">(Wang et al. 2016a)</ref> 91.63% MIMTL  95.29% P-LSTM  95.35% STGC K 97.67% Deep STGC K 99.07% <ref type="table">Table 3</ref>: Comparisons on HDM05 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy Accuracy RSR-ML <ref type="bibr" target="#b9">(Harandi, Salzmann, and Hartley 2014)</ref> 40.0% -Cov-RP <ref type="bibr" target="#b34">(Tuzel, Porikli, and Meer 2006)</ref> 58.9% -Ker-RP <ref type="bibr" target="#b38">(Wang et al. 2015a)</ref> 66.2% -SPDNet <ref type="bibr">(Huang and</ref>  </p><formula xml:id="formula_33">N N i=1 x i , where x i ‚àà R 3</formula><p>is a 3D coordinate of the i-th joint, N is the number of joints. Specially, for NTU, we preprocess the joint coordinates in a way similar to Shahroudy et al. . To enhance the robustness of model training, we perform data augmentation as widely used in previous deep learning literature <ref type="bibr" target="#b30">Shahroudy et al. 2016)</ref>. Concretely, for each action sequence, we split the sequence into several equal sized subsequences, here 12 segments, and then pick one frame from each segment randomly to generate a large amount of training sequences. In addition, we randomly scale the skeletons by multiplying a factor in [0.98, 1.02] for the sake of the adaptive capability of scaling.</p><p>Model Configuration For the undirected attribute graph, we simply design edge connections according to human bones. If two joints are bridged with a bone, the edge weight is assigned to 1, otherwise 0. The signals of each node are set to its 3D coordinate. For the receptive field function, we use the simplest polynomial term, i.e., œà k (L) = L k , which represents k-hop neighbors. According to Theory 1, we normalize/clip L = L/Œª max (Œª max = 2), W k,ii ‚â• 0, k |W k |1 &lt; 1 after the gradient update at each iteration, to make sure the model stability. The outputs of recursive model are concatenated into a softmax layer for classification. In the default case, the dimension of output signals is d = 32. For deep STGC, we empirically observe that stacking two layers is good enough to these datasets, thus we only employ two-layer network for Deep STGC. The outputs of each layer in deep STGC are 32, 64 dimensions. The most important factor of our model is the scale of convolutional kernels, which is analyzed in the next Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Selection of Convolutional Kernel Size</head><p>In the STGC K model, the parameters {K 1 , K 2 } control the receptive field sizes in temporal domain and spatial domain. When increasing K 1 , K 2 , the local filtering region will cover   <ref type="bibr" target="#b35">(Vemulapalli, Arrate, and Chellappa 2014)</ref> 52.76% 50.08% Dynamic Skeletons <ref type="bibr" target="#b10">(Hu et al. 2015)</ref> 65.22% 60.23% HBRNN <ref type="bibr" target="#b7">(Du, Wang, and Wang 2015)</ref> 63.97% 59.07% LieNet <ref type="bibr" target="#b12">(Huang et al. 2016)</ref> 66.95% 61.37% Deep LSTM  67.29% 60.69% P-LSTM  70.27% 62.93% ST-LSTM  77.70% 69.20% STA-LSTM <ref type="bibr" target="#b32">(Song et al. 2017)</ref> 81.20% 73.40% GCA-LSTM  82.80% 74.40% Geometric Features <ref type="bibr" target="#b48">(Zhang, Liu, and Xiao 2017)</ref> 82.39% 70.26% Clips + CNN + MTLN <ref type="bibr" target="#b14">(Ke et al. 2017</ref> the farther hoping neighbors. To check the effect of different scales, we conduct an experiment on HDM05 dataset by searching the temporal kernel scale K 1 in {1, 2, 3} and the spatial kernel scale K 2 in {1, 2, 4, 6, 8}. Considering the continuous convolution filtering is performed along time slices like a stacking CNN, in practice we should employ a smaller K 1 than K 2 . The cross-comparison results are reported in <ref type="figure" target="#fig_0">Figure 1</ref>. The best performance is obtained at K 1 = 2 and K 2 = 6, which are used as the default parameters. Note that, K = 1 means STGC(w/o L), which convolves only on the node itself without any neighbors. If K 1 = 1 and K 2 &gt; 1, only spatial filtering is taken. Conversely, K 1 &gt; 1 and K 2 = 1 for only temporal filtering. We can observe that the spatial and temporal filtering together contribute the gain for action recognition. , which doesn't use any adjacent relationship, i.e., implementing a multi-channel mapping like 1√ó1 convolution on images. When introducing the convolutional kernel with 1-neighborhood, i.e., STGC(w/ L), the performance is improved. As discussed in Section 4.3, we set K 1 = 2 and K 2 = 6 as default for multi-scale STGC K (indep.), STGC K (dep.), where the former is the version of independent signals while the latter is the general one. STGC K (dep.) is slightly superior to STGC K (indep.) due to the consideration of signal interaction. When extending STGC into the deep architecture of two layers, we can achieve the best performance on all four benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Verification of STGC Structure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparisons with State-of-the-Art</head><p>We compare state-of-the-art methods on Florence, HDM05, LSC, NTU, respectively, which are summarized in <ref type="table" target="#tab_1">Table 2</ref>, 3, 4 and 5. As observed from these results, we have the following observations. The proposed spatio-temporal graph convolution method is superior to the recent graph-based method <ref type="bibr" target="#b40">(Wang et al. 2016a</ref>) and LSTM-based methods <ref type="bibr" target="#b18">Liu et al. 2016;</ref><ref type="bibr" target="#b32">Song et al. 2017)</ref>. As shown in Table 2, our STGC has a large improvement (more than 7%) in contrast to the graph-based work <ref type="bibr" target="#b40">(Wang et al. 2016a</ref>). In principle, our STGC is very different from this work <ref type="bibr" target="#b40">(Wang et al. 2016a</ref>), although graph is used for both. Our method falls into a recursively convolutional architecture, while the work <ref type="bibr" target="#b40">(Wang et al. 2016a</ref>) follows the conventional graph kernel matching technique. Also, different from those LSTM-based methods, which only model dynamics of sequences by revising LSTM, our method absorbs the success of convolutional filtering into a recursive learning with a theoretical guarantee.</p><p>Our proposed STGC improves the current state-of-theart on most datasets. On the Florence dataset, our method achieves a nearly perfect performance 99.07%. On the current largest dataset NTU, under the same recursive idea, the performance is pushed to the higher 86.28% and 74.85%, from 82.80% and 74.40% for GCA-LSTM. Recently, the CNN-based methods <ref type="bibr" target="#b14">(Ke et al. 2017;</ref><ref type="bibr" target="#b20">Liu, Liu, and Chen 2017)</ref> converted skeletons into images and then employed the sophisticated CNN feature extraction techniques by using the pre-training on ImageNet <ref type="bibr" target="#b5">(Deng et al. 2009</ref>). Even so, without the use of extra training data, our STGC is still more competitive over them.</p><p>Deep learning based methods are more effective than those shallow learning methods. The advanced nonlinear dynamic networks, variations of LSTM <ref type="bibr" target="#b18">Liu et al. 2016;</ref><ref type="bibr" target="#b32">Song et al. 2017</ref>) and CNNbased models <ref type="bibr" target="#b14">(Ke et al. 2017;</ref><ref type="bibr" target="#b20">Liu, Liu, and Chen 2017)</ref>, largely improve the action recognition performance, due to their robust representation ability. For the conventional matrix-based descriptors (e.g., covariance or its variants), although the deep manifold learning strategies <ref type="bibr" target="#b12">(Huang et al. 2016;</ref><ref type="bibr" target="#b11">Huang and Van Gool 2017)</ref> are developed recently, the matrix-based representations limit their representation capability because the only second-order statistic relationship of skeletal joints is preserved, whereas first-order statistics is also informative <ref type="bibr" target="#b27">(Ranzato and Hinton 2010)</ref>.</p><p>Different datasets have different performances. Florence 3D is the simplest dataset with 215 sequences and 9 action classes, thus most methods obtain a higher accuracy. The most difficult dataset should be the largest dataset NTU, which consists of 56880 sequences and covers various of daily actions and pair actions. The cross subject accuracy is still less than 80% due to various entangled actions. Cross subject is more difficult than cross view or cross sample. The phenomenon is observed from <ref type="table" target="#tab_3">Table 4</ref>, <ref type="table" target="#tab_4">Table 5, and Table 3</ref> (the left/right column w.r.t cross subject/cross sample). It is easy to understand, in the cross subject task, more unforeseeable information exists in the testing set, compared to the other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a spatio-temporal graph convolution approach for combining the successes of local convolution filtering and the recursive learning power of autoregressive moving average. To locally filter on spatio-temporal structures, we introduced multi-scale graphical convolutional kernels, which were composed of the receptive field matrices defined by polynomials of adjacency matrix, and signal mappings. The multi-scale convolutional kernels were simultaneously performed on hidden states of sequences for encoding the motion variations and input state for extracting spatial graphical feature. In theory, we proved the convergency of the proposed model and provided an upper-bound. Moreover, we extended the basic model into a multi-layer deep architecture. We verified the representation ability of the proposed STGC and its deep version. We also demonstrated the improvements with STGC on four public skeletal datasets, including the current largest NTU RGB+D dataset. As a generic model, the proposed model may be generalized into many problems modeled by dynamic graphs, which will be one of our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparisons of different convolutional kernel scale K 1 , K 2 on HDM05 (Protocol 1) for our SGTC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparisons with different configurations for our model. STGC K (indep.) is the multi-scale filtering version under the assumption of signal independency. ‚Ä¢ STGC K (dep.) is the general multi-scale filtering version, where signals are dependent on each other. ‚Ä¢ Deep STGC K (indep.) is the deep scheme by stacking STGC K (indep.). ‚Ä¢ Deep STGC K (dep.) is the deep scheme by stacking STGC</figDesc><table><row><cell>Method</cell><cell>Florence</cell><cell cols="2">HDM05 Protocol 1 Protocol 2</cell><cell cols="2">Cross Sample</cell><cell>LSC</cell><cell cols="2">Cross Subject</cell><cell cols="2">NTU Cross View Cross Subject</cell></row><row><cell></cell><cell cols="2">Accuracy Accuracy</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell cols="2">Precision</cell><cell>Recall</cell><cell>Accuracy</cell><cell>Accuracy</cell></row><row><cell>STGC (w/o L)</cell><cell>98.14%</cell><cell>73.82%</cell><cell>81.92%¬±1.04</cell><cell>82.51%</cell><cell cols="2">80.80%</cell><cell>82.44%</cell><cell>81.12%</cell><cell>80.87%</cell><cell>70.29%</cell></row><row><cell>STGC (w/ L)</cell><cell>98.60%</cell><cell>75.76%</cell><cell>82.24%¬±1.18</cell><cell>83.65%</cell><cell cols="2">82.04%</cell><cell>83.63%</cell><cell>80.24%</cell><cell>81.11%</cell><cell>71.00%</cell></row><row><cell>STGC K (indep.)</cell><cell>98.14%</cell><cell>77.08%</cell><cell>82.49%¬±1.49</cell><cell>84.70%</cell><cell cols="2">82.60%</cell><cell>82.08%</cell><cell>78.17%</cell><cell>79.71%</cell><cell>70.62%</cell></row><row><cell>STGC K (dep.)</cell><cell>97.67%</cell><cell>77.99%</cell><cell>83.40%¬±1.30</cell><cell>86.44%</cell><cell cols="2">85.10%</cell><cell>84.00%</cell><cell>82.12%</cell><cell>81.84%</cell><cell>72.09%</cell></row><row><cell>Deep STGC K (indep.)</cell><cell>98.60%</cell><cell>78.19%</cell><cell>86.17%¬±1.25</cell><cell>86.73%</cell><cell cols="2">86.18%</cell><cell>84.58%</cell><cell>81.11%</cell><cell>83.57%</cell><cell>73.60%</cell></row><row><cell>Deep STGC K (dep.)</cell><cell>99.07%</cell><cell>78.68%</cell><cell>85.29%¬±1.33</cell><cell cols="5">88.11% 87.03% 85.44% 83.42%</cell><cell>86.28%</cell><cell>74.85%</cell></row><row><cell cols="4">‚Ä¢ STGC (w/o L) is the standard baseline without consid-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ering neighbor nodes, i.e., only performing channel map-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ping;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">‚Ä¢ STGC w/ L uses convolutional kernels with the receptive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">filed of one hop neighborhood;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>‚Ä¢</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>K (dep.)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons on Florence 3D dataset.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>Multi-part Bag-of-Poses</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on Large Scale Combined dataset.</figDesc><table><row><cell>Method</cell><cell cols="4">Cross Sample Precision Recall Precision Recall Cross Subject</cell></row><row><cell>HON4D (Oreifej and Liu 2013)</cell><cell>84.6%</cell><cell>84.1%</cell><cell>63.1%</cell><cell>59.3%</cell></row><row><cell>Dynamic Skeletons (Hu et al. 2015)</cell><cell>85.9%</cell><cell>85.6%</cell><cell>74.5%</cell><cell>73.7%</cell></row><row><cell>P-LSTM (Shahroudy et al. 2016)</cell><cell>84.2%</cell><cell>84.9%</cell><cell>76.3%</cell><cell>74.6%</cell></row><row><cell>STGC K</cell><cell>86.4%</cell><cell>85.1%</cell><cell>84.0%</cell><cell>82.1%</cell></row><row><cell>Deep STGC K</cell><cell>88.1%</cell><cell>87.0%</cell><cell>85.4%</cell><cell>83.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparisons on NTU RGB+D dataset.</figDesc><table><row><cell>Method</cell><cell>Cross View</cell><cell>Cross Subject</cell></row><row><cell>Lie Group</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>For our proposed model STGC itself, we conduct a series of experiments with different configurations on four benchmark datasets. The results are summarized inTable 1. They include six configurations: STGC(w/o L), STGC(w/ L), STGC K (indep.), STGC K (dep.), deep STGC K (indep.) and deep STGC K (dep.). The standard baseline should be STGC(w/o L)</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic feature selection for online action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HBU</title>
		<meeting>HBU</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="64" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">G3d: A gaming action dataset and real time action recognition evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bio-inspired dynamic 3d discriminative skeletal features for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3-d human action recognition by shape analysis of motion trajectories on riemannian manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1340" to="1352" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The statistical theory of linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deistler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From manifold to manifold: Geometry-aware dimensionality reduction for spd matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A riemannian network for spd matrix learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep learning on lie groups for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05877</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from rgb-d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="951" to="970" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<idno>arXiv:1711.06427</idno>
		<title level="m">Action-attending graphic neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Gated graph sequence neural networks</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3d points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatiotemporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Upper bounds for the infinity norm of the inverse of sdd and s-sdd matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moraƒça</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="666" to="678" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>R√∂der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Clausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kr√ºger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<title level="m">Documentation mocap database hdm05</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rgbd-hudaact: A colordepth video database for human daily activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCVW</title>
		<meeting>ICCVW</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1147" to="1153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pr≈æulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling pixel means and covariances using factorized third-order boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2551" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recognizing actions from depth cameras as weakly aligned multipart bag-of-poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Varano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="479" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07659</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An end-toend spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unstructured human activity detection from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="842" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Region covariance: A fast descriptor for detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="589" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beyond covariance: Feature representation with nonlinear kernel matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4570" to="4578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convnets-based action recognition from depth maps through virtual cameras and pseudocoloring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACMMM</title>
		<meeting>ACMMM</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph based skeleton motion representation and similarity measurement for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="370" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Action recognition from depth maps using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Human Mach. Syst</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="509" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD</title>
		<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Discriminative multi-instance multitask learning for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="519" to="529" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A survey on human motion analysis from depth data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Time-of-Flight and Depth Imaging. Sensors, Algorithms, and Applications</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="149" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A large scale rgb-d dataset for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep manifoldto-manifold transforming network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10732</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On geometric features for skeleton-based action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
