<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingdong</forename><surname>He</surname></persName>
							<email>heqingdong@std.uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengning</forename><surname>Wang</surname></persName>
							<email>zhengning.wang@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zeng</surname></persName>
							<email>haozeng@std.uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zeng</surname></persName>
							<email>zengyi@std.uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
							<email>liushuaicheng@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
							<email>eezeng@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate 3D object detection from point clouds has become a crucial component in autonomous driving. However, the volumetric representations and the projection methods in previous works fail to establish the relationships between the local point sets. In this paper, we propose Sparse Voxel-Graph Attention Network (SVGA-Net), a novel end-to-end trainable network which mainly contains voxel-graph module and sparse-to-dense regression module to achieve comparable 3D detection tasks from raw LIDAR data. Specifically, SVGA-Net constructs the local complete graph within each divided 3D spherical voxel and global KNN graph through all voxels. The local and global graphs serve as the attention mechanism to enhance the extracted features. In addition, the novel sparse-to-dense regression module enhances the 3D box estimation accuracy through feature maps aggregation at different levels. Experiments on KITTI detection benchmark demonstrate the efficiency of extending the graph representation to 3D object detection and the proposed SVGA-Net can achieve decent detection accuracy. * Corresponding author.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the widespread popularity of LIDAR sensors in autonomous driving <ref type="bibr" target="#b5">[6]</ref> and augmented reality <ref type="bibr" target="#b18">[19]</ref>, 3D object detection from point clouds has become a mainstream research direction. Compared to RGB images from video cameras, point clouds could provide accurate depth and geometric information which can be used not only to locate the object, but also to describe the shape of the object. However, the properties of unordered, sparsity and relevance of point clouds make it a challenging task to utilize point clouds for 3D object detection directly.</p><p>In recent years, several pioneering approaches have been proposed to tackle these challenges for 3D object detection on point clouds. The main ideas for processing point clouds data are to project point clouds to different views <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> or divide the point clouds into equally spaced voxels <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b37">38]</ref>. Then convolutional neural networks and mature 2D objection detection frameworks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref> are applied to extract features. However, because projection alone cannot capture the object's geometric information well, many methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref> have to combine RGB images in the designed network. While the methods using only voxelization do not make good use of the properties of the point clouds and bring a huge computational burden <ref type="bibr" target="#b16">[17]</ref> as resolution increases. Apart from converting point clouds into other formats, some works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b41">42]</ref> take Pointnets <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> as backbone to process point clouds directly. Although Pointnets build a hierarchical network and use a symmetric function to maintain permutation invariance, they fail to construct the neighbour relationships between the grouped point sets <ref type="bibr" target="#b34">[35]</ref>.</p><p>Considering the properties of point clouds, we should notice the superiority of graphs in dealing with the irregular data. In fact, in the domain of point clouds for segmentation and classification tasks, the method of processing with graphs has been deeply studied by many works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref>. However, few researches have used graphs to make 3D object detection from point clouds. To our knowledge, Point-GNN <ref type="bibr" target="#b30">[31]</ref> may be the first to prove the potential of using the graph neural network as a new approach for 3D object detection. Point-GNN introduces auto-registration mechanism to reduce translation variance and designs box merging and scoring operation to combine detection results from multiple vertices accurately. However, similar to ShapeContextNet <ref type="bibr" target="#b36">[37]</ref> and Pointnet++ <ref type="bibr" target="#b21">[22]</ref>, the relationship between point sets is not well established in the feature extraction process and a large number of matrix operations will bring heavy calculation burden and memory cost.</p><p>In this paper, we propose the sparse voxel-graph attention network (SVGA-Net) for 3D object detection. SVGA-Net is an end-to-end trainable network which takes raw point clouds as input as outputs the category and bounding boxes information of the object. Specifically, SVGA-Net mainly consists of voxel-graph network module and sparse-to-dense regression module. Instead of normalized rectangle voxels, we divide the point clouds into 3D spherical space with a fixed radius. The voxel-graph network aims to construct local complete graph for each voxel and global KNN graph for all voxels. The local and global serve as the attention mechanism that can provide a parameter supervision factor for the feature vector of each point. In this way, the local aggregated features can be combined with the global point-wise features. Then we design the sparse-to-dense regression module to predict the category and 3D bounding box by processing the features at different scales. Evaluation on KITTI benchmark demonstrates that our proposed method can achieve comparable results with the state-of-the-art approaches.</p><p>Our key contributions can be summarized as follows:</p><p>• We propose a new end-to-end trainable 3D object detection network from point clouds which uses graph representations without converting to other formats.</p><p>• We design a voxel-graph network, which constructs the local complete graph within each spherical voxel and the global KNN graph through all voxels to learn the discriminative feature representation simultaneously.</p><p>• We propose a novel 3D boxes estimation method that aggregates features at different scales to achieve higher 3D localization accuracy.</p><p>• Our proposed SVGA-Net achieves decent experimental results with the state-of-the-art methods on the challenging KITTI 3D detection dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Projection-based methods for point clouds. To align with RGB images, series of works process point clouds through projection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b14">15]</ref>. Among them, MV3D <ref type="bibr" target="#b2">[3]</ref> projects point clouds to bird view and trains a Region Proposal Network (RPN) to generate positive proposals. It extracts features from LiDAR bird view, LIDAR front view and RGB image, for every proposal to generate refined 3D bounding boxes. AVOD <ref type="bibr" target="#b10">[11]</ref> improves MV3D by fusing image and bird view features and merges features from multiple views in the RPN phase to generate positive proposals. Note that accurate geometric information may be lost in the high-level layers with this scheme.</p><p>Volumetric methods for point clouds. Another typical method for processing point clouds is voxelization. VoxelNet <ref type="bibr" target="#b42">[43]</ref> is the first network to process point clouds with voxelization, which use stacked VFE layers to extract features tensors. Following it, a large number of methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4]</ref> divide the 3D space into regular grids and group the points in a grid as a whole. However, they often need to stack heavy 3D CNN layers to realize geometric pose inference which bring large computation.</p><p>Pointnet-based methods for point clouds. To process point clouds directly, PointNet <ref type="bibr" target="#b20">[21]</ref> and PonintNet++ <ref type="bibr" target="#b21">[22]</ref> are the two groundbreaking works to design parallel MLPs to extract features from the raw irregular data, which improve the accuracy greatly. Taking them as backbone, many works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b40">41]</ref> begin to design different feature extractors to achieve better performance. Although Pointnets are effective to abstract features, they still suffer feature loss between the local and global point sets.</p><p>Graph-based methods for point clouds. Constructing graphs to learn the order-invariant representation of the irregular point clouds data has been explored in classification and segmentation tasks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">35]</ref>. Graph convolution operation is efficient to compute features between points. DGCNN <ref type="bibr" target="#b34">[35]</ref> proposes EdgeConv in the neighbor point sets to fuse local features in a KNN graph. SAWNet <ref type="bibr" target="#b8">[9]</ref> extends the ideas of PointNet and DGCNN to learn both local and global information for points. Surprisingly, few researches have considered applying graph for 3D object detection. Point-GNN may be the first work to design a GNN for 3D object detection. Point-GNN <ref type="bibr" target="#b30">[31]</ref> designs a one-stage graph neural network to predict the category and shape of the object with an auto-registration mechanism, merging and scoring operation, which demonstrate the potential of using the graph neural network as a new approach for 3D object detection. In this section, we detail the architecture of the proposed SVGA-Net for 3D detection from point clouds. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our SVGA-Net architecture mainly consists of two modules: voxelgraph network and spare-to-dense regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Voxel-graph network architecture</head><p>Spherical voxel grouping. Consider the original point clouds are represented as</p><formula xml:id="formula_0">G = {V, D}, where V = {p 1 , p 2 , ..., p n } indicting n points in a D dimensional metric space. In our practice, D is set to 4 so each point in 3D space is defined as v i = [x i , y i , z i ]</formula><p>, where x i , y i , z i denote the coordinate values of each point along the axes X, Y, Z and the fourth dimension is the laser reflection intensity which denoted as s i .</p><p>Then in order to cover the entire point set better, we use the iterative farthest point sampling <ref type="bibr" target="#b21">[22]</ref> to choose N farthest points</p><formula xml:id="formula_1">P = {p i = [v i , s i ] T ∈ R 4 } i=1,2,.</formula><p>. . N . According to each point in P , we search its nearest neighbor within a fixed radius r to form a local voxel sphere:</p><formula xml:id="formula_2">b i = {p 1 , p 2 , ...p i , ..., p j , ... | v i − v j 2 &lt; r}<label>(1)</label></formula><p>In this way, we can subdivide the 3D space into N 3D spherical voxels</p><formula xml:id="formula_3">B = {b 1 , b 2 , ..., b N }.</formula><p>Local point-wise feature. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, for each spherical voxel b i = {p j = [x j , y j , z j , s j ] T } j=1,2,...,t with t points (t varies for different voxel sphere), the coordinate information of all points inside form the input vector. We extract the local point-wise features for each voxel sphere by learning a mapping:</p><formula xml:id="formula_4">f (b i ) = M LP (p j ) j=1,2,...,t</formula><p>(2) Then, we could obtain the local point-wise feature representation for each voxel sphere F = {f i , i = 1, . . . , t}, which are transformed by the subsequent layers for deeper feature learning.</p><p>Local point-attention layer. Taken the features of each nodes as input, the local point-attention layer outputs the refined features F = {f i , i = 1, ..., t} through series of information aggregation. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we construct a complete graph for each local node set and KNN graph for all the spherical voxels. We aggregate the information of each node according to the local and global attention score. The feature aggregation of j-th node is represented as:</p><formula xml:id="formula_5">f j = β m · f j + k∈ (pj ) α j,k · f j,k<label>(3)</label></formula><p>where f j denotes the dynamic updated feature of node p j and f j is the input feature of node p j . (p j ) denotes the index of the other nodes inside the same sphere. f j,k denotes the feature of the k-th nodes inside the same sphere. α j,k is the local attention score between node p j and the other nodes inside the same sphere. β m is the global attention score from the global KNN graph in the m-th iterations.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a), we construct a complete graph for all nodes within a voxel sphere to learn the features constrained by each other. And the local attention score α j,k is calculated by:</p><formula xml:id="formula_6">α j,k = sof tmax j (f j , f j,k ) = exp(f T j · f j,k ) k∈ (pj ) exp(f T j · f j,k )<label>(4)</label></formula><p>(a) local complete graph (b) global KNN graph Global attention layer. By constructing the local complete graph, the aggregated features can only describe the local feature and do not integrate with the global information. So we design the global attention layer to learn the global feature of each spherical voxel and offer a feature factor aligned to each node.</p><p>For the points within each b i in N 3D spherical voxels B = {b 1 , b 2 , ..., b N }, we calculate the physical centers of all voxels which denoted as {c i } i=1,...,N . Each center is learned by a 3-layer MLP to get the initial global feature F g = {f g,1 , f g,2 , ..., f g,N }. As <ref type="figure" target="#fig_1">Figure 2</ref> (b) shows, we construct a KNN graph for the N voxel sphere. For each node f g,i , the attention score between node f g,i and its l-th neighbor is calculated as follows:</p><formula xml:id="formula_7">β m = f T g,i · f g,i,l l∈ (fg,i) f T g,i · f g,i,l<label>(5)</label></formula><p>where (f g,i ) denotes the index of the neighbors of node f g,i . m is the number of the point attention layers.</p><p>Voxel-graph features representation. The point attention operation on each spherical voxel can combine the parameter factor from both local and global, each of which is inserted with a 2-layer MLP with a nonlinear activation to transform each updated feature f j . By stacking multiple point attention layers, both local aggregated feature and global point-wise feature can be learned. We then apply maxpool on the aggregated feature to obtain the final feature vector. To process all the spherical voxel, we obtain a set of voxel sphere features, each of which corresponds to the spatial coordinates of the voxels and is taken as input of the sparse-to-dense regression module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparse-to-dense regression</head><p>For each 3D bounding box in 3D space, the predicted box information is represented as (x, y, z, l, w, h, θ), where (x, y, z) is the center coordinate of the bounding box, (l, w, h) is the size information alongside length, width and height respectively, and θ is the heading angle. Feature map from the voxel-graph network is processed by region proposal regression module. The architecture of the specified sparse-to-dense regression(SDR) module is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. SDR module first apply three similar blocks as <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b12">13]</ref> to generate smaller the spatial resolution from top to down. Each block consist of series of Conv2D(f in , f out , k, s, p) layers, followed by Batch-Norm and a ReLU, where f in and f out are the number of input and output channels, k, s, p represent the kernel size, stride size and padding size respectively. The stride size is set to 2 for the first layer of each block to downsample the feature map by half, followed by sequence of convolutions with stride 1.</p><p>And the output of the three blocks is denoted as b 1 , b 2 , b 3 respectively.</p><p>In order to combine highresolution features with large receptive fields and lowresolution features with small receptive fields, we concat the output of the second and third modules b 2 , b 3 with the output of the first and second modules b 1 , b 2 after upsampling. In this way, the dense feature range of the lower level can be well combined with the sparse feature range of the higher level. Then a series of convolution operations with an upsampling layer are performed in parallel on three scale channels to generate three feature maps with the same scale size, which are denoted as F 1 , F 2 , F 3 .</p><p>In addition, we consider that the features output of F 1 , F 2 , F 3 are more densely fit to our final goal than the original three modules. Therefore, in order to combine the original sparse feature map and the series of processed dense feature maps, we combine the original output b 1 , b 2 , b 3 after upsampling and F 1 , F 2 , F 3 by element-wise addition. The final output F s is obtained by concatenating the fused feature maps after a 3 × 3 convolution layer. And F s is taken as input to perform category classification and 3D bounding box regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss function</head><p>We use a multi-task loss to train our network. Each prior anchor and ground truth bounding box are parameterized as (x a , y a , z a , l a , w a , h a , θ a ) and (x gt , y gt , z gt , l gt , w gt , h gt , θ gt ) respectively. The regression residuals between anchors and ground truth are computed as:</p><formula xml:id="formula_8">∆x = x gt − x a d a , ∆y = y gt − y a d a , ∆z = z gt − z a h a ∆w = log( w gt w a ), ∆l = log( l gt l a ), ∆h = log( h gt h a ) ∆θ = sin(θ gt − θ a )<label>(6)</label></formula><p>where d a = (w a ) 2 + (l a ) 2 . And we use Smooth L1 loss <ref type="bibr" target="#b6">[7]</ref> as our 3D bounding box regression loss L reg .</p><p>For the object classification loss, we apply the classification binary cross entropy loss.</p><formula xml:id="formula_9">L cls = γ 1 1 N pos i L cls (p pos i , 1) + γ 2 1 N neg i L cls (p neg i , 0).<label>(7)</label></formula><p>where N pos and N neg are the number of the positive and negative anchors. p pos i and p neg i are the softmax output for positive and negative anchors respectively. γ 1 and γ 2 are positive constants to balance the different anchors, which are set to 1.5 and 1 respectively in our practice.</p><p>Our total loss is composed of two parts, the classification loss L cls and the bounding box regression loss L reg . The total loss is denoted as:</p><formula xml:id="formula_10">L total = αL cls + β 1 N pos t∈{x,y,z,l,w,h,θ} L reg (∆t * , ∆t).<label>(8)</label></formula><p>where ∆t * and ∆t are the predicated residual and the regression target respectively. Weighting parameters α and β are used to balance the relative importance of different parts, and their values are set to 1 and 2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our method on the widely used KITTI 3D object detection benchmark <ref type="bibr" target="#b5">[6]</ref>. It includes 7481 training samples and 7518 test samples with three categories: car, pedestrian and cyclist. For each category, detection results are evaluated based on three levels of difficulty: easy, moderate and hard. Following <ref type="bibr" target="#b1">[2]</ref>, we divide the training data into a training set (3712 images and point clouds) and a validation set (3769 images and point clouds) at a ratio of about 1: 1 (Ablation studies are conducted on this split). We train our model on train split and compare our results with state-of-the-art methods on both val split and test split. For evaluation, the average precision (AP) metric is to compare with different methods and the 3D IoU of car, cyclist, and pedestrian are 0.7, 0.5, and 0.5 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Network Architecture. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, in the local point-wise feature and global attention layer, the point sets are first processed by 3-layer MLP and the sizes are all <ref type="figure" target="#fig_0">(64, 128, 128</ref>). In the local point attention layer, we stack n = 3 local point-attention graph to aggregate the features, each followed by a 2-layer MLP. And the sizes of the three MLPs are (128, 128), (128, 256) and (512, 1024) respectively. Following <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42]</ref>, we train two networks, one for cars and another for both pedestrians and cyclists. For cars, we sample N = 1024 to form the initial point sets. To construct the local complete graph, we choose r = 1.8m. For anchors, an anchor is considered as positive if it has the highest IoU with a ground truth or its IoU score is over 0.6. An anchor is considered as negative if the IoU with all ground truth boxes is less than 0.45. To reduce redundancy, we apply IoU threshold of 0.7 for NMS. For cyclist and pedestrian, the number of the initial point sets is n = 512. We set r = 0.8 to construct the local graph. The anchor is considered as positive if its highest IoU score with a ground truth box or an IoU score is over than 0.5. And an anchor is considered as negative if its IoU score with ground truth box is less than 0.35. The IoU threshold of NMS is set to 0.6.</p><p>Training. The network is trained in an end-to-end manner on GTX 1080 GPU. The ADAM optimizer <ref type="bibr" target="#b9">[10]</ref> is employed to train our network and its initial learning rate is 0.001 for the first 140 epoches and is decayed by 10 times in every 20 epoches. We train our network for 200 epoches with a batch size of 16 on 4 GPU cards. Furthermore, we also apply data augmentation as <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref> do to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparing with state-of-the-art methods</head><p>Performance on KITTI test dataset. We evaluate our method on the 3D detection benchmark and the bird's eye view detection benchmark of the KITTI test server. As shown in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref>, we compare our results with state-of-the-art RGB+Lidar and Lidar only methods for the 3D object detection and the bird's view detection task. Our proposed method outperforms the most effective RGB+Lidar methods MMF <ref type="bibr" target="#b14">[15]</ref> by (4.86%, 6.2%, 6.22%) and (6.33%, 4.2%, 5.79%) for car category on three difficulty levels of 3D detection and BEV detection.</p><p>Compared with the Lidar-based methods, our SVGA-Net can still show decent performance on the three categories. In particular, we are far superior to Point-GNN <ref type="bibr" target="#b30">[31]</ref> using the same graph representation method in the detection of the three categories. We believe that this may benefit from our construction of local and global graphs to better capture the feature information of point clouds.</p><p>The slight inferiority of the hard difficulty level in the two detection tasks may be due to the fact that the local graph cannot be constructed for objects with occlusion ratio exceeding 80%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation studies</head><p>In this section, we conduct series of extensive ablation studies on the validation split of KITTI to illustrate the role of each module in improving the final result and our parameter selection. All ablation studies are implemented on the car class which contains the largest amount of training examples. The evaluation metric is the average precision (AP %) on the val set.</p><p>Results on KITTI validation dataset. For the most important car category, we first report the performance of our method on KITTI val split and the results are shown in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table" target="#tab_3">Table 4</ref>. For car, our proposed method achieves better or comparable results than state-of-the-art methods on three difficulty levels which illustrate the superiority of our method.   Effect of different design choice. In the local point attention layer, we stack several local complete layers to extract aggregated features. In order to show the impact of the number of the point attention layer, we train our network with n varying from 1 to 4. As shown in <ref type="table" target="#tab_4">Table 5</ref>, when the local feature information is transmitted on the 1st to 3rd layers, the detection accuracy is continuously improved because the features are continuously aggregated to the object itself. When n increases to 4, the detection accuracy decreases slightly, and we believe that the network should be over-learning.</p><p>Furthermore, we study the importance of the global attention layer in improving the detection accuracy. As shown in <ref type="table" target="#tab_4">Table 5</ref>, the AP values on both detection tasks are greatly reduced when we remove this module from the network, which proves the importance of this design in providing global feature information for each point.</p><p>In the last three rows of <ref type="table" target="#tab_4">Table 5</ref>, we aim to explore the effect of different design in the spare-to-dense regression module. SR is to remove the concatenation of b 1 , b 2 with the upsampled b 2 , b 3 and DR is to remove the addition of b i with F i . Results show that only the design of sparse-to-dense regression ranks the first in improving detection accuracy.</p><p>Running time. Our network is written in Python and implemented in Pytorch for GPU computation. The average inference time for one sample is 62 ms, including 14.5%(9 ms) for data reading and pre-processing, 66.1%(41 ms) for local and global features aggregation and 19.4%(12 ms) for final boxes detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have proposed a novel sparse voxel-graph attention network(SVGA-Net) for 3D Object Detection from raw Point Clouds. We introduce graph representation to process point clouds.</p><p>By constructing a local complete graph in the divided spherical voxel space, we can get a better local representation of the point feature, and the information between the point and its neighborhood can be fused. By constructing a global graph, we can better supervise and learn the features of points. In addition, the sparse-to-dense regression module can also fuse feature maps at different scales. Experiments have demonstrated the efficiency of the design choice in our network. Future work will extend SVGA-Net to combine RGB images to further improve detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our work aims to solve the problem of detection of road objects in autonomous driving, which is particularly critical in autonomous driving scenarios. The improvement of detection accuracy can accelerate the implementation of unmanned vehicles. Our paper introduces a new method of representing road point clouds data, which will bring new ideas to further improve the accuracy to a certain extent. However, we have to admit that the popularization of autonomous driving will bring a series of traffic safety problems, and the resulting car accidents are also incalculable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of the proposed SVGA-Net. The voxel-graph network takes raw point clouds as input, partitions the space into spherical voxels, transforms the points in each sphere to a vector representing the feature information. The sparse-to-dense regression module takes the aggregated features as input as generates the final boxes information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Graph construction. Each node with different color indicates the aggregated feature and arrows direction represents the information propagation direction with independent attention calculations scores. (a) local complete graph: for each node, we aggregate the information of all the nodes within the same spherical voxel according to the attention score. (b) global 3-NN graph: we aggregate the information of the three nearest neighbours around each node according to the attention score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FsFigure 3 :</head><label>3</label><figDesc>The architecture of the sparse-to-dense regression module. Features from the voxel-graph network are processed by series of region proposal extraction operations to generate the final classification and regression maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative 3D detection results of SVGA-Net on the KITTI test set. The detected objects are shown with green 3D bounding boxes and the relative labels. The upper row in each image is the 3D object detection result projected onto the RGB image and the bottom is the result in the corresponding point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on KITTI 3D object detection for car, pedestrian and cyclists.The evaluation metrics is the average precision (AP) on the official test set. 'R' denotes RGB images input and 'L' denotes Lidar point clouds input. R+L 81.20 70.39 62.19 51.21 44.89 40.23 71.96 56.77 50.39 AVOD-FPN[11] R+L 81.94 71.88 66.38 50.80 42.81 40.88 64.00 52.18 46.61 F-ConvNet[36] R+L 85.88 76.51 68.08 52.37 45.61 41.49 79.58 64.68 57.82.95 74.63 55.21 47.71 44.56 79.22 66.13 57.64</figDesc><table><row><cell>Method</cell><cell>Modality</cell><cell>APcar(%)</cell><cell cols="6">AP pedestrian (%) AP cyclist (%)</cell></row><row><cell></cell><cell></cell><cell cols="7">Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard</cell></row><row><cell>MV3D[3]</cell><cell cols="3">R+L 71.09 62.35 55.12 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>F-Pointnet[20]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>03</cell></row><row><cell>MMF[15]</cell><cell cols="3">R+L 86.81 76.75 68.41 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Voxelnet[43]</cell><cell>L</cell><cell cols="7">77.47 65.11 57.73 39.48 33.69 31.51 61.22 48.36 44.37</cell></row><row><cell>SECOND[38]</cell><cell>L</cell><cell cols="7">83.13 73.66 66.20 51.07 42.56 37.29 70.51 53.85 46.90</cell></row><row><cell>PointPillars[13]</cell><cell>L</cell><cell cols="7">79.05 74.99 68.30 52.08 43.43 41.49 75.78 59.07 52.92</cell></row><row><cell>PointRCNN[29]</cell><cell>L</cell><cell cols="7">85.94 75.76 68.32 49.43 41.78 38.63 73.93 59.60 53.59</cell></row><row><cell>STD[42]</cell><cell>L</cell><cell cols="7">86.61 77.63 76.06 53.08 44.24 41.97 78.89 62.53 55.77</cell></row><row><cell>3DSSD[41]</cell><cell>L</cell><cell cols="2">88.36 79.57 74.55 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SA-SSD[8]</cell><cell>L</cell><cell cols="2">88.75 79.79 74.16 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PV-RCNN [28]</cell><cell>L</cell><cell cols="2">90.25 81.43 76.82 -</cell><cell>-</cell><cell cols="4">-78.60 63.71 57.65</cell></row><row><cell>Point-GNN[31]</cell><cell>L</cell><cell cols="7">88.33 79.47 72.29 51.92 43.77 40.14 78.60 63.48 57.08</cell></row><row><cell>SVGA-Net(ours)</cell><cell>L</cell><cell>91.67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on KITTI bird's eye view detection for car, pedestrian and cyclists. The evaluation metrics is the average precision (AP) on the official test set. 'R' denotes RGB images input and 'L' denotes Lidar point clouds input.</figDesc><table><row><cell>As shown in Figure 4, we il-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lustrate some qualitative pre-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dicted bounding results of our proposed SVGA-Net on the</cell><cell>Method</cell><cell>Modality</cell><cell>APcar(%)</cell><cell>AP pedestrian (%)</cell><cell>AP cyclist (%)</cell></row><row><cell>test split on KITTI dataset. For better visualization, we project the 3D bounding</cell><cell cols="5">Easy Moderate Hard Easy Moderate Hard Easy Moderate Hard R+L 86.02 76.90 68.49 ------R+L 88.70 84.00 75.33 58.09 50.22 47.20 75.38 61.96 54.68 AVOD-FPN[11] R+L 88.53 83.79 77.90 58.75 51.05 47.54 68.09 57.48 50.77 MV3D[3] F-Pointnet[20]</cell></row><row><cell>boxes into RGB images and BEV in point clouds. From</cell><cell>F-ConvNet[36] MMF[15] Voxelnet[43]</cell><cell cols="4">R+L 89.69 83.08 74.56 58.90 50.48 46.72 82.59 68.62 60.62 R+L 89.49 87.47 79.10 ------L 89.35 79.26 77.39 46.13 40.74 38.11 66.70 54.76 50.55</cell></row><row><cell>the figures we could see</cell><cell>SECOND[38]</cell><cell>L</cell><cell cols="3">88.07 79.37 77.95 55.10 46.27 44.76 73.67 56.04 48.78</cell></row><row><cell>that our proposed network could estimate accurate 3D</cell><cell>PointPillars[13] PointRCNN[29] STD[42]</cell><cell>L L L</cell><cell cols="3">88.35 86.10 79.83 58.66 50.23 47.19 79.14 62.25 56.00 89.47 85.58 79.10 ---81.52 66.77 60.78 89.66 87.76 86.89 60.99 51.39 45.89 81.04 65.32 57.85</cell></row><row><cell>bounding boxes in different scenes. Surprisingly, SVGA-Net can still produce accurate</cell><cell>SA-SSD[8] PV-RCNN [28] Point-GNN[31] SVGA-Net(ours)</cell><cell>L L L L</cell><cell cols="3">95.03 91.03 85.96 -94.98 90.65 86.14 -93.11 89.17 83.90 55.36 47.07 44.61 81.17 67.28 59.67 -------82.49 68.89 62.41 95.82 91.67 84.89 61.13 50.71 46.91 83.55 69.71 61.52</cell></row><row><cell>3D bounding boxes even un-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>der poor lighting conditions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>and severe occlusion.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on KITTI 3D object detection val set for car class.</figDesc><table><row><cell>Method</cell><cell>Modality</cell><cell></cell><cell>APcar (%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>MV3D [3]</cell><cell>R+L</cell><cell>71.29</cell><cell>62.68</cell><cell>56.56</cell></row><row><cell>F-Pointnet [20]</cell><cell>R+L</cell><cell>83.76</cell><cell>70.92</cell><cell>63.65</cell></row><row><cell>AVOD-FPN [11]</cell><cell>R+L</cell><cell>84.41</cell><cell>74.44</cell><cell>68.65</cell></row><row><cell>Cont-Fuse [16]</cell><cell>R+L</cell><cell>86.32</cell><cell>73.25</cell><cell>67.81</cell></row><row><cell>F-ConvNet[36]</cell><cell>R+L</cell><cell>89.02</cell><cell>78.80</cell><cell>77.09</cell></row><row><cell>Voxelnet [43]</cell><cell>L</cell><cell>81.97</cell><cell>65.46</cell><cell>62.85</cell></row><row><cell>SECOND [38]</cell><cell>L</cell><cell>87.43</cell><cell>76.48</cell><cell>69.10</cell></row><row><cell>PointRCNN [29]</cell><cell>L</cell><cell>88.88</cell><cell>78.63</cell><cell>77.38</cell></row><row><cell>Fast PointRCNN [4]</cell><cell>L</cell><cell>89.12</cell><cell>79.00</cell><cell>77.48</cell></row><row><cell>STD[42]</cell><cell>L</cell><cell>89.70</cell><cell>79.80</cell><cell>79.30</cell></row><row><cell>SA-SSD[8]</cell><cell>L</cell><cell>90.15</cell><cell>79.91</cell><cell>78.78</cell></row><row><cell>3DSSD[41]</cell><cell>L</cell><cell>89.71</cell><cell>79.45</cell><cell>78.67</cell></row><row><cell>Point-GNN[31]</cell><cell>L</cell><cell>87.89</cell><cell>78.34</cell><cell>77.38</cell></row><row><cell>SVGA-Net(ours)</cell><cell>L</cell><cell>90.59</cell><cell>80.23</cell><cell>79.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison on KITTI bird's eye view detection val set for car class.</figDesc><table><row><cell>Method</cell><cell>Modality</cell><cell></cell><cell>APcar (%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>MV3D [3]</cell><cell>R+L</cell><cell>86.55</cell><cell>78.10</cell><cell>76.67</cell></row><row><cell>F-Pointnet [20]</cell><cell>R+L</cell><cell>88.16</cell><cell>84.02</cell><cell>76.44</cell></row><row><cell>F-ConvNet[36]</cell><cell>R+L</cell><cell>90.23</cell><cell>88.79</cell><cell>86.84</cell></row><row><cell>Voxelnet [43]</cell><cell>L</cell><cell>89.60</cell><cell>84.81</cell><cell>78.57</cell></row><row><cell>SECOND [38]</cell><cell>L</cell><cell>89.96</cell><cell>87.07</cell><cell>79.66</cell></row><row><cell>Fast PointRCNN [4]</cell><cell>L</cell><cell>90.12</cell><cell>88.10</cell><cell>86.24</cell></row><row><cell>STD[42]</cell><cell>L</cell><cell>90.50</cell><cell>88.50</cell><cell>88.10</cell></row><row><cell>Point-GNN[31]</cell><cell>L</cell><cell>89.82</cell><cell>88.31</cell><cell>87.16</cell></row><row><cell>SVGA-Net(ours)</cell><cell>L</cell><cell>90.27</cell><cell>89.16</cell><cell>88.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison with different design choice. n is the number of pointattention layers. 'w/o.' denotes whether to keep the global attention layer. SDR denotes the sparse-to-dense regression.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">3DAPcar (%)</cell><cell cols="3">BEV APcar (%)</cell></row><row><cell></cell><cell></cell><cell cols="6">Easy Moderate Hard Easy Moderate Hard</cell></row><row><cell></cell><cell>1</cell><cell>86.77</cell><cell>75.37</cell><cell cols="2">74.19 87.54</cell><cell>86.11</cell><cell>83.72</cell></row><row><cell>n</cell><cell>2 3</cell><cell>88.86 90.59</cell><cell>78.81 80.23</cell><cell cols="2">78.03 89.04 79.15 90.27</cell><cell>88.44 89.16</cell><cell>87.05 88.11</cell></row><row><cell></cell><cell>4</cell><cell>89.62</cell><cell>79.26</cell><cell cols="2">77.58 89.72</cell><cell>88.51</cell><cell>87.17</cell></row><row><cell>w/o.</cell><cell cols="2">o. w. 90.59 88.42</cell><cell>78.11 80.23</cell><cell cols="2">76.54 89.71 79.15 90.27</cell><cell>87.45 89.16</cell><cell>84.33 88.11</cell></row><row><cell></cell><cell cols="2">SR 87.53</cell><cell>77.81</cell><cell cols="2">76.22 86.95</cell><cell>86.62</cell><cell>85.04</cell></row><row><cell></cell><cell cols="2">DR 88.39</cell><cell>78.44</cell><cell cols="2">76.56 87.91</cell><cell>86.82</cell><cell>86.73</cell></row><row><cell></cell><cell cols="2">SDR 90.59</cell><cell>80.23</cell><cell cols="2">79.15 90.27</cell><cell>89.16</cell><cell>88.11</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph-based object classification for neuromorphic vision sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bourtsoulatze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andreopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="491" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9775" to="9784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1355" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Structure aware single-stage 3d object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sawnet: A spatially aware deep neural network for 3d point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pears</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07650</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1513" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="963" to="973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Tanet: Robust 3d object detection from point clouds with triple attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiple 3d object tracking for augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE/ACM International Symposium on Mixed and Augmented Reality, ISMAR &apos;08</title>
		<meeting>the 7th IEEE/ACM International Symposium on Mixed and Augmented Reality, ISMAR &apos;08<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="117" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.13192</idno>
		<title level="m">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Part-aˆ2 net: 3d part-aware and aggregation neural network for object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Complexer-yolo: Real-time 3d object detection and tracking on semantic point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Honer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Samann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaulbersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Michael</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mvx-net: Multimodal voxelnet for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7276" to="7282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno>146:1-146:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1742" to="1749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hdnet: Exploiting hd maps for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10187</idno>
		<title level="m">Point-based 3d single stage object detector</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
