<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dilated Convolution with Dilated GRU for Music Source Separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for IT Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
							<email>yang@citi.sinica.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for IT Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dilated Convolution with Dilated GRU for Music Source Separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Stacked dilated convolutions used in Wavenet have been shown effective for generating high-quality audios. By replacing pooling/striding with dilation in convolution layers, they can preserve highresolution information and still reach distant locations. Producing high-resolution predictions is also crucial in music source separation, whose goal is to separate different sound sources while maintaining the quality of the separated sounds. Therefore, this paper investigates using stacked dilated convolutions as the backbone for music source separation. However, while stacked dilated convolutions can reach wider context than standard convolutions, their effective receptive fields are still fixed and may not be wide enough for complex music audio signals. To reach information at remote locations, we propose to combine dilated convolution with a modified version of gated recurrent units (GRU) called the 'Dilated GRU' to form a block. A Dilated GRU unit receives information from k steps before instead of the previous step for a fixed k. This modification allows a GRU unit to reach a location with fewer recurrent steps and run faster because it can execute partially in parallel. We show that the proposed model with a stack of such blocks performs equally well or better than the state-ofthe-art models for separating vocals and accompaniments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Music source separation has received much attention and has obtained impressive progress in recent years <ref type="bibr" target="#b6">[Stöter et al., 2018]</ref>. It has different use cases such as generating accompaniment from pop songs for karaoke <ref type="bibr" target="#b6">[Rafii et al., 2018]</ref>, separating specific sources as a pre-processing tool for other tasks such as music transcription <ref type="bibr" target="#b5">[Paulus and Virtanen, 2005]</ref>, and DJ-related applications <ref type="bibr">[Vande Veire and De Bie, 2018]</ref>.</p><p>In recent years, neural network-based methods have obtained promising result for music source separation <ref type="bibr">[Nugraha et al., 2016;</ref><ref type="bibr" target="#b6">Uhlich et al., 2017;</ref><ref type="bibr" target="#b6">Takahashi et al., 2018;</ref><ref type="bibr" target="#b6">Stöter et al., 2018]</ref>. Among them, <ref type="bibr">[Nugraha et al., 2016]</ref> proposed a DNN architecture with fully-connected layers, and it is one of the first models based on neural networks for music source separation. Music source separation requires the model to produce highresolution predictions. This has mostly been achieved by either using encoder-decoder architecture with skip connections <ref type="bibr" target="#b1">[Jansson et al., 2017;</ref><ref type="bibr" target="#b6">Takahashi and Mitsufuji, 2017;</ref><ref type="bibr" target="#b6">Takahashi et al., 2018;</ref><ref type="bibr" target="#b2">Liu and Yang, 2018;</ref><ref type="bibr" target="#b6">Stoller et al., 2018]</ref>, or recurrent neural networks <ref type="bibr" target="#b6">[Uhlich et al., 2017]</ref>.</p><p>We notice that the stacked dilated convolutions used in Wavenet <ref type="bibr" target="#b6">[van den Oord et al., 2016]</ref> might also work well for this purpose. In Wavenet, the kernels in convolution layers are dilated more and more as the network goes deeper, so the entire network can access neighboring information as well as distant information, depending on how many dilated convolution layers are stacked.</p><p>Dilated convolutions are used in audio generation <ref type="bibr" target="#b6">[van den Oord et al., 2016]</ref>, machine translation <ref type="bibr">[Kalchbrenner et al., 2017]</ref>, speech recognition <ref type="bibr" target="#b6">[Sercu and Goel, 2016]</ref>, semantic segmentation <ref type="bibr" target="#b7">[Yu and Koltun, 2016]</ref>, and video generation <ref type="bibr">[Kalchbrenner et al., 2017]</ref>. To the best of our knowledge, dilated convolutions have not been used for audio regression problems such as music source separation.</p><p>Music audio signals are usually very long, even using spectrogram as the feature. For example, a 3-minute audio under 44,100 Hz sampling rate and 1,024-sample hop size for shorttime Fourier transform has 7,752 frames in its spectrogram representation. It is not easy to stack enough dilated convolutions to reach that far with limited computational resources.</p><p>We propose to combine a recurrent layer, specifically a gated recurrent unit (GRU) <ref type="bibr" target="#b1">[Cho et al., 2014;</ref><ref type="bibr" target="#b1">Chung et al., 2014]</ref>, and a dilated convolution to form a block. A GRU can in theory reach very far away in one layer if the information does not decay too fast. As aforementioned, music audio signals can be very long. This can sometimes be a problem for a recurrent layer like GRU, because it has to process its input sequence sequentially.</p><p>We use the Dilated GRU to alleviate this problem. A Dilated GRU unit receives information from k-step before instead of the previous step for a fixed k. This modification allows a GRU unit to reach a location with fewer recurrent steps and run faster because it can execute partially in parallel. The dilated version of recurrent layers is also used in other contexts. For example, <ref type="bibr" target="#b0">[Chang et al., 2017]</ref> stacked multiple recurrent layers with increasing dilations for speaker identifica-tion, while <ref type="bibr">[Vezhnevets et al., 2017]</ref> used them as managers in reinforcement learning, both of which are pure RNN architectures. In contrast, we combine them with dilated grouped convolutions to form processing blocks. We call these blocks the 'Dilated recurrent-Dilated convolution' (D2) blocks.</p><p>We conduct extensive experiments to verify the capability of the proposed model, as well as the relative importance of its components. We also investigate how the D2 blocks work. Our evaluation shows that our model (GRU dilation=1) outperforms the state-of-the-art model <ref type="bibr" target="#b6">[Takahashi et al., 2018]</ref> by 0.25 dB and 0.57 dB in the signal-to-distortion ratio (SDR) for vocal separation and accompaniment, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed model</head><p>Our model works on the magnitude of spectrograms. For a D × T tensor M mix of the mixture magnitude, the goal of source separation is to predict a D × T tensor M s for each source s ∈ S, where D, T , and S denotes the feature dimension, the sequence length, and the set of sources, respectively. Our model takes M mix as the input, and predicts all the source tensors M s at once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dilated GRU</head><p>GRU 1 <ref type="bibr" target="#b1">[Cho et al., 2014;</ref><ref type="bibr" target="#b1">Chung et al., 2014</ref>] is a popular design choice of recurrent layers. In this paper, we use a modified version of GRU where the unit at a temporal point receives information from k-step before instead of the previous step for a fixed k. The same idea of dilating the temporal connections of an RNN has also been used by <ref type="bibr">[Vezhnevets et al., 2017;</ref><ref type="bibr" target="#b0">Chang et al., 2017]</ref>.</p><p>Dilated GRU with dilation k involves the following operations:</p><formula xml:id="formula_0">r t = σ(W ir x t + b ir + W hr h (t−k) + b hr ) , z t = σ(W iz x t + b iz + W hz h (t−k) + b hz ) , n t = tanh(W in x t + b in + r t (W hn h (t−k) + b hn )) , h t = (1 − z t )n t + z t h (t−k) ,</formula><p>where W stands for matrices, b the bias terms, and σ the sigmoid function. r, z, n and h are all vectors. The temporal indices are partitioned into k disjoint sets, meaning that they can be processed independently in parallel. <ref type="figure" target="#fig_0">Figure 1</ref> provides an illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dilated Recurrent-Dilated Convolution Block (D2 block)</head><p>In contrast to the a conventional convolution, the kernel of a dilated convolution is dilated so that it takes as input the further neighbors instead of the immediate neighbors. We propose to stack a dilated grouped convolution on top of a Dilated GRU layer with bidirectional connections. The input to the block, the output of the Dilated GRU and the output of the dilated grouped convolution are summed together  <ref type="bibr">, 2016]</ref> are applied to all the convolution layers.</p><p>In stacked dilated convolutions, the model could potentially pay less attention to neighboring locations as the dilation in convolution layers increases. We hope to alleviate this problem by using vertical skip connections from the block input to the block output, as also shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The grouping in a convolution layer is also used in architectures such as MobileNet <ref type="bibr" target="#b1">[Howard et al., 2017]</ref> and Shuf-fleNet <ref type="bibr" target="#b7">[Zhang et al., 2018]</ref>. It reduces the connections between input and output and hence reduces the memory usage and computation while still maintaining the capacity of the network. In one layer, the different groups have their own inputs and do not communicate with each other. Different groups cannot directly communicate with each other, so the grouped convolutions are usually followed some mixing layers such as fully-connected layers <ref type="bibr" target="#b1">[Howard et al., 2017]</ref> or shuffle layers <ref type="bibr" target="#b7">[Zhang et al., 2018]</ref>. We do not use these extra layers in the proposed model, but instead delegate this communication task to the GRU layer, since the computation in GRU is fully-connected across channels. Therefore, the Dilated GRU at the beginning of each block will mix the information from different groups in addition to its other task of aggregating information through time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Full model</head><p>The full model is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. The input is first processed by a convolution layer. Then, several D2 blocks are stacked together. By the end, the output of the last D2 block is processed by a convolution. It then outputs the separation predictions of all the sources at once.</p><p>Most top models in SiSEC2018 use certain forms of denoising auto-encoders that down-sample and up-sample along the temporal axis in the encoders and decoders respectively <ref type="bibr" target="#b1">[Jansson et al., 2017;</ref><ref type="bibr" target="#b6">Takahashi and Mitsufuji, 2017;</ref><ref type="bibr" target="#b6">Takahashi et al., 2018;</ref><ref type="bibr" target="#b2">Liu and Yang, 2018]</ref>. They also use symmetric skip connections connecting a pair of encoder and decoder to compensate the loss of high-resolution information due to down-sampling. In contrast, our models maintain the temporal resolution without down-sampling and up-    sampling during the whole process, so the high-resolution information will not be lost. The blocks can be easily stacked without considering the symmetry of skip connections across different blocks in this approach.</p><p>More details of our model can be found in <ref type="table">Table 1.</ref> 3 Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Setup</head><p>We evaluate the proposed model for music source separation on the <ref type="formula">MUSDB18</ref>  Output convolution 1D Conv (3, 1) 2048 4×2049 <ref type="table">Table 1</ref>: Model specification. The two numbers in the parentheses after '1D Conv' are kernel size and dilation size, respectively. All the 1D convolution layers in the Blocks 1, 2 and 3 have 32 groups, where each group has 64 input channels and 64 output channels. 2018]. MUSDB18 contains 100 songs for training and 50 songs for evaluation, all with 44,100 Hz sampling rate. Each song contains the source audios of 'vocals,' 'drums,' 'bass' and 'other.' The evaluation is conducted by using the evaluation package provided by the SiSEC2018 organizers. <ref type="bibr">3</ref> The evaluation metrics are computed by taking the median over all the tracks in the evaluation set as done in the official report <ref type="bibr" target="#b6">[Stöter et al., 2018]</ref>, using the SiSEC analysis package. <ref type="bibr">4</ref> We evaluate the performance of the models mainly with objective metrics commonly used in music source separation. Unless otherwise specified, we report the SDR in the tables.</p><p>Our models use the log-scale magnitudes of the complex spectrogram as the feature. First, complex spectrograms are derived by applying a short-term Fourier transform (STFT) to waveforms, with 4,096-sample window size and 3/4 overlapping. Then, the magnitudes of the complex spectrograms are computed. log(1 + magnitude) of the mixture audios and the source audios are used as the inputs and the training targets, respectively. Mean square error is used as the loss function for updating the network, and Adam [Kingma and <ref type="bibr" target="#b1">Ba, 2015]</ref> is used to update the weights. As data augmentation has been found useful in the literature <ref type="bibr" target="#b6">[Takahashi et al., 2018;</ref><ref type="bibr" target="#b6">Uhlich et al., 2017;</ref><ref type="bibr" target="#b2">Liu and Yang, 2018]</ref>, we use data augmentation in the training process by randomly shuffling the audio clips in each source and then collecting the audio clips from the four sources in the shuffled orders. A mixture clip is formed by summing the collected source clips. The source audio clips are shuffled at the beginning of every epoch. Batch sizes 20 and 5 are used for 5-sec and 20-sec training respectively so that they have roughly the same number of weight updating in training. 1/10 of the training set is used for validation. The weights in the epoch with the best validation loss during 500-epoch training are kept for a model.</p><p>To convert the outputs of the model to waveforms, the above process is reversed. The phases of the mixture complex spectrograms are used with the predicted spectrogram magnitudes to construct the complex spectrogram. Before converting back to waveforms, multi-channel Wiener filter is applied to the complex spectrograms as widely done in recent source separation systems <ref type="bibr">[Nugraha et al., 2016;</ref><ref type="bibr" target="#b6">Uhlich et al., 2017;</ref><ref type="bibr" target="#b6">Takahashi et al., 2018;</ref><ref type="bibr" target="#b2">Liu and Yang, 2018]</ref>. <ref type="table" target="#tab_2">Table 2</ref> shows the performance of the proposed models and the top-performing models of SiSEC2018 <ref type="bibr" target="#b6">[Rafii et al., 2018]</ref>. <ref type="bibr">5</ref> We show the top 5 models (one top model from each group) that are trained without extra data in SiSEC2018. In this subsection, each setting is run three times, and the mean score and standard deviation of the three runs is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with Participants of SiSEC2018</head><p>First, we can see that the model using stacked dilated convolutions alone without recurrent layers and skip connections already have performance comparable with the top-3 model JY3 in SiSEC2018. This verifies our intuition that stacked di-lated convolutions are strong not only in generation problems but also in music source separation.</p><p>The best performance in both 'vocals' and 'accompaniment' is achieved by the proposed model with dilation 1 trained with 20-sec clips. In general, models trained with longer and shorter clips have similar performance.</p><p>Dilation 2 and dilation 1 are both strong models. The performance difference between the proposed models with dilation 1 and dilation 2 are not as large as the difference between the difference between the proposed models and TAK1. In practice, the proposed model with dilation 2 has the benefit of parallel computation.</p><p>Our models are strong in 'vocals,' 'other,' and overall 'accompaniment.' They perform slightly worse in 'drums' and 'bass' compared to TAK1 and UHL2. Our conjecture is that the different sources are competing for resources in the model because we train one model for all four sources. The sounds in 'vocals' and 'other' are usually louder than 'drums' and 'bass,' so they are weighted more in the loss function. In contrast, TAK1 and UHL2 do not have this problem because they train one model for each source <ref type="bibr" target="#b6">[Takahashi et al., 2018;</ref><ref type="bibr" target="#b6">Uhlich et al., 2017]</ref>. This can be a trade-off between resources and performance.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, we compare our models with TAK1, including other performance metrics in addition to SDR. Our models consistently outperform TAK1 in 'vocals' and 'other' in all metrics. We also show the track-by-track comparison in <ref type="figure" target="#fig_3">Figure 3</ref>. In general, our proposed model performs better in vocals in almost all tracks, while our model and TAK1 have their own advantages in accompaniment in different tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We consider ablated versions of our model in this evaluation. Specifically, we use the model with dilation 2 and trained with 5-sec subclips as the baseline. The score of one training run for each setting is reported in this subsection. We will refer to the model in Section 2.2 and <ref type="table">Table 1</ref> as the proposed model.</p><p>First, we compare different block designs. The first one is the proposed block, the second one, 'Dense,' is similar to the proposed block with an additional skip connection adding the input of the block to the output of Dilated GRU, the third one, 'Residual,' has a residual connection for each layer including Dilated GRU and convolutions, and the fourth one has the same skip connections as the proposed block but the dilated convolution layer is before the Dilated GRU. To investigate the effectiveness of GRU, we replace GRU with a convolution in the fifth row. The performance of the variants are presented in <ref type="table" target="#tab_4">Table 4</ref>. The proposed one has the best performance.</p><p>Second, we compare different GRU dilations in <ref type="table" target="#tab_5">Table 5</ref>. Dilation 1 and 2 perform better either in vocals or accompaniment than dilation 4. <ref type="bibr" target="#b0">[Chang et al., 2017]</ref> constructed a recurrent neural network with increasing dilations in the recurrent layers. We also evaluate our model with increasing dilations, but found no improvement as shown in the last row of <ref type="table" target="#tab_5">Table 5</ref>.</p><p>To evaluate the running speed, we run the model with one batch 10 times for each setting and compute the average running time. The average are 423±4.90, 340±8.91, and Proposed, GRU dilation 2 20-sec training 6.76 ± 0.06 5.85 ± 0.07 4.84 ± 0.09 4.49 ± 0.08 13.19 ± 0.02 Proposed, GRU dilation 2 5-sec training 6.78 ± 0.05 5.66 ± 0.09 4.96 ± 0.03 4.48 ± 0.09 13.22 ± 0.05 Proposed, GRU dilation 1 20-sec training 6.85 ± 0.04 5.86 ± 0.12 4.86 ± 0.05 4.65 ± 0.04 13.40 ± 0.11 Proposed, GRU dilation 1 5-sec training 6.81 ± 0.15 5.72 ± 0.06 4.58 ± 0.10 4.48 ± 0.11 13.26 ± 0.08     327±10.2 ms for models with dilation 1, dilation 2, and dilation 4, respectively. The running time is reduced about 20% from dilation 1 to dilation 2, but there is only marginal reduction from dilation 2 to dilation 4. Third, models with different number of D2 blocks are compared in <ref type="table" target="#tab_6">Table 6</ref>. 3 blocks and 4 blocks have close results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">How the Model Works</head><p>We investigate here how our model works. In a D2 block, the block input is added to the output of the last layer as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. This operation intuitively could fix the semantic of each channel, starting from the output of the first convolution to the output of the final block. To verify this intuition, we remove the blocks from the top of the stack one at a time in the trained dilation-2 model. This results in an altered model without any blocks, an altered model with block 1, an altered model with block 1 and 2, and the original model with block 1, 2 and 3. We apply these altered models to songs and convert them back to audios just like how we evaluate the full separation model described in Section 3.1. Note that these altered models use the same weights as the full model and are not re-trained. We find that the output from these altered models have very good sound quality subjectively. In terms of separation performance, it gets better as more blocks are returned (added) to the model. The fact that the separations from these altered models are audible verifies the intuition that the semantics of the channels are fixed to some degree.</p><p>By plotting the waveforms converted from the outputs of the altered models, we can have a grasp of how the original model works. <ref type="figure" target="#fig_5">Figure 5</ref> shows such plots for 'other' and 'vocals.' We observe that 'vocals,' 'drums,' and 'other' get more and more activations as more blocks are used. In contrast, 'other' gets less and less activations as more blocks are used. These observations give us some hints on the strategy the model has developed. In the lower blocks, the model stores most of the information in the 'other' source. As the process moves forward, the 'vocals' recovers more and more information from the channels related to the other three sources. The changes in 'drums' and 'bass' are relatively smaller compared to the change in 'vocals.'</p><p>The objective metrics also give us some hints on the process, as shown in <ref type="table" target="#tab_8">Table 7</ref>. 'Drums' and 'bass' are relatively simple signals compared to 'vocals' and 'other,' so the model can already roughly separate these two sources even without any blocks. In contrast, 'vocals' and 'other' are very poor without any blocks. The capacity of the model increases as the blocks are added back.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented a model for music source separation. It uses a stack of dilated convolutions as the backbone and consists of a stack of D2 blocks. In a D2 block, a Dilated GRU is combined with a dilated convolution to form a unit. Dilated GRU runs faster than standard GRU while still maintaining vocals drums bass other  the performance. The proposed model achieves state-of-theart performance in separating 'vocals' and 'accompaniment.' Currently, music source separation focuses on separating sources in Pop music with vocals. In the future, we aim to separate different sources such as different instruments in symphony and rich electronic sounds in EDM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Parallel branches in Dilated GRU. The locations with odd indices and even indices can be processed in parallel with dilation 2. to form the output of the block. This is illustrated in Figure 2. Leaky ReLU and weight normalization [Salimans and Kingma</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Dilated GRU-Dilated convolution block (D2 block). A Dilated GRU layer is followed by a dilated convolution with groups. The input of the block, the output of Dilated GRU and the output of the dilated convolution are added together to the output of the block. Two views of the proposed architecture are shown, since the architecture contains temporal connections (better seen in the Temporal view) as well as grouped channels in convolutions (better seen in the Channel view).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Track-by-track comparison of the 50 test songs. The SDR of a track is represented by a block colored with a color on the yellow-green-black color map (high to low). The 'Proposed' represents the model of GRU dilation 1 trained with 20-sec clips.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Network architecture of the proposed model. A convolution layer processes the input feature map, followed by a stack of D2 blocks. A convolution layer processes the output of the last D2 block and outputs all the predicted separations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Waveform evolving as the blocks are returned (added) to a trained model. The model with no block, the one with block 1, the one with blocks 1 and 2, and the one with all blocks 1, 2 and 3 are represented in gray-scale shades from the darkest to the lightest shades. The 'other' source and the other three soruces show opposite behaviors as the number of blocks increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>dataset 2 used in SiSEC2018 [Stöter et al.,</figDesc><table><row><cell>Unit</cell><cell>Spec.</cell><cell>Input Output</cell></row><row><cell>Input convolution</cell><cell cols="2">1D Conv (3, 1) 2049 2048</cell></row><row><cell>Block 1</cell><cell>Dilated GRU</cell><cell>2048 2048</cell></row><row><cell></cell><cell cols="2">1D Conv (3, 2) 2048 2048</cell></row><row><cell>Block 2</cell><cell>Dilated GRU</cell><cell>2048 2048</cell></row><row><cell></cell><cell cols="2">1D Conv (3, 4) 2048 2048</cell></row><row><cell>Block 3</cell><cell>Dilated GRU</cell><cell>2048 2048</cell></row><row><cell></cell><cell cols="2">1D Conv (3, 8) 2048 2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with models in SiSEC2018 (in SDR). The table shows the top models in SiSEC2018 in the upper part, the baseline stacked dilated convolutions in the middle part, and the proposed models in the lower part. All the proposed models contain three blocks. ± .06 5.85 ± .07 4.84 ± .09 4.49 ± .08 13.19 ± .02 D=1 6.85 ± .04 5.86 ± .12 4.86 ± .05 4.65 ± .04 13.40 ± .11 ± .30 11.65 ± .13 9.44 ± .13 7.37 ± .27 18.22 ± .24 D=1 14.33 ± .31 12.26 ± .21 9.20 ± .40 7.58 ± .13 18.43 ± .31 ± .18 6.05 ± .12 5.89 ± .19 4.98 ± .02 13.63 ± .12 D=1 6.56 ± .05 6.13 ± .23 5.82 ± .10 4.88 ± .06 13.72 ± .18 D=2 13.24 ± .05 10.62 ± .24 9.63 ± .28 9.56 ± .17 22.38 ± .13 D=1 13.50 ± .10 10.69 ± .20 10.20 ± .31 9.58 ± .12 22.39 ± .31</figDesc><table><row><cell>SDR vocals</cell><cell>drums</cell><cell>bass</cell><cell>other</cell><cell>accomp.</cell></row><row><cell>TAK1 6.60</cell><cell>6.43</cell><cell>5.16</cell><cell>4.15</cell><cell>12.83</cell></row><row><cell>D=2 6.76 SIR vocals</cell><cell>drums</cell><cell>bass</cell><cell>other</cell><cell>accomp.</cell></row><row><cell>TAK1 14.37</cell><cell>11.81</cell><cell>10.54</cell><cell>6.41</cell><cell>16.69</cell></row><row><cell>D=2 14.60 SAR vocals</cell><cell>drums</cell><cell>bass</cell><cell>other</cell><cell>accomp.</cell></row><row><cell>TAK1 6.37</cell><cell>6.64</cell><cell>5.69</cell><cell>4.83</cell><cell>14.08</cell></row><row><cell>D=2 6.50 ISR vocals</cell><cell>drums</cell><cell>bass</cell><cell>other</cell><cell>accomp.</cell></row><row><cell>TAK1 11.56</cell><cell>12.02</cell><cell>9.92</cell><cell>9.86</cell><cell>22.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison with the state-of-the-art model (TAK1) [Takahashi et al., 2018] using SDR and other metrics.</figDesc><table><row><cell>Our</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Performance comparison (in SDR) of different block de-</cell></row><row><cell>signs, using dilation 2 and 3 blocks. 'DGRU' represents Dilated</cell></row><row><cell>GRU with dilation 2, 'DGConv' represents the dilated grouped con-</cell></row><row><cell>volution described in Section 2.2.'DGRU-Conv' is the one presented</cell></row><row><cell>in Table 1. 'Conv-DGConv' replaces all GRUs with non-grouped 1D</cell></row><row><cell>convolution with kernel size 1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>SDR of the models with different GRU dilations, all with 3 blocks.</figDesc><table><row><cell cols="5"># blocks vocals drums bass other accomp.</cell></row><row><cell>2</cell><cell>6.49</cell><cell>5.49 4.89</cell><cell>4.32</cell><cell>13.01</cell></row><row><cell>3</cell><cell>6.74</cell><cell>5.71 5.00</cell><cell>4.61</cell><cell>13.25</cell></row><row><cell>4</cell><cell>6.81</cell><cell>5.64 4.84</cell><cell>4.50</cell><cell>13.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>SDR of variants of our model with different number of blocks, all with GRU dilation 2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison (in SDR) as the blocks are added to a trained model. In 'No blocks,' all D2 blocks are removed from the model. Then, the blocks are added one by one.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the GRU version implemented by PyTorch (https: //pytorch.org/docs/stable/nn.html#gru), which is slightly different from the original version.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://sigsep.github.io/datasets/musdb.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/sigsep/sigsep-mus-eval 4 https://github.com/sigsep/sigsep-mus-2018-analysis/blob/ master/sisec-2018-paper-figures/boxplot.py 5 The raw data of the evaluation metrics are available at [Online] https://github.com/sigsep/sigsep-mus-2018</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<editor>Xiaodong Cui, Michael Witbrock, Mark Hasegawa-Johnson, and Thomas S</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aäron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mo-bileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Denoising auto-encoder with recurrent skip connections and residual regression for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang ; Jen-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Machine Learning and Applications</title>
		<meeting>IEEE Int. Conf. Machine Learning and Applications</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="773" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The 2016 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liutkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LVA/ICA</title>
		<meeting>LVA/ICA</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="323" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monaural singing voice separation with skip-filtering connections and recurrent inference of time-frequency mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mimilakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoustics, Speech and Signal Processing</title>
		<editor>Aditya Arie Nugraha, Antoine Liutkus, and Emmanuel Vincent</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1748" to="1752" />
		</imprint>
	</monogr>
	<note>Proc. European Signal Processing Conf.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jouni Paulus and Tuomas Virtanen. Drum transcription with non-negative spectrogram factorisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virtanen</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Signal Processing Conf</title>
		<meeting>European Signal essing Conf</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Len Vande Veire and Tijl De Bie. From raw audio to a seamless mix: creating an automated DJ system for drum and bass</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Rafii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio</title>
		<editor>Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu</editor>
		<meeting><address><addrLine>Alex Graves</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3540" to="3549" />
		</imprint>
		<respStmt>
			<orgName>Vande Veire and De Bie</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proc. Int. Conf. Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koltun ; Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proc. IEEE Conf. Computer Vision and Pattern Recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
