<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRANSFORMER-BASED ACOUSTIC MODELING FOR HYBRID SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Mahadeokar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Fuegen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TRANSFORMER-BASED ACOUSTIC MODELING FOR HYBRID SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-hybrid speech recognition</term>
					<term>acoustic modeling</term>
					<term>transformer</term>
					<term>recurrent neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose and evaluate transformer-based acoustic models (AMs) for hybrid speech recognition. Several modeling choices are discussed in this work, including various positional embedding methods and an iterated loss to enable training deep transformers. We also present a preliminary study of using limited right context in transformer models, which makes it possible for streaming applications. We demonstrate that on the widely used Librispeech benchmark, our transformer-based AM outperforms the best published hybrid result by 19% to 26% relative when the standard n-gram language model (LM) is used. Combined with neural network LM for rescoring, our proposed approach achieves state-of-the-art results on Librispeech. Our findings are also confirmed on a much larger internal dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since the introduction of deep learning in automatic speech recognition (ASR) <ref type="bibr" target="#b0">[1]</ref>, a variety of neural network architectures for acoustic modeling have been explored <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Among them, recurrent neural networks (RNNs), especially long short-term memory (LSTM) <ref type="bibr" target="#b6">[7]</ref> neural networks, are widely used, either in conventional hybrid systems (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>), sequence-to-sequence-based (e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>) or neural-transducer-based end-to-end systems (e.g. <ref type="bibr" target="#b10">[11]</ref>). However, RNNs have several well-known limitations: 1) due to the vanishing or exploding gradient problem discovered in <ref type="bibr" target="#b11">[12]</ref>, RNNs cannot model long term temporal dependencies well; 2) the recurrence nature of RNNs makes it difficult to process speech signal in parallel. To address these issues, a variety of neural network architectures have been proposed to replace RNNs, including time delay neural networks (TDNN) <ref type="bibr" target="#b4">[5]</ref>, feed-forward sequential memory networks (FSMN) <ref type="bibr" target="#b5">[6]</ref>, and convolution neural networks (CNN) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>, while only limited success has been achieved.</p><p>Recently, self-attention network <ref type="bibr" target="#b13">[14]</ref> has demonstrated promising results in a variety of natural language processing tasks (e.g., <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>). Different from RNNs and CNNs, self-attention connects arbitrary pairs of positions in the input sequence directly. To forward (or backward) signals between two positions that are n steps away in the input, it only needs one step to traverse the network, compared with O(n) steps in RNNs and O(log n) in CNNs. Moreover, computation in self-attention can be easily parallelized. On top of self-attention, the transformer model <ref type="bibr" target="#b13">[14]</ref> leverages multi-head attention and interleaves with feed-forward layers. Self-attention and transformer models were also used for ASR, mostly in the sequenceto-sequence architecture <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> with notable exceptions of <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>In this work, we propose and evaluate transformer-based acous-Equal contribution; † Work was done when Andros was an intern at Facebook. tic models (AMs) for hybrid ASR. We explore several modeling choices, including methods to encode either absolute or relative positional information into the input of transformer and an iterated loss to enable training deep transformers. Though our focus in this work is to investigate the potential of transformer-based AMs without any constraint, we do explore streamable transformers and present our initial experimental results. We show that our proposed transformer-based AMs can yield significant word error rate (WER) improvement over very strong bi-directional LSTM (BLSTM) baselines, both on the widely-used Librispeech benchmark and our internal dataset. The results we obtained on Librispeech improve over the previous best hybrid WER by 19% to 26% when the standard 4-gram language model (LM) is used; combined with neural LM rescoring, our system achieves state-of-the-art performance on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Hybrid Architecture</head><p>In hybrid ASR <ref type="bibr" target="#b21">[22]</ref>, an acoustic encoder is used to encode an input sequence x1, · · · , xT to a sequence of high level embedding vectors z1, · · · , zT . These embedding vectors are used to produce a posterior distribution of tied states of hidden Markov model (HMM), such as senone <ref type="bibr" target="#b22">[23]</ref> or chenone <ref type="bibr" target="#b23">[24]</ref>, for each frame. These posterior distributions are then combined with other knowledge sources such as lexicons and LMs to construct a search graph. A decoder is then used to find the best hypothesis. Different neural networks can be used as the encoder: in DNN, TDNN and CNN, zt is a function of xt and its fixed number of neighboring frames; in uni-directional RNNs, zt is a function of x1 to xt, while in bi-directional RNNs, zt is a function of the entire input sequence.</p><p>Though compared with the sequence-to-sequence or neural transducer architecture, the hybrid approach is admittedly less appealing as it is not end-to-end trained, it is still the best performing system for authors' practical problems. It also has the advantage that it can be easily integrated with other knowledge sources (e.g., personalized lexicon) that may not be available during training. In this work, we aim to leverage the transformer to improve hybrid acoustic modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Acoustic Modeling Using Transformer</head><p>In this section, we first briefly review the transformer network and discuss various modeling choices when using the transformer as the acoustic encoder. Relation to other works is also discussed in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-Attention and Multi-Head Attention</head><p>Self-attention first computes the attention distribution over the input sequence using dot-product attention, i.e., for every xt ∈ R d i , a arXiv:1910.09799v2 [cs.CL] 30 Apr 2020 distribution αt is obtained by:</p><formula xml:id="formula_0">αtτ = exp(β · x T t W T q W k xτ ) τ exp(β · x T t W T q W k x τ )<label>(1)</label></formula><p>where Wq, W k ∈ R d k ×d i transforms xt to query and key space,</p><formula xml:id="formula_1">β = 1 √ d i</formula><p>is a scaling factor. Note that for language modeling, the dot-products between the current position and future positions are masked to −∞ to prevent future information leaking to the current embedding. Though for acoustic modeling, it is possible to attend to the entire sequence, in many applications, we only attend to limited right context frames to enable streaming processing of speech signals (i.e., dot-product between t and τ, τ &gt; t + R is masked to −∞). Given αt, the output embedding of self-attention is obtained via:</p><formula xml:id="formula_2">zt = τ Dropout(αtτ ) · Wvxτ<label>(2)</label></formula><p>where Wv ∈ R dv ×d i maps the input vectors to value space. Self-attention is often combined with multi-head attention (MHA), where h self-attention heads are applied individually on the input sequences, and the output of each head are concatenated and linearly transformed to a common space, i.e.,</p><formula xml:id="formula_3">zt = Wo   ... τ Dropout(α (i) tτ ) · W (i) v xτ ...   (3) where Wo ∈ R d i ×hdv , α (i) tτ and W (i)</formula><p>v are the attention weights and the value matrix of the i-th head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture of Transformer</head><p>In addition to the MHA sub-layer, each transformer layer contains a fully-connected feed-forward network (FFN), which is composed by two linear transformations and a nonlinear activation function in between. The FFN network is applied to each position in the sequence separately and identically. To allow stacking many transformer layer together, residual connections are added to the MHA and FFN sublayers. Dropouts are also applied after MHA and linear transformation as a form of regularization. <ref type="figure" target="#fig_0">Figure 1</ref> summarizes the architecture of one transformer layer. Note that different from <ref type="bibr" target="#b13">[14]</ref>, layer normalization <ref type="bibr" target="#b24">[25]</ref> is applied before MHA and FFN and the third layer normalization (LN3 in <ref type="figure" target="#fig_0">Figure 1</ref>) is necessary to prevent bypassing the transformer layer entirely. Note, following <ref type="bibr" target="#b14">[15]</ref>, we use "gelu" non-linearity <ref type="bibr" target="#b25">[26]</ref> in the FFN network.  <ref type="bibr" target="#b24">[25]</ref>; "FC" means fully connected linear transformation; "gelu" means the gelu nonlinear activation <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Positional Embedding</head><p>One obvious limitation of the transformer layer is that the output is invariant to the input order permutation, i.e., for any permutation π applied on the input sequence x1, · · · , xT , the output of the transformer layer can be obtained by applying the same permutation π on z1, · · · , zT . This means that transformer does not model the order of the input sequence. In the original transformer work <ref type="bibr" target="#b13">[14]</ref>, this is solved by injecting information about absolute positions into the input sequence via sinusoid positional embeddings. We argued that different from NLP applications, relative position could be more useful for speech signals. In this work, we compare a few ways to encode positional information into the input of transformer:</p><p>• Sinusoid positional embedding: a sinusoid positional embedding p t is added to xt, where the i-th element of p t is sin((t/10000) i/d i ) for even i and cos((t/10000) (i−1)/d i for odd i. This encodes absolute positional information; • Frame stacking: a simple way to break the permutation invariance is to stack n contextual vectors together, i.e., xt = (</p><formula xml:id="formula_4">x T t , x T t+1 , · · · , x T t+n−1 ) T .</formula><p>This encodes the relative positional information;</p><p>• Convolutional embedding: inspired by <ref type="bibr" target="#b26">[27]</ref>, we use 2D convolutional layers to implicitly encode the relative positional information. Convolutional embedding implicitly performs frame stacking as well as learns useful short-range spectraltemporal patterns <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Deep Transformers</head><p>Transformer layers can be stacked many times to form a very deep network. In our initial experiments, we found better accuracies can be obtained by deeper networks. However, after stacking many layers, it becomes difficult to train and often gets stuck in a bad local optimum. To enable training deep transformer, we used iterated loss <ref type="bibr" target="#b28">[29]</ref>, in which output of some intermediate transformer layers is also used to calculate auxiliary cross entropy losses. These auxiliary losses are interpolated to make the final loss function. Note that intermediate-layer-specific parameters (e.g., the linear transformation before the softmax operation) are discarded after training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Relation to Other Works</head><p>The original transformer paper <ref type="bibr" target="#b13">[14]</ref> proposed to use self-attention and cross-attention to replace the recurrence in encoder and decoder in a sequence-to-sequence model. Since we focus on hybrid speech recognition, we only use self-attention to replace the RNNs in the acoustic encoder in this work. Self-attention based acoustic modeling has been explored in the past. In <ref type="bibr" target="#b19">[20]</ref>, self-attention is modified to attend to a fixed number of left and right context frames, and only one attention layer was used. By comparison, in our work attention heads attend to all the past frames, and we use both self-attention and FFN networks with a very deep structure, which is critical to achieve a good model accuracy.</p><p>In <ref type="bibr" target="#b29">[30]</ref>, transformers are compared with RNNs in the sequence-tosequence architecture. In <ref type="bibr" target="#b17">[18]</ref>, various positional embedding methods were investigated for a sequence-to-sequence model, where it is found that replacing the FFN network with a LSTM layer to make the self-attention layer position aware yielded better performance. Following <ref type="bibr" target="#b26">[27]</ref>, we use convolution layers as pre-processors for the transformer layer's input and compare it with other positional encoding methods in Section 4.2. In <ref type="bibr" target="#b30">[31]</ref>, a loss function similar to the iterated loss is used to enable training very deep transformers for character-level LMs; we demonstrate that it is also crucial for training deep transformer-based AMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate the effectiveness of the proposed transformer-based acoustic model, we first perform experiments on the Librispeech corpus <ref type="bibr" target="#b31">[32]</ref>. This corpus contains about 960 hours of read speech data for training, and 4 development and test sets ({dev, test} -{clean,other}), where other sets are more acoustic challenging. No segmentation is performed for these test sets. The standard 4-gram language model (LM) with a 200K vocabulary is used for all first-pass decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setups</head><p>Following <ref type="bibr" target="#b23">[24]</ref>, we use context-and position-dependent graphemes (i.e., chenones) in all experiments. We bootstrap our HMM-GMM system using the standard Kaldi <ref type="bibr" target="#b32">[33]</ref> Librispeech recipe. We use 1-state HMM topology with fixed self-loop and forward transition probability (both 0.5). 80-dimensional log Mel-filter bank features are extracted with a 10ms frame shift. A reduced 20ms frame rate is achieved either by stacking-and-striding 2 consecutive frames or by a stride-2 pooling in the convolution layer if it is used. We found that this not only reduces the computation but also slightly improves the recognition accuracy. Speed perturbation <ref type="bibr" target="#b33">[34]</ref> and SpecAugment <ref type="bibr" target="#b9">[10]</ref> (LD policy without time warping) are used. We focus on cross-entropy (CE) trained models and only selectively perform sMBR <ref type="bibr" target="#b34">[35]</ref> training on top of the best CE setup.</p><p>Neural network training is performed using an in-house developed speech extension of the PyTorch-based fairseq <ref type="bibr" target="#b35">[36]</ref> toolkit. Adam optimizer <ref type="bibr" target="#b36">[37]</ref> is used in all experiments; the learning rate linearly warms up from 1e-5 to 1e-3 in the first 8000 iterations and stays at 1e-3 during the rest of training. We mainly compare full-context transformer with BLSTM in this work though we do have an initial investigation of transformers using limited right context. Dropout is used in all experiments: 0.1 for transformer and 0.2 for BLSTM. To improve training throughput, our batch size is dynamically determined so that we can occupy as much GPU memory as possible. For most of the experiments in this work, a batch contains around 10,000 to 20,000 frames, including padding frames. We train models using 32 Nvidia P100 GPUs for at most 100 epochs; training is usually done within 4 days. We did not perform thorough architecture searches for either transformer or BLSTM. For transformers, we mainly use a 12-layer transformer architecture with di = 768: perhead dimension is always 64 and the FFN dimension is always set to 4di. This model has about 90M parameters. For BLSTMs, we follow <ref type="bibr" target="#b23">[24]</ref> and consider two architectures, a 5-layer BLSTM with 800 units per layer per direction (about 94M parameters), and a 6-layer BLSTM with 1000 units (about 163M parameters) <ref type="bibr" target="#b0">1</ref> .</p><p>Training transformers requires some tricks. Due to the quadratically growing computation cost with respect to the input sequence length, we segment the training utterances into segments that are not longer than 10 seconds 2 . Though this creates a mismatch between training and testing, preliminary results show that training on shorter segments not only increases the training throughput but also helps the final WERs. We also found that transformers are more prone to over-fitting, thus require some regularization. We found SpecAugment <ref type="bibr" target="#b9">[10]</ref> is effective: without it, WER starts to increase after only 3 epochs, while WER continues to improve during training with SpecAugment.</p><p>A fully-optimized, static 4-gram decoding graph is built using Kaldi. This decoding graph is used for first-pass decoding and n-best generation for neural LM rescoring. Test set WERs are obtained using the best model based on WER on the development set 3 . Follow- <ref type="bibr" target="#b0">1</ref> We did not obtain further WER improvements by increasing number of parameters in BLSTM beyond 163M. <ref type="bibr" target="#b1">2</ref> This is achieved by aligning audio against the reference using an existing latency-controlled BLSTM acoustic model. <ref type="bibr" target="#b2">3</ref> We also average the last 10 epoch checkpoints to form an extra candidate.</p><p>ing <ref type="bibr" target="#b37">[38]</ref>, the best checkpoints for test-clean and test-other are selected separately on the corresponding development sets 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effect of Positional Embedding</head><p>In the first set experiment, we investigate the effect of four positional embeddings (PE) methods for transformer-based acoustic models.</p><p>In the first method, we stack-and-stride every 2 frames: it does not break the permutation invariance in transformers, thus denoted as None. In the second method, the Sinusoid PE proposed in the original transformer paper <ref type="bibr" target="#b13">[14]</ref>, which encodes the absolute positional information, is used. In the third method, Frame Stacking, we stack the current frame and next 8 future frames followed by a stride-2 sampling to form a new input sequence to transformers. Note that since the stacked frames are partially overlapped with its neighboring stacked frames, the permutation invariance no longer holds. This method encodes relative positional information. In the fourth method, Convolution, we use two VGG blocks <ref type="bibr" target="#b38">[39]</ref> beneath transformer layers: each VGG block contains 2 consecutive convolution layers with a 3-by-3 kernel followed by a ReLu non-linearity and a pooling layer; 32 channels are used in the convolution layer of the first VGG block and increase to 64 for the second block. Maxpooling is performed at a 2-by-2 grid, with stride 2 in the first block and 1 in the second block. For an input sequence of 80-dim feature vector at a 10ms rate, this VGG network produces a 2560-dim feature vector sequence at a 20ms rate. Note that the perception field of each feature vector output by the VGG network consists of 80ms left-context and 80ms right context, the same right context length as Frame Stacking. A linear projection is used to project the feature vector to the dimension accepted by transformers, in this case, 768. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transformer vs. BLSTM</head><p>In the second set of experiments, we compare the transformer architecture with BLSTM. For a fair comparison, we try to build transformer and BLSTM-based models using similar number of parameters. First we compare a BLSTM model, BLSTM(800, 5), i.e., 5 layers with 800 hidden units per layer per direction, with the transformer model in row 3, <ref type="table" target="#tab_0">Table 1</ref>, dubbed Trf-FS since it uses Frame Stacking. To be able to compare our best performing transformer-based model with Convolution PE, we combine the same VGG blocks in row 4, <ref type="table" target="#tab_0">Table 1</ref> with BLSTM, producing vg-gBLSTM(800, 5). Lastly, with about 163M parameters, we build the largest vggBLSTM model, vggBLSTM(1000,6). To match the number of parameters of this model, we increase the number of transformer layers from 12 to 20. As shown in <ref type="table" target="#tab_1">Table 2</ref>, transformerbased models consistently outperform BLSTM-based models by 2-4% on test-clean and 7-11% on test-other. <ref type="table" target="#tab_1">Table 2</ref> shows that simply increasing the depth of transformers to 20 layers, we obtained about 5.5% relative WER reduction (6.10 vs. 6.46). Inspired by this, we try to increase the number of transformer  On top of this vggTrf(512, 24) model, we further perform sMBR training and it slightly improves to 2.60% and 5.59% on test-clean and test-other. We compare our results with some published state-of-the-art systems on Librispeech in <ref type="table" target="#tab_3">Table  4</ref>: when the standard 4-gram LM is used in decoding, our system achieves 19% and 26% WER reduction on test-clean and test-other respectively, over previous best 4-gram only hybrid system <ref type="bibr" target="#b23">[24]</ref>  <ref type="bibr" target="#b5">6</ref> . We also built a transformer LM similar to the setup in <ref type="bibr" target="#b15">[16]</ref> on the 800M text tokens provided by the Librispeech benchmark and performed n-best rescoring on the first pass decoding output. To the best of our knowledge, our final WERs (2.26/4.85) are state-of-the-art results on this widely used benchmark.   <ref type="table" target="#tab_1">Table 2</ref>) and force every layer to attend to a fixed limited right context during inference. Interestingly, though this creates a large mismatch between training and inference, the resultant systems can still yield reasonable WERs if the number of right context frames is large enough. Note that though each layer only requires limited right context frames, the overall right context length is added up by the right context length of every transformer layer, therefore we still end up with a large look-ahead window into the future, which makes it less possible to be used in a streaming ASR application. We will investigate transformer-based acoustic models with the streaming constraint in our future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Iterated Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Large Scale Experiments</head><p>Finally, we perform a large scale experiment on one of our internal tasks, English video ASR. The training set consists of 13.7K hours of videos (from 941.6K video clips) shared publicly by users; only the audio part of those videos are used in our experiments. These data are completely anonymized; both transcribers and researchers do not have access to any user-identifiable information. Due to the data nature, it is a very diverse and challenging task. About 9 hours (from 620 video clips) data are held out for dev set. 3 test sets are used for evaluation purpose: an 8.5-hour curated set of carefully select very clean videos, an 19-hour clean set and a 18.6-hour noisy set. For our initial evaluation purpose, both training and test sets are segmented into maximum 10 second segments.</p><p>Due to time limit, we only built vggTrf(768, 12) without the iterated loss and vggBLSTM(800, 5) on this task. <ref type="table" target="#tab_5">Table 6</ref> shows that on this task, the proposed transformer-based acoustic model outperform vggBLSTM by 4.0-7.6%. We will report more results in our future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions And Conclusions</head><p>In this work, we proposed and evaluated transformer-based acoustic models for hybrid speech recognition. A couple of model modeling choices are discussed and compared. We demonstrated that transformer can significantly outperforms BLSTM and give the best acoustic models on Librispeech benchmark. Initial study on a much larger and more challenging dataset also confirms our findings.</p><p>There are many works we are yet to explore. For example, our experiments did not show to what extent transformer's superior performance comes from replacing recurrence with self-attention, while other modeling techniques from transformer can be borrowed to improve RNNs as well <ref type="bibr" target="#b41">[42]</ref>. The quadratically growing cost with respect to the length of speech signals is still a major blocker for transformer-based acoustic models to be used in practice. These questions will be studied in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Architecture of one transformer layer. "LN" means layer normalization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effect of Positional Embeddings (PE) for Transformer.</figDesc><table><row><cell>PE</cell><cell cols="2">test-clean test-other</cell></row><row><cell>None</cell><cell>3.11</cell><cell>6.94</cell></row><row><cell>Sinusoid</cell><cell>3.13</cell><cell>6.67</cell></row><row><cell>Frame Stacking</cell><cell>3.04</cell><cell>6.64</cell></row><row><cell>Convolution</cell><cell>2.87</cell><cell>6.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Architecture comparison on the Librispeech benchmark</figDesc><table><row><cell>Model Arch</cell><cell cols="3">#Params (M) test-clean test-other</cell></row><row><cell>BLSTM (800,5)</cell><cell>79</cell><cell>3.11</cell><cell>7.44</cell></row><row><cell>Trf-FS (768,12)</cell><cell>91</cell><cell>3.04</cell><cell>6.64</cell></row><row><cell>vggBLSTM (800,5)</cell><cell>95</cell><cell>2.99</cell><cell>6.95</cell></row><row><cell>vggTrf. (768,12)</cell><cell>93</cell><cell>2.87</cell><cell>6.46</cell></row><row><cell>vggBLSTM (1000,6)</cell><cell>163</cell><cell>2.86</cell><cell>6.63</cell></row><row><cell>vggTrf. (768, 20)</cell><cell>149</cell><cell>2.77</cell><cell>6.10</cell></row><row><cell cols="4">layers further. To make the model size manageable, we use a smaller</cell></row><row><cell cols="4">embedding dimension, 512, for deep transformer models. Our initial</cell></row><row><cell cols="4">attempt was not successful; deep transformer models (deeper than</cell></row><row><cell cols="4">20 layers) often got stuck in training and made little progress for a</cell></row><row><cell cols="4">long time. We solved the problem with the iterated loss used in [29]:</cell></row><row><cell cols="4">the output embeddings of the 6/12/18-th transformer layers are non-</cell></row><row><cell cols="4">linearly transformed (projected to a 256-dimensional space with a</cell></row><row><cell cols="4">linear transformation followed by a Relu non-linearity) and auxiliary</cell></row><row><cell cols="4">CE losses are calculated separately. These additional CE losses are</cell></row><row><cell cols="4">interpolated with the original CE loss with a 0.3 weight. With this</cell></row><row><cell cols="4">iterated loss, we were able to train a 24-layer transformer model with</cell></row><row><cell cols="4">only 81M model parameters in decoding 5 and obtain a 7% and 13%</cell></row><row><cell cols="4">WER reduction on test-clean and test-other, respectively,</cell></row><row><cell cols="2">over the vggTrf(768, 12) baseline.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Using iterated loss to train deep transformer models.</figDesc><table><row><cell>Model Arch</cell><cell cols="3">Iter Loss test-clean test-other</cell></row><row><cell>vggTrf. (768, 12)</cell><cell>N</cell><cell>2.87</cell><cell>6.46</cell></row><row><cell>(Params: 93M)</cell><cell>Y</cell><cell>2.77</cell><cell>6.10</cell></row><row><cell>vggTrf. (512, 24)</cell><cell>N</cell><cell cols="2">not converged</cell></row><row><cell>(Params: 81M)</cell><cell>Y</cell><cell>2.66</cell><cell>5.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison with previous best results on Librispeech. "4g" means the stand 4-gram LM is used; "NNLM" means a neural LM is used.</figDesc><table><row><cell>Arch.</cell><cell>System</cell><cell>LM</cell><cell>test-clean</cell><cell>test-other</cell></row><row><cell>LAS</cell><cell>Park et al. [10] Karita et al. [30]</cell><cell>NNLM + 4g NNLM</cell><cell>2.5 2.6</cell><cell>5.8 5.7</cell></row><row><cell></cell><cell>RWTH [38]</cell><cell>4g +NNLM</cell><cell>3.8 2.3</cell><cell>8.8 5.0</cell></row><row><cell>Hybrid</cell><cell>Han et al. [41]</cell><cell>4g +NNLM</cell><cell>2.9 2.2</cell><cell>8.3 5.8</cell></row><row><cell></cell><cell>Le et al. [24]</cell><cell>4g</cell><cell>3.2</cell><cell>7.6</cell></row><row><cell></cell><cell>Ours</cell><cell>4g +NNLM</cell><cell>2.60 2.26</cell><cell>5.59 4.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Forcing transformer models to use limited right context (RC) per layer during inference. Given a 12-layer transformer, an RC of 10 frames translates to 2.48 seconds of total lookahead. Limited Right Context All the transformer-based experiments so far used full context. To understand to what extent the transformer relies on future frames to derive embeddings for the current frames, we take the vggTrf(768, 12) model (row 4,</figDesc><table><row><cell>RC</cell><cell cols="2">test-clean test-other</cell></row><row><cell>∞</cell><cell>2.87</cell><cell>6.45</cell></row><row><cell>50</cell><cell>3.01</cell><cell>7.12</cell></row><row><cell>20</cell><cell>3.29</cell><cell>8.10</cell></row><row><cell>10</cell><cell>3.65</cell><cell>9.01</cell></row><row><cell>4.5</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Experiment results on our internal English video ASR task.</figDesc><table><row><cell>Model</cell><cell cols="2">curated clean noisy</cell></row><row><cell>vggBLSTM(800,5)</cell><cell>10.72</cell><cell>15.97 22.13</cell></row><row><cell>vggTrf(768,12)</cell><cell>9.90</cell><cell>15.26 21.25</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This is only to follow the same experimental protocol set by the prior work in<ref type="bibr" target="#b37">[38]</ref> -most of the experimental results on both test sets, including the best WERs we reported inTable 4, are actually achieved by the same model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">There are 6M extra parameters only used in training.<ref type="bibr" target="#b5">6</ref> Note that<ref type="bibr" target="#b23">[24]</ref> used LC-BLSTM<ref type="bibr" target="#b39">[40]</ref> instead of full-context BLSTM.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Conversational speech transcription using context-dependent deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on audio</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1533" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>and language processing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Feedforward sequential memory neural networks without recurrent feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02693</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end attentionbased large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Streaming end-to-end speech recognition for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Wav2letter: an endto-end convnet-based speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Self-attentional acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09519</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Syllable-based sequence-tosequence speech recognition with the transformer in mandarin Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10752</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A time-restricted self-attention layer for asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hossein Hadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahremani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5874" to="5878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-attention networks for connectionist temporal classification in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7115" to="7119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<title level="m">Connectionist speech recognition: a hybrid approach</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">247</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Subphonetic modeling with markov states-senone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 1992</title>
		<meeting>ICASSP, 1992</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">From senones to chenones: Tied context-dependent graphemes for hybrid speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01493</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11660</idno>
		<title level="m">Transformers with convolutional context for asr</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4845" to="4849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deja-vu: Double feature presentation and iterated loss in deep transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>To appear ICASSP, 2020</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A Comparative Study on Transformer vs RNN in Speech Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06317</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3159" to="3166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequencediscriminative training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veselỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="2345" to="2349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">fairseq: A Fast, Extensible Toolkit for Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Myle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Angela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">RWTH ASR Systems for Lib-riSpeech: Hybrid vs Attention-w/o Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03072</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Highway long short-term memory RNNs for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
		<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5755" to="5759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">State-of-the-art speech recognition using multi-stream self-attention with dilated 1d convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00716</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09849</idno>
		<title level="m">The best of both worlds: Combining recent advances in neural machine translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
