<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
							<email>zhaojian90@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National University of Defense Technology 3 Panasonic R&amp;D Center Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Cheng</surname></persName>
							<email>yi.cheng@sg.panasonic.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<email>yangyang@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochong</forename><surname>Lan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
							<email>yan.xu@sg.panasonic.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
							<email>jianshu@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugiri</forename><surname>Pranata</surname></persName>
							<email>sugiri.pranata@sg.panasonic.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengmei</forename><surname>Shen</surname></persName>
							<email>shengmei.shen@sg.panasonic.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
							<email>jlxing@nlpr.ia.ac.cn</email>
							<affiliation key="aff4">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>7 Qihoo 360</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">AI Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Liu</surname></persName>
							<email>hengzhuliu@nudt.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Defense Technology 3 Panasonic R&amp;D Center Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages still remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identitypreserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, we develop effective and novel training strategies for end-to-end learning the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR and several other cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-thearts. Benchmarking our model on one of the most popular unconstrained face recognition datasets IJB-C additionally verifies the promising generalizability of AIM in recogniz- *</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 National University of Singapore, 2 National University of Defense Technology <ref type="bibr" target="#b2">3</ref> Panasonic R&amp;D Center Singapore, <ref type="bibr" target="#b3">4</ref> Nanyang Technological University <ref type="bibr" target="#b4">5</ref> Inception Institute of Artificial Intelligence, <ref type="bibr" target="#b5">6</ref> Institute of Automation, Chinese Academy of Sciences, <ref type="bibr" target="#b6">7</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages still remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identitypreserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, we develop effective and novel training strategies for end-to-end learning the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR and several other cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-thearts. Benchmarking our model on one of the most popular unconstrained face recognition datasets IJB-C additionally verifies the promising generalizability of AIM in recogniz-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition is one of the most widely studied topics in computer vision and artificial intelligence fields. Recently, some approaches claim to have achieved <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b55">56]</ref> or even surpassed <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55]</ref> human performance on several benchmarks.</p><p>Despite the exciting progress, age variations still form a major bottleneck for many practical applications. For ex-ample, in law enforcement scenarios, finding missing children after years, identifying wanted fugitives based on mug shots and verifying passports usually involve recognizing faces across ages and/or synthesizing photorealistic age regressed/progressed 1 face images. These are extremely challenging due to several reasons: 1) Human face rejuvenation/aging is a complex process whose patterns differ from one individual to another. Both intrinsic factors (like heredity, gender and ethnicity) and extrinsic factors (like environment and living styles) affect the aging process and lead to significant intra-class variations. 2) Facial shapes and textures dramatically change over time, making learning age-invariant patterns difficult. 3) Current learning based cross-age face recognition models are limited by existing cross-age databases <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54]</ref> due to their small size, narrow elapse per subject and unbalanced genders, ethnicities and age span. As such, the performance of most face recognition models degrades by over 13% from general recognition on faces of (almost) the same age to crossage face recognition <ref type="bibr" target="#b5">[6]</ref>. In this work, we aim to improve automatic models for recognizing unconstrained faces with large age variations.</p><p>According to recent studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b47">48]</ref>, face images of different individuals usually share common aging characteristics (e.g., wrinkles), and face images of the same individual contain intrinsic features that are relatively stable across ages. Facial representations of a person in the latent space can hence be decomposed into an age-specific component which reflects the aging effect and an identity-specific component which preserves intrinsic identity information. The latter would be invariant to age variations and ideal for cross-age face recognition when achievable. This finding inspires us to develop a novel and unified deep neural network, termed as Age Invariant Model (AIM). The AIM jointly learns disentangled identity representations that are invariant to age, and photorealistic cross-age face image synthesis that can highlight important latent representations among the disentangled ones end-to-end. Thus they mutually boost each other to achieve age-invariant face recognition. AIM takes as input face images of arbitrary ages with other potential distracting factors like various illumination, expressions, poses and occlusion. It outputs facial representations invariant to age variations and meanwhile preserves discriminativeness across different identities. As shown in <ref type="figure">Fig. 1</ref>, the AIM can learn age-invariant representations and effectively synthesize natural age regressed/progressed faces.</p><p>In particular, AIM extends from an auto-encoder based Generative Adversarial Network (GAN) and includes a disentangled Representation Learning sub-Net (RLN) and a Face Synthesis sub-Net (FSN) for age-invariant face recognition. RLN consists of an encoder and a discriminator that compete with each other to learn discriminative and ageinvariant representations. It introduces cross-age domain adversarial training to promote encoded features that are indistinguishable w.r.t. the shift between multi-age domains, and cross-entropy regularization with a label smoothing strategy to constrain cross-age representations with ambiguous separability. The discriminator incorporates dual agents to encourage the representations to be uniformly distributed to smooth the age transformation while preserving identity information. The representations are then concatenated with a continuous age condition code to synthesize age regressed/progressed face images, such that the learned representations are explicitly disentangled from age variations. FSN consists of a decoder and a local-patch based discriminator that compete with each other to synthesize photorealistic cross-age face images. FSN uses an attention mechanism to guarantee robustness to large background complexity and illumination variance. The discriminator incorporates dual agents to add realism to synthesized cross-age faces while forcing the generated faces to exhibit desirable rejuvenation/aging effects.</p><p>Moreover, we propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and future research on age-invariant face recognition. CAFR contains 1,446,500 face images from 25,000 subjects annotated with age, identity, gender, race and landmark labels. Extensive experiments on both our CAFR and other standard cross-age datasets (MORPH <ref type="bibr" target="#b33">[34]</ref>, CACD <ref type="bibr" target="#b5">[6]</ref> and FG-NET <ref type="bibr" target="#b0">[1]</ref>) demonstrate the superiority of AIM over the state-of-the-arts. Benchmarking AIM on one of the most popular unconstrained face recognition datasets IJB-C <ref type="bibr" target="#b26">[27]</ref> additionally verifies its promising generalizability in recognizing faces in the wild. Our code and trained models are available at https:// github.com/ZhaoJ9014/High_Performance_ Face_Recognition/tree/master/src/Look% 20Across%20Elapse-%20Disentangled% 20Representation%20Learning%20and% 20Photorealistic%20Cross-Age%20Face% 20Synthesis%20for%20Age-Invariant% 20Face%20Recognition.TensorFlow.</p><p>Our dataset and online demo will be released soon.</p><p>Our contributions are summarized as follows.</p><p>• We propose a novel deep architecture unifying crossage face synthesis and recognition in a mutual boosting way.</p><p>• We develop effective end-to-end training strategies for the whole deep architecture to generate powerful ageinvariant facial representations explicitly disentangled from the age variations.</p><p>• The proposed model achieves continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding the requirement of paired data and true age of testing samples.</p><p>• We propose a new large-scale benchmark dataset CAFR to advance the frontiers of age-invariant face recognition research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Age-Invariant Representation Learning</head><p>Conventional approaches often leverage robust local descriptors <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref> and metric learning <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7]</ref> to tackle age variance. For instance, <ref type="bibr" target="#b30">[31]</ref> develop a Bayesian classifier to recognize age difference and perform face verification across age progression. <ref type="bibr" target="#b12">[13]</ref> propose Hidden Factor Analysis (HFA) for age-invariant face recognition that separates aging variations from identity-specific features. <ref type="bibr" target="#b46">[47]</ref> improve the performance by distance metric learning. <ref type="bibr" target="#b24">[25]</ref> propose Gradient Orientation Pyramid (GOP) for cross-age face verification. In contrast, deep learning models often handle age variance through using a single age-agnostic or several age-specific models with pooling and specific loss functions <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b44">45]</ref>. For instance, <ref type="bibr" target="#b8">[9]</ref> propose an enforced softmax optimization strategy to learn effective and compact deep facial representations with reduced intra-class variance and enlarged inter-class distance. <ref type="bibr" target="#b47">[48]</ref> propose a Latent Factor guided Convolutional Neural Network (LF-CNN) model to learn age-invariant deep features. <ref type="bibr" target="#b56">[57]</ref> propose an Age Estimation guided CNN (AE-CNN) model to separate aging variations from identity-specific features. <ref type="bibr" target="#b44">[45]</ref> propose an Orthogonal Embedding CNN (OE-CNN) model to decompose deep facial representations into two orthogonal components to represent age-and identity-specific features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cross-Age Face Synthesis</head><p>Previous methods can be roughly divided into physical modeling based and prototype based. The former approaches model the biological patterns and physical mechanisms of aging, including muscles <ref type="bibr" target="#b40">[41]</ref>, wrinkles <ref type="bibr" target="#b32">[33]</ref>, and facial structure <ref type="bibr" target="#b31">[32]</ref>. However, they usually require massive annotated cross-age face data with long elapse per subject which are hard to collect, and they are computationally expensive. Prototype-based approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref> often divide faces into groups by ages and select the average face of each group as the prototype. The differences in prototypes between two age groups are then considered as the aging pattern. However, the aged face generated from the averaged prototype may lose personality information. Most of subsequent approaches <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b52">53]</ref> are data-driven and do not rely much on the biological prior knowledge, and the aging patterns are learned from train-ing data. Though improve the results, these methods suffer ghosting artifacts on the synthesized faces. More recently, deep generative networks are exploited. For instance, <ref type="bibr" target="#b43">[44]</ref> propose a smooth face aging process between neighboring groups by modeling the intermediate transition states with Recurrent Neural Network (RNN). <ref type="bibr" target="#b53">[54]</ref> propose a Conditional Adversarial Auto-Encoder (CAAE) and achieve face age regression/progression in a holistic framework. <ref type="bibr" target="#b57">[58]</ref> propose a Conditional Multi-Adversarial Auto-Encoder with Ordinal Regression (CMAAE-OR) to predict facial rejuvenation and aging. <ref type="bibr" target="#b38">[39]</ref> propose a Dual conditional GANs (Dual cGANs) where the primal cGAN transforms a face image to other ages based on the age condition, while the dual one learns to invert the task.</p><p>Our model differs from them in following aspects: 1) AIM jointly performs cross-age face synthesis and recognition end-to-end to allow them to mutually boost each other for addressing large age variance in unconstrained face recognition. 2) AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identitypreserving properties, and do not require paired data and true age of testing samples. 3) AIM generates powerful ageinvariant face representations explicitly disentangled from age variations through cross-age domain adversarial training and cross-entropy regularization with a label smoothing strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Age-Invariant Model</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the proposed Age-Invariant Model (AIM) extends from an auto-encoder based GAN, and consists of a disentangled Representation Learning sub-Net (RLN) and a Face Synthesis sub-Net (FSN) that jointly learn discriminative and robust facial representations disentangled from age variance and perform attention-based face rejuvenation/aging end-to-end. We now detail each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Disentangled Representation Learning</head><p>Matching face images across ages is demanded in many real-world applications. It is mainly challenged by variations of an individual at different ages (i.e. large intraclass variations) or caused by aging (e.g. facial shape and texture changes), and inevitable entanglement of unrelated (statistically independent) components in the deep features extracted from a general-purpose face recognition model. Large intra-class variations usually result in erroneous cross-age face recognition and entangled facial representations potentially weaken the model's robustness in recognizing faces with age variations. We propose a GAN-like Representation Learning sub-Net (RLN) to learn discriminative and robust identity-specific facial representations disentangled from age variance, as illustrated in  RLN consists of an encoder (G θ E ) and a discriminator (D φ 1 ) that compete with each other to learn discriminative and robust facial representations (f ) disentangled from age variance. It is augmented by cross-age domain adversarial training (L cad ) and crossentropy regularization with a label smoothing strategy (Lcer). FSN consists of a decoder (G θ D ) and a local-patch based discriminator (D φ 2 ) that compete with each other to achieve continuous face rejuvenation/aging (x) with remarkable photorealistic and identitypreserving properties. It introduces an attention mechanism to guarantee robustness to large background complexity and illumination variance. Note AIM does not require paired training data nor true age of testing samples. Best viewed in color.</p><p>In particular, RLN takes the encoder G θ E (with learnable parameters θ E ) as the generator : R H×W ×C → R C for facial representation learning, where H, W , C and C denote the input image height, width, channel number and the dimensionality of the encoded feature f , respectively. f preserves the high-level identity-specific information of the input face image through several carefully designed regularizations. We further concatenate f with a continuous age condition code to synthesize age regressed/progressed face images, such that the learned representations are explicitly disentangled from age variations.</p><p>Formally, denote the input RGB face image as x and the learned facial representation as f . Then</p><formula xml:id="formula_0">f := G θ E (x).<label>(1)</label></formula><p>The key requirements for G θ E include three aspects. 1) The learned representation f should be invariant to age variations and also well preserve the identity-specific component. 2) It should be barely possible for an algorithm to identify the domain of origin of the observation x regardless of the underlying gap between multi-age domains. 3) f should obey uniform distribution to smooth the age transformation.</p><p>To this end, we propose to learn θ E by minimizing the following composite losses:</p><formula xml:id="formula_1">L G θ E = −λ 1 L cad + λ 2 L cer − λ 3 L adv1 + λ 4 L ip − λ 5 L adv2 + λ 6 L ae + λ 7 L mc + λ 8 L tv + λ 9 L att ,<label>(2)</label></formula><p>where L cad is the cross-age domain adversarial loss for facilitating age-invariant representation learning via domain adaption, L cer is the cross-entropy regularization loss for constraining cross-age representations with ambiguous separability, L adv1 is the adversarial loss for imposing the uniform distribution on f , L ip is the identity preserving loss for preserving identity information, L adv2 is the adversarial loss for adding realism to the synthesized images and alleviating artifacts, L ae is the age estimation loss for forcing the synthesized faces to exhibit desirable rejuvenation/aging effect, L mc is the manifold consistency loss for encouraging input-output space manifold consistency, L tv is the total variation loss for reducing spiky artifacts, L att is the attention loss for facilitating robustness enhancement via an attention mechanism, and {λ k } k= <ref type="bibr" target="#b8">9</ref> 1 are weighting parameters among different losses.</p><p>In order to enhance the age-invariant representation learning capacity, we adopt L cad to promote emergence of features encoded by G θ E that are indistinguishable w.r.t. the shift between multi-age domains, which is defined as</p><formula xml:id="formula_2">L cad = 1 N i −yilog[Cϕ(fi)] − (1 − yi)log[1 − Cϕ(fi)],<label>(3)</label></formula><p>where ϕ denotes the learnable parameters for the domain classifier, and y i ∈ {0, 1, . . . } indicates which domain f i is from. Minimizing L cad can reduce the domain discrepancy and help the generator achieve similar facial representations across different age domains, even if training samples from a domain are limited. Such adapted representations are provided by augmenting the encoder of G θ E with a few standard layers as the domain classifier C ϕ , and a new gradient reversal layer to reverse the gradient during optimizing the encoder (i.e., gradient reverse operator as in <ref type="figure" target="#fig_0">Fig. 2</ref>), as inspired by <ref type="bibr" target="#b11">[12]</ref>. If using L cad alone, the results tend to be sub-optimal, because searching for a local minimum of L cad may go through a path that resides outside the manifold of desired cross-age representations with ambiguous separability. Thus, we combine L cad with L cer to ensure the search resides in that manifold and produces age-invariant facial representations, where L cer is defined as</p><formula xml:id="formula_3">Lcer = 1 N i −ȳilog[R ψ (fi)] − (1 −ȳi)log[1 − R ψ (fi)],<label>(4)</label></formula><p>where ψ denotes the learnable parameters for the regularizer, andȳ i ∈ { 1 n , 1 n , . . . } denotes the smoothed domain indicator.</p><p>L adv1 is introduced to impose a prior distribution (e.g., uniform distribution) on f to evenly populate the latent space with no apparent "holes", such that smooth age transformation can be achieved:</p><formula xml:id="formula_4">L adv 1 = 1 N i −yilog[D φ 1 (fi)] − (1 − yi)log[1 − D φ 1 (f * i )],<label>(5)</label></formula><p>where φ 1 denotes the learnable parameters for the discriminator, f * i ∼ U (f ) denotes a random sample from uniform distribution U (f ), and y i denotes the binary distribution indicator.</p><p>To facilitate this process, we leverage a Multi-Layer Perceptron (MLP) as the discriminator D φ1 , which is very simple to avoid typical GAN tricks. We further augment D φ1 with an auxiliary agent L ip to preserve identity information: <ref type="bibr" target="#b5">(6)</ref> where y i denotes the identity ground truth.</p><formula xml:id="formula_5">Lip = 1 N i −yilog[D φ 1 (fi)] − (1 − yi)log[1 − D φ 1 (fi)],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention-based Face Rejuvenation/Aging</head><p>Photorealistic cross-age face images are important for face recognition with large age variance. A natural scheme is to generate reference age regressed/progressed faces from face images of arbitrary ages to match target age before feature extraction or serve as augmented data for learning discriminative models. We then propose a GAN-like Face Synthesis sub-Net (FSN) to learn a synthesis function that can achieve both face rejuvenation and aging in a holistic, end-to-end manner, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>In particular, FSN leverages the decoder G θ D (with learnable parameters θ D ) as the generator: R C +C → R H×W ×C for cross-age face synthesis, where C denotes the dimensionality of the age condition code concatenated with f . The synthesized results present natural effects of rejuvenation/aging with robustness to large background complexity and bad lighting conditions through the carefully designed learning schema.</p><p>Formally, denote the age condition code as c and the synthesized face image asx. Then</p><formula xml:id="formula_6">x := G θ D (f, c).<label>(7)</label></formula><p>The key requirements for G θ D include two aspects. 1) The synthesized face imagex should visually resemble a real one and preserve the desired rejuvenation/aging effect. 2) Attention should be paid to the most salient regions of the image that are responsible for synthesizing the novel aging phase while keeping the rest elements such as glasses, hats, jewelery and background untouched.</p><p>To this end, we propose to learn θ D by minimizing the following composite losses:</p><formula xml:id="formula_7">LG θ D = −λ10L adv 2 + λ11Lae + λ12Lmc + λ13Ltv + λ14Latt,<label>(8)</label></formula><p>where {λ k } k= <ref type="bibr">14 10</ref> are weighting parameters among different losses.</p><p>L adv2 is introduced to push the synthesized image to reside in the manifold of photorealistic age regressed/progressed face images, prevent blur effect, and produce visually pleasing results:</p><formula xml:id="formula_8">L adv 2 = 1 N i −yilog[D φ 2 (xi, ci,j )]−(1−yi)log[1−D φ 2 (x R i , ci,j )],<label>(9)</label></formula><p>where φ 2 denotes the learnable parameters for the discriminator, c i,j denotes the age condition code to transform f i into the j th age phase, and x R i denotes a real face image with (almost) the same age withx i (not necessarily belong to the same person).</p><p>To facilitate this process, we modify a CNN backbone as a local-patch based discriminator D φ2 to prevent G θ D from over-emphasizing certain image features to fool the current discriminator network. We further augment D φ2 with an auxiliary agent L ae to preserve the desired rejuvenation/aging effect. In this way, G θ D not only learns to render photorealistic samples but also learns to satisfy the target age encoded by c:</p><formula xml:id="formula_9">Lae = 1 N i ĉi,j − ci,j 2 2 + c R i,j − ci,j 2 2 ,<label>(10)</label></formula><p>whereĉ i,j and c R i,j denote the estimated ages fromx i and x R i , respectively. L mc is introduced to enforce the manifold consistency between the input-output space, defined as x − x 2 2 /|x|, where |x| is the size of x. L TV is introduced as a regularization term on the synthesized results to reduce spiky artifacts:</p><formula xml:id="formula_10">LTV = H,W i,j xi,j+1 −xi,j 2 2 + xi+1,j −xi,j 2 2 .<label>(11)</label></formula><p>In order to make the model focus on the most relevant features, we adopt L att to facilitate robustness enhancement via an attention mechanism:</p><formula xml:id="formula_11">Latt = H,W i,j x A i,j+1 −x A i,j 2 2 + x A i+1,j −x A i,j 2 2 + x A i,j 2 2 ,<label>(12)</label></formula><p>where x A denotes the attention score map which serves as the guidance, and attends to the most relevant regions during cross-age face synthesis. The final synthesized results can be obtained bŷ</p><formula xml:id="formula_12">x = x A · x F + (1 − x A ) · x,<label>(13)</label></formula><p>where x F denotes the feature map predicted by the last fractionally-strided convolution block.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Inference</head><p>The goal of AIM is to use sets of real targets to learn two GAN-like sub-nets that mutually boost each other and jointly accomplish age-invariant face recognition. Each separate loss serves as a deep supervision within the hinged structure benefiting network convergence. The overall objective function for AIM is</p><formula xml:id="formula_13">L AIM = −λ 1 L cad + λ 2 L cer − λ 3 L adv1 + λ 4 L ip − λ 5 L adv2 + λ 6 L ae + λ 7 L mc + λ 8 L tv + λ 9 L att .<label>(14)</label></formula><p>During testing, we simply feed the input face image x and desired age condition code c into AIM to obtain the disentangled age-invariant representation f from G θ E and the synthesized age regressed/progressed face imagex from G θ D . Example results are visualized in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Cross-Age Face Recognition Benchmark</head><p>In this section, we introduce a new large-scale "Cross-Age Face Recognition (CAFR)" benchmark dataset to push the frontiers of age-invariant face recognition research with several appealing properties. 1) It contains 1,446,500 face images from 25,000 subjects annotated with age, identity, gender, race and landmark labels, which is larger and more comprehensive than previous similar attempts <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b44">45]</ref>. 2) The images within CAFR are collected from real-world scenarios, involving humans with various expressions, poses, occlusion and resolution.</p><p>3) The background of images in CAFR is more complex and diverse than previous datasets. Some examples and statistics w.r.t. data distribution on the image number per age phase and the image number per subject are illustrated in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>, (b) and (c), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Collection and Annotation</head><p>We select a sub-set from the celebrity name list of MS-Celeb-1M <ref type="bibr" target="#b14">[15]</ref> for data collection based on below considerations. 1) Each individual must have many cross-age face images available on the Internet for retrieval. 2) Both gender balance and racial diversity should be considered. Accordingly, we manually specify some keywords (such as name, face image, event, year, etc.) to ensure the accuracy and diversity of returned results. Based on these specifications, corresponding cross-age face images are located by performing Internet searches over Google and Bing image search engines. For each identified image, the corresponding URL is stored in a spreadsheet. Automated scrapping software is used to download the cross-age imagery and stores all relevant information (e.g., identity) in a database. Moreover, a pool of self-collected children face images with age variations is also constructed to augment and complement Internet scraping results.</p><p>After curating the imagery, semi-automatic annotation is conducted with three steps. 1) Data cleaning. We perform face detection with an off-the-shelf algorithm <ref type="bibr" target="#b18">[19]</ref> to filter the images without any faces and manually wipe off duplicated images and false positive images (i.e., faces that do not belong to that subject). 2) Data annotation. We combine the prior information on identity and apply off-the-shelf age estimator <ref type="bibr" target="#b35">[36]</ref> and landmark localization algorithm <ref type="bibr" target="#b19">[20]</ref> to annotate the ground truths on age, identity, gender, race and landmarks. 3) Manual inspection. After annotation, manual inspection is performed on all images and corresponding annotations to verify the correctness. In cases where annotations are erroneous, the information is manually rectified by 7 well-informed analysts. The whole work took around 2.5 months to accomplish by 10 professional data annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dataset Splits and Statistics</head><p>In total, there are 1,446,500 face images from 25,000 subjects in the CAFR dataset. Each subject has 57.86 face images on average. The statistical comparisons between our CAFR and existing cross-age datasets are summarized in Tab. 1. CAFR is the largest and most comprehensive benchmark dataset for age-invariant face recognition to date. Following random selection, we divide the data into 10 splits with a pair-wise disjoint of subjects in each split. Each split contains 2,500 subjects and we randomly generate 5 genuine and 5 imposter pairs for each subject with various age gaps, resulting in 25,000 pairs per split. The remained data are preserved for algorithm development and parameter selection. We suggest evaluation systems to report the average Accuracy (Acc), Equal Error Rate (EER), Area Under the Curve (AUC) and Receiver Operating Characteristic (ROC) curve as 10-fold cross validation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate AIM qualitatively and quantitatively under various settings for face recognition in the wild. In particular, we evaluate age-invariant face recognition performance on the CAFR dataset proposed in this work, as well as the MORPH <ref type="bibr" target="#b33">[34]</ref>, CACD <ref type="bibr" target="#b5">[6]</ref> and FG-NET <ref type="bibr" target="#b0">[1]</ref> benchmark datasets. We also evaluate unconstrained face recognition results on the IJB-C benchmark dataset <ref type="bibr" target="#b26">[27]</ref> to verify the generalizability of AIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We apply integrated Face Analytics Network (iFAN) <ref type="bibr" target="#b19">[20]</ref> for face Region of Interest (RoI) extraction, 68 landmark localization (if not provided), and alignment; throughout the experiments, the sizes of the RGB image x, the attention score map x A , the feature map x F , the synthesized face imagex are fixed as 128 × 128; the pixel values of x,x and x R are normalized to <ref type="bibr">[-1,1]</ref>; the sizes of the input local patches (w/o overlapping) to the discriminator D φ2 are fixed as 32 × 32; the dimensionality of learned facial representation f and sample f * drawn from prior distribution U (f ) are fixed as 256; the age condition code c is a 7-dimension one-hot vector to encode different age phases 2 , based on which continuous face rejuvenation/aging results can be achieved through interpolation during inference; the element of c is also confined to [-1,1], where -1 corresponds to 0; the element of smoothed labels for L cer is 1 7 ; the constraint factors {λ k } k= <ref type="bibr">14 1</ref> are empirically fixed as 0.1, 0.1, 0.01, 1.0, 0.01, 0.05, 0.1, 10 −5 , 0.03, 0.01, 0.05, 0.1, 10 −5 and 0.03, respectively; the encoder G θ E is initialized with the Light CNN-29 <ref type="bibr" target="#b49">[50]</ref> architecture by eliminating the linear classifier and replacing the activation function of the last fully-connected layer with hyperbolic tangent; the decoder G θ D is initialized with 3 hidden fractionally-strided convolution layers with kernels 3 × 3 × 512/2, 3 × 3 × 256/2 and 3 × 3 × 128/2, activated with Retified Linear Unit (ReLU) <ref type="bibr" target="#b9">[10]</ref>, appended with a convolution layer with kernel 1 × 1 × 1 activated with sigmoid and a convolution layer with kernel 1 × 1 × 3 acti- <ref type="bibr" target="#b1">2</ref> We divide the whole age span into 7 age phases: ≤20, 20-25, <ref type="bibr">25-30, 30-40, 40-50, 50-60, ≥60.</ref> vated with scaled sigmoid for attention score map x A and feature map x F prediction, respectively; the domain classifier C ϕ and the regularizer R ψ are initialized with the same MLP architectures (which are learned separately), containing a hidden 256-way fully-connected layer activated with Leaky ReLU <ref type="bibr" target="#b25">[26]</ref> and a final 7-way fully-connected layer; the discriminator D φ1 is initialized with a MLP containing a hidden 256-way fully-connected layer activated with Leaky ReLU, appended with a 1-way fully-connected layer activated by sigmoid and a n-way fully-connected layer (n is the identity number of the training data) as the dual agents for L adv1 and L ip , respectively; the discriminator D φ2 is initialized with a VGG-16 <ref type="bibr" target="#b37">[38]</ref> architecture by eliminating the linear classifier, and appending a new 1-way fully-connected layer activated by sigmoid and a new 7-way fully-connected layer activated by hyperbolic tangent as the dual agents for L adv2 and L ae , respectively; the newly added layers are randomly initialized by drawing weights from a zeromean Gaussian distribution with standard deviation 0.01; Batch Normalization <ref type="bibr" target="#b16">[17]</ref> is adopted in G θ E and G θ D ; the dropout <ref type="bibr" target="#b9">[10]</ref> ratio is empirically fixed as 0.7; the weight decay and batch size are fixed as 5 × 10 3 and 32, respectively; We use an initial learning rate of 10 −5 for pre-trained layers, and 2×10 −4 for newly added layers in all our experiments; we decrease the learning rate to <ref type="bibr" target="#b0">1</ref> 10 of the previous one after 20 epochs and train the network for roughly 60 epochs one after another; the proposed network is implemented based on the publicly available TensorFlow <ref type="bibr" target="#b1">[2]</ref> platform, which is trained using Adam (α=2×10 −4 , β 1 =0.5) on two NVIDIA GeForce GTX TITAN X GPUs with 12G memory; the same training setting is utilized for all our compared network variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluations on the CAFR Benchmark</head><p>Our newly proposed CAFR dataset is the largest and most comprehensive age-invariant face recognition benchmark to date, which contains 1,446,500 images annotated with age, identity, gender, race and landmarks. Examples are visualized in <ref type="figure" target="#fig_2">Fig. 3</ref>. The data are randomly organized into 10 splits, each consisting of 25,000 verification pairs with various age variations. Evaluation systems report Acc, EER, AUC and ROC as 10-fold cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Component Analysis and Quantitative Comparison</head><p>We first investigate different architectures and loss function combinations of AIM to see their respective roles in ageinvariant face recognition. We compare 10 variants from four aspects: baseline (Light CNN-29 <ref type="bibr" target="#b49">[50]</ref>), different network structures (w/o C ϕ , R ψ , w/o attention mechanism), different loss function combinations (w/o L ip , L adv1 , L ae , L mc , L adv2 ), and our proposed AIM. The performance comparison w.r.t. Acc, EER and AUC on CAFR is reported in Tab. 2. The corresponding ROC curve is provided in <ref type="figure">Fig. 4 (a)</ref>. By comparing the results from the 1 st v.s. 4 th panels, we observe that our AIM consistently outperforms the baseline by a large margin: 11.25% in Acc, 13.95% in EER, and 14.88% in AUC. Light-CNN is a general-purpose face recognition model, with representations entangled with age variations and suffering difficulties to distinguish cross-age faces. Comparatively, AIM jointly performs disentangled representation learning through cross-age domain adversarial training and cross-entropy regularization, and photorealistic cross-age face synthesis with attention mechanism in a mutual boosting way. By comparing the results from the 2 nd v.s. 4 th panels, we observe that AIM consistently outperforms the 3 variants in terms of network structure. In particular, w/o C ϕ refers to truncating the domain classifier from AIM, leading to 5.96%, 4.30% and 4.07% performance drop for all metrics. This verifies the necessity of cross-age domain adversarial training, which promotes encoded features to be indistinguishable w.r.t. the shift between multi-age domains to facilitate age-invariant representation learning. w/o R ψ refers to truncating the cross-entropy regularizer from AIM, leading to 4.42%, 2.55% and 2.32% performance drop for all metrics. This verifies the necessity of cross-entropy regularization with label smoothing strategy that constrains cross-age representations with ambiguous separability to serve as an auxiliary assistance for C ϕ . The superiority of incorporating attention mechanism to crossage face synthesis can be verified by comparing w/o Att. with AIM, i.e., 2.56%, 0.83% and 0.58% differences for all metrics. Identity-preserving quality is crucial for face recognition applications, the superiority of which is verified by comparing w/o L ip with AIM, i.e., 17.17%, 28.18% and 33.70% decline for all metrics. The superiority of incorporating adversarial learning to specific process can be verified by comparing w/o L advi , i ∈ {1, 2} with AIM, i.e., 3.79%, 1.89% and 1.74%; 2.51%, 0.61% and 0.52% decrease for all metrics. The superiorities of incorporating age estimation and manifold consistency constraints are verified by comparing w/o L ae and w/o L mc with AIM, i.e., 2.98%, 1.41% and 0.97%; 2.78%, 0.90% and 0.74% drop for all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Qualitative Comparison</head><p>Most previous works on age-invariant face recognition address this problem considering either only robust representation learning or only face rejuvenation/aging. It is commonly believed simultaneously modeling both is a highly non-linear transformation, thus it is difficult for a model to learn discriminative and age-invariant facial representations while generating faithful cross-age face images. However, with enough training data and proper architecture and objective function design of AIM, it is feasible to take the best of both worlds, as shown in <ref type="figure">Fig. 1</ref>. For more detailed results across a wide range of ages in high resolution, please refer to <ref type="figure" target="#fig_5">Fig. 8</ref>. Our AIM consistently provides discriminative and age-invariant representations and high-fidelity age regressed/progressed face images for all cases. This well verifies that the joint learning scheme of age-invariant representation and attention-based cross-age face synthesis is effective, and both results are beneficial to face recognition in the wild.</p><p>We then visually compare the qualitative face rejuvenation and aging results by our AIM with previous state-ofthe-art method CAAE <ref type="bibr" target="#b53">[54]</ref> in <ref type="figure" target="#fig_4">Fig. 7</ref> 1 st block and showcase the facial detail transformation over time with AIM in <ref type="figure">Fig. 5</ref>. It can be observed that AIM achieves simultaneous face rejuvenation and aging with photorealistic and accurate age transformation effect (e.g., wrinkles, eyes, mouth, moustache, laugh lines), thanks to the novel network structure and training strategy. In contrast, results of previous work may suffer from blur and ghosting artifacts, and be fragile to variations in illumination, expression and pose. This further shows effectiveness of the proposed AIM.</p><p>To demonstrate the capacity of AIM to synthesize crossage face images with continuous and smooth transition between identities and ages, and show that the learned representations are identity-specific and explicitly disentangled from age variations, we further visualize the learned face manifold in <ref type="figure">Fig. 6</ref> by performing interpolation upon both f and c. In particular, we take two images of different subjects x 1 and x 2 , extract the encoded features from G θ E and perform interpolation between f x1 and f x2 . We also interpolate between two neighboring age condition codes to generate face images with continuous ages. The interpolated f and c are then fed to G θ D to synthesize face images. These smooth semantic changes indicate that the model has learned to produce identity-specific representations disentangled from age variations for age-invariant face recognition.</p><p>Finally, we visualize the cross-age face verification results for CAFR split1 to gain insights into age-invariant face recognition with AIM. After computing the similarities for all pairs of probe and reference sets, we sort the results into a ranking list. Each row shows a probe and reference pair. Between pairs are the matching similarities. <ref type="figure" target="#fig_6">Fig. 9</ref> (a) and (b) show the best matched and non-matches examples, respectively. We note that most of these cases are under mild conditions in terms of age gap and other unconstrained factors like resolution, expression and pose. <ref type="figure" target="#fig_6">Fig. 9</ref> (c) and (d) show the worst matched and non-matched examples, respectively, representing failed matching. We note that most of error cases are with large age gaps blended with other challenging scenarios like blur, extreme expressions, heavy make-up and large poses, which are even hard for humans to recognize. This confirms that CAFR aligns well with reality and deserves more research attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluations on the MORPH Benchmark</head><p>MORPH is a large-scale public longitudinal face database, collected in real-world conditions with variations in age, pose, expression and lighting conditions. It has two separate datasets: Album1 and Album2. Album 1 contains 1,690 face images from 515 subjects while Album 2 contains 78,207 face images from 20,569 subjects. Statistical details are provided in Tab. 1. Both albums include meta data for age, identity, gender, race, eye coordinates and date    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Setting-1/Setting-2 HFA <ref type="bibr" target="#b12">[13]</ref> 91.14/-CARC <ref type="bibr" target="#b4">[5]</ref> 92.80/-MEFA <ref type="bibr" target="#b13">[14]</ref> 93.80/-GSM <ref type="bibr" target="#b23">[24]</ref> -/94.40 MEFA+SIFT+MLBP <ref type="bibr" target="#b13">[14]</ref> 94.59/-LPS+HFA <ref type="bibr" target="#b21">[22]</ref> 94.87/-LF-CNN <ref type="bibr" target="#b47">[48]</ref> 97.51/-AE-CNN <ref type="bibr" target="#b56">[57]</ref> -/98.13 OE-CNN <ref type="bibr" target="#b44">[45]</ref> 98. <ref type="bibr" target="#b54">55</ref> of acquisition. For fair comparisons, Album2 is used for evaluation. Following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13]</ref>, Album2 is partitioned into a training set of 20,000 face images from 10,000 subjects with each subject represented by two images with largest gap, and an independent testing set consisting of a gallery set and a probe set from the remaining subjects under two settings. Setting-1 consists of 20,000 face images from 10,000 subjects with each subject represented by a youngest face image as gallery and an oldest face image as probe while </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VS.</head><p>Method Acc (%) CAN <ref type="bibr" target="#b51">[52]</ref> 92.30 VGGFace <ref type="bibr" target="#b29">[30]</ref> 96.00 Center Loss <ref type="bibr" target="#b48">[49]</ref> 97.48 MFM-CNN <ref type="bibr" target="#b49">[50]</ref> 97.95 LF-CNN <ref type="bibr" target="#b47">[48]</ref> 98.50 Marginal Loss <ref type="bibr" target="#b10">[11]</ref> 98.95 DeepVisage <ref type="bibr" target="#b15">[16]</ref> 99.13 OE-CNN <ref type="bibr" target="#b44">[45]</ref> 99.20 Human, avg. <ref type="bibr" target="#b5">[6]</ref> 85.70 Human, voting <ref type="bibr" target="#b5">[6]</ref> 94.20 AIM (Ours) 99.38 AIM + CAFR (Ours) 99.76</p><p>Setting-2 consists of 6,000 face images from 3,000 subjects with the same criteria. Evaluation systems report the Rank-1 identification rate. The face recognition performance comparison of the proposed AIM with other state-of-the-arts on MORPH <ref type="bibr" target="#b33">[34]</ref> Album2 in Setting-1 and Setting-2 is reported in Tab. 3. With the mutual boosting learning scheme of age-invariant representation and attention-based cross-age face synthesis, our method outperforms the 2 nd -best by 0.58% and 0.14% for Setting-1 and Setting-2, respectively. By incorporating CAFR during training, the rank-1 recognition rates are further improved by 0.52% and 0.45% for Setting-1 and Setting-2, respectively. This confirms that our AIM is highly effective and the proposed CAFR dataset is beneficial for advancing age-invariant face recognition performance. Visual comparison of face rejuvenation/aging results by AIM and CAAE <ref type="bibr" target="#b53">[54]</ref> is provided in <ref type="figure" target="#fig_4">Fig. 7</ref> 2 nd block, also validating advantages of AIM over existing solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluations on the CACD Benchmark</head><p>CACD is a large-scale public dataset for face recognition and retrieval across ages, with variations in age, illumination, makeup, expression and pose, aligned with the real-world scenarios better than MORPH <ref type="bibr" target="#b33">[34]</ref>. It contains 163,446 face images from 2,000 celebrities. Statistical details are provided in Tab. 1. The meta data include age, identity and landmark. However, CACD contains some incorrectly labeled samples and duplicate images. For fair comparison, following <ref type="bibr" target="#b5">[6]</ref>, a carefully annotated version CACD Verification Sub-set (CACD-VS) is used for evaluation. It consists of 10 splits including 4,000 image pairs in total. Each split contains 200 genuine pairs and 200 imposter pairs for cross-age verification task. Evaluation systems report Acc and ROC as 10-fold cross validation.</p><p>The face recognition performance comparison of the proposed AIM with other state-of-the-arts on CACD-VS <ref type="bibr" target="#b5">[6]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Rank-1 (%) Park et al. <ref type="bibr" target="#b28">[29]</ref> 37.40 Li et al. <ref type="bibr" target="#b22">[23]</ref> 47.50 HFA <ref type="bibr" target="#b12">[13]</ref> 69.00 MEFA <ref type="bibr" target="#b13">[14]</ref> 76.20 CAN <ref type="bibr" target="#b51">[52]</ref> 86.50 LF-CNN <ref type="bibr" target="#b47">[48]</ref> 88.10 AIM (Ours) 93.20</p><p>is reported in Tab. 4. The corresponding ROC curve is provided in <ref type="figure">Fig. 4 (b)</ref>. Our method dramatically surpasses human performance and other state-of-the-arts. In particular, AIM improves the Acc of the 2 nd -best by 0.18%. AIM also outperforms human voting performance by 5.18%. To our best knowledge, this is the new state-of-the-art, including unpublished technical reports. This shows the learned facial representations by AIM are discriminative and robust even with in-the-wild variations. With the injection of CAFR as augmented training data, our method further gains 0.38%. Visual comparison of face rejuvenation/aging results by AIM and four state-of-the-art methods is provided in <ref type="figure" target="#fig_4">Fig. 7</ref> 3 rd block, which again verifies effectiveness of our method for high-fidelity cross-age face synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluations on the FG-NET Benchmark</head><p>FG-NET is a popular public dataset for cross-age face recognition, collected in realistic conditions with huge variability in age covering from child to elder. It contains 1,002 face images from 82 non-celebrity subjects. Statistical details are provided in Tab. 1. The meta data include age, identity and landmark. Since the size of FG-NET is small, we follow the leave-one-out setting of <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13]</ref> for fair comparisons with previous methods. In particular, we leave one image as the testing sample and train (finetune) the model with remaining 1,001 images. We repeat this procedure 1,002 times and report the average rank-1 recognition rate.</p><p>The face recognition performance comparison of the proposed AIM with other state-of-the-arts on FG-NET <ref type="bibr" target="#b0">[1]</ref> is reported in Tab. 5. AIM improves the 2 nd -best by 5.10%. Qualitative comparisons for face rejuvenation/aging are provided in <ref type="figure" target="#fig_4">Fig. 7 4</ref> th block, which well shows the promising potential of our method for challenging unconstrained face recognition contaminated with age variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Evaluations on the IJB-C Benchmark</head><p>IJB-C contains 31,334 images and 11,779 videos from 3,531 subjects, which are split into 117,542 frames, 8.87 images and 3.34 videos per subject, captured from in-thewild environments to avoid the near frontal bias. For fair comparison, we follow the template-based setting and evaluate models on the standard 1:1 verification protocol in <ref type="table">Table 6</ref>: Face recognition performance comparison on IJB-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>TAR@FAR=10 −5 TAR@FAR=10 −4 TAR@FAR=10 −3 TAR@FAR=10 −2 GOTS <ref type="bibr" target="#b26">[27]</ref> 0.066 0.147 0.330 0.620 FaceNet <ref type="bibr" target="#b36">[37]</ref> 0.330 0.487 0.665 0.817 VGGFace <ref type="bibr" target="#b29">[30]</ref> 0.437 0.598 0.748 0.871 VGGFace2 ft <ref type="bibr" target="#b3">[4]</ref> 0.768 0.862 0.927 0.967 MN-vc <ref type="bibr" target="#b50">[51]</ref> 0.771 0.862 0.927 0.968 AIM 0.826 0.895 0.935 0.962 terms of True Acceptance Rate (TAR)@False Acceptance Rate (FAR). The face recognition performance comparison of the proposed AIM with other state-of-the-arts on IJB-C <ref type="bibr" target="#b26">[27]</ref> unconstrained face verification protocol is reported in Tab. 6. The corresponding ROC curve is provided in <ref type="figure">Fig. 4 (c)</ref>. Our AIM beats the 2 nd -best by 5.50% in TAR@FAR=10 −5 , which verifies its remarkable generalizability for recognizing faces in the wild. Qualitative comparisons for face rejuvenation/aging are provided in <ref type="figure" target="#fig_4">Fig. 7 5</ref> th block, which further shows the superiority of our method for cross-age face synthesis under unconstrained condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a novel Age-Invariant Model (AIM) for joint disentangled representation learning and photorealistic cross-age face synthesis to address the challenging face recognition with large age variations. Through carefully designed network architecture and optimization strategies, AIM learns to generate powerful age-invariant facial representations explicitly disentangled from the age variation while achieving continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding requirements of paired data and true age of testing samples. Moreover, we propose a new large-scale Cross-Age Face Recognition (CAFR) dataset to spark progress in age-invariant face recognition. Comprehensive experiments demonstrate the superiority of AIM over the state-ofthe-arts. We envision the proposed method and benchmark dataset would drive the age-invariant face recognition research towards real-world applications with presence of age gaps and other complex unconstrained distractors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Age-Invariant Model (AIM) for face recognition in the wild. AIM extends from an auto-encoder based GAN and includes a disentangled Representation Learning sub-Net (RLN) and a Face Synthesis sub-Net (FSN) that jointly learn end-to-end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Cross-Age Face Recognition (CAFR) dataset. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>ROC performance curve on (a) CAFR; (b) CACD-VS; (c) IJB-C. Best viewed in color. Facial attributes transformation over time in terms of (a) wrinkles &amp; eyes, (b) mouth &amp; moustache and (c) laugh lines, which is automatically learned by AIM instead of physical modelling. Illustration of learned face manifold with continuous transitions in age (horizontal axis) and identity (vertical axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative comparison of face rejuvenation/aging results on CAFR, MORPH, CACD, FG-NET and IJB-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Age-invariant face recognition example results on CAFR. Col. 1 &amp; 18: Input faces of distinct identities with various challenging factors (e.g., neutral, illumination, expression, and pose). Col. 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17: Synthesized age regressed/progressed faces by our proposed AIM. Col. 9 &amp; 10: Learned facial representations by AIM, which are explicitly disentangled from age variations. These examples indicate facial representations learned by AIM are robust to age variance, and synthesized cross-age face images retain the intrinsic details. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Age-invariant face recognition analysis on CAFR split1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Qihoo 360 AI Institute {zhaojian90, e0321276, yang yang, jianshu}@u.nus.edu, {yi.cheng, yan.xu, sugiri.pranata, shengmei.shen}@sg.panasonic.com lanh0001@e.ntu.edu.sg, {zhaofang0627, bruinxiongmac}@gmail.com, jlxing@nlpr.ia.ac.cn, hengzhuliu@nudt.edu.cn, {eleyans, elefjia}@nus.edu.sg</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Data distribution on age. (c) Data distribution on # images/subject. (a) Example images.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>[61 ,99 ]</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>[51 ,60 ]</cell><cell></cell><cell></cell></row><row><cell>Will Smith</cell><cell></cell><cell>Age Range</cell><cell>[26 ,30 ] [31 ,40 ] [41 ,50 ]</cell><cell></cell><cell></cell></row><row><cell>[0,20]</cell><cell>[21,25]</cell><cell>[41,50]</cell><cell>[21 ,25 ]</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>[0,2 0]</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>0. 1</cell><cell>0. 2</cell><cell>0. 3</cell><cell>0. 4</cell></row><row><cell>Adam Rodriguez</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Frequency</cell></row><row><cell cols="4">[20 ,39 ] [40 ,59 ] [60 ,79 ] [80 ,99 ] (b) [0,20] # Image/Subject [31,40] [41,50]</cell><cell></cell><cell></cell></row><row><cell>Zoe Saldana</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>[0,1 9]</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>0. 1</cell><cell>0. 2</cell><cell>0. 3</cell><cell>0. 4</cell><cell>0. 5</cell></row><row><cell>[0,20]</cell><cell>[26,30]</cell><cell>[41,50]</cell><cell></cell><cell></cell><cell cols="2">Frequency</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics for publicly available cross-age face datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="5"># Images # Subjects # Images/Subject Age Span Average Age</cell></row><row><cell>FG-NET [1]</cell><cell>1,002</cell><cell>82</cell><cell>avg. 12.22</cell><cell>0-69</cell><cell>15.84</cell></row><row><cell>MORPH Album1 [34]</cell><cell>1,690</cell><cell>515</cell><cell>avg. 3.28</cell><cell>15-68</cell><cell>27.28</cell></row><row><cell>MORPH Album2 [34]</cell><cell>78,207</cell><cell>20,569</cell><cell>avg. 3.80</cell><cell>16-99</cell><cell>32.69</cell></row><row><cell>CACD [6]</cell><cell>163,446</cell><cell>2,000</cell><cell>avg. 81.72</cell><cell>16-62</cell><cell>38.03</cell></row><row><cell>IMDB-WIKI [36]</cell><cell>523,051</cell><cell>20,284</cell><cell>avg. 25.79</cell><cell>0-100</cell><cell>38.00</cell></row><row><cell>AgeDB [28]</cell><cell>16,488</cell><cell>568</cell><cell>avg. 29.03</cell><cell>1-101</cell><cell>50.30</cell></row><row><cell>CAF [45]</cell><cell>313,986</cell><cell>4,668</cell><cell>avg. 67.26</cell><cell>0-80</cell><cell>29.00</cell></row><row><cell>CAFR</cell><cell>1,446,500</cell><cell>25,000</cell><cell>avg. 57.86</cell><cell>0-99</cell><cell>28.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Face recognition performance comparison on CAFR.The results are averaged over 10 testing splits.</figDesc><table><row><cell>Model</cell><cell>Acc (%)</cell><cell>EER (%)</cell><cell>AUC (%)</cell></row><row><cell cols="4">Light CNN 73.56±1.39 31.62±1.68 75.96±1.63</cell></row><row><cell>[50]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Architecture ablation of AIM</cell><cell></cell></row><row><cell>w/o Cϕ</cell><cell cols="3">78.85±1.39 21.97±1.18 86.77±1.01</cell></row><row><cell>w/o R ψ</cell><cell cols="3">80.39±1.19 20.22±1.25 88.52±0.82</cell></row><row><cell>w/o Att.</cell><cell cols="3">82.25±1.03 18.50±1.04 90.26±0.94</cell></row><row><cell cols="3">Training loss ablation of AIM</cell><cell></cell></row><row><cell>w/o Lip</cell><cell cols="3">67.64±0.88 45.85±2.59 57.14±2.59</cell></row><row><cell>w/o L adv 1</cell><cell cols="3">81.02±1.10 19.56±1.00 89.10±0.83</cell></row><row><cell>w/o Lae</cell><cell cols="3">81.83±1.29 19.08±1.03 89.87±0.76</cell></row><row><cell>w/o Lmc</cell><cell cols="3">82.03±0.98 18.57±0.98 90.10±0.83</cell></row><row><cell>w/o L adv 2</cell><cell cols="3">82.30±0.99 18.28±1.02 90.32±0.71</cell></row><row><cell>AIM</cell><cell cols="3">84.81±0.93 17.67±0.90 90.84±0.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Rank-1 recognition rates (%) on MORPH Album2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Face recognition performance comparison on CACD-</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Face recognition performance comparison on FG-NET.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Face regression (a.k.a face rejuvenation) and face progression (a.k.a face aging) refers to rendering the natural rejuvenation/aging effect for a given face, respectively.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The work of Jian Zhao was partially supported by China Scholarship Council (CSC) grant 201503170248.</p><p>The work of Junliang Xing was partially supported by the National Science Foundation of Chian 61672519.</p><p>The work of Jiashi Feng was partially supported by NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-133 and MOE Tier-II R-263-000-D17-112.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://webmail.cycollege.ac.cy/alanitis/fgnetaging/" />
		<title level="m">Fg-net aging database</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<title level="m">Tensorflow: A system for large-scale machine learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perception of age in adult caucasian male faces: Computer graphic manipulation of shape and colour information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Perrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. R. Soc. Lond. B</title>
		<imprint>
			<biblScope unit="volume">259</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="137" to="143" />
			<date type="published" when="1355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-age reference coding for age-invariant face recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="768" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition and retrieval using cross-age reference coding with cross-age celebrity dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-MM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3025" to="3032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust local features for remote face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kellokumpu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="46" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Know you at one glance: A compact vector representation for low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karlekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1924" to="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for lvcsr using rectified linear units and dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8609" to="8613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Marginal loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hidden factor analysis for age invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A maximum entropy feature descriptor for age invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepvisage: Making face recognition simple yet with powerful generalization skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohné</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Milgram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gentric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Illumination-aware age progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3334" to="3341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Towards a comprehensive face detector in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Integrated face analytics networks through crossdataset hybrid training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust face recognition with deep multi-view representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1068" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Aging face recognition: a hierarchical learning model based on local patterns selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>T-IP</publisher>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A discriminative model for age invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-IFS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crossdomain visual matching via generalized similarity measure and feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face verification across age progression using discriminative methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-IFS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-c: Face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICB</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Agedb: The first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>T-PAMI</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Face verification across age progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3349" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling age progression in young faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="387" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling shape and textural variations in aging faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Morph: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FGR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dex: Deep expectation of apparent age from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dex: Deep expectation of apparent age from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dual conditional gans for face aging and rejuvenation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="899" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiview discriminative learning for age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sungatullina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A concatenational graph evolution aging model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2083" to="2096" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recurrent face aging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2378" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Orthogonal deep features decomposition for age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Combining tensor space analysis and active appearance models for aging effect simulation on face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T SYST MAN CY B</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1107" to="1118" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Latent factor guided convolutional neural networks for age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A light cnn for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-IFS</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Multicolumn networks for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09192</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Age invariant face recognition and retrieval by coupled auto-encoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Face aging effect simulation using hidden factor analysis joint sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>T-IP</publisher>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2493" to="2507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">3d-aided deep pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karlekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pranata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1184" to="1190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dual-agent gans for photorealistic and identity preserving profile face synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Jayashree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Pranata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Age estimation guided convolutional neural network for age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Facial aging and rejuvenation by conditional multi-adversarial autoencoder with ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02740</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
