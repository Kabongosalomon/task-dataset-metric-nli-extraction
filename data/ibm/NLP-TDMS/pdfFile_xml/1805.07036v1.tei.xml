<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
							<email>twhui@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>FlowNet2 <ref type="bibr" target="#b13">[14]</ref>, the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that outperforms FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: <ref type="formula">(1)</ref> We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at https://github.com/twhui/LiteFlowNet.</p><p>There are two general principles to improve the design of FlowNet2 and SPyNet. The first principle is pyramidal feature extraction. The proposed network, dubbed Lite-FlowNet, consists of an encoder and a decoder. The encoder maps the given image pair, respectively, into two pyramids 1 arXiv:1805.07036v1 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Optical flow estimation is a long-standing problem in computer vision. Due to the well-known aperture problem, optical flow is not directly measurable <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Hence, the estimation is typically solved by energy minimization in a coarse-to-fine framework <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref>. This class of techniques, however, involves complex energy optimization and thus it is not scalable for applications that demand real-time estimation.</p><p>FlowNet <ref type="bibr" target="#b8">[9]</ref> and its successor FlowNet2 <ref type="bibr" target="#b13">[14]</ref>, have marked a milestone by using CNN for optical flow estimation. Their accuracies especially the successor are approaching that of state-of-the-art energy minimization approaches, while the speed is several orders of magnitude faster. To push the envelop of accuracy, FlowNet2 is designed as a cascade of variants of FlowNet that each network in the cascade refines the preceding flow field by contributing on the flow increment between the first image and the warped second image. The model, as a result, comprises over 160M parameters, which could be formidable in many applications. A recent network termed SPyNet <ref type="bibr" target="#b20">[21]</ref> attempts a network with smaller size of 1.2M parameters by adopting image warping in each pyramid level. Nonetheless, the accuracy can only match that of FlowNet but not FlowNet2. The objective of this study is to explore alternative CNN architectures for accurate flow estimation yet with high efficiency. Our work is inspired by the successes of FlowNet2 and SPyNet, but we further drill down the key elements to fully unleash the potential of deep convolutional network combined with classical principles. of multi-scale high-dimensional features. The decoder then estimates the flow field in a coarse-to-fine framework. At each pyramid level, the decoder infers the flow field by selecting and using the features of the same resolution from the feature pyramids. This design leads to a lighter network compared to FlowNet2 that adopts U-Net architecture <ref type="bibr" target="#b22">[23]</ref> for flow inference. In comparison to SPyNet, our network separates the process of feature extraction and flow estimation. This helps us to better pinpoint the bottleneck of accuracy and model size.</p><p>The second general principle is feature warping. FlowNet2 and SPyNet warp the second image towards the first image in the pair using the previous flow estimate, and then refine the estimate using the feature maps generated by the warped and the first images. Warping an image and then generating the feature maps of the warped image are two ordered steps. We find that the two steps can be reduced to a single one by directly warping the feature maps of the second image, which have been computed by the encoder. This one-step feature warping process reduces the more discriminative feature-space distance instead of the RGB-space distance between the two images. This makes our network more powerful and efficient in addressing the flow problem.</p><p>We now highlight the more specific differences between our network and existing CNN-based optical flow estimation frameworks: 1) Cascaded flow inference -At each pyramid level, we introduce a novel cascade of two lightweight networks. Each of them has a feature warping (f-warp) layer to displace the feature maps of the second image towards the first image using the flow estimate from the previous level. Flow residue is computed to further reduce the feature-space distance between the images. This design is advantageous to the conventional design of using a single network for flow inference. First, the cascade progressively improves flow accuracy thus allowing an early correction of the estimate without passing more errors to the next level. Second, this design allows seamless integration with descriptor matching. We assign a matching network to the first inference. Consequently, pixel-accuracy flow field can be generated first and then refined to sub-pixel accuracy in the subsequent inference network. Since at each pyramid level the feature-space distance between the images has been reduced by feature warping, we can use a rather short displacement than <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref> to establish the cost volume. Besides, matching is performed only at sampled positions and thus a sparse cost-volume is aggregated. This effectively reduces the computational burden raised by the explicit matching.</p><p>2) Flow regularization -The cascaded flow inference resembles the role of data fidelity in energy minimization methods. Using data term alone, vague flow boundaries and undesired artifacts exist in flow fields. To tackle this problem, local flow consistency and co-occurrence between flow boundaries and intensity edges are commonly used as the cues to regularize flow field. Some of the representative methods include anisotropic image-driven <ref type="bibr" target="#b31">[32]</ref>, image-and flow-driven <ref type="bibr" target="#b27">[28]</ref>, and complementary <ref type="bibr" target="#b35">[36]</ref> regularizations. After cascaded flow inference, we allow the flow field to be further regularized by our novel feature-driven local convolution (f-lconv) layer 1 at each pyramid level. The kernels of such a local convolution are adaptive to the pyramidal features from the encoder, flow estimate and occlusion probability map. This makes the flow regularization to be both flow-and image-aware. To our best knowledge, state-ofthe-art CNNs do not explore such a flow regularization.</p><p>The effectiveness of the aforementioned contributions are depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. In summary, we propose a compact LiteFlowNet to estimate optical flow. Our network innovates the useful elements from conventional methods. e.g., brightness constraint in data fidelity to pyramidal CNN features and image warping to CNN feature warping. More specifically, we present a cascaded flow inference with feature warping and flow regularization in each pyramid level, which are new in the literature. Overall, our network outperforms FlowNet <ref type="bibr" target="#b8">[9]</ref> and SPyNet <ref type="bibr" target="#b20">[21]</ref> and is on par with or outperforms the recent FlowNet2 <ref type="bibr" target="#b13">[14]</ref> on public benchmarks, while having 30 times fewer parameters and being 1.36 times faster than FlowNet2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Here, we briefly review some of the major approaches for optical flow estimation. Variational methods. Since the pioneering work by Horn and Schunck <ref type="bibr" target="#b11">[12]</ref>, variational methods have dominated optical flow estimation. Brox et al. address illumination changes by combining the brightness and gradient constancy assumptions <ref type="bibr" target="#b5">[6]</ref>. Brox et al. integrate rich descriptors into variational formulation <ref type="bibr" target="#b6">[7]</ref>. In DeepFlow <ref type="bibr" target="#b30">[31]</ref>, Weinzaepfel et al. propose to correlate multi-scale patches and incorporate this as the matching term in functional. In Patch-Match Filter <ref type="bibr" target="#b15">[16]</ref>, Lu et al. establish dense correspondence using the superpixel-based PatchMatch <ref type="bibr" target="#b3">[4]</ref>. Revaud et al. propose a method EpicFlow that uses externally matched flows as initialization and then performs interpolation <ref type="bibr" target="#b21">[22]</ref>. Zimmer et al. design the complementary regularization that exploits directional information from the constraints imposed in data term <ref type="bibr" target="#b35">[36]</ref>. Our network that infers optical flow and performs flow regularization is inspired by the use of data fidelity and regularization in variational methods.  <ref type="bibr" target="#b25">[26]</ref> that captures higher order spatial statistics <ref type="bibr" target="#b24">[25]</ref>. Sun et al. study the probabilistic model of brightness inconstancy in a highorder random field framework <ref type="bibr" target="#b27">[28]</ref>. Nir et al. represent image motion using the over-parameterization model <ref type="bibr" target="#b18">[19]</ref>. Rosenbaum et al. model the local statistics of optical flow using Gaussian mixtures <ref type="bibr" target="#b23">[24]</ref>. Given a set of sparse matches, Wulff et al. propose to regress them to a dense flow field using a set of basis flow fields (PCA-Flow) <ref type="bibr" target="#b32">[33]</ref>. It can be shown that the parameterized model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref> can be efficiently implemented using CNN. CNN-based methods. In the work of Fischer et al. termed FlowNet <ref type="bibr" target="#b8">[9]</ref>, a post-processing step that involves energy minimization is required to reduce smoothing effect across flow boundaries. This process is not end-to-end trainable. In our work, we present an end-to-end approach that performs in-network flow regularization using the proposed f-lconv layer, which plays similar role as the regularization term in variational methods. In FlowNet2 <ref type="bibr" target="#b13">[14]</ref>, Ilg et al. introduce a huge network cascade (over 160M parameters) that consists of variants of FlowNet. The cascade improves flow accuracy with an expense of model size and computational complexity. Our model uses a more efficient architecture containing 30 times fewer parameters than FlowNet2 while the performance is on par with it. A compact network termed SPyNet <ref type="bibr" target="#b20">[21]</ref> from Ranjan et al. is inspired from spatial pyramid. Nevertheless, the accuracy is far below FlowNet2. A small-sized variant of our network outperforms SPyNet while being 1.33 times smaller in the model size. Zweig et al. present a network to interpolate third-party sparse flows but requiring off-the-shelf edge detector <ref type="bibr" target="#b36">[37]</ref>. DeepFlow <ref type="bibr" target="#b30">[31]</ref> that involves convolution and pooling operations is however not a CNN, since the "filter weights" are non-trainable image patches. According to the terminology used in FlowNet, DeepFlow uses correlation.</p><p>An alternative approach for establishing point correspondence is to match image patches. Zagoruyko et al. first introduce to CNN-feature matching <ref type="bibr" target="#b34">[35]</ref>. Güney et al. find feature representation and formulate optical flow estimation in MRF <ref type="bibr" target="#b10">[11]</ref>. Bailer et al. <ref type="bibr" target="#b1">[2]</ref> use multi-scale features and then perform feature matching as Flow Fields <ref type="bibr" target="#b0">[1]</ref>. Although pixel-wise matching can establish accurate point correspondence, the computational demand is too high for practical use (it takes several seconds even a GPU is used). As a tradeoff, Fischer et al. <ref type="bibr" target="#b8">[9]</ref> and Ilg et al. <ref type="bibr" target="#b13">[14]</ref> perform feature matching only at a reduced spatial resolution. We reduce the computational burden of feature matching by using a short-ranged matching of warped CNN features at sampled positions and a sub-pixel refinement at every pyramid level.</p><p>We are inspired by the feature transformation used in Spatial Transformer <ref type="bibr" target="#b14">[15]</ref>. Our network uses the proposed f-warp layer to displace each channel 2 of the given vector- <ref type="bibr" target="#b1">2</ref> We can also use f-warp layer to displace each channel differently when valued feature according to the provided flow field. Unlike Spatial Transformer, f-warp layer is not fully constrained and is a relaxed version of it as the flow field is not parameterized. While transformation in FlowNet2 and SPyNet is limited to images, our decider network is a more generic warping network that warps high-level CNN features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LiteFlowNet</head><p>LiteFlowNet is composed of two compact sub-networks that are specialized in pyramidal feature extraction and optical flow estimation as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. Since the spatial dimension of feature maps is contracting in feature extraction and that of flow fields is expanding in flow estimation, we call the two sub-networks as NetC and NetE respectively. NetC transforms any given image pair into two pyramids of multi-scale high-dimensional features. NetE consists of cascaded flow inference and regularization modules that estimate coarse-to-fine flow fields.</p><p>Pyramidal Feature Extraction. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, NetC is a two-stream network in which the filter weights are shared across the two streams. Each of them functions as a feature descriptor that transforms an image I to a pyramid of multi-scale high-dimensional features {F k (I)} from the highest spatial resolution (k = 1) to the lowest spatial resolution (k = L). The pyramidal features are generated by stride-s convolutions with the reduction of spatial resolution by a factor s up the pyramid. In the following, we omit the subscript k that indicates the level of pyramid for brevity. We use F i to represent CNN features for I i . When we discuss the operations in a pyramid level, the same operations are applicable to other levels.</p><p>Feature Warping. At each pyramid level, a flow field is inferred from high-level features F 1 and F 2 of images I 1 and I 2 . Flow inference becomes more challenging if I 1 and I 2 are captured far away from each other. With the motivation of image warping used in conventional methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref> and recent CNNs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref> for addressing large-displacement flow, we propose to reduce feature-space distance between F 1 and F 2 by feature warping (f-warp). Specifically, F 2 is warped towards F 1 by f-warp via flow estimateẋ to</p><formula xml:id="formula_0">F 2 (x) F 2 (x +ẋ) ∼ F 1 (x)</formula><p>. This allows our network to infer residual flow between F 1 and F 2 that has smaller flow magnitude (more details in Section 3.1) but not the complete flow field that is more difficult to infer. Unlike conventional methods, f-warp is performed on high-level CNN features but not on images. This makes our network more powerful and efficient in addressing the optical flow problem. To allow end-to-end training, F is interpolated to F multiple flow fields are supplied. The usage, however, is beyond the scope of this work. for any sub-pixel displacementẋ as follows:</p><formula xml:id="formula_1">F(x) = x i s ∈N (xs) F(x i s ) 1 − x s − x i s 1 − y s − y i s ,<label>(1)</label></formula><p>where x s = x +ẋ = (x s , y s ) denotes the source coordinates in the input feature map F that defines the sample point, x = (x, y) denotes the target coordinates of the regular grid in the interpolated feature map F, and N (x s ) denotes the four pixel neighbors of x s . The above bilinear interpolation allows back-propagation during training as its gradients can be efficiently computed <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cascaded Flow Inference</head><p>At each pyramid level of NetE, pixel-by-pixel matching of high-level features yields coarse flow estimate. A subsequent refinement on the coarse flow further improves it to sub-pixel accuracy. First Flow Inference (descriptor matching). Point correspondence between I 1 and I 2 is established through computing correlation of high-level feature vectors in individual pyramidal features F 1 and F 2 as follows:</p><formula xml:id="formula_2">c(x, d) = F 1 (x) · F 2 (x + d)/N,<label>(2)</label></formula><p>where c is the matching cost between point x in F 1 and point x + d in F 2 , d ∈ Z is the displacement vector from x, and N is the length of the feature vector. A cost volume C is built by aggregating all the matching costs into a 3D grid. We reduce the computational burden raised by costvolume processing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref> in three ways: 1) We perform short-range matching at every pyramid level instead of longrange matching at a single level. 2) We reduce feature-space distance between F 1 and F 2 by warping F 2 towards F 1 using our proposed f-warp through flow estimate 3ẋ from previous level. 3) We perform matching only at the sampled positions in the pyramid levels of high-spatial resolution. The sparse cost volume is interpolated in the spatial dimension to fill the missed matching costs for the unsampled positions. The first two techniques effectively reduce the searching space needed, while the last technique reduces the frequency of matching per pyramid level.</p><p>In the descriptor matching unit M , residual flow ∆ẋ m is inferred by filtering the cost volume C as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. A complete flow fieldẋ m is computed as follows:</p><formula xml:id="formula_3">x m = M C(F 1 , F 2 ; d) ∆ẋm +sẋ ↑s .<label>(3)</label></formula><p>Second Flow Inference (sub-pixel refinement). Since the cost volume in descriptor matching unit is aggregated by measuring pixel-by-pixel correlation, flow estimateẋ m from the previous inference is only up to pixel-level accuracy. We introduce the second flow inference in the wake of descriptor matching as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. It aims to refine the pixel-level flow fieldẋ m to sub-pixel accuracy. This prevents erroneous flows being amplified by upsampling and passing to the next pyramid level. Specifically, F 2 is warped to F 2 via flow estimateẋ m . Sub-pixel refinement unit S yields a more accurate flow fieldẋ s by minimizing feature-space distance between F 1 and F 2 through 3ẋ from previous level needs to be upsampled in spatial resolution (denoted by "↑s") and magnitude (multiplied by a scalar s) to sẋ ↑s for matching the spatial resolution of the pyramidal features at the current level. computing residual flow ∆ẋ s as the following:</p><formula xml:id="formula_4">x s = S F 1 , F 2 ,ẋ m ∆ẋs +ẋ m .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Flow Regularization</head><p>Cascaded flow inference resembles the role of data fidelity in conventional minimization methods. Using data term alone, vague flow boundaries and undesired artifacts commonly exist in flow field <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36]</ref>. To tackle this problem, we propose to use a feature-driven local convolution (f-lcon) to regularize flow field from the cascaded flow inference. The operation of f-lcon is well-governed by the Laplacian formulation of diffusion of pixel values <ref type="bibr" target="#b29">[30]</ref>. In contrast to local convolution (lcon) used in conventional CNNs <ref type="bibr" target="#b28">[29]</ref>, f-lcon is more generalized. Not only is a distinct filter used for each position of feature map, but the filter is adaptively constructed for individual flow patches.</p><p>Consider a general case, a vector-valued feature F that has to be regularized has C channels and a spatial dimension M × N . Define G = {g} as the set of filters used in f-lcon layer. The operation of f-lcon to F can be formulated as follow:</p><formula xml:id="formula_5">f g (x, y, c) = g(x, y, c) * f (x, y, c),<label>(5)</label></formula><p>where " * " denotes convolution, f (x, y, c) is a w × w patch centered at position (x, y) of channel c in F , g(x, y, c) is the corresponding w × w regularization filter, and f g (x, y, c) is a scalar output for x = (x, y) and c = 1, 2, ..., C. To be specific for regularizing flow fieldẋ s from the cascaded flow inference, we replace F toẋ s . Flow regularization module R is defined as follows:</p><formula xml:id="formula_6">x r = R(ẋ s ; G).<label>(6)</label></formula><p>The f-lcon filters need to be specialized for smoothing flow field. It should behave as an averaging filter if the variation of flow vectors over the patch is smooth. It should also not over-smooth flow field across flow boundary. We define a feature-driven CNN distance metric D that estimates local flow variation using pyramidal feature F 1 , flow fieldẋ s from the cascaded flow inference, and occlusion probability map <ref type="bibr" target="#b3">4</ref> O. In summary, D is adaptively constructed by a CNN unit R D as follows:</p><formula xml:id="formula_7">D = R D (F 1 ,ẋ s , O).<label>(7)</label></formula><p>With the introduction of feature-driven distance metric D, each filter g of f-lcon is constructed as follows:</p><formula xml:id="formula_8">g(x, y, c) = exp(−D(x, y, c) 2 ) (xi,yi)∈N (x,y) exp(−D(x i , y i , c) 2 ) ,<label>(8)</label></formula><p>where N (x, y) denotes the neighborhood containing ω × ω pixels centered at position (x, y).</p><p>Here, we provide a mechanism to perform f-lcon efficiently. For a C-channel input F , we use C tensors G(1), ...,Ḡ(C) to store f-lcon filter set G. As illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>, each f-lcon filter g(x, y, c) is folded into a 1 × 1 × w 2 3D column and then packed into the (x, y)entry of a M × N × w 2 3D tensorḠ(c). Same folding and packing operations are also applied to each patch in each channel of F . This results C tensorsF (1), ...,F (C) for F . In this way, Equation (5) can be reformulated to:</p><formula xml:id="formula_9">F g (c) =Ḡ(c) F (c),<label>(9)</label></formula><p>where " " denotes element-wise dot product between the corresponding columns of the tensors. With the abuse of notation, F g (c) means the c-th xy-slice of the regularized C-channel feature F g . Equation <ref type="formula" target="#formula_9">(9)</ref> reduces the dimension of tensors from M × N × w 2 (right-hand side in prior to the dot product) to M × N (left-hand side).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Network Details. In LiteFlowNet, NetC generates 6-level pyramidal features and NetE predicts flow fields for levels 6 to 2. Flow field in level 2 is upsampled to yield flow field in level 1. We set the maximum searching radius in costvolume to 3 pixels (levels 6 to 4) or 6 pixels (levels 3 to 2). Matching is performed at each position in pyramidal features, except for levels 3 to 2 that it is performed at a regularly sampled grid (a stride of 2). All convolution layers use 3 × 3 filters, except each last layer in descriptor matching M , sub-pixel refinement S, and flow regularization R units uses 5×5 (levels 4 to 3) or 7×7 (level 2) filters. Each convolution layer is followed by a leaky rectified linear unit layer, except f-lcon and the last layer in M , S and R CNN units. More details can be found in the supplementary material. Training Details. We train our network stage-wise by the following steps: 1) NetC and M 6 :S 6 of NetE is trained for 300k iterations. 2) R 6 together with the trained network in step 1 is trained for 300k iterations. 3) For levels k ∈ [5, 2], M k :S k followed by R k is added into the trained network each time. The new network cascade is trained for 200k (level 2: 300k) iterations. Filter weights are initialized from previous level. Learning rates are initially set to 1e-4, 5e-5, and 4e-5 for levels 6 to 4, 3 and 2 respectively. We reduce it by a factor of 2 starting at 120k, 160k, 200k, and 240k iterations. We use the same loss weight, L2 training loss, Adam optimization, data augmentation (including noise injection), and training schedule 5 (Chairs <ref type="bibr" target="#b8">[9]</ref> → Things3D <ref type="bibr" target="#b16">[17]</ref>) as FlowNet2 <ref type="bibr" target="#b13">[14]</ref>. We denote LiteFlowNet-pre and LiteFlowNet as the networks trained on Chairs and Chairs → Things3D, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>We compare several variants of LiteFlowNet to stateof-the-art methods on public benchmarks including Fly-ingChairs (Chairs) <ref type="bibr" target="#b8">[9]</ref>, Sintel clean and final <ref type="bibr" target="#b7">[8]</ref>, KITTI12 <ref type="bibr" target="#b9">[10]</ref>, KITTI15 <ref type="bibr" target="#b17">[18]</ref>, and Middlebury <ref type="bibr" target="#b2">[3]</ref>. FlyingChairs. We first compare the intermediate results of different well-performing networks trained on Chairs alone in <ref type="table" target="#tab_0">Table 1</ref>. Average end-point error (AEE) is reported. LiteFlowNet-pre outperforms the compared networks. No intermediate result is available for FlowNet2 <ref type="bibr" target="#b13">[14]</ref> as each cascade is trained on the Chairs → Things3D schedule individually. Since FlowNetC, FlowNetS (variants of FlowNet <ref type="bibr" target="#b8">[9]</ref>), and SPyNet <ref type="bibr" target="#b20">[21]</ref> have fewer parameters than FlowNet2 and the later two models do not perform feature matching, we also construct a small-size counterpart LiteFlowNetX-pre by removing the matching part and shrinking the model sizes of NetC and NetE by about 4 and 5 times, respectively. Despite that LiteFlowNetX-pre is 43 and 1.33 times smaller than FlowNetC and SPyNet, respectively, it still outperforms these networks and is on par with FlowNetC that uses explicit matching.</p><p>MPI Sintel. In <ref type="table">Table 2</ref>, LiteFlowNetX-pre outperforms FlowNetS (and C) <ref type="bibr" target="#b8">[9]</ref> and SPyNet <ref type="bibr" target="#b20">[21]</ref> that are trained on Chairs on all cases except the Middlebury benchmark. LiteFlowNet, trained on the Chairs → Things3D schedule, performs better than LiteFlowNet-pre as expected. LiteFlowNet also outperforms SPyNet, FlowNet2-S (and -C) <ref type="bibr" target="#b13">[14]</ref>. We also fine-tuned LiteFlowNet on a mixture of Sintel clean and final training data (LiteFlowNet-ft) using the generalized Charbonnier loss <ref type="bibr" target="#b26">[27]</ref>. No noise augmentation was performed but we introduced image mirroring to improve the diversity of the training set. LiteFlowNetft outperforms FlowNet2-ft-sintel <ref type="bibr" target="#b13">[14]</ref> and EpicFlow <ref type="bibr" target="#b21">[22]</ref> for Sintel final testing set. Despite DC Flow <ref type="bibr" target="#b33">[34]</ref> (a hybrid method consists of CNN and post-processing) performs better than LiteFlowNet, its GPU runtime requires several seconds that makes it formidable in many applications. <ref type="figure" target="#fig_5">Figure 5</ref> shows some examples of flow fields on Sintel dataset. LiteFlowNet-ft and FlowNet2-ft-sintel perform the best among the compared methods. As LiteFlowNet has flow regularization module, sharper flow boundaries and lesser artifacts can be observed in the generated flow fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI. LiteFlowNet consistently performs better than</head><p>LiteFlowNet-pre especially on KITTI15 as shown in Table 2. It also outperforms SPyNet <ref type="bibr" target="#b20">[21]</ref> and FlowNet2-S (and C) <ref type="bibr" target="#b13">[14]</ref>. We also fine-tuned LiteFlowNet on a mixture of KITTI12 and KITTI15 training data (LiteFlowNetft) using the same augmentation as the case of Sintel except that we reduced the amount of augmentation for spatial motion to fit the driving scene. After fine-tuning, Lite-FlowNet generalizes well to real-world data. LiteFlowNetft outperforms FlowNet2-ft-kitti <ref type="bibr" target="#b13">[14]</ref>. <ref type="figure" target="#fig_6">Figure 6</ref> shows some examples of flow fields on KITTI. As in the case for Sintel, LiteFlowNet-ft and FlowNet2-ft-kitti performs the best among the compared methods. Even though LiteFlowNet and its variants perform pyramidal descriptor matching in a limited searching range, it yields reliable large-displacement flow fields for real-world data due to the feature warping (f-warp) layer introduced. More analysis <ref type="table">Table 2</ref>: AEE of different methods. The values in parentheses are the results of the networks on the data they were trained on, and hence are not directly comparable to the others. Fl-all: Percentage of outliers averaged over all pixels. Inliers are defined as EPE &lt;3 pixels or &lt;5%. The best number for each category is highlighted in bold. (Note: <ref type="bibr" target="#b0">1</ref> The values are reported from <ref type="bibr" target="#b13">[14]</ref>. <ref type="bibr" target="#b1">2</ref> We re-trained the model using the code provided by the authors. <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5</ref> The values are computed using the trained models provided by the authors. <ref type="bibr" target="#b3">4</ref> Large discrepancy exists as the authors mistakenly evaluated the results on the disparity dataset. <ref type="bibr" target="#b4">5</ref> Up-to-date dataset is used. <ref type="bibr" target="#b5">6</ref>   will be presented in Section 4.3.</p><p>Middlebury. LiteFlowNet has comparable performance with conventional methods. It outperforms FlowNetS (and C) <ref type="bibr" target="#b8">[9]</ref>, FlowNet2-S (and C) <ref type="bibr" target="#b13">[14]</ref>, SPyNet <ref type="bibr" target="#b20">[21]</ref>, and FlowNet2 <ref type="bibr" target="#b13">[14]</ref>. On the benchmark, LiteFlowNet-ft refers to the one fine-tuned on Sintel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Runtime and Parameters</head><p>We measure runtime of a CNN using a machine equipped with an Intel Xeon E5 2.2GHz and an NVIDIA GTX 1080. Timings are averaged over 100 runs for Sintel image pairs of size 1024 × 436. As summarized in <ref type="table" target="#tab_2">Table 3</ref>, LiteFlowNet has about 30 times fewer parameters than FlowNet2 <ref type="bibr" target="#b13">[14]</ref> and is 1.36 times faster in runtime. LiteFlowNetX, a variant of LiteFlowNet having a smaller model size and without descriptor matching, has about 43 times fewer parameters than FlowNetC <ref type="bibr" target="#b8">[9]</ref> and a comparable runtime. LiteFlowNetX also has 1.33 times fewer parameters than SPyNet <ref type="bibr" target="#b20">[21]</ref>. LiteFlowNet and its variants are currently the most compact CNNs for flow estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We investigate the role of each component in LiteFlowNet-pre trained on Chairs by evaluating the per-Image overlay</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>FlowNetC <ref type="bibr" target="#b8">[9]</ref> FlowNet2 <ref type="bibr" target="#b13">[14]</ref> LiteFlowNet First image FlowNetC <ref type="bibr" target="#b8">[9]</ref> FlowNet2 <ref type="bibr" target="#b13">[14]</ref> FlowNet2-ft-sintel <ref type="bibr" target="#b13">[14]</ref> LiteFlowNet-ft Image overlay Ground truth FlowNetC <ref type="bibr" target="#b8">[9]</ref> FlowNet2 <ref type="bibr" target="#b13">[14]</ref> LiteFlowNet First Image FlowNetC <ref type="bibr" target="#b8">[9]</ref> FlowNet2 <ref type="bibr" target="#b13">[14]</ref> FlowNet2-ft-kitti <ref type="bibr" target="#b13">[14]</ref> LiteFlowNet-ft formance of different variants with some of the components disabled. The AEE results are summarized in <ref type="table" target="#tab_3">Table 4</ref> and examples of flow fields are illustrated in <ref type="figure" target="#fig_7">Figure 7</ref>.</p><p>Feature Warping. We consider two variants LiteFlowNetpre (WM and WMS) and compare them to the counterparts with warping disabled (M and MS). Flow fields from M and MS are more vague. Large degradation in AEE is noticed especially for KITTI12 (33%) and KITTI15 (25%). With feature warping, pyramidal features that input to flow inference are closer to each other. This facilitates flow estimation in subsequent pyramid level by computing residual flow.</p><p>Descriptor Matching. We compare the variant WSR without descriptor matching for which the flow inference part is made as deep as that in the unamended LiteFlowNetpre (ALL). No noticeable difference between the flow fields from WSR and ALL. Since the maximum displacement of the example flow field is not very large (only 14.7 pixels), accurate flow field can still be yielded from WSR. For evaluation covering a wide range of flow displacement (especially large-displacement benchmark, KITTI), degradation in AEE is noticed for WSR. This suggests that descriptor matching is useful in addressing large-displacement flow.</p><p>Sub-Pixel Refinement. The flow field generated from WMS is more crisp and contains more fine details than that generated from WM with sub-pixel refinement disabled. Less small-magnitude flow artifacts (represented by light color on the background) are also observed. Besides, WMS achieves smaller AEE. Since descriptor matching establishes pixel-by-pixel correspondence, sub-pixel refinement is necessary to yield detail-preserving flow field. Regularization. In comparison WMS with regularization disabled to ALL, undesired artifacts exist in homogeneous regions (represented by very dim color on the background) of the flow field generated from WMS. Flow bleeding and vague flow boundaries are observed. Degradation in AEE is also noticed. This suggests that the proposed feature-driven local convolution (f-lcon) plays the vital role to smooth flow field and maintain crisp flow boundaries as regularization term in conventional variational methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a compact network for accurate flow estimation. LiteFlowNet outperforms FlowNet <ref type="bibr" target="#b8">[9]</ref> and is on par with or outperforms the state-of-the-art FlowNet2 <ref type="bibr" target="#b13">[14]</ref>  on public benchmarks while being faster in runtime and 30 times smaller in model size. Pyramidal feature extraction and feature warping (f-warp) help us to break the de facto rule of accurate flow network requiring large model size. To address large-displacement and detail-preserving flows, LiteFlowNet exploits short-range matching to generate pixel-level flow field and further improves the estimate to sub-pixel accuracy in the cascaded flow inference. To result crisp flow boundaries, LiteFlowNet regularizes flow field through feature-driven local convolution (f-lcon). With its lightweight, accurate, and fast flow computation, we expect that LiteFlowNet can be deployed to many applications such as motion segmentation, action recognition, SLAM, 3D reconstruction and more.</p><p>Acknowledgement. This work is supported by SenseTime Group Limited and the General Research Fund sponsored by the Research Grants Council of the Hong Kong SAR (CUHK 14241716, 14224316, 14209217).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>LiteFlowNet consists of two compact sub-networks, namely NetC and NetE. NetC is a two-steam network in which the two network streams share the same set of filters. The input to NetC is an image pair (I 1 , I 2 ). The network architectures of the 6-level NetC and NetE at pyramid level 5 are provided in <ref type="table" target="#tab_4">Table 5</ref> and Tables 6 to 8, respectively. We use suffixes "M", "S" and "R" to highlight the layers that are used in descriptor matching, sub-pixel refinement, and flow regularization units in NetE, respectively. We declare a layer as "flow" to highlight when the output is a flow field. Our code and trained models are available at https://github.com/twhui/LiteFlowNet. A video clip (https://www.youtube.com/watch?v= pfQ0zFwv-hM) and a supplementary material are available on our project page (http://mmlab.ie.cuhk. edu.hk/projects/LiteFlowNet/) to showcase the performance of LiteFlowNet and the effectiveness of the proposed components in our network.  <ref type="table">Table 6</ref>: The network details of the descriptor matching unit (M) of NetE in LiteFlowNet at pyramid level 5. "upconv", "f-warp", "corr", and "loss" denote the fractionally strided convolution (so-called deconvolution), feature warping, correlation, and the layer where training loss is applied, respectively. Furthermore, "conv5a' and "conv5b" denote the high-dimensional features of images I1 and I2 generated from NetC at pyramid level 5.   <ref type="table">Table 8</ref>: Network details of the flow regularization unit (R) of NetE in LiteFlowNet at pyramid level 5. "rgb-warp", "norm", "negsq", "softmax", and "f-lcon" denote the image warping, L2 norm of the RGB brightness difference between the two input images, negativesquare, normalized exponential operation over each 1 × 1 × (# Ch. In) column in the 3-D tensor, and feature-driven local convolution, respectively. Furthermore, "conv dist" that highlights the output of the convolution layer is used as the feature-driven distance metric D Eq. (7) in the main manuscript. "im5a" and "im5b" denote the down-sized images of I1 and I2 at pyramid level 5, respectively </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer name</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples demonstrate the effectiveness of the proposed components in LiteFlowNet for i) feature warping, ii) cascaded flow inference, and iii) flow regularization. Enabled components are indicated with bold black fonts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Machine learning methods. Black et al. propose to represent complex image motion as a linear combination of the learned basis vectors [5]. Roth et al. formulates the prior probability of flow field as Field-of-Experts model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The network structure of LiteFlowNet. For the ease of representation, only a 3-level design is shown. Given an image pair (I1 and I2), NetC generates two pyramids of high-level features (F k (I1) in pink and F k (I2) in red, k ∈<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>). NetE yields multi-scale flow fields that each of them is generated by a cascaded flow inference module M :S (in blue color, including a descriptor matching unit M and a sub-pixel refinement unit S) and a regularization module R (in green color). Flow inference and regularization modules correspond to data fidelity and regularization terms in conventional energy minimization methods respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>A cascaded flow inference module M :S in NetE. It consists of a descriptor matching unit M and a sub-pixel refinement unit S. In M , f-warp transforms high-level feature F2 to F2 via upscaled flow field 2ẋ ↑2 estimated at previous pyramid level. In S, F2 is warped byẋm from M . In comparison to residual flow ∆ẋm, more flow adjustment exists at flow boundaries in ∆ẋs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Folding and packing of f-lcon filters {g}. The (x, y)entry of 3D tensorḠ(c) is a 3D column with size 1 × 1 × w 2 . It corresponds to the unfolded w × w f-lcon filter g(x, y, c) to be applied at position (x, y) of channel c in vector-valued feature F .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Examples of flow fields from different methods on Sintel training sets for clean (top row), final (middle row) passes, and the testing set for final pass (last row). Fine details are well preserved and less artifacts can be observed in the flow fields of LiteFlowNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Examples of flow fields from different methods on the training set (top) and the testing set (bottom) of KITTI15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Examples of flow fields from different variants of LiteFlowNet-pre trained on Chairs with some of the components disabled. LiteFlowNet-pre is denoted as "All". W = Feature Warping, M = Descriptor Matching, S = Sub-Pixel Refinement, R = Regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>AEE on the Chairs testing set. Models are trained on the Chairs training set.</figDesc><table><row><cell cols="5">FlowNetS FlowNetC SPyNet LiteFlowNetX-pre LiteFlowNet-pre</cell></row><row><cell>2.71</cell><cell>2.19</cell><cell>2.63</cell><cell>2.25</cell><cell>1.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Trained on Driving and Monkaa [17])</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">Sintel clean</cell><cell cols="2">Sintel final</cell><cell cols="2">KITTI12</cell><cell></cell><cell>KITTI15</cell><cell></cell><cell cols="2">Middlebury</cell></row><row><cell></cell><cell></cell><cell>train</cell><cell>test</cell><cell>train</cell><cell>test</cell><cell>train</cell><cell>test</cell><cell>train</cell><cell cols="2">train (Fl-all) test (Fl-all)</cell><cell>train</cell><cell>test</cell></row><row><cell>Conventional</cell><cell>LDOF 1 [7] DeepFlow 1 [31] Classic+NLP [27] PCA-Layers 1 [33] EpicFlow 1 [22] FlowFields 1 [1]</cell><cell>4.64 2.66 4.49 3.22 2.27 1.86</cell><cell>7.56 5.38 6.73 5.73 4.12 3.75</cell><cell>5.96 3.57 7.46 4.52 3.56 3.06</cell><cell>9.12 7.21 8.29 7.89 6.29 5.81</cell><cell>10.94 4.48 -5.99 3.09 3.33</cell><cell>12.4 5.8 7.2 5.2 3.8 3.5</cell><cell>18.19 10.63 -12.74 9.27 8.33</cell><cell>38.11% 26.52% -27.26% 27.18% 24.43%</cell><cell>-29.18% --27.10% -</cell><cell cols="2">0.44 0.56 0.25 0.42 0.22 0.32 0.66 -0.31 0.39 0.27 0.33</cell></row><row><cell>Hybrid</cell><cell>Deep DiscreteFlow [11] Bailer et al. [2] DC Flow [34]</cell><cell>---</cell><cell>3.86 3.78 -</cell><cell>---</cell><cell>5.73 5.36 5.12</cell><cell>---</cell><cell>3.4 3.0 -</cell><cell>---</cell><cell>---</cell><cell>21.17% 19.44% 14.86%</cell><cell>---</cell><cell>---</cell></row><row><cell></cell><cell>FlowNetS [9]</cell><cell>4.50</cell><cell>7.42</cell><cell>5.45</cell><cell>8.43</cell><cell>8.26</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.09</cell><cell>-</cell></row><row><cell>Heavyweight CNN</cell><cell>FlowNetS-ft [9] FlowNetC [9] FlowNetC-ft [9] FlowNet2-S 3 [14] FlowNet2-S re-trained 2 FlowNet2-C 3 [14] FlowNet2 [14] FlowNet2-ft-sintel [14]</cell><cell cols="4">(3.66) 6.96 4.31 7.28 (3.78) 6.85 3.79 -3.96 -3.04 -2.02 3.96 (1.45) 4.16 (2.19 4 ) 5.74 (4.44) 7.76 5.87 8.81 (5.28) 8.51 4.99 -5.37 -4.60 -3.54 4 6.02</cell><cell>7.52 9.35 8.79 7.26 7.31 5.79 4.01 5 3.54 5</cell><cell>9.1 -------</cell><cell>---14.28 14.51 11.49 10.08 5 9.94 5</cell><cell>---51.06% 51.38% 44.09% 29.99% 5 28.02% 5</cell><cell>--------</cell><cell cols="2">0.98 1.15 0.93 1.04 1.13 0.98 0.35 0.52 ------0.35 -</cell></row><row><cell></cell><cell>FlowNet2-ft-kitti [14]</cell><cell>3.43</cell><cell>-</cell><cell>4.83 4</cell><cell>-</cell><cell>(1.43 5 )</cell><cell>1.8</cell><cell>(2.36 5 )</cell><cell>(8.88% 5 )</cell><cell>11.48%</cell><cell>0.56</cell><cell>-</cell></row><row><cell>Lightweight CNN</cell><cell>SPyNet [21] SPyNet-ft [21] LiteFlowNetX-pre LiteFlowNetX LiteFlowNet-pre LiteFlowNet LiteFlowNet-ft</cell><cell cols="2">4.12 (3.17) 6.64 6.69 3.70 -3.58 -2.78 -2.48 -(1.35) 4.54</cell><cell>5.57 (4.32) 4.82 4.79 4.17 4.04 (1.78)</cell><cell>8.43 8.36 ----5.38</cell><cell>9.12 3.36 6 6.81 6.38 4.56 4.00 (1.05)</cell><cell>-4.1 ----1.6</cell><cell>--16.64 15.81 11.58 10.39 (1.62)</cell><cell>--36.64% 34.90% 32.59% 28.50% (5.58%)</cell><cell>-35.07% ----9.38%</cell><cell cols="2">0.33 0.58 0.33 0.58 0.45 -0.46 -0.45 -0.39 -0.30 0.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Number of training parameters and runtime. The model for which the runtime is in parentheses is measured using Torch, and hence are not directly comparable to the others using Caffe. Abbreviation LFlowNet refers to LiteFlowNet.</figDesc><table><row><cell></cell><cell cols="2">Shallow</cell><cell>Deep</cell><cell cols="2">Very Deep</cell></row><row><cell>Model</cell><cell>FlowNetC</cell><cell>SPyNet</cell><cell>LFlowNetX</cell><cell cols="2">LFlowNet FlowNet2</cell></row><row><cell># layers</cell><cell>26</cell><cell>35</cell><cell>74</cell><cell>99</cell><cell>115</cell></row><row><cell># param. (M)</cell><cell>39.16</cell><cell>1.20</cell><cell>0.90</cell><cell>5.37</cell><cell>162.49</cell></row><row><cell>Runtime (ms)</cell><cell>32.28</cell><cell>(129.83)</cell><cell>35.83</cell><cell>90.25</cell><cell>122.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>AEE of different variants of LiteFlowNet-pre trained on Chairs dataset with some of the components disabled.</figDesc><table><row><cell>Variants</cell><cell>M</cell><cell>MS</cell><cell>WM</cell><cell cols="2">WSR WMS</cell><cell>ALL</cell></row><row><cell>Feature Warping</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Descriptor Matching</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sub-pix. Refinement</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Regularization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FlyingChairs (train)</cell><cell>3.75</cell><cell>2.70</cell><cell>2.98</cell><cell>1.63</cell><cell>1.82</cell><cell>1.57</cell></row><row><cell>Sintel clean (train)</cell><cell>4.70</cell><cell>4.17</cell><cell>3.54</cell><cell>3.19</cell><cell>2.90</cell><cell>2.78</cell></row><row><cell>Sintel final (train)</cell><cell>5.69</cell><cell>5.30</cell><cell>4.81</cell><cell>4.63</cell><cell>4.45</cell><cell>4.17</cell></row><row><cell>KITTI12 (train)</cell><cell>9.22</cell><cell>8.01</cell><cell>6.17</cell><cell>5.03</cell><cell>4.83</cell><cell>4.56</cell></row><row><cell>KITTI15 (train)</cell><cell cols="5">18.24 16.19 14.52 13.20 12.32</cell><cell>11.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The network details of NetC in LiteFlowNet. "# Ch. In / Out" means the number of channels of the input or the output features. "conv" denotes convolution.</figDesc><table><row><cell cols="4">Layer name Kernel Stride # Ch. In / Out</cell><cell>Input</cell></row><row><cell>conv1</cell><cell>7×7</cell><cell>1</cell><cell>3 / 32</cell><cell>I 1 or I 2</cell></row><row><cell>conv2 1</cell><cell>3×3</cell><cell>2</cell><cell>32 / 32</cell><cell>conv1</cell></row><row><cell>conv2 2</cell><cell>3×3</cell><cell>1</cell><cell>32 / 32</cell><cell>conv2 1</cell></row><row><cell>conv2 3</cell><cell>3×3</cell><cell>1</cell><cell>32 / 32</cell><cell>conv2 2</cell></row><row><cell>conv3 1</cell><cell>3×3</cell><cell>2</cell><cell>32 / 64</cell><cell>conv2 3</cell></row><row><cell>conv3 2</cell><cell>3×3</cell><cell>1</cell><cell>64 / 64</cell><cell>conv3 1</cell></row><row><cell>conv4 1</cell><cell>3×3</cell><cell>2</cell><cell>64 / 96</cell><cell>conv3 2</cell></row><row><cell>conv4 2</cell><cell>3×3</cell><cell>1</cell><cell>96 / 96</cell><cell>conv4 1</cell></row><row><cell>conv5</cell><cell>3×3</cell><cell>2</cell><cell>96 / 128</cell><cell>conv4 2</cell></row><row><cell>conv6</cell><cell>3×3</cell><cell>2</cell><cell>128 / 192</cell><cell>conv5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Network details of the sub-pixel refinement unit (S) of NetE in LiteFlowNet at pyramid level 5.</figDesc><table><row><cell>Layer name</cell><cell>Kernel</cell><cell>Stride</cell><cell># Ch. In / Out</cell><cell>Input(s)</cell></row><row><cell>f-warp5 S</cell><cell>-</cell><cell>-</cell><cell>(128, 2) / 128</cell><cell>conv5b, flow5 M</cell></row><row><cell>conv5 1 S</cell><cell>3×3</cell><cell>1</cell><cell>258 / 128</cell><cell>conv5a, f-warp5 S, flow5 M</cell></row><row><cell>conv5 2 S</cell><cell>3×3</cell><cell>1</cell><cell>128 / 64</cell><cell>conv5 1 S</cell></row><row><cell>conv5 3 S</cell><cell>3×3</cell><cell>1</cell><cell>64 / 32</cell><cell>conv5 2 S</cell></row><row><cell>conv5 4 S</cell><cell>3×3</cell><cell>1</cell><cell>32 / 2</cell><cell>conv5 3 S</cell></row><row><cell cols="3">flow5 S, loss5 S element-wise sum</cell><cell>(2, 2) / 2</cell><cell>flow5 M, conv5 4 S</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We name it as feature-driven local convolution (f-lconv) layer in order to distinguish it from local convolution (lconv) layer of which filter weights are locally fixed in conventional CNNs<ref type="bibr" target="#b28">[29]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the brightness error ||I 2 (x+ẋ)−I 1 (x)|| 2 between the warped second image and the first image as the occlusion probability map.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We excluded a small amount of training data in Things3D undergoing extremely large flow displacement as advised by the authors (https:// github.com/lmb-freiburg/flownet2/issues).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
		<title level="m">Flow Fields: Dense correspondence fields for highly accurate large displacement optical flow estimation. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">CNN-based patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<title level="m">PatchMatch: A randomized correspondence algorithm for structural image editing. SIGGRAGH</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning parameterized models of image motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yacoobt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepsont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleets</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Large displacement optical flow: Descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mailk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="500" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="611" to="625" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">FlowNet: Learning optical flow with convolutional networks. ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Are we ready for autonomous driving? CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">Deep discrete flow. ACCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arifical Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Determining motion directly from normal flows upon the use of a spherical eye platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2267" to="2274" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">FlowNet2.0: Evolution of optical flow estimation with deep networks. CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PatchMatch Filter: Efficient edge-aware filtering meets randomized search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1854" to="1861" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Husser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3061" to="3070" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overparameterized variational optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="216" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Highly accurate optic flow computation with theoretically justified warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Didas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="4161" to="4170" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1164" to="1172" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MIC-CAI</publisher>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning the local statistics of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2373" to="2381" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the spatial statistics of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fields of experts: A framework for learning image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="860" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A quantitative analysis of current practices in optical flow estimation and the principles behind them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="137" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<title level="m">Learning optical flow. ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Vector-valued image regularization with PDEs: A common framework for different applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tschumperlé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="506" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepflow</surname></persName>
		</author>
		<title level="m">Large displacement optical flow with deep matching. ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Trobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anisotropic Huber-L 1 optical flow</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient sparse-to-dense optical flow estimation using a learned basis and layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Accurate optical flow via direct cost volume processings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optic flow in harmony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="368" to="388" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Interponet</surname></persName>
		</author>
		<title level="m">A brain inspired neural network for optical flow dense interpolation. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4563" to="4572" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
