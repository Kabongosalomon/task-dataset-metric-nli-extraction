<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Bird&apos;s-Eye-View Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathrin</forename><surname>Elich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Technical University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tuebingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Bird&apos;s-Eye-View Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2[0000−0002−3269−6976] , Francis Engelmann 1[0000−0001−5745−2137] , Theodora Kontogianni 1[0000−0002−8754−8356] , and Bastian Leibe 1[0000−0003−4225−0051]</p><p>Abstract. Recent deep learning models achieve impressive results on 3D scene analysis tasks by operating directly on unstructured point clouds. A lot of progress was made in the field of object classification and semantic segmentation. However, the task of instance segmentation is currently less explored. In this work, we present 3D-BEVIS (3D bird's-eye-view instance segmentation), a deep learning framework for joint semantic-and instance-segmentation on 3D point clouds. Following the idea of previous proposal-free instance segmentation approaches, our model learns a feature embedding and groups the obtained feature space into semantic instances. Current point-based methods process local sub-parts of a full scene independently, followed by a heuristic merging step. However, to perform instance segmentation by clustering on a full scene, globally consistent features are required. Therefore, we propose to combine local point geometry with global context information using an intermediate bird's-eye view representation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent progress in deep learning techniques along with the rapid availability of commodity 3D sensors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15]</ref> has allowed the community to leverage classical tasks such as semantic segmentation and object detection from the 2D image space into the 3D world. In this work, we tackle the joint task of semantic segmentation and instance segmentation of 3D point clouds. Specifically, given a 3D reconstruction of a scene in the form of a point cloud, our goal is not only to estimate a semantic label for each point but also to identify each object's instance. Progress in this area is interesting to a number of computer vision applications such as automatic scene parsing, robot navigation and virtual or augmented reality.</p><p>The main differences between semantic and instance segmentation can be described as follows. While the semantic segmentation task can be interpreted as a classification task for a fixed number of known labels, the object indices for instance segmentation are invariant to permutation and the total number of objects is unknown a priori. Currently, there are two main directions to tackle instance segmentation: Proposal-based methods first look for interesting regions and then segment them into foreground and background <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref> proposal-free approaches learn a feature embedding space for the pixels within the image. The pixels are subsequently grouped according to their feature vector <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. In this work, we follow the latter direction since it is straightforward to jointly perform semantic and instance segmentation for every point in the scene. Moreover, proposal-based approaches generally rely on multi-stage architectures which can be challenging to train.</p><p>Two fundamental issues need to be addressed for proposal-free instance segmentation: First, we need to learn point representations that can be grouped to object instances. Although some attempts have been made for 2D instance segmentation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref>, it remains unclear what is the best way to learn instance features on 3D point clouds. This strongly relates to the second issue which deals with the scale of point clouds. A typical point cloud can have multiple millions of points along with high dimensional features, including position, color or normals. The usual approach to deal with large scenes consists in splitting the point cloud into chunks and processing them separately <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. This is problematic for instance segmentation as large instances can extend over multiple chunks. An alternative is to downsample the original point cloud to a manageable size <ref type="bibr" target="#b32">[33]</ref> which leads to obvious draw-backs (e.g. loss of detail) and can still fail with very large point clouds, such as in dense outdoor scenes.</p><p>In this work, we introduce a hybrid network architecture (see <ref type="figure" target="#fig_0">Fig. 1</ref>) that learns global instance features on a 2D representation of the full scene and then propagates the learned features onto subsets of the full 3D point cloud. In order to achieve this effect, we need a network architecture that supports propagation over unstructured data. The recently presented graph neural network by Wang et al. <ref type="bibr" target="#b34">[35]</ref> for learning a semantic segmentation on point clouds is an adequate choice for this purpose. We present results for our model on the Stanford Indoor 3D scenes dataset <ref type="bibr" target="#b2">[3]</ref> and the more recent ScanNet v2 dataset <ref type="bibr" target="#b10">[11]</ref>.</p><p>The key contributions of this work are as follows: <ref type="bibr" target="#b0">(1)</ref> We present a hybrid 2D-3D network architecture for performing joint semantic and instance segmentation on large scale point clouds. <ref type="bibr" target="#b1">(2)</ref> We show how to combine features learned from a regular 2D representation and unstructured 3D point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>2D Feature Learning for Instance Segmentation. Fully convolutional networks (FCN) <ref type="bibr" target="#b30">[31]</ref> have been used as part of many successful semantic segmentation methods to provide dense semantic predictions and features <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7]</ref>. Similarly, for proposal-free instance segmentation, pixel-wise features need to be inferred based on which the image pixels can subsequently be clustered. Fathi et al. <ref type="bibr" target="#b17">[18]</ref> compute a cross-entropy loss on randomly sampled points for each object instance. Hsu et al. <ref type="bibr" target="#b20">[21]</ref> treat the FCN-features as multinomial distributions. and rely on the KL-divergence to measure similarities between pixel distributions. Kong et al. <ref type="bibr" target="#b22">[23]</ref> map pixel embeddings to a hypersphere. These embeddings are then clustered using a recurrent implementation of the mean-shift algorithm. Similar to our approach, Brabandere et al. <ref type="bibr" target="#b5">[6]</ref> use a discriminative loss function to penalize large distances between pixels of the same instance and small distances between the mean embeddings of different instances.</p><p>While the above approaches are only used for 2D images, we examine instance segmentation on 3D point clouds. However, our model utilizes an additional 2D representation which has proven to be useful in previous 3D scene understanding tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref>. Building on top of these ideas for 2D feature learning, our model includes a U-shaped <ref type="bibr" target="#b29">[30]</ref> FCN to process a 2D bird's-eye view learning globally consistent instance features for an entire scene.</p><p>Deep Learning on 3D point clouds. Most approaches in 2D vision tasks are taking advantage of powerful features, learned through 2D convolutions. Extending the use of convolutions to unstructured 3D point cloud data is non-trivial and has become a very active field of research <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. The seminal work of Qi et al. <ref type="bibr" target="#b26">[27]</ref> introduced feature learning directly on raw point clouds through a series of multi-layer perceptrons (MLPs) and max-pooling. Hierarchical features are added in the follow-up work <ref type="bibr" target="#b27">[28]</ref>. In both works, the max-pooling is only able to extract global shape information. In dynamic graph CNN (DGCNN) <ref type="bibr" target="#b34">[35]</ref>, PointNets are further generalized by EdgeConvs adding local neighborhood information over a k-nearest neighbor graph. In this work, we rely on DGCNN to learn strong geometric features and simultaneously utilize it as a message passing graph network to propagate learned instance features.</p><p>3D Instance Segmentation. While recently several approaches were presented for 3D semantic segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref> and object detection <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32]</ref>, the combined problem of these was mainly disregarded so far. The only published work that conducts instance segmentation directly on raw 3D point clouds is SGPN <ref type="bibr" target="#b33">[34]</ref>. A pair-wise similarity matrix is computed and subsequently thresholded to generate proposals which are merged according to a confidence score. As point clouds are split into smaller blocks that are processed separately, a heuristic GroupMerging algorithm is required to merge identical instances. In contrast, the instance features in this work are globally coherent across a scene such that the instances can directly be extracted without the need of a merging algorithm or thresholding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In the following, we will present the architecture of our model 3D-BEVIS for semantic instance segmentation on 3D point clouds as visualized in <ref type="figure" target="#fig_1">Fig. 2</ref>. The input for our model is a point cloud P = {x i } N i=1 , i.e. a set of points x i ∈ R F where F is the dimension of the input point features. In our model, we use F = 9 for XYZ-position, RGB-color and normalized position with respect to the room size as in <ref type="bibr" target="#b26">[27]</ref>. The model predicts semantic labels</p><formula xml:id="formula_0">L = {l i } N i=1 and instance features F inst = {f i } N i=1 with f i ∈ R D which are grouped to extract the semantic instance labels I = {I i } N i=1 .</formula><p>The entire framework consists of the combination of a 2D and a 3D feature network to learn point-wise instance features, followed by a clustering procedure to obtain the final instance segmentation. First, an intermediate 2D representation of the scene is utilized to learn globally consistent instant features for a scattered subset of points. These features are subsequently propagated towards the remaining points of the point cloud by applying a 3D feature propagation network. A clustering with respect to these learned features yields the final objects instances. Next, the single stages are explained in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">2D Instance Feature Network.</head><p>To efficiently process the entire scene at once, we consider an intermediate representation B ∈ R H×W ×C in the form of a bird's-eye view projection of the point cloud P (see <ref type="figure" target="#fig_2">Fig. 3</ref>). In contrast to previous methods <ref type="bibr" target="#b33">[34]</ref> that independently process small chunks of the full point cloud, we are thereby able to learn instance features which are globally consistent across the point cloud. For generating this view, the points are projected onto a grid on the ground plane. If several points fall into the same cell, only the highest point above the ground plane is taken into account. We use color and height-above-ground as input channels, thus C = 4. The projections B are precomputed offline. The resulting 2D representation is the input to a fully convolutional network (FCN) <ref type="bibr" target="#b30">[31]</ref> which predicts the instance feature map E ∈ R H×W ×D . The FCN can process rooms of changing size during testing. We utilize a simple encoder-decoder architecture inspired by U-Net <ref type="bibr" target="#b29">[30]</ref> and the FCN applied in <ref type="bibr" target="#b20">[21]</ref>. Convolutions use a 3x3 kernel size with batch normalization, ReLU non-linearities, and skip-connections. The full architecture is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. There are two output branches, one for semantic segmentation and one for instance segmentation. The corresponding losses are L 2D inst and L 2D sem . L 2D sem is the cross-entropy loss for semantic segmentation. The instance segmentation loss L 2D inst is based on a similarity measure for pairs of pixels: s i,j = x i − x j 2 . From this, we define the entire loss as</p><formula xml:id="formula_1">Input: B GT instance seg. Output: E</formula><formula xml:id="formula_2">L 2D inst = L var + L dist<label>(1)</label></formula><p>with</p><formula xml:id="formula_3">L var = C c=1 xi,xj ∈Sc [s i,j − δ var ] + , L dist = C c,c =1 c =c xi∈Sc xj ∈S c [δ dist − s i,j ] +<label>(2)</label></formula><p>This ensures feature vectors of points belonging to the same object to be similar while encouraging a large distance in the feature space between features corresponding to different instances. Whereas the margin δ var allows instance features to be spread within a certain range, δ dist enforces a minimum distance between to feature vectors. [·] + denotes the hinge function max(0, ·).</p><p>To compute the instance loss, we use the same sampling strategy as applied in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>. Instead of comparing all pairs of feature vectors, we sample a subset S c containing M pixels for each instance c.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">3D Feature Propagation Network.</head><p>At this stage, we have instance features E for all the points P B ⊂ P visible in B. These features are globally consistent and can thus be used as a basis for later grouping. Due to occlusion in the bird's-eye view projection, a fraction of the points was unregarded so far. Therefore, in this part, we use a graph neural network to propagate existing features and predict instance features for all points in P. Specifically, we concatenate the initial point cloud features x i with the learned instance features from B to obtain P . When generating B, we keep track of point indices to map the learned instance features back to the point cloud P. The instance features of unseen points in P \ P B are set to zero. As graph neural network, we use the architecture from DGCNN <ref type="bibr" target="#b34">[35]</ref> which was originally presented for learning a semantic segmentation on point clouds. Similar to the 2D instance feature network, the graph neural network has two output branches, each with an assigned loss function. The semantic segmentation loss L 3D sem is again the cross-entropy loss. The instance segmentation loss L 3D inst is defined as:</p><formula xml:id="formula_4">L 3D inst = F inst − F target<label>(3)</label></formula><p>where F target ∈ R N ×D are target instance features. The target instance feature for a point x i is the mean over all instance features in E which lie in the same ground truth instance I j . If an instance is not visible in B, there will be no target instance feature. Such instances are not part of the loss during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Instance Grouping.</head><p>The last component obtains the final instance labels I by clustering the predicted instance features F inst using the MeanShift <ref type="bibr" target="#b8">[9]</ref> algorithm. MeanShift does not require a pre-determined number of clusters and is thus suited for the task of instance segmentation with an arbitrary number of instances. The semantic labels L directly correspond to the category with the highest prediction in the semantic output branch of the propagation network. As a final post-processing step, we found it beneficial to split up instances with an inconsistent semantic labeling. More specifically, we obtain a new instance I c for every class c if at least th c points in I have predicted semantic label c. th c is chosen to be proportional to the average number of points per instance of the respective category. This helps to distinguish between objects from different classes that are hardly identified from the bird's-eye view like windows and walls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training details</head><p>We train the 2D instance feature network on bird's-eye-view projections at a resolution of 3 cm (S3DIS ) or 5 cm (ScanNet) per pixel. Depending on the room size, images are either cropped or padded. We deal with ceiling points by heuristically removing the highest points in each point cloud up to a threshold. As the network is fully convolutional, we can process the full image at test time. We perform data augmentation on the bird's-eye views B by random rotation at angles of 90 • , scaling and horizontal/vertical flipping.</p><p>To optimize the loss of the 3D feature propagation network, we pick a random position and extract 1024 points from a cylindric block with diameter 1 m 2 or 1.5 m 2 on the ground plane. This is comparable to the proceeding in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref>. The semantic losses are weighted with the negative logarithm of the class frequency. The networks are trained with the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> using exponential learning rate decay with an initial rate of 10 −3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>To evaluate our method, we need point cloud datasets with point-wise instance labels and semantic labels for each instance.</p><p>Stanford Large-Scale 3D Indoor Spaces (S3DIS) <ref type="bibr" target="#b2">[3]</ref> contains dense 3D point clouds from 6 large-scale indoor areas consisting of 271 rooms from 3 different buildings. The points are annotated with 13 semantic classes and grouped into instances. We follow the usual 6-fold cross validation strategy for training and testing as used in <ref type="bibr" target="#b26">[27]</ref>.</p><p>ScanNet v2 <ref type="bibr" target="#b10">[11]</ref> contains 3D meshes of a wide variety of indoor scenes including apartments, hotels, conference rooms and offices. The dataset contains 20 Metrics. For semantic segmentation, we adopt the predominant metrics from the field: intersection over union and overall accuracy. The overall accuracy is an inadequate measure as it favors classes with many points, as it is also noted in <ref type="bibr" target="#b32">[33]</ref>. To report scores on instance segmentation we follow the evaluation scheme applied in <ref type="bibr" target="#b33">[34]</ref> to which we compare. We report the average precision (AP) of the predicted instances with an overlap of 50 % with the ground truth instances for the single categories as well as the AP with 25 % and 75 % overlap. We also report results on the official ScanNet benchmark challenge <ref type="bibr" target="#b11">[12]</ref> which uses a stricter metric that is adapted from the CityScapes <ref type="bibr" target="#b9">[10]</ref> evaluation. Specifically, this metric penalizes wrong semantic labels even if the instance labels are predicted correctly. Moreover, false negatives are taken into account for the precision score.</p><p>Baselines. We compare our method to SGPN <ref type="bibr" target="#b33">[34]</ref>, the only published work so far in the field of semantic instance segmentation operating directly on point clouds. SGPN uses PointNet <ref type="bibr" target="#b26">[27]</ref> as the initial feature extraction network. We conducted an additional baseline experiment SGPN DGCNN which replaces Point-Net by DGCNN <ref type="bibr" target="#b34">[35]</ref>. We used the source code provided by the authors of <ref type="bibr" target="#b33">[34]</ref>, although it required some modifications to run. Due to a lack of information regarding the test split of the dataset used for the provided model, we re-trained the model. On the ScanNet dataset, we also include the PMRCNN (Projected MaskRCNN ) baseline experiment provided by the authors of the ScanNet benchmark challenge <ref type="bibr" target="#b11">[12]</ref>. Their method projects predictions on 2D color images into 3D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>We present quantitative and qualitative results for semantic instance segmentation. Tab. 1 summarizes our results on S3DIS. Category-wise scores for AP 50%  <ref type="table">Table 3</ref>. Category-wise AP0.5 on ScanNet. The presented scores for SGPN are extracted from <ref type="bibr" target="#b33">[34]</ref>. As they did not provide scores for other furniture, this category does not contribute to the mean score.</p><p>are presented in Tab. 2. Our model outperforms both versions of SGPN over all overlap thresholds and most categories. The relatively low result for the category ceiling is due to omitting the ceiling in the bird's eye view. Therefore, the distinction of several such elements is never learned. We see that DGCNN is a powerful method, it can help to significantly improve the existing approach regarding both the instance and semantic segmentation. Please note that our scores differ from the ones reported in SGPN <ref type="bibr" target="#b33">[34]</ref>. The difficulty of reproducibility might be due to the considerable number of heuristic thresholds.</p><p>We present detailed results on ScanNet for AP 50% in Tab. 3. In Tab. 4, we report our scores on the ScanNet v2 benchmark 3D instance segmentation challenge. We get decent results compared to our baseline SGPN. Other recently submitted scores are included as well. Hou et al. <ref type="bibr" target="#b19">[20]</ref> use multi-view RGB-D images as additional input. Yi et al. <ref type="bibr" target="#b35">[36]</ref> predict object proposals on point clouds.</p><p>We show qualitative results of our method for instance and semantic segmentation on S3DIS <ref type="bibr" target="#b2">[3]</ref> in <ref type="figure">Fig. 6</ref> and ScanNet <ref type="bibr" target="#b10">[11]</ref> in <ref type="figure">Fig. 7</ref> at the end this paper. Our model can successfully distinguish between several objects of the same category as can be seen e.g. regarding multiple chairs within a scene. Visualized inferred features for the 2D bird's eye views are depicted in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>The bird's-eye view used in this work has proven to be very powerful to compute globally consistent features. However, there are intrinsic limitations, e.g. vertically oriented objects are not well visible in this 2D representation. The same is true for scenes including numerous occluded objects. An obvious extension could be to include multiple 2D views of the scene. Compared to previous work <ref type="bibr">[</ref>  <ref type="table">Table 4</ref>. ScanNet v2 Benchmark Challenge. We report the mean average precision AP at overlap 25% (AP0.25), overlap 50% (AP0.5) and for overlaps in the range [0.5,0.95] with step size 0.05 (AP). We report additional submitted scores from concurrent work that was recently accepted for publication (*). Scores from <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB depth</head><p>Segmentation (GT) semantic instance</p><p>Inst. features full scene. Thus, the presented method overcomes the necessity for a heuristic post-processing step to merge instances. In this work, we explored the relatively new field of instance segmentation on 3D point clouds. We have proposed a 2D-3D deep learning framework combining a U-shaped fully convolution network to learn globally consistent instance features from a bird's-eye view in combination with a graph neural network to propagate and predict point features in the 3D point cloud. Future work could look at alternative 2D representations to overcome the limitations of the bird'seye view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGBD Input</head><p>Semantic Segmentation GT pred.</p><p>Instance Segmentation GT pred. <ref type="figure">Fig. 6</ref>. Qualitative results on S3DIS <ref type="bibr" target="#b2">[3]</ref>. Left to right: Input RGB point cloud, semantic segmentation (ground truth, prediction), instance segmentation (ground truth, prediction). While we have a fixed color for each class, the color mapping for the single instances is arbitrary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGBD Input</head><p>Semantic Segmentation GT pred.</p><p>Instance Segmentation GT pred. <ref type="figure">Fig. 7</ref>. Qualitative results on ScanNet <ref type="bibr" target="#b10">[11]</ref>. Left to right: Input RGB point cloud, semantic segmentation (ground truth, prediction), instance segmentation (ground truth, prediction). While we have a fixed color for each class, the color mapping for the single instances is arbitrary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance features</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2DFig. 1 .</head><label>1</label><figDesc>. Alternatively, arXiv:1904.02199v3 [cs.CV] 1 Aug 2019 We present a 2D-3D deep model for semantic instance segmentation on 3D point clouds. From left to right: The input 3D point cloud, our network architecture combining a 2D U-shaped convolutional network and a 3D graph convolutional network, actual predictions from our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>3D BEVIS framework. Given a point cloud P, our model predicts instance labels I and semantic labels L. The entire pipeline consists of three stages: First, the 2D instance feature network learns instance features E from a bird's-eye-view B of the scene. After concatenating the instance features to the original point cloud features, a 3D feature propagation network propagates and predicts instance features for all points in the scene. Our model finally predicts semantic labels L and instance features F inst which are clustered to instance labels I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Left to right: Input bird's-eye view B, ground truth instance labels, predicted instance features E colored according to the GT instance labels. For visualization, we project the D-dimensional instance features E to 2D with PCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>2D Instance Feature Network. U-shaped fully convolution network to learn instances features E from the input bird's-eye view B. During training the network predicts semantic labels and instance features. At test time, we only forward the instance features E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Predicted instance features for 2D BEV. Left to right: Input RGB and depth images, ground truth semantic and instance segmentation, instance features. Instance features are mapped into RGB space by applying PCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>In this table, we compare methods that jointly predict semantic labels and instance labels. Our presented method yields the best results for instance segmentation compared to both versions of SGPN. The semantic scores mainly depend on the 3D feature network and are thus comparable for SGPN (DGCNN) and 3D-BEVIS. Using DGCNN as a feature network gives an important improvement.</figDesc><table><row><cell></cell><cell cols="3">Instance Seg.</cell><cell>Semantic Seg.</cell></row><row><cell></cell><cell cols="4">AP0.25 AP0.5 AP0.75 mIoU mAcc</cell></row><row><cell>SGPN [34]</cell><cell>62.47</cell><cell>42.91</cell><cell>23.89</cell><cell>48.27 71.07</cell></row><row><cell>SGPN (DGCNN)</cell><cell>70.73</cell><cell>58.56</cell><cell>39.73</cell><cell>59.29 80.71</cell></row><row><cell cols="4">Ours (3D-BEVIS) 78,45 65,66 46,72</cell><cell>58.37 83.69</cell></row><row><cell cols="5">Table 1. Instance and semantic segmentation results on the S3DIS [3]</cell></row><row><cell cols="5">dataset. semantic classes. We use the public training, validation and test split of 1201,</cell></row><row><cell cols="2">312 and 100 scans, respectively.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Mean ceiling floor wall beam column window door table chair sofa bookcase board SGPN 42.90 78.15 80.27 48.90 33.65 16.97 49.63 44.48 30.33 52.22 23.12 28.50 28.62 SGPNDGCNN 58.56 85.85 83.15 61.65 52.82 47.60 55.12 62.22 34.97 66.02 42.50 55.93 54.85 3D-BEVIS 65.66 71.00 96.70 79.37 45.10 64.38 64.63 70.15 57.22 74.22 47.92 57.97 59.27 Category-wise AP0.5 on S3DIS. We receive the best results in nearly all categories. .09 46.90 79.00 34.10 43.80 63.60 36.80 40.70 0.00 0.00 22.40 0.00 26.90 22.80 61.10 24.50 21.70 60.50 35.80 46.20 -3D-BEVIS 57.73 70.30 97.00 29.70 78.30 75.60 65.00 68.50 36.80 37.40 65.00 21.30 14.50 37.50 57.80 71.40 56.40 68.10 57.40 88.90 38.80</figDesc><table><row><cell cols="6">Mean wall floor cabi-bed chair sofa table door win-book-pic-coun-desk cur-fridge shower toilet sink bath-other</cell></row><row><cell>net</cell><cell>dow</cell><cell>ture ter</cell><cell>tain</cell><cell>curtain</cell><cell>tub furniture</cell></row><row><cell>SGPN* [34] 35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b33">34]</ref>, our model is able to learn global instance features which are consistent over a</figDesc><table><row><cell></cell><cell cols="2">AP AP0.5 AP0.25</cell></row><row><cell cols="2">PMRCCN [12] 2.1 5.3</cell><cell>22.7</cell></row><row><cell>SGPN [34]</cell><cell cols="2">4.9 14.3 39.0</cell></row><row><cell cols="3">Our method 11.0 22.5 35.0</cell></row><row><cell cols="3">3D-SIS* [20] 16.1 38.2 55.8</cell></row><row><cell>GSPN* [36]</cell><cell cols="2">15.8 30.6 54.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">ExperimentsWe evaluate our method using two benchmark datasets on which we conduct experiments on the task of semantic and instance segmentation. We show qualitative and quantitative results on both tasks.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Intel RealSense Stereoscopic Depth Cameras</title>
		<idno>abs/1705.05548</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Computing Research Repository CoRR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Matterport: 3D models of interior spaces</title>
		<ptr target="http://matterport.com" />
		<imprint>
			<date type="published" when="2019-08-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D Semantic Parsing of Large-Scale Indoor Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<title level="m">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<title level="m">SnapNet: 3D Point Cloud Semantic Labeling with 2D Deep Segmentation Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic Instance Segmentation with a Discriminative Loss Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-View 3D Object Detection Network for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<title level="m">Mean shift: A Robust Approach Toward Feature Space Analysis</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scan-Net: Richly-annotated 3D Reconstructions of Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<ptr target="http://kaldir.vc.in.tum.de/scannet_benchmark/" />
		<title level="m">ScanNet Benchmark Challenge</title>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
	<note>Online; accessed 19</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Instance-aware Semantic Segmentation via Multi-task Network Cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">FabScan-Affordable 3D Laser Scanning of Physical Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1907.12046</idno>
		<title level="m">Dilated Point Convolutions: On the Receptive Field of Point Convolutions. Computing Research Repository CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Know What Your Neighbors Do: 3D Semantic Segmentation of Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops (ECCV&apos;W)</title>
		<meeting>the European Conference on Computer Vision Workshops (ECCV&apos;W)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno>abs/1703.10277</idno>
		<title level="m">Semantic Instance Segmentation via Deep Metric Learning. Computing Research Repository CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Mask R-CNN</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to Cluster for Proposal-Free Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent Pixel Embedding for Instance Grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pixels to Graphs by Associative Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to Segment Object Candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<title level="m">Frustum PointNets for 3D Object Detection from RGB-D Data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully-Convolutional Point Networks for Large-Scale Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Fully Convolutional Networks for Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Complex-YOLO: Real-time 3D Object Detection on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gross</surname></persName>
		</author>
		<idno>abs/1803.06199</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository CoRR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tangent Convolutions for Dense Prediction in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dynamic Graph CNN for Learning on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno>abs/1801.07829</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository CoRR</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
