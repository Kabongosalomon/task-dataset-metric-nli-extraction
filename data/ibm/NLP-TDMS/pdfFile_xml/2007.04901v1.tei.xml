<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Modal Weighting Network for RGB-D Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongyang</forename><surname>Li</surname></persName>
							<email>ligongyang@shu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Manitoba</orgName>
								<address>
									<settlement>Winnipeg</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
							<email>ywang@cs.umanitoba.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Manitoba</orgName>
								<address>
									<settlement>Winnipeg</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Huawei Technologies</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
							<email>hling@cs.stonybrook.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Modal Weighting Network for RGB-D Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>RGB-D salient object detection · Cross-modal weighting · Depth-to-RGB weighting · RGB-to-RGB weighting</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth maps contain geometric clues for assisting Salient Object Detection (SOD). In this paper, we propose a novel Cross-Modal Weighting (CMW) strategy to encourage comprehensive interactions between RGB and depth channels for RGB-D SOD. Specifically, three RGB-depth interaction modules, named CMW-L, CMW-M and CMW-H, are developed to deal with respectively low-, middle-and high-level cross-modal information fusion. These modules use Depth-to-RGB Weighing (DW) and RGB-to-RGB Weighting (RW) to allow rich cross-modal and cross-scale interactions among feature layers generated by different network blocks. To effectively train the proposed Cross-Modal Weighting Network (CMWNet), we design a composite loss function that summarizes the errors between intermediate predictions and ground truth over different scales. With all these novel components working together, CMWNet effectively fuses information from RGB and depth channels, and meanwhile explores object localization and details across scales. Thorough evaluations demonstrate CMWNet consistently outperforms 15 state-of-the-art RGB-D SOD methods on seven popular benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Salient object detection (SOD) aims to pick the regions/objects in an image that are most attractive to human visual attention. It has a wide range of applications as summarized in recent surveys <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b38">39]</ref>. Most existing SOD solutions take as input an RGB image (or video), which is convenient in many application scenarios, but may suffer from challenges such as low contrast and disturbing background. Alternatively, one can seek help from the depth information typically provided with an RGB-D input. In fact, with the popularity of depth sensors/devices, RGB-D SOD has received extensive attention recently, and numerous approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref> have been proposed to extract salient objects from paired RGB images and depth maps.</p><p>Starting with the first stereoscopic image SOD dataset STEREO <ref type="bibr" target="#b27">[28]</ref>, traditional RGB-D SOD methods mainly apply contrast cue <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>, fusion framework <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> and measure strategy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref> to extract the complementary information in depth maps. These well-designed hand-crafted features-based methods, which are influenced by RGB SOD solutions, have achieved remarkable results. However, salient objects in the generated saliency maps are sometimes blocky because of inappropriate over-segmentation, while salient objects may be confused by complex scenes.</p><p>Recently, with the rapid development of deep learning, convolutional neural networks (CNNs) have shown strong dominance in many computer vision problems. Many CNN-based RGB-D SOD methods have been proposed and greatly outperformed traditional ones. Early CNN-based methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref> feed the superpixel-based hand-crafted features of RGB-D pairs into CNNs, but their results are still patch-based. Subsequent methods instead assign saliency values for each pixel based on the RGB image and depth map in an end-to-end manner. Among these methods, the two-stream architecture <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref> fuses cross-modal features/saliency maps in the middle/late stage, while the single-stream architecture <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref> directly handles RGB-D pairs. These methods, despite achieving great performance gain, do not take full advantage of rich interactive information between different modalities and scales of CNN blocks.</p><p>Motivated by the above observation, in this paper, we propose a novel Cross-Modal Weighting Network (CMWNet) that significantly improves RGB-depth interactions, and hence boosts RGB-D SOD performances as demonstrated in our thorough experiments. Our key idea is to jointly explore the information carried by both RGB and depth channels, and to encourage cross-modal and crossscale RGB-depth interactions among different CNN feature blocks. This way, our algorithm can capture both microscopic details carried by shallow blocks and macroscopic object location information carried by deep blocks. CMWNet adopts a three-level representation, capturing low-, middle-, and high-level information respectively; and multiple blocks at different scales are allowed to be within a level. The cross-modal cross-scale interactions are modeled through the novel Cross-Modal Weighting (CMW) modules to highlight salient objects.</p><p>In particular, we propose three CMW modules, CMW-L, CMW-M and CMW-H. For low-and middle-level parts, CMW-L and CMW-M are used to enhance salient object details in a cross-scale manner. For high-level part, CMW-H is used to enhance salient object localization, which plays a crucial role in subsequent prediction of salient objects. The key components in these CMW modules are the proposed Depth-to-RGB Weighting (DW) and RGB-to-RGB Weighting (RW) operations that enhance RGB features in each channel based on corresponding response maps. In addition to the encoder, a three-level decoder is designed to connect the three-level enhanced features to predict the final salient objects. In this way, the proposed CMWNet effectively exploits the properties of CNN features and strengthens the cross-modal and cross-scale interactions, resulting in excellent performance.</p><p>Our major contributions are summarized as follows:</p><p>-We explore the complex complementarity between RGB image and depth map in a three-level encoder-decoder structure, and propose a novel Cross-Modal Weighting Network (CMWNet) to encourage the cross-modal and cross-scale interactions, boosting the performance of RGB-D SOD. -We propose three novel RGB-depth interaction modules to effectively enhance both salient object details (CMW-L and CMW-M) and salient object localization (CMW-H).</p><p>-Extensive experiments on seven popular public datasets under six commonly used evaluation metrics show that the proposed method achieves the best performance compared with 15 state-of-the-art RGB-D SOD methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditional RGB-D SOD. Starting from the first work for saliency detection <ref type="bibr" target="#b20">[21]</ref>, the contrast-based approaches are the mainstream for saliency detection. This trend has spread to traditional RGB-D SOD. Numerous contrastbased RGB-D SOD methods have been proposed, such as disparity contrast <ref type="bibr" target="#b27">[28]</ref>, depth contrast <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>, and multi-contextual contrast <ref type="bibr" target="#b28">[29]</ref>. Song et al. <ref type="bibr" target="#b35">[36]</ref> employed the multi-scale fusion to merge saliency maps to obtain the final RGB-D saliency map, which is similar to methods based on two-stream saliency fusion <ref type="bibr" target="#b28">[29]</ref> and multiple-cues fusion <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref>. By adopting the objectness measure <ref type="bibr" target="#b24">[25]</ref>, depth confidence measure <ref type="bibr" target="#b8">[9]</ref> and salient structure measure <ref type="bibr" target="#b15">[16]</ref>, the performance gets clear improvement. Besides, other methods (i.e., global prior <ref type="bibr" target="#b31">[32]</ref>, cellular automata <ref type="bibr" target="#b17">[18]</ref>, transformation strategy <ref type="bibr" target="#b7">[8]</ref>) have been proposed for RGB-D SOD. However, these traditional methods are often based on superpixels, regions and patches, which cause saliency maps to appear blocky and saliency values to be scattered. CNN-based RGB-D SOD. In recent years, numerous CNN-based RGB-D SOD methods <ref type="bibr">[4-6, 10, 13, 20, 26, 31, 33, 38, 42, 44]</ref> have been proposed. As pioneering work based on CNNs, Qu et al. <ref type="bibr" target="#b30">[31]</ref> fed the superpixel-based RGB-D saliency features into a five-layer CNN. Shigematsu et al. <ref type="bibr" target="#b32">[33]</ref> sent ten superpixelbased depth features to a network. Being patch-based methods, these methods sometimes generate results that appear blocky. To overcome the limitation, Han et al. <ref type="bibr" target="#b19">[20]</ref> proposed a transfer and fusion based network to predict pixel-level saliency values. The single-stream architecture <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref> adopts a straightforward way to handle the four-channel RGB-D pair. This architecture does not effectively capture the cross-modal interactions between the RGB image and the depth map, so the performance depends largely on the network structure rather than the cross-modal interactions. The two-stream architecture employs two separate networks to extract features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b43">44]</ref> and saliency maps <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b37">38]</ref>, and then fuse them with various strategies. Some works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref> only fuse saliency maps and high-level features. As a result, they do not capture more complex cross-modal interactions at other levels of the network. Some other works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> consider cross-modal CNN features, but the same module is used to process cross-modal CNN features at different blocks. Consequently, these methods ignore the different properties of CNN features at different blocks and cannot provide specific enhancements to object details and object localization.</p><p>In this work, we propose a novel three-level CMWNet to encourage interactions between RGB and depth channels and propose several modules to treat differently detail features and localization features carried in CNN feature blocks at various scales. Moreover, we process CNN features in a cross-modal and crossscale manner to effectively capture the interactions across modalities and scales. Thus, our network can accurately enhance the details and localization of salient objects in the encoder part and precisely infer salient objects in the three-level decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we start with the overview of Cross-Modal Weighting Network (CMWNet) ( § 3.1). In § 3.2, we provide the details of low-and middle-level cross-modal weighting modules, i.e. CMW-L and CMW-M, and then in § 3.3 we introduce the high-level cross-modal weighting module CMW-H. Finally, we describe the implementation details in § 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Overview and Motivation</head><p>The proposed CMWNet follows a three-level Siamese encoder-decoder structure, as summarized in <ref type="figure" target="#fig_0">Fig. 1</ref>. Three-level Encoder. We adopt the VGG16 <ref type="bibr" target="#b33">[34]</ref> as the backbone. The depth map branch and the RGB image branch share the same weights. In the Siamese encoder part, five CNN blocks of the depth map and the RGB image are denoted as D-E (l) and R-E (l) (l ∈{1, 2, 3, 4, 5} is the block index), respectively. Considering the unique properties of features, we divide the first and second CNN blocks into low-level part, the third and fourth CNN blocks into middle-level part, and the last CNN block into high-level part. Low-and Middle-level Cross-Modal Weighting Modules. The weighting mechanism <ref type="bibr" target="#b42">[43]</ref> is an extended version of attention mechanism, and it aims to modulate features of each channel according to particular response maps. The abundant geometric knowledge of depth maps is helpful to provide object details and object localization for SOD. We novelly extend the weighting mechanism with cross-modal information (i.e. RGB image and depth map), and propose cross-modal RGB-depth interaction modules, which adopt weighting mechanism to reweight the RGB features based on depth response maps and RGB response maps to focus on salient objects. Considering that the low-and middle-level parts carry abundant information about object details, we treat the features in these two levels as responsible for object details enhancement, and propose CMW-L and CMW-M. Each of the lowand middle-level contains the two adjacent CNN blocks, one contains relatively macroscopic information while the other relatively microscopic. To balance these two types of information within a level, we use the higher depth block to enhance the lower RGB block, and use the lower depth block to enhance the higher RGB block, namely cross-scale Depth-to-RGB weighting. It is an important component of CMW-L and CMW-M. Concretely, the higher depth response maps are in charge of modulating the lower RGB features, and the lower depth response maps are responsible to modulate the higher RGB features. Such a cross-scale way captures cross-scale complementarity of cross-modal features. Besides, the Depth-to-RGB weighting is executed between two adjacent blocks, which can capture the continuity of features.</p><p>On the other hand, RGB features have the ability to modulate themselves. For this purpose, we introduce the RGB-to-RGB weighting to CMW-L and CMW-M. RGB features are enhanced by RGB response maps, which are generated from RGB features. This allows our weighting modules to learn and adjust salient parts in an adaptive manner. Depth-to-RGB weighting and RGB-to-RGB</p><formula xml:id="formula_0">Conv + Sigmoid ⊕ Summation ⊗ Multiplication Deconv + Sigmoid Deconv R-Conv1_2 R-Conv2_2 3 x 3, 64 3 x 3, 64 3x3, 64 r =5 7 x 7, 64 ⊚ 2 x 2, 64 ⊗ 3 x 3, 64 ⊗ ⊚ ⊚ 2 x 2, 128 ⊗ 3 x 3, 128 ⊗ ⊕ ⊕ 2 x 2, 64</formula><p>RW <ref type="bibr" target="#b0">(1)</ref> RW <ref type="bibr" target="#b1">(2)</ref> DW <ref type="bibr" target="#b0">(1)</ref>   weighting complement each other, improving the stability and robustness of our inference. Thus, for example, in CMW-M, R-E (3) is enhanced by D-E (4) and R-E <ref type="bibr" target="#b2">(3)</ref> , and R-E (4) is enhanced by D-E (3) and R-E <ref type="bibr" target="#b3">(4)</ref> . Notably, CMW-L and CMW-M perform the same cross-scale scheme, but with different resolutions. The multi-resolution enhanced object details of features benefit SOD.</p><formula xml:id="formula_1">DW (2) D-Conv1_2 D-Conv2_2 3 x 3,</formula><p>High-level Cross-Modal Weighting Module. The high-level part is distinct from the other two parts, and it contains rich global information. Thus, in the high-level part, we adopt CNN features of the highest blocks (i.e., D-E <ref type="bibr" target="#b4">(5)</ref> and R-E <ref type="bibr" target="#b4">(5)</ref> ) to accurately locate salient objects. We propose the CMW-H module, which is the modified variant of CMW-L and CMW-M, to enhance the macroscopic localization of salient objects. The RGB features of R-E <ref type="bibr" target="#b4">(5)</ref> are enhanced by the depth response maps generated from D-E <ref type="bibr" target="#b4">(5)</ref> and the RGB response maps generated from R-E <ref type="bibr" target="#b4">(5)</ref> .</p><p>Three-level Decoder. The decoder can make good use of features from the encoder with skip-connections. To fuse all the enhanced features for effective inference, the decoder part also consists of three levels, i.e., D <ref type="bibr" target="#b4">(5)</ref> , D (3&amp;4) and D (1&amp;2) as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, corresponding to high-level, middle-level and low-level encoder parts. Between the two adjacent levels, we employ the deconvolutional layer for 4× upsampling. Specifically, to effectively train the proposed CMWNet, we adopt the deep scale supervision <ref type="bibr" target="#b39">[40]</ref> behind each level to force features of the decoder network to focus on salient objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Low-and Middle-level Cross-Modal Weighting</head><p>We perform the enhancement on RGB features at low-level and middle-level parts with the CMW-L and CMW-M, respectively. The details of CMW-L and CMW-M are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. There are two types of weighting in each module, i.e., cross-scale Depth-to-RGB weighting (DW) and RGB-to-RGB weighting (RW). For each encoder block, those two types of weighting only apply to the last feature layer. So we denote the last layer of features in D-E (l) and R-E (l) as f</p><formula xml:id="formula_2">(l) d and f (l)</formula><p>r , respectively. We provide a simplified version to explain the principle of DW and RW. For a feature map F ∈ R C0×W0×H0 , there are two groups of weighting response maps r 1 ∈ [0, 1] C0×W0×H0 and r 2 ∈ [0, 1] C0×W0×H0 to modulate it at pixel level, and then the two types of weighting can be formulated as:</p><formula xml:id="formula_3">EF = F + r 1 ⊗ F + r 2 ⊗ F,<label>(1)</label></formula><p>where EF ∈ R C0×W0×H0 is the enhanced feature, ⊗ is the element-wise multiplication, and + is the element-wise summation. r 1 ⊗ F and r 2 ⊗ F can be regarded as the DW operation and RW operation, respectively. If r 1 and r 2 have good responses to salient objects (i.e., the pixel value is close to 1 on salient objects and close to 0 on background), F will be accurately modulated to focus on the desired salient parts and EF will have a stronger representation for salient objects. Thus, we apply the two types of weighting to RGB features and depth features for enhancement of salient object details in the CMW-L and CMW-M modules.</p><p>Depth-to-RGB Weighting. In our network, the Depth-to-RGB weighting is the most important operation to mine the complementarity of depth maps. It works in a cross-modal and cross-scale manner. To expand the receptive field and increase the feature diversity, we design a comprehensive structure of filters to generate the local and global features f</p><formula xml:id="formula_4">(l)</formula><p>lg . Concretely, we adopt two convolutional layers with 3×3 kernel as local filters. We also adopt a dilated convolution <ref type="bibr" target="#b40">[41]</ref> with 3 × 3 kernel and rate = 5 and a convolutional layers with 7×7 kernel as global filters, as shown in DW (l) of <ref type="figure" target="#fig_1">Fig. 2</ref>. The global filters in the comprehensive structure expand the receptive field of convolution operations. The obtained global features can capture macro-level information of depth features, which are complementary to the local features. For each f</p><formula xml:id="formula_5">(l) d , f (l)</formula><p>lg can be computed as:</p><formula xml:id="formula_6">f (l) lg = concat C(f (l) d ; W (l1) loc ), C(f (l) d ; W (l2) loc ), C(f (l) d ; W (l1) glo ), C(f (l) d ; W (l2) glo ) ,<label>(2)</label></formula><p>where concat(·) denotes the cross-channel concatenation, C( * ; W  Then, the multi-scale features in f <ref type="bibr">(l)</ref> lg are fused to generate the depth response maps r (l) dw . Specifically, to make r (l) dw have the same resolution as the corresponding cross-scale RGB features, the fusion operation is a convolutional layer with stride 2 for 2× downsampling for DW <ref type="bibr" target="#b0">(1)</ref> and DW <ref type="bibr" target="#b2">(3)</ref> , while a deconvolutional layer is used for DW <ref type="bibr" target="#b1">(2)</ref> and DW <ref type="bibr" target="#b3">(4)</ref> for 2× upsampling. Thus, r (l) dw can be computed as:</p><formula xml:id="formula_7">r (l) dw = σ(C(f (l) lg ; W (l) dw )), l = 1, 3 σ(De(f (l) lg ; W (l) dw )), l = 2, 4 ,<label>(3)</label></formula><p>where σ(·) is the sigmoid function, and De( * ; W (l) dw ) is the deconvolutional layer with parameters W (l) dw , which are 2×2 kernel with stride 2. Finally, r (l) dw is used to enhance the cross-scale RGB features as follows:</p><formula xml:id="formula_8">f (l) dw = r (l+1) dw ⊗ f (l) r , l = 1, 3 r (l−1) dw ⊗ f (l) r , l = 2, 4 .<label>(4)</label></formula><p>RGB-to-RGB Weighting. Considering that the RGB features of low-and middle-level parts also contain rich information about details of salient objects, we propose the RGB-to-RGB weighting to adaptively enhance RGB features with the RGB response maps r (l) rw , which are generated from f (l) r as follows:</p><formula xml:id="formula_9">r (l) rw = σ(C(f (l) r ; W (l) rw )).<label>(5)</label></formula><p>The details of filters in RW (l) are also presented in <ref type="figure" target="#fig_1">Fig. 2</ref>. Then, similar as depth response maps, r</p><p>rw can enhance f (l) r as follows:</p><formula xml:id="formula_11">f (l) rw = r (l) rw ⊗ f (l) r .<label>(6)</label></formula><p>Aggregation of Double Weighting Features. After the DW and RW operations, the RGB features are enhanced twice and can capture the details of salient objects. To preserve the original color information, we add RGB features to these two groups of enhanced features to produce the double enhanced features f (l)</p><p>de . The aggregation of double weighting features is defined as:</p><formula xml:id="formula_12">f (l) de = f (l) r + f (l) dw + f (l) rw .<label>(7)</label></formula><p>Combining Eq. 4, Eq. 6 and Eq. 7, we find that Eq. 7 is similar to Eq. 1. Thus, for the CMW-L and CMW-M, the output features f (k) cmw can be computed as:</p><formula xml:id="formula_13">f (k) cmw = Cat(f (2k−1) de , De(f (2k) de ; W (2k) cmw )), k = 1, 2.<label>(8)</label></formula><p>Then, f</p><p>cmw and f <ref type="bibr" target="#b0">(1)</ref> cmw boost the salient object inference in D (3&amp;4) and D (1&amp;2) , respectively, through skip-connections (i.e., the dashed line in <ref type="figure" target="#fig_0">Fig. 1</ref>). More detailed parameters of CMW-L and CMW-M are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>In <ref type="figure">Fig. 3</ref>, we visualize features of RGB conv2 2 in CMW-L to verify the effectiveness of the double weighting enhancement. By comparing "conv2 2" and "Ours", salient objects are highlighted more clearly in "Ours". If we delete f</p><formula xml:id="formula_15">(l) dw (w/o DW) or f (l)</formula><p>rw (w/o RW), salient objects are more indistinct than "Ours". We also show the features with triple linear enhancement ("3 * conv2 2") to demonstrate that the double weighting enhancement is more effective than the conventional linear enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">High-level Cross-Modal Weighting</head><p>As for the high-level part, we modify the cross-scale CMW-L to the same-scale manner to effectively utilize the macroscopic semantic information. We propose the CMW-H for object localization enhancement. For DW operation in CMW-H, the RGB features are enhanced by depth response maps of the same layer. So, the DW operation in Eq. 4 is modified as follows:</p><formula xml:id="formula_16">f (l) dw = σ(C(f (l) lg ; W (l) dw )) ⊗ f (l) r , l = 5.<label>(9)</label></formula><p>Other operations, such as the RW and features aggregation, are the same as those in CMW-L. Notably, the output of CMW-H is f <ref type="bibr" target="#b4">(5)</ref> de , which is directly fed to the decoder part. It leads the inference process of SOD, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The detailed structure of CMW-H is present in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Loss Function. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we add a convolutional layer after R-E <ref type="bibr" target="#b4">(5)</ref> , D <ref type="bibr" target="#b4">(5)</ref> and D (3&amp;4) to generate intermediate predictions S <ref type="bibr" target="#b3">(4)</ref> , S (3) and S (2) at the training phase. Then, we utilize different scales of ground truth (GT) to supervise them and the final prediction S <ref type="bibr" target="#b0">(1)</ref> with the softmax loss. The total loss L can be defined as:</p><formula xml:id="formula_17">L = 4 t=1 α t · L (t) sm (S (t) , G (t) ),<label>(10)</label></formula><p>where L (t) sm (·, ·) is the softmax loss, α t is the loss weight and set to 1, and G (t) is a GT of the same resolution as S (t) . Network Training Protocol. Our CMWNet is implemented in Caffe <ref type="bibr" target="#b21">[22]</ref> with an NVIDIA Titan X GPU. The parameters of the Siamese encoder part are initialized by the VGG16 model <ref type="bibr" target="#b33">[34]</ref>, except that the conv1 1 of the depth stream is initialized by the Gaussian distribution with a standard deviation of 0.01. Other newly added layers are initialized using the Xavier initialization <ref type="bibr" target="#b16">[17]</ref>. Following <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>, the training set consists of 1,400 triplets from NJU2K <ref type="bibr" target="#b22">[23]</ref> and 650 triplets from NLPR <ref type="bibr" target="#b28">[29]</ref>. We resize all training triplets to 288×288, and then we adopt the rotation (90 • , 180 • and 270 • ) and mirror reflection for augmentation, resulting in 10.25K training triplets. We employ the SGD <ref type="bibr" target="#b2">[3]</ref> to train the network for 22.5K iterations. The learning rate, batch size, iteration size, momentum and weight decay are set to 10 −7 , 1, 8, 0.9 and 0.0001, respectively. The learning rate will be divided by 10 after 12.5K iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>Datasets. We evaluate the proposed method and all compared methods on seven public benchmark datasets, including STEREO <ref type="bibr" target="#b27">[28]</ref>, NJU2K <ref type="bibr" target="#b22">[23]</ref>, LFSD <ref type="bibr" target="#b24">[25]</ref>, DES <ref type="bibr" target="#b6">[7]</ref>, NLPR <ref type="bibr" target="#b28">[29]</ref>, SSD <ref type="bibr" target="#b23">[24]</ref> and SIP <ref type="bibr" target="#b12">[13]</ref>. Evaluation Metrics. We evaluate the performance of our method and other methods using six widely used evaluation metrics including maximum F-measure (F β , β 2 = 0.3), weighted F-measure (F w β , β 2 = 1) <ref type="bibr" target="#b26">[27]</ref>, mean absolute error (MAE, M), precision-recall (PR) curve, S-measure (S λ , λ = 0.5) <ref type="bibr" target="#b10">[11]</ref>, and maximum E-measure (E ξ ) <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-art Methods</head><p>Comparison Methods. We compare the proposed CMWNet with 6 state-ofthe-art traditional methods, which are LBE <ref type="bibr" target="#b15">[16]</ref>, DCMC <ref type="bibr" target="#b8">[9]</ref>, SE <ref type="bibr" target="#b17">[18]</ref>, CDCP <ref type="bibr" target="#b44">[45]</ref>, MDSF <ref type="bibr" target="#b35">[36]</ref> and DTM <ref type="bibr" target="#b7">[8]</ref>, and 9 state-of-the-art CNN-based methods, which are DF <ref type="bibr" target="#b30">[31]</ref>, CTMF <ref type="bibr" target="#b19">[20]</ref>, PCF <ref type="bibr" target="#b3">[4]</ref>, AFNet <ref type="bibr" target="#b37">[38]</ref>, MMCI <ref type="bibr" target="#b5">[6]</ref>, TANet <ref type="bibr" target="#b4">[5]</ref>, CPFP <ref type="bibr" target="#b41">[42]</ref>, DMRA <ref type="bibr" target="#b29">[30]</ref> and D3Net <ref type="bibr" target="#b12">[13]</ref>. The saliency maps of all compared methods are provided by authors or obtained by running their released codes. Notably, we retest DMRA <ref type="bibr" target="#b29">[30]</ref> on STEREO dataset with 1,000 images, which results in different performances from the original DMRA paper. Quantitative Comparison. We evaluate our method and the other 15 state-ofthe-art methods under four quantitative metrics, including S-measure S λ , max F-measure F β , max E-measure E ξ and MAE M. As shown in Tab. 1, our method favorably outperforms all compared methods under these four metrics, and the recently proposed CNN-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42]</ref> and our method are superior to traditional methods by a large margin. Comparing to the second best results in Tab. 1, the performance of our method on the largest and challenging dataset <ref type="table">Table 1</ref>. Quantitative results of 15 state-of-the-art methods on 7 datasets: STEREO <ref type="bibr" target="#b27">[28]</ref>, NJU2K <ref type="bibr" target="#b22">[23]</ref>, LFSD <ref type="bibr" target="#b24">[25]</ref>, DES <ref type="bibr" target="#b6">[7]</ref>, NLPR <ref type="bibr" target="#b28">[29]</ref>, SSD <ref type="bibr" target="#b23">[24]</ref>, and SIP <ref type="bibr" target="#b12">[13]</ref>. "-T" indicates the results on the test set of this dataset. ↑ and ↓ stand for larger and smaller is better, respectively. The best two results are marked in red and blue. The corner note of each method is the publication year.</p><p>Models STEREO <ref type="bibr" target="#b27">[28]</ref> NJU2K-T <ref type="bibr" target="#b22">[23]</ref> LFSD <ref type="bibr" target="#b24">[25]</ref> DES <ref type="bibr" target="#b6">[7]</ref> NLPR-T <ref type="bibr" target="#b28">[29]</ref> SSD <ref type="bibr" target="#b23">[24]</ref> SIP <ref type="bibr" target="#b12">[13]</ref> Sλ  STEREO <ref type="bibr" target="#b27">[28]</ref> is improved by 1.6% and 2.0% in S λ and F β , respectively. The improvement on the relatively small dataset DES <ref type="bibr" target="#b6">[7]</ref> is remarkable, with an increase of 3.0% and 4.2% in S λ and F β , respectively. For the salient person detection, our method improves the performance by 1.2% in F β on SIP <ref type="bibr" target="#b12">[13]</ref>.</p><p>In addition, we also present the PR curves, F-measure curves and S λ (Xaxis) -F w β (Y-axis) coordinates in <ref type="figure" target="#fig_5">Fig. 4</ref>. The performance under these metrics is consistent with that in Tab. 1. The superiority of our method is more visible on STEREO <ref type="bibr" target="#b27">[28]</ref>, LFSD <ref type="bibr" target="#b24">[25]</ref> and DES <ref type="bibr" target="#b6">[7]</ref>. Both Tab. 1 and <ref type="figure" target="#fig_5">Fig. 4</ref>   <ref type="bibr" target="#b12">[13]</ref>, DMRA <ref type="bibr" target="#b29">[30]</ref>, PCF <ref type="bibr" target="#b3">[4]</ref>, CTMF <ref type="bibr" target="#b19">[20]</ref>, DF <ref type="bibr" target="#b30">[31]</ref>) and three traditional methods (DTM <ref type="bibr" target="#b7">[8]</ref>, CDCP <ref type="bibr" target="#b44">[45]</ref>, SE <ref type="bibr" target="#b17">[18]</ref>).</p><p>that our method is consistently better than all compared methods in terms of different evaluation metrics. Visual Comparison. We show visual comparisons with 8 representative methods in <ref type="figure">Fig. 5</ref>. Each row in <ref type="figure">Fig. 5</ref> represents a challenging scenario for SOD, including low contrast (1 st row), disturbing background (2 nd row), salient person detection (3 rd row), object with fine structures (4 th row), multiple objects (5 th row) and small object (6 th row). Regardless of different scene, our method can accurately highlight salient objects with fine details. Notably, in the 4 th row, the mask has a fine structure with three holes on it. Thanks to the DW and RW operations in our method, our method successfully highlights the mask with three holes, while other methods fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>We conduct detailed ablation studies of our CMWNet on a big dataset, NJU2K <ref type="bibr" target="#b22">[23]</ref>, and a small but challenging dataset, SSD <ref type="bibr" target="#b23">[24]</ref>. Specifically, we assess 1) the rationality of enhancing RGB features with depth features; 2) the individual contributions of CMW-L&amp;M and CMW-H; 3) the importance of weighting; 4) the rationality of cross-scale weighting of CMW-L&amp;M; and 5) the necessity of deep scale supervision of decoder part. We change one component at a time to evaluate individual contributions. Rationality of Enhancing RGB Features with Depth Features. In our method, we use depth features to enhance RGB features ("DeR"). To study the rationality of this enhancement manner, we explore another baseline variant: adopting RGB features to enhance depth features ("ReD"). From Tab. 2, we observe that the performance on both datasets has dropped (e.g. M : 0.046 → 0.056 on NJU2K and 0.051 → 0.063 on SSD). This confirms that using depth features to enhance RGB features is more reasonable than the other direction for extracting the cross-modal complementarity.</p><p>In addition, we remove the depth map input in our network to evaluate the power of depth map (w/o depth). This variant is for RGB SOD. The performance of w/o depth drops sharply (e.g. M : 0.046 → 0.056 on NJU2K). This confirms that the way of exploring complementary distance information of depth map in our network is effective. Individual Contributions of CMW-L&amp;M and CMW-H. The proposed three RGB-depth interaction modules can be divided into two types. CMW-L and CMW-M (CMW-L&amp;M) are responsible for object details enhancement, and CMW-H for object localization enhancement. Thus, we provide two variants of our network: removing the CMW-L&amp;M (w/o CMW-L&amp;M) and removing the CMW-H (w/o CMW-H). From Tab. 2, we observe a significant performance degradation (e.g. S λ : 0.903 → 0.891 on NJU2K and 0.875 → 0.849 on SSD) of w/o CMW-L&amp;M. This confirms that the proposed CMW-L&amp;M are momentous to our network, and they enhance the details of salient object in low-and middlelevel features clearly. Some enhanced features in CMW-L are shown in the third column of <ref type="figure">Fig. 3</ref>. The performance drop (e.g. F β : 0.902 → 0.894 on NJU2K and 0.871 → 0.845 on SSD) of w/o CMW-H means that the proposed CMW-H is also important to our network and it enhances the salient object localization in high-level features accurately. Importance of Weighting. To study the importance of two types of weighting, we derive three variants: removing the Depth-to-RGB weighting (w/o DW), removing the RGB-to-RGB weighting (w/o RW), and using concatenation of depth features and RGB features instead of two types of weighting (w/o Wei). Specifically, w/o DW is the same as w/o depth, i.e., the depth map does not participate in SOD. The depth map is still utilized in w/o Wei, which is not equal to w/o (DW+RW). It focuses on evaluating the impact of weighting mechanism. According to the statistics in Tab. 2, we observe the performance of these three variants is worse than our complete CMWNet. This demonstrates that the two types of weighting can help our CMWNet to better highlight salient objects with effective feature enhancement. We also provide two variants, i.e. DW w/o GF and RW w/ GF, to confirm the rationality of specific global filters (GF) in DW.</p><p>In addition, the visualization of features w/o DW and w/o RW is shown in <ref type="figure">Fig. 3</ref>, in which the boundaries of salient objects w/o RW are much clearer than w/o DW. This demonstrates that the depth map does assist the RGB-D SOD, and the enhancement effect of DW is more effective than RW. Rationality of Cross-Scale Weighting in CMW-L&amp;M. To study the rationality of cross-scale weighting in CMW-L&amp;M, we modify the cross-scale DW to the same-scale manner (w/o CS), which is the same as CMW-H. The double enhanced features of each scale in CMW-L&amp;M are concatenated for inference. By comparing w/o CS and Ours in Tab. 2, we find that the performance of w/o CS decreases on both NJU2K and SSD. This demonstrates that the DW of CMW-L&amp;M in the cross-scale manner is rational, and this manner can enhance interactions between different scales to further boost performance.</p><p>Besides, to study the rationality of performing cross-scale weighting between adjacent CNN blocks, we provide a variant which performs cross-scale weighting between nonadjacent CNN blocks, i.e., R-E (1) is enhanced by D-E (3) , R-E (2) is enhanced by D-E (4) , R-E (3) is enhanced by D-E (1) and R-E (4) is enhanced by D-E (2) (C2S). As the results presented in Tab. 2, we observe that the results of C2S are worse than Ours. The reason behind this is that the weighting performed across two scales (i.e. C2S) may lose the continuity of features, causing the depth response maps to fail to highlight the salient objects of RGB features. In contrast, the weighting performed between two adjacent CNN blocks (i.e. Ours) can capture the continuity of features and precisely increase cross-scale interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Necessity of Deep Scale Supervision in Decoder.</head><p>To study the necessity of deep scale supervision, we provide a baseline with only one supervision of the final prediction S (1) (w/o DS). As shown in Tab. 2, we observe that network training with additional supervision is better than the single supervision. This verifies that the multiple scale supervision during network training can improve the testing performance. Besides, the intermediate predictions S <ref type="bibr" target="#b3">(4)</ref> , S (3) and S <ref type="bibr" target="#b1">(2)</ref> are also shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We can observe that the refinement process of prediction from coarse (S (4) ) to fine (S <ref type="bibr" target="#b0">(1)</ref> ) benefits from the deep scale supervision in decoder part, which visually confirms the necessity of deep scale supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel Cross-Modal Weighting Network (CMWNet) for RGB-D SOD. In particular, three novel cross-modal cross-scale weighting modules (CWM-L, CMW-M and CMW-H) are designed to encourage feature interactions for improving SOD performance. Based on these improvements, a three-level decoder progressively refines salient objects. Extensive experiments are conducted to validate our CMWNet, which achieves the best performance on seven public RGB-D SOD benchmarks in comparison with 15 state-of-the-arts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the proposed CMWNet. For both RGB and depth channel, the Siamese encoder network is employed to extract feature blocks organized in three levels. Three Cross-Modal Weighting (CMW) modules, CMW-L, CMW-M and CMW-H, are proposed to capture the interactions at corresponding level, and provide inputs for the decoder. The decoder progressively aggregates all the cross-modal cross-scale information for the final prediction. For training, multi-scale pixel-level supervision for intermediate predictions are utilized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Details of the three proposed RGB-depth interaction modules: CMW-L, CMW-M and CMW-H. All modules consist of Depth-to-RGB Weighting (DW) and RGB-to-RGB Weighting (RW) as key operations. Notably, the DW in CMW-L and CMW-M is performed in the cross-scale manner between two adjacent blocks, which effectively captures the feature continuity and activates cross-modal cross-scale interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>loc ) is a convolutional layer with parameters W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>loc are 3×3 kernel) for producing local features, C( * ; W (l1) glo ) is a convolutional layer with parameters W (l1) glo (i.e., W (l1) glo is 7×7 kernel), and C( * ; W (l2) glo ) is the dilated convolution with parameters W (l2) glo (i.e., W (l2)glo is 3×3 kernel with rate = 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>RGB conv2 2 2 Fig. 3 .</head><label>223</label><figDesc>Ours w/o DW w/o RW 3 * conv2 Visualizing features of RGB conv2 2 in CMW-L. w/o DW: without adding DW features; w/o RW: without adding RW features; 3 * conv2 2: features with triple linear enhancement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>WeightedFig. 4 .</head><label>4</label><figDesc>Quantitative comparisons on PR curve, F-measure curve and S λ -F w β coordinates. The top 5 methods on PR and F-measure curves are shown in color. For S λ -F w β coordinates, we only compare with several representative methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.787 .250 .695 .748 .803 .153 .736 .726 .804 .208 .703 .788 .890 .208 .762 .745 .855 .081 .621 .619 .736 .278 .727 .751 .853 .200 DCMC16 [9] .731 .740 .819 .148 .686 .715 .799 .172 .753 .817 .856 .155 .707 .666 .773 .111 .724 .648 .793 .117 .704 .711 .786 .169 .683 .618 .743 .186 SE16 [18] .708 .755 .846 .143 .664 .748 .813 .169 .698 .791 .840 .167 .741 .741 .856 .090 .756 .713 .847 .091 .675 .710 .800 .165 .628 .661 .771 .164 CDCP17 [45] .713 .664 .786 .149 .669 .621 .741 .180 .717 .703 .786 .167 .709 .631 .811 .115 .727 .645 .820 .112 .603 .535 .700 .214 .595 .505 .721 .224 MDSF17 [36] .728 .719 .809 .176 .748 .775 .838 .157 .700 .783 .826 .190 .741 .746 .851 .122 .805 .793 .885 .095 .673 .703 .779 .192 .717 .698 .798 .167 DTM19 [8] .747 .743 .837 .168 .706 .716 .799 .190 .783 .825 .853 .160 .752 .697 .858 .123 .733 .677 .833 .145 .677 .651 .773 .199 .690 .659 .778 .203 DF17 [31] .757 .757 .847 .141 .763 .804 .864 .141 .791 .817 .865 .138 .752 .766 .870 .093 .802 .778 .880 .085 .747 .735 .828 .142 .653 .657 .759 .185 CTMF18 [20] .848 .831 .912 .086 .849 .845 .913 .085 .796 .791 .865 .119 .863 .844 .932 .055 .860 .825 .929 .056 .776 .729 .865 .099 .716 .694 .829 .139 PCF18 [4] .875 .860 .925 .064 .877 .872 .924 .059 .794 .779 .835 .112 .842 .804 .893 .049 .874 .841 .925 .044 .841 .807 .894 .062 .842 .838 .901 .071 AFNet19 [38] .825 .823 .887 .075 .772 .775 .853 .100 .738 .744 .815 .133 .770 .728 .881 .068 .799 .771 .879 .058 .714 .687 .807 .118 .720 .712 .819 .118 MMCI19 [6] .873 .863 .927 .068 .858 .852 .915 .079 .787 .771 .839 .132 .848 .822 .928 .065 .856 .815 .913 .059 .813 .781 .882 .082 .833 .818 .897 .086 TANet19 [5] .871 .861 .923 .060 .878 .874 .925 .060 .801 .796 .847 .111 .858 .827 .910 .046 .886 .863 .941 .041 .839 .810 .897 .063 .835 .830 .895 .075 CPFP19 [42] .879 .874 .925 .051 .878 .877 .923 .053 .828 .826 .872 .088 .872 .846 .923 .038 .888 .867 .932 .036 .807 .766 .852 .082 .850 .851 .903 .064 DMRA19 [30] .835 .847 .911 .066 .886 .886 .927 .051 .847 .856 .900 .075 .900 .888 .943 .030 .899 .879 .947 .031 .857 .844 .906 .058 .806 .821 .875 .085 D3Net19 [13] .891 .881 .930 .054 .895 .889 .932 .051 .832 .819 .864 .099 .904 .885 .946 .030 .906 .885 .946 .034 .866 .847 .910 .058 .864 .862 .903 .063 Ours .905 .901 .944 .043 .903 .902 .936 .046 .876 .883 .912 .066 .934 .930 .969 .022 .917 .903 .951 .029 .875 .871 .930 .051 .867 .874 .913 .062</figDesc><table><row><cell></cell><cell cols="3">↑ Fβ ↑ Eξ ↑ M ↓ Sλ ↑ Fβ ↑ Eξ ↑ M ↓ Sλ ↑ Fβ ↑ Eξ ↑ M ↓ Sλ ↑ Fβ ↑ Eξ ↑ M ↓ Sλ ↑ Fβ ↑ Eξ ↑ M ↓ Sλ ↑ Fβ ↑ Eξ ↑ M ↓ Sλ ↑ Fβ ↑ Eξ ↑ M ↓</cell></row><row><cell>LBE16 [16]</cell><cell>0.66 0.72 0.78 .660 .633 0.78 0.84</cell><cell>0.83</cell><cell>0.88</cell></row><row><cell></cell><cell></cell><cell>Structure Similarity</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies on NJU2K<ref type="bibr" target="#b22">[23]</ref> and SSD<ref type="bibr" target="#b23">[24]</ref>. The best result of each column is bold. Details are introduced in § 4.3.</figDesc><table><row><cell>Models</cell><cell></cell><cell cols="2">NJU2K-T [23]</cell><cell></cell><cell></cell><cell cols="2">SSD [24]</cell><cell></cell></row><row><cell></cell><cell cols="8">Sλ ↑ Fβ ↑ Eξ ↑ M ↓ Sλ ↑ Fβ ↑ Eξ ↑ M ↓</cell></row><row><cell>Ours (DeR)</cell><cell cols="8">.903 .902 .936 .046 .875 .871 .930 .051</cell></row><row><cell>ReD</cell><cell>.889</cell><cell>.887</cell><cell>.927</cell><cell>.056</cell><cell>.864</cell><cell>.850</cell><cell>.909</cell><cell>.063</cell></row><row><cell>w/o depth (w/o DW)</cell><cell>.886</cell><cell>.886</cell><cell>.924</cell><cell>.056</cell><cell>.855</cell><cell>.842</cell><cell>.915</cell><cell>.064</cell></row><row><cell>w/o CMW-L&amp;M</cell><cell>.891</cell><cell>.886</cell><cell>.932</cell><cell>.053</cell><cell>.849</cell><cell>.839</cell><cell>.909</cell><cell>.066</cell></row><row><cell>w/o CMW-H</cell><cell>.896</cell><cell>.894</cell><cell>.929</cell><cell>.051</cell><cell>.853</cell><cell>.845</cell><cell>.908</cell><cell>.063</cell></row><row><cell>w/o RW</cell><cell>.901</cell><cell>.899</cell><cell>.933</cell><cell>.046</cell><cell>.868</cell><cell>.861</cell><cell>.919</cell><cell>.056</cell></row><row><cell>w/o Wei</cell><cell>.900</cell><cell>.898</cell><cell>.933</cell><cell>.048</cell><cell>.858</cell><cell>.839</cell><cell>.899</cell><cell>.061</cell></row><row><cell>DW w/o GF</cell><cell>.900</cell><cell>.898</cell><cell>.933</cell><cell>.048</cell><cell>.868</cell><cell>.858</cell><cell>.923</cell><cell>.054</cell></row><row><cell>RW w/ GF</cell><cell>.901</cell><cell>.900</cell><cell>.934</cell><cell>.046</cell><cell>.870</cell><cell>.867</cell><cell>.924</cell><cell>.052</cell></row><row><cell>w/o CS</cell><cell>.901</cell><cell>.898</cell><cell>.932</cell><cell>.047</cell><cell>.864</cell><cell>.861</cell><cell>.922</cell><cell>.060</cell></row><row><cell>C2S</cell><cell>.900</cell><cell>.899</cell><cell>.933</cell><cell>.049</cell><cell>.864</cell><cell>.847</cell><cell>.906</cell><cell>.060</cell></row><row><cell>w/o DS</cell><cell>.898</cell><cell>.898</cell><cell>.933</cell><cell>.049</cell><cell>.866</cell><cell>.862</cell><cell>.923</cell><cell>.055</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported by the National Natural Science Foundation of China under Grant 61771301. Linwei Ye and Yang Wang were supported by NSERC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="150" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>COMPSTAT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM ICIMCS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Going from RGB to RGBD saliency: A depth-guided transformation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/pdf_IBM/10.1109/TCYB.2019.2932005</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TCYB</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth-aware saliency detection using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representatio</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06781</idno>
		<title level="m">Rethinking RGB-D salient object detection: Models, datasets, and largescale benchmarks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Salient region detection for stereoscopic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE DSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narwaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2625" to="2636" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Local background enclosure for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccarthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Salient object detection for RGB-D image via saliency evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Salient object detection in RGB-D image based on saliency fusion and propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM ICIMCS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CNNs-based RGB-D saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCYB</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3171" to="3183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICIP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A three-pathway psychobiological framework of salient object detection using stereoscopic technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCVW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Salient object detection for RGB-D image by single stream recurrent convolution neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">RGBD salient object detection: A benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RGBD salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Exploiting global priors for RGB-D saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning RGB-D salient object detection using background enclosure, depth contrast, and top-down features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shigematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Saliency detection for RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM ICIMCS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4204" to="4216" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">RGB-D salient object detection via minimum barrier distance transform and saliency fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="663" to="667" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive fusion for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="55277" to="55284" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09146</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Contrast prior and fluid pyramid integration for RGBD salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Global and local sensitivity guided key salient object re-augmentation for video saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07480</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">PDNet: Prior-model guided depthenhanced network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">An innovative salient object detection using center-dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
