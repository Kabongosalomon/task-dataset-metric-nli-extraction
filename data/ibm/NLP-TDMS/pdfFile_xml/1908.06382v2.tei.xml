<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RankSRGAN: Generative Adversarial Networks with Ranker for Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Zhang</surname></persName>
							<email>wl.zhang1@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<email>chao.dong@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RankSRGAN: Generative Adversarial Networks with Ranker for Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of perceptual metrics. Specifically, we first train a Ranker which can learn the behavior of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics. Project page: https://wenlongzhang0724.github.io/ Projects/RankSRGAN</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image super resolution aims at reconstructing/generating a high-resolution (HR) image from a lowresolution (LR) observation. Thanks to the strong learning capability, Convolutional Neural Networks (CNNs) have demonstrated superior performance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b44">42</ref>] to the conventional example-based <ref type="bibr" target="#b42">[40]</ref> and interpolation-based <ref type="bibr" target="#b43">[41]</ref> algorithms. Recent CNN-based methods can be divided into two groups. The first one regards SR as a reconstruction problem and adopts MSE as the loss function to achieve high PSNR values. However, due to the conflict between the reconstruction accuracy and visual quality, they tend to produce overly smoothed/sharpened images. To favor better visual quality, the second group casts SR as an image generation problem <ref type="bibr" target="#b23">[22]</ref>. By incorporating the perceptual loss <ref type="bibr">[6,</ref><ref type="bibr" target="#b18">18]</ref> and adversarial learning <ref type="bibr" target="#b23">[22]</ref>, these perceptual SR methods have potential to generate realistic textures and details, thus attracted increasing attention in recent years.</p><p>The most challenging problem faced with perceptual SR methods is the evaluation. Most related works resort to user study for subjectively evaluating the visual quality <ref type="bibr">[2,</ref><ref type="bibr" target="#b35">34]</ref>. However, without an objective metric like PSNR/SSIM, it is hard to compare different algorithms on a fair platform, which largely prevents them from rapid development. To address this issue, a number of no-reference image quality assessment (NR-IQA) metrics are proposed, and some of them are proven to be highly correlated with human ratings <ref type="bibr">[2]</ref>, such as NIQE <ref type="bibr" target="#b30">[29]</ref> (correlation 0.76) and PI <ref type="bibr">[2]</ref> (correlation 0.83). Specially, the PIRM2018-SR challenge <ref type="bibr">[2]</ref> introduced the PI metric as perceptual criteria and successfully ranked the entries. Nevertheless, most of these NR-IQA metrics are not differentiable (e.g., they include handcrafted feature extraction or statistic regression operation), making them infeasible to serve as loss functions. Without considering NR-IQA metrics in optimization, existing perceptual SR methods could not show stable performance in the orientation of objective perceptual criteria.</p><p>To overcome this obstacle, we propose a general and differentiable model -Ranker, which can mimic any NR-IQA metric and provide a clear goal (as loss function) for optimizing perceptual quality. Specifically, Ranker is a Siamese CNN that simulates the behavior of the perceptual metric by learning to rank approach <ref type="bibr">[7]</ref>. Notably, as NR-IQA metrics have various dynamic ranges, Ranker learns their output ranking orders instead of absolute values. Just like in the real world, people tend to rank the quality of images rather than give a specific value. We equip Ranker with the standard SRGAN model and form a new perceptual SR framework -RankSRGAN (Super-Resolution Generative Adversarial Networks with Ranker). In addition to SRGAN, the proposed framework has a rank-content loss using a welltrained Ranker to measure the output image quality. Then the SR model can be stably optimized in the orientation of specific perceptual metrics.</p><p>To train the proposed Ranker, we prepare another training dataset by labeling the outputs of different SR algorithms. Then the Ranker, with a Siamese-like architecture, could learn these ranking orders with high accuracy.</p><p>The effectiveness of the Ranker is largely determined by the selected SR algorithms. To achieve the best performance, we adopt two state-of-the-art perceptual SR models -SRGAN <ref type="bibr" target="#b23">[22]</ref> and ESRGAN <ref type="bibr" target="#b36">[35]</ref>. As the champion of PIRM2018-SR challenge <ref type="bibr">[2]</ref>, ESRGAN is superior to SR-GAN on average scores, but can not outperform SRGAN on all test images. When evaluating with NIQE <ref type="bibr" target="#b30">[29]</ref>, we obtain mixed orders for these two methods. Then the Ranker will favor different algorithms on different images, rather than simply classifying an image into a binary class (SR-GAN/ESRGAN). After adopting the rank-content loss, the generative network will output results with higher ranking scores. In other words, the learned SR model could combine the better parts of SRGAN and ESRGAN, and achieve superior performance both in perceptual metric and visual quality. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of RankSRGAN, which fuses the imagery effects of SRGAN and ESRGAN and obtains better NIQE score.</p><p>We have done comprehensive ablation studies to further validate the effectiveness of the proposed method. First, we distinguish our Ranker from the regression/classification network that could also mimic the perceptual metric. Then, we train and test RankSRGAN with several perceptual metrics (i.e. NIQE <ref type="bibr" target="#b30">[29]</ref>, Ma <ref type="bibr" target="#b27">[26]</ref>, PI <ref type="bibr">[2]</ref>). We further show that adopting different SR algorithms to build the dataset achieves different performance. Besides, we have also investigated the effect of different loss designs and combinations. With proper formulation, our method can clearly surpass ESRGAN and achieve state-of-the-art performance.</p><p>In summary, the contributions of this paper are threefold. (1) We propose a general perceptual SR framework -RankSRGAN that can optimize generator in the direction of indifferentiable perceptual metrics and achieve the state-ofthe-art performance. (2) We, for the first time, utilize results of other SR methods to build training dataset. The proposed method combines the strengths of different SR methods and generates better results. (3) The proposed SR framework is highly flexible and produce diverse results given different rank datasets, perceptual metrics, and loss combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Super resolution. Since Dong et al. <ref type="bibr" target="#b9">[10]</ref> first introduced convolutional neural networks (CNNs) to the SR task, a series of learning-based works <ref type="bibr" target="#b42">[40,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref> have achieved great improvements in terms of PSNR. For example, <ref type="bibr">Kim et al. [20]</ref> propose a deep network VDSR with gradient clipping. The residual and dense block <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b44">42]</ref> are explored to improve the super-resolved results. In addition, SRGAN <ref type="bibr" target="#b23">[22]</ref> is proposed to generate more realistic images. Then, texture matching <ref type="bibr" target="#b32">[31]</ref> and semantic prior <ref type="bibr" target="#b35">[34]</ref> are introduced to improve perceptual quality. Furthermore, the perceptual index <ref type="bibr">[2]</ref> consisting of NIQE <ref type="bibr" target="#b30">[29]</ref> and Ma <ref type="bibr" target="#b27">[26]</ref> is adopted to measure the perceptual SR methods in the PIRM2018-SR Challenge at ECCV <ref type="bibr">[2]</ref>. In the Challenge, ESRGAN <ref type="bibr" target="#b36">[35]</ref> achieves the state-of-the-art performance by improving network architecture and loss functions.</p><p>CNN for NR-IQA. No-reference Image Quality Assessment (NR-IQA) can be implemented by learning-based models, which extract hand-crafted features from Natural Scene Statistics (NSS), such as CBIQ <ref type="bibr" target="#b38">[37]</ref>, NIQE <ref type="bibr" target="#b30">[29]</ref>, and Ma <ref type="bibr" target="#b27">[26]</ref>, etc. In <ref type="bibr" target="#b24">[23]</ref>, Li et al. develop a general regression neural network to fit human subjective opinion scores with pre-extracted features. Kang et al. <ref type="bibr" target="#b20">[19,</ref><ref type="bibr">4]</ref> integrate a general CNN framework which can predict image quality on local regions. In addition, Liu et al. <ref type="bibr" target="#b26">[25]</ref> propose RankIQA to tackle the problem of lacking human-annotated data in NR-IQA. They first generate large distorted images in different distortion level. Then they train a Siamese Network to learn the rank of the quality of those images, which can improve the performance of the image quality scores.</p><p>Learning to rank. It has been demonstrated that learning to rank approach is effective in computer vision. For instance, Devi Parikh et al. <ref type="bibr" target="#b31">[30]</ref> model relative attributes using a well-learned ranking function. Yang et al. <ref type="bibr" target="#b37">[36]</ref> first employ CNN for relative attribute ranking in a unified framework. One of the most relevant studies to our work is RankCGAN <ref type="bibr" target="#b33">[32]</ref>, which investigates the use of GAN to tackle the task of image generation with semantic attributes. Unlike standard GANs that generate the image from noise input (CGAN <ref type="bibr" target="#b29">[28]</ref>), RankCGAN incorporates a pairwise Ranker into CGAN architecture so that it can handle continuous attribute values with subjective measures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of RankSRGAN</head><p>The proposed framework is built upon the GAN-based <ref type="bibr" target="#b23">[22]</ref> SR approach, which consists of a generator and a discriminator. The discriminator network tries to distinguish the ground-truth images from the super-resolved results, while the generator network is trained to fool the discriminator. To obtain more natural textures, we propose to add additional constraints on the standard SRGAN <ref type="bibr" target="#b23">[22]</ref> by exploiting the prior knowledge of perceptual metrics to improve the visual quality of output images. The overall framework of our approach is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. The pipeline involves the following three stages:</p><p>Stage 1: Generate pair-wise rank images. First, we employ different SR methods to generate super-resolved images on public SR datasets. Then we apply a chosen perceptual metric (e.g. NIQE) on the generated images. After that, we can pick up two images of the same content to form a pair and rank the pair-wise images according to the quality score calculated by the perceptual metric. Finally, we obtain the pair-wise images and the associated ranking labels. More details will be presented in Section 4.1.</p><p>Stage 2: Train Ranker. The Ranker adopts a Siamese architecture to learn the beheviour of perceptual metrics and the network structure is depicted in Section 3.2. We adopt margin-ranking loss, which is commonly used in "learning to rank" <ref type="bibr">[7]</ref>, as the cost function to optimize Ranker. The learned Ranker is supposed to have the ability to rank images according to their perceptual scores.</p><p>Stage 3: Introduce rank-content loss. Once the Ranker is well-trained, we use it to define a rank-content loss for a standard SRGAN to generate visually pleasing images. Please see the rank-content loss in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ranker</head><p>Rank dataset. Similar to <ref type="bibr">[8,</ref><ref type="bibr" target="#b26">25]</ref>, we use superresolution results of different SR methods to represent different perceptual levels. With a given perceptual metric, we can rank these results in a pair-wise manner. Picking any two SR images, we can get their ranking order according to the quality score measured by the perpetual metric. These pair-wise data with ranking labels form a new dataset, which is defined as the rank dataset. Then we let the proposed Ranker learn the ranking orders. Specifically, given two input images y 1 , y 2 , the ranking scores s 1 and s 2 can be obtained by</p><formula xml:id="formula_0">s1 = R(y1; ΘR) (1) s2 = R(y2; ΘR),<label>(2)</label></formula><p>where Θ R represents the network weights and R(.) indicates the mapping function of Ranker. In order to make the Ranker output similar ranking orders as the perceptual metric, we can formulate:</p><formula xml:id="formula_1">s1 &lt; s2 if my 1 &lt; my 2 s1 &gt; s2 if my 1 &gt; my 2 ,<label>(3)</label></formula><p>where m y1 and m y2 represent the quality scores of image y 1 and image y 2 , respectively. A well-trained Ranker could guide the SR model to be optimized in the orientation of the given perceptual metric. Siamese architecture. The Ranker uses a Siamese-like architecture <ref type="bibr">[5,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b39">38]</ref>, which is effective for pair-wise inputs. The architecture of Ranker is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. It has two identical network branches which contain a series of convolutional, LeakyReLU, pooling and full-connected layers. Here we use a Global Average Pooling layer after the Feature Extractor, thus the architecture can get rid of the limit of input size. To obtain the ranking scores, we employ a fully-connected layer as a regressor to quantify the rank results. Note that we do not aim to predict the real values of the perceptual metric since we only care about the ranking information. Finally, the outputs of two branches are passed to the margin-ranking loss module, where we can compute the gradients and apply back-propagation to update parameters of the whole network.</p><p>Optimization. To train Ranker, we employ marginranking loss that is commonly used in sorting problems <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b26">25]</ref>. The margin-ranking loss is given below:</p><formula xml:id="formula_2">L(s1,s2; γ) = max(0, (s1 − s2) * γ + ε) γ = −1 if my 1 &lt; my 2 γ = 1 if my 1 &gt; my 2 ,<label>(4)</label></formula><p>where the s 1 and s 2 represent the ranking scores of pairwise images. The γ is the rank label of the pair-wise training images. The margin ε can control the distance between s 1 and s 2 . Therefore, the N pair-wise training images can be optimized by:</p><formula xml:id="formula_3">Θ = arg min Θ R 1 N N i=1 L(s (i) 1 , s (i) 2 ; γ (i) ) = arg min Θ R 1 N N i=1 L(R(y (i) 1 ; ΘR), R(y (i) 2 ; ΘR); γ (i) )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">RankSRGAN</head><p>RankSRGAN consists of a standard SRGAN and the proposed Ranker, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Compared with existing SRGAN, our framework simply adds a well-trained Ranker to constrain the generator in SR space. To obtain visually pleasing super-resolved results, adversarial learning <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b32">31]</ref> is applied to our framework where the generator and discriminator are jointly optimized with the objective given below:</p><formula xml:id="formula_4">min θ max η Ey p HR logDη(y) + Ey p LR log(1 − Dη(G θ (x))),<label>(6)</label></formula><p>where p HR and p LR represent the probability distributions of HR and LR samples, respectively. In order to demonstrate the effectiveness of the proposed Ranker, we do not use complex architectural designs of GAN <ref type="bibr" target="#b36">[35]</ref> but use the general SRGAN <ref type="bibr" target="#b23">[22]</ref>. Perceptual loss. In <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b18">18]</ref>, the perceptual loss is proposed to measure the perceptual similarity between two images. Instead of computing distances in image pixel space, the images are first mapped into feature space and the perceptual loss can be presented as:</p><formula xml:id="formula_5">LP = i ||φ(ŷi) − φ(yi)|| 2 2 ,<label>(7)</label></formula><p>where φ(y i ) and φ(ŷ i ) represent the feature maps of HR and SR images, respectively. Here φ is obtained by the 5-th con-volution (before maxpooling) layer within VGG19 network <ref type="bibr" target="#b34">[33]</ref>. Adversarial loss. Adversarial training <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b32">31]</ref> is recently used to produce natural-looking images. A discriminator is trained to distinguish the real image from the generated image. This is a minimax game approach where the generator loss L G is defined based on the output of discriminator:</p><formula xml:id="formula_6">LG = −logD(G(xi)),<label>(8)</label></formula><p>where x i is the LR image, D(G(x i )) represents the probability of the discriminator over all training samples.</p><p>Rank-content loss. The generated image is put into the Ranker to predict the ranking score. Then, the rank-content loss can be defined as:</p><formula xml:id="formula_7">L R = sigmoid(R(G(x i ))),<label>(9)</label></formula><p>where R(G(x i )) is the ranking score of the generated image. A lower ranking score indicates better perceptual quality. After applying the sigmoid function, L R represents ranking-content loss ranging from 0 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Analysis of Ranker</head><p>The proposed Ranker possesses an appealing property: by elaborately selecting the SR algorithms and the perceptual metric, the RankSRGAN has the potential to surpass the upper bound of these methods and achieve superior performance. To validate this comment, we select the stateof-the-art perceptual SR methods -SRGAN <ref type="bibr" target="#b23">[22]</ref> and ESR-GAN <ref type="bibr" target="#b36">[35]</ref> to build the rank dataset. Then we use the perceptual metric NIQE <ref type="bibr" target="#b30">[29]</ref> for evaluation. NIQE is demonstrated to be highly correlated with human ratings and easy to implement. A lower NIQE value indicates better perceptual quality. When measured with NIQE on the PIRM-Test <ref type="bibr">[2]</ref> dataset, the average scores of SRGAN and ESRGAN are 2.70 and 2.55, respectively. ESRGAN obtains better NIQE scores for most images but not all images, indicating that SRGAN and ESRGAN have mixed ranking orders with NIQE.</p><p>In order to examine the effectiveness of our proposed Ranker, we compare two ranking strategies -metric rank and model classification. Metric rank, which is our proposed method, uses perceptual metrics to rank the images. For example, in each image pair, the one with a lower score is labeled to 1 and the other is 2. The model classification, as the comparison method, ranks images according to the used SR methods, i.e., all results of ESRGAN are labeled to 1 and those of SRGAN are labeled to 2. We then give an analysis of the upper bound of these two methods. The upper bound can be calculated as:   (Perceptual Metric) is the perceptual score for each image in the corresponding class. (SR1, SR2) represents two SR results of the same LR. Subscripts −L and −H indicate the Lower and Higher perceptual score in (SR1, SR2). We use (SRGAN, ESRGAN) as (SR1, SR2) to obtain the upper bound of these methods, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Obviously, metric rank could combine the better parts of different algorithms and exceed the upper bound of a single algorithm. We further conduct SR experiments to support the above analysis. We use the metric rank and model classification approach to label the rank dataset. Then the Ranker-MC (model classification) and Ranker-MR (metric rank) are used to train separate RankSRGAN models. <ref type="figure" target="#fig_4">Figure 4</ref> shows the quantitative results (NIQE), where RankSRGAN-MR outperforms ESRGAN and RankSRGAN-MC. This demonstrates that our method can exceed the upper bound of all chosen SR algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training details of Ranker</head><p>Datasets. We use DIV2K (800 images) <ref type="bibr">[1]</ref> and Flickr2K (2650 images) <ref type="bibr">[1]</ref> datasets to generate pair-wise images as rank dataset for training. Three different SR algorithms (SRResNet <ref type="bibr" target="#b23">[22]</ref>, SRGAN <ref type="bibr" target="#b23">[22]</ref> and ESRGAN <ref type="bibr" target="#b36">[35]</ref>) are used to generate super-resolved images as three perceptual levels, as shown in <ref type="table" target="#tab_1">Table 1</ref> We extract patches from those pair-wise images with a stride of 200 and size of 296 × 296. For one perceptual level (SR algorithm), we can generate 150 K patches (10% for validation, 90% for training). Inspired by PIRM2018-SR Challenge <ref type="bibr">[2]</ref>, we use NIQE <ref type="bibr" target="#b30">[29]</ref> as the perceptual metric, while other metrics will be investigated in Section 4.4. Finally, we label every image pair to (3,2,1) according to the order of corresponding NIQE value (the one with the best NIQE value is set to 1). Implementation details. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we utilize VGG <ref type="bibr" target="#b34">[33]</ref> structure to implement the Ranker <ref type="bibr" target="#b26">[25]</ref>, which includes 10 convolutional layers, a series of batch  normalization and LeakyReLU operations. Instead of maxpooling, we apply convolutional layer with a kernel size 4 and stride 2 to downsample the features. In one iteration, two patches with different perceptual levels are randomly selected as the input of Ranker. For optimization, we use Adam <ref type="bibr" target="#b22">[21]</ref> optimizer with weight decay 1 × 10 −4 . The learning rate is initialized to 1 × 10 −3 and decreases with a factor 0.5 of every 10 × 10 4 iterations for total 30 × 10 4 iterations. The margin of margin-ranking loss is set to 0.5. For weight initialization, we use He. <ref type="bibr" target="#b17">[17]</ref> method to initialize the weights of Ranker.</p><p>Evaluation. The Spearman Rank Order Correlation Coefficient (SROCC) <ref type="bibr" target="#b26">[25]</ref> is a traditional evaluation metric to evaluate the performance of image quality assessment algorithms. In our experiment, SROCC is employed to measure the monotonic relationship between the label and the ranking score. Given N images, the SROCC is computed as:</p><formula xml:id="formula_9">SROCC = 1 − 6 N i=1 (yi −ŷi) 2 N (N 2 − 1) ,<label>(11)</label></formula><p>where y i represents the order of label, andŷ i is the order of output score of the Ranker. SROCC has the ability to measure the accuracy of Ranker. The larger value of SROCC represents the better accuracy of Ranker. For validation dataset, the ranker achieves a SROCC of 0.88, which is an adequate performance compared with those in the related work <ref type="bibr">[8,</ref><ref type="bibr" target="#b26">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training details of RankSRGAN</head><p>We use the DIV2K <ref type="bibr">[1]</ref> dataset to train RankSRGAN. The patch sizes of HR and LR are set to 296 and 74, respectively. For testing, we use benchmark datasets Set14 <ref type="bibr" target="#b40">[39]</ref>, BSD100 <ref type="bibr" target="#b28">[27]</ref> and PIRM-test <ref type="bibr">[2]</ref>. PIRM-test is used to measure the perceptual quality of SR methods in PIRM2018-SR <ref type="bibr">[2]</ref>. Following the settings of SRGAN <ref type="bibr" target="#b23">[22]</ref>, we employ a standard SRGAN <ref type="bibr" target="#b23">[22]</ref> as our base model. The generator is built with 16 residual blocks, and the batch-normalization layers are removed <ref type="bibr" target="#b36">[35]</ref>. The discriminator utilizes the VGG network <ref type="bibr" target="#b34">[33]</ref> with ten convolutional layers. The mini-batch size is set to 8. At each training step, the combination of loss functions (Section 3.3) for the generator is:</p><formula xml:id="formula_10">L total = L P + 0.005L G + 0.03L R ,<label>(12)</label></formula><p>where the weights of L G and L R are determined empirically to obtain high perceptual improvement <ref type="bibr">[8,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b36">35]</ref>.  Adam <ref type="bibr" target="#b22">[21]</ref> optimization method with β 1 = 0.9 is used for training. For generator and discriminator, the initial learning rate is set to 1 × 10 −4 which is reduced by a half for multi-step [50 × 10 3 , 100 × 10 3 , 200 × 10 3 , 300 × 10 3 ]. A total of 600 × 10 3 iterations are executed by PyTorch. In training procedure, we add Ranker to the standard SRGAN. The Ranker takes some time to predict the ranking score, thus the traning time is a little slower (about 1.18 times) than standard SRGAN <ref type="bibr" target="#b23">[22]</ref>. For the generator, the number of parameters remains the same as SRGAN <ref type="bibr" target="#b23">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the-state-of-the-arts</head><p>We compare the performance of the proposed method with the state-of-the-art perceptual SR methods ESRGAN <ref type="bibr" target="#b36">[35]</ref>/ SRGAN <ref type="bibr" target="#b23">[22]</ref> and the PSNR-orientated methods FS-RCNN <ref type="bibr" target="#b10">[11]</ref> and SRResNet <ref type="bibr" target="#b23">[22]</ref> 1 . The evaluation metrics include NIQE <ref type="bibr" target="#b30">[29]</ref>, PI <ref type="bibr">[2]</ref> and PSNR. <ref type="table">Table 2</ref> shows their performance on three test datasets -Set14, BSD100 and PIRM-Test. Note that lower NIQE/PI indicates better visual quality. When comparing our method with SRGAN and ES-RGAN, we find that RankSRGAN achieves the best NIQE and PI performance on all test sets. Furthermore, the improvement of perceptual scores does not come at the price of PSNR. Note that in PIRM-Test, RankSRGAN also obtains the highest PSNR values among perceptual SR methods. <ref type="figure" target="#fig_5">Figure 5</ref> shows some visual examples, where we observe that our method could generate more realistic textures without introducing additional artifacts (please see the win-  As the results may vary across different iterations, we further show the convergence curves of RankSRGAN in <ref type="figure" target="#fig_6">Figure 6</ref>. Their performance on NIQE and PSNR are relatively stable during the training process. For PSNR, they obtain comparable results. But for NIQE, RankSRGAN is consistently better than SRGAN by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>Effect of different rank datasets. The key factor that influences the performance of Ranker is the choice of SR algorithms. In the main experiments, we use (SRResNet, SR-GAN, ESRGAN) to generate the rank dataset. Then what if we select other SR algorithms? Will we always obtain better results than SRGAN? To answer the question, we first analyze the reason of using these three algorithms, then conduct another experiment using a different combination.</p><p>As our baseline model is SRGAN, we need the Ranker to have the ability to rank the outputs of SRGAN. Since the training of SRGAN starts from the pre-trained model SR-ResNet, the Ranker should recognize the results between SRResNet and SRGAN. That is the reason why we choose SRResNet and SRGAN. Then the next step is to find a better algorithm that could guide the model to achieve better results. We choose ESRGAN as it surpasses SRGAN by a large margin in the PIRM-SR2018 challenge <ref type="bibr">[2]</ref>. Therefore, we believe that a better algorithm than SRGAN could always lead to better performance. <ref type="bibr">Method</ref>  To validate this comment, we directly use the ground truth HR as the third algorithm, which is the extreme case. We still apply NIQE for evaluation. Interestingly, although HR images have infinite PSNR values, they cannot surpass all the results of SRGAN on NIQE. Similar to ESR-GAN, HR and SRGAN have mixed ranking orders. We train our Ranker with (SRResNet, SRGAN, HR) and obtain the new SR model -RankSRGAN-HR. <ref type="table" target="#tab_5">Table 3</ref> compares its results with SRGAN and RankSRGAN. As expected, RankSRGAN-HR achieves better NIQE values than SRGAN. But at the same time, RankSRGAN-HR also im-proves the PSNR by almost 0.4 dB. It achieves a good balance between the perceptual metric and PSNR. This also indicates that the model could always have an improvement space as long as we have better algorithms for guidance.</p><p>Effect of different perceptual metrics. As we claim that Ranker can guide the SR model to be optimized in the direction of perceptual metrics, we need to verify whether it works for other perceptual metrics. We choose Ma <ref type="bibr" target="#b28">[27]</ref> and PI <ref type="bibr">[2]</ref>, which show high correlation with Mean-Opinion-Score (Ma: 0.61, PI: 0.83) in <ref type="bibr">[2]</ref>. We use Ma and PI as the evaluation metric to generate the rank dataset. All other settings remain the same as RankSRGAN with NIQE. The only difference in these experiments is the ranking labels in the rank dataset. The results are summarized in <ref type="table" target="#tab_6">Table  4</ref>, where we observe that the Ranker could help RankSR-GAN achieve the best performance in the chosen metric. This shows that our method can generalize well on different perceptual metrics. Effect of Ranker: Rank VS. Regression. To train our Ranker, we choose to use the ranking orders instead of the real values of the perceptual metric. Actually, we can also let the network directly learn the real values. In <ref type="bibr">[8]</ref>, Choi et al. use a regression network to predict a subjective score for a given image and define a corresponding subjective score loss. To compare these two strategies, we train a "regression" Ranker with MSE loss instead of the margin-ranking loss. The labels in the rank dataset are real values of the perceptual metric. We use NIQE and Ma to generate the labels of rank dataset. All the other settings remain the same as RankSRGAN. Theoretically, the real values of perceptual metrics may distribute unevenly among different algorithms. For example, SRGAN and ESRGAN are very close to each other on NIQE values. This presents a difficulty for the learning of regression. On the contrary, learning ranking orders can simply ignore these variances. In experiments, we first measure the distances between the outputs of SRGAN and ESRGAN with different strategies. <ref type="table" target="#tab_7">Table 5</ref> shows the mean absolute distances of these two strategies. Obviously, results with rank have larger distances than results with regression. When applying these Rankers in SR training, the rank strategy achieves better performance than the regression strategy on the selected perceptual metric. Results are shown in <ref type="table">Table 6</ref>. Effect of different losses. To test the effects of rankcontent loss, we expect to add MSE loss to achieve improvement in PSNR. <ref type="table">Table 7</ref> shows the performance of our method trained with the combination of loss functions. As expected, increasing the contribution of the MSE loss with the larger α results in higher PSNR values. On the other hand, the NIQE values are increased which is a tradeoff between PSNR and NIQE as mentioned in <ref type="bibr">[3]</ref>, and our method has a capability to deal with the priorities by adjusting the weights of the loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">User study</head><p>To demonstrate the effectiveness and superiority of RankSRGAN, we conduct a user study against state-of-theart models, i.e. SRGAN <ref type="bibr" target="#b23">[22]</ref> and ESRGAN <ref type="bibr" target="#b36">[35]</ref>. In the first session, two different SR images are shown at the same time where one is generated by the proposed RankSRGAN and the other is generated by SRGAN or ESRGAN. The participants are required to pick the image that is more visually pleasant (more natural and realistic). We use the PIRM-Test <ref type="bibr">[2]</ref> dataset as the testing dataset. There are a total of 100 images, from which 30 images are randomly selected for each participant. To make a better comparison, one small patch from the image is zoomed in. In the second session,  <ref type="figure">Figure 7</ref>. The results of user studies, comparing our method with SRGAN <ref type="bibr" target="#b23">[22]</ref> and ESRGAN <ref type="bibr" target="#b36">[35]</ref>.</p><p>we focus on the perceptual quality of different typical SR methods in a sorting manner. The participants are asked to rank 4 versions of each image: SRResNet <ref type="bibr" target="#b23">[22]</ref>, ESRGAN <ref type="bibr" target="#b36">[35]</ref>, RankSRGAN, and the Ground Truth (GT) image according to their visual qualities. Similar to the first session, 20 images are randomly shown for each participant. There are totally 30 participants to finish the user study. As suggested in <ref type="figure">Figure 7</ref>, RankSRGAN has achieved better visual performance against ESRGAN and SRGAN. Since RankSRGAN consists of a base model SRGAN and the proposed Ranker, it can naturally inherit the characteristics of SRGAN and achieve better performance in perceptual metric. Thus, RankSRGAN performs more similar to SRGAN than ESRGAN. <ref type="figure">Figure 8</ref> shows the ranking results of different SR methods. As RankSRGAN has the best performance in perceptual metric, the ranking results of RankSRGAN are second to GT images, but sometimes it even produces images comparable to GT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>For perceptual super-resolution, we propose RankSR-GAN to optimize SR model in the orientation of perceptual metrics. The key idea is introducing a Ranker to learn the behavior of the perceptual metrics by learning to rank approach. Moreover, our proposed method can combine the strengths of different SR methods and generate better results. Extensive experiments well demonstrate that our RankSRGAN is a flexible framework, which can achieve superiority over state-of-the-art methods in perceptual metric and have the ability to recover more realistic textures. To analyze the effects of Ranker on RankSRGAN, we use SRResNet <ref type="bibr">[3]</ref>, SRGAN <ref type="bibr">[3]</ref> and ESRGAN <ref type="bibr">[9]</ref> to generate two rank datasets with different sizes. We first employ DIV2K <ref type="bibr">[1]</ref> to generate rank dataset1 with 15 K image pairs. Besides, we use DIV2K+Flickr2k <ref type="bibr">[1]</ref> to generate rank dataset2 with 150 K image pairs. Then, we utilize rank dataset1 and rank dataset2 to train Ranker1 and Ranker2, respectively. Finally, the well-trained Ranker1 and Ranker2 are applied on RankSRGAN. <ref type="table" target="#tab_1">Table 1</ref> shows that more data leads to better SROCC, and the Ranker2 with higher SROCC can reach better performance in NIQE and PSNR. The convergence curves are shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Details of Ranker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Output Distribution</head><p>In <ref type="table" target="#tab_7">Table 5</ref> and 6 of the main paper, we quantitatively evaluate the effects of Ranker and "regression" Ranker on RankSR-GAN. To better understand the effects, we provide the histograms of NIQE [6]/Ma <ref type="bibr">[4]</ref> label value in the validation dataset of the rank dataset. Furthermore, we plot the histograms of the output scores of different Rankers ("regression" Ranker and our Ranker) in <ref type="figure" target="#fig_3">Figure 3</ref>. Comparing <ref type="figure" target="#fig_3">Figure 3</ref> (b) and (c), Ranker successfully enlarges the distance between SRGAN and ESRGAN. The "regression" Ranker tends to learn the distribution of the NIQE label, while the NIQE values of SRGAN are close ESRGAN. The same observation is also found in Ma metric as shown in <ref type="figure" target="#fig_4">Figure 4</ref>. <ref type="figure" target="#fig_3">Figure 3</ref>. The histograms of (a) NIQE label value, (b) the regression score of "regression" Ranker and (c) the ranking score of Ranker (ours). These graphs illustrate that Ranker (ours) successfully managed to separate the different perceptual levels. (Better view in color version) <ref type="figure" target="#fig_4">Figure 4</ref>. The histograms of (a) Ma label value, (b) the regression score of "regression" Ranker and (c) the ranking score of Ranker (ours). These graphs illustrate that Ranker (ours) successfully managed to separate the different perceptual levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Details of RankSRGAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convergence curves for RankSRGAN-(NIQE,Ma, and PI)</head><p>As mentioned in the main paper, Ranker can guide the SR model to be optimized in the direction of perceptual metrics. We further present the curves showing that our RankSRGAN can achieve a constant improvement compared with the baseline SRGAN. We provide the curves of RankSRGAN-N, RankSRGAN-M, and RankSRGAN-PI (N: Ranker with NIQE <ref type="bibr">[6]</ref>, M: Ranker with Ma <ref type="bibr">[4]</ref>, and PI: Ranker with PI [2]) in <ref type="figure" target="#fig_5">Figure 5</ref>. Besides, we add the curves of RankSRGAN-N-re and RankSRGAN-M-re (re: "regression" Ranker). We observe that Ranker could help RankSRGAN achieve state-of-the-art performance in the chosen metric. This shows that our method can generalize well on different perceptual metrics. Compared with "regression" Ranker, Ranker can accelerate the convergence of RankSRGAN-N. For RankSRGAN-M, Ranker can still reach state-of-the-art performance (less than 1.40 in ESRGAN), while the "regression" Ranker cannot outperform ESRGAN (1.40). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Convergence curves for RankSRGAN-HR</head><p>As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, we present the curves of RankSRGAN and RankSRGAN-HR. To improve the performance of NIQE evaluation, we use (SRResNet, SRGAN, ESRGAN) to generate rank dataset to train Ranker in RankSRGAN. <ref type="figure" target="#fig_6">Figure  6</ref> shows that RankSRGAN is consistently better than SRGAN by a large margin. Furthermore, we directly use the ground truth HR to replace ESRGAN . We train our Ranker with the rank dataset (SRResNet, SRGAN, HR) and obtain the new model RankSRGAN-HR. In <ref type="figure" target="#fig_6">Figure 6</ref>, RankSRGAN-HR achieves better NIQE values than SRGAN. But at the same time, RankSRGAN-HR also constantly improves the PSNR. It achieves a good balance between the perceptual metric and PSNR. <ref type="figure" target="#fig_6">Figure 6</ref>. The convergence curves of RankSRGAN-HR in PSNR and NIQE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The comparison of RankSRGAN and the state-of-theart perceptual SR methods on ×4. NIQE: lower is better. PSNR: higher is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed method. Stage 1: Generate pair-wise rank images by different SR models in the orientation of perceptual metrics. Stage 2: Train Siamese-like Ranker network. Stage 3: Introduce rank-content loss derived from well-trained Ranker to guide GAN training. RankSRGAN consists of a generator(G), discriminator(D), a fixed Feature extractor(F) and Ranker(R).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>U</head><label></label><figDesc>BMC = M ean(P MSR2−L + P MSR2−H ) U BMR = M ean(P MSR2−L + P MSR1−L) where : P MSR1−L &lt; P MSR2−H ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The upper bound (average NIQE value) of SRGAN, ESRGAN, model rank and model classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The NIQE of RankSRGAN-MR exceeds that of SR-GAN, ESRGAN and RankSRGAN-MC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Visual results comparison of our model with other works on ×4 super-resolution. Lower NIQE value indicates better perceptual quality, while higher PSNR indicates less distortion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Convergence curves of RankSRGAN in PSNR/ NIQE. dows in Img 233 and feathers in Img 242).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>The convergence curves of RankSRGANN , RankSRGANM and RankSRGANPI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where U B M C and U B M R represent the upper bound of model classification and metric rank, respectively. P M</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>2.47</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NIQE</cell><cell></cell><cell></cell><cell></cell><cell>2.55 2.55</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.7</cell><cell></cell></row><row><cell>2.35</cell><cell>2.4</cell><cell>2.45</cell><cell>2.5</cell><cell>2.55</cell><cell>2.6</cell><cell>2.65</cell><cell>2.7</cell><cell>2.75</cell></row><row><cell></cell><cell>Metric Rank</cell><cell></cell><cell cols="2">Model Classification</cell><cell></cell><cell>ESRGAN</cell><cell>SRGAN</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>.</cell><cell></cell><cell></cell></row><row><cell cols="4">PIRM-Test SRResNet SRGAN ESRGAN</cell></row><row><cell>NIQE</cell><cell>5.968</cell><cell>2.705</cell><cell>2.557</cell></row><row><cell>PSNR</cell><cell>28.33</cell><cell>25.62</cell><cell>25.30</cell></row></table><note>The performance of super-resolved results with three per- ceptual levels in PIRM-Test [2]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Comparison with RankSRGAN and RankSRGAN-HR.</figDesc><table><row><cell></cell><cell cols="3">SRGAN RankSRGAN RankSRGAN-HR</cell></row><row><cell>NIQE PSNR</cell><cell>2.70 25.62</cell><cell>2.51 25.60</cell><cell>2.58 26.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>The performance of RankSRGAN with different Rankers.</figDesc><table><row><cell>Method</cell><cell cols="2">NIQE 10-Ma</cell><cell>PI</cell><cell>PSNR</cell></row><row><cell>SRGAN</cell><cell>2.71</cell><cell>1.47</cell><cell cols="2">2.09 25.62</cell></row><row><cell>ESRGAN</cell><cell>2.56</cell><cell>1.40</cell><cell cols="2">1.98 25.30</cell></row><row><cell>RankSRGANN RankSRGANM RankSRGANP I</cell><cell>2.51 2.65 2.49</cell><cell>1.39 1.38 1.39</cell><cell cols="2">1.95 25.62 2.01 25.21 1.94 25.49</cell></row></table><note>N : Ranker with NIQE [29], M : Ranker with Ma [26] and P I: Ranker with PI [2].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>The distance between SR1 and SR2 with regression and rank.</figDesc><table><row><cell>Metric NIQE NIQE</cell><cell>Method regression rank</cell><cell>E(|SR1 − SR2|) 0.06 0.11</cell></row><row><cell>Ma Ma</cell><cell>regression rank</cell><cell>0.09 0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>1.1. DatasetFigure 1. The convergence curves of RankSRGAN with Ranker1 and Ranker2 in NIQE and PSNR.</figDesc><table><row><cell>Method</cell><cell>Ranker</cell><cell>Dataset</cell><cell cols="4">Data Size (k) SROCC NIQE PSNR</cell></row><row><cell cols="2">RankSRGAN-R-D1 Ranker1</cell><cell>DIV2K</cell><cell>15</cell><cell>0.78</cell><cell>2.53</cell><cell>24.54</cell></row><row><cell cols="3">RankSRGAN-R-D2 Ranker2 DIV2K+Flickr2K</cell><cell>150</cell><cell>0.88</cell><cell>2.51</cell><cell>25.62</cell></row><row><cell cols="7">Table 1. The performance of RankSRGAN with Ranker1 and Ranker2. R-D1: rank dataset1 (15 K), R-D2: rank dataset2 (150 K)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our implementation of SRResNet and SRGAN achieve even better performance than that reported in the original paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>This work is partially supported by National Natural Science Foundation of China (61876176, U1613211), Shenzhen Basic Research Program (JCYJ20170818164704758), the Joint Lab of CAS-HK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">More Qualitative Results</head><p>In this section, we provide additional qualitative results (×4 enlargement) to clearly show the effectiveness of our RankSR-GAN. We compare the proposed RankSRGAN with the state-of-the-art perceptual SR methods SRGAN [3]  / ESRGAN [9]   and PSNR-oriented method SRResNet [3]. We employ NIQE and PSNR to evaluate those SR methods. Lower NIQE value indicates better perceptual quality while higher PSNR indicates that there is less distortion with the Ground-Truth image.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this supplementary file, we first present more details and additional experimental results of our proposed Ranker. Then, we provide the curves showing the performance of different RankSRGAN models in the ablation study. Finally, we provide more additional qualitative results to compare our networks with the state-of-the-art methods.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07517</idno>
		<title level="m">pirm challenge on perceptual image super-resolution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6228" to="6237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for no-reference and full-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Maniry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Superresolution with deep convolutional sufficient statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05666</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep learning-based image super-resolution considering quantitative and perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Hyuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manri</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Seok</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04789</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Suppressing model overfitting for image super-resolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blind super-resolution with iterative kernel correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1604" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modulating image restoration with continual levels via adaptive feature modification layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1733" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Andrew P Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using a general regression neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="793" to="799" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition (CVPR) workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1707.08347v1" />
		<title level="m">Rankiqa: Learning from rankings for no-reference image quality assessment. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning a no-reference quality metric for single-image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Eighth IEEE International Conference on</title>
		<meeting>Eighth IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Making a&quot; completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="503" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ranking cgans: Subjective control over semantic image attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassir</forename><surname>Saquil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang</forename><forename type="middle">In</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of British Machine Vision Conference (BMVC)</title>
		<meeting>of British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Recovering realistic texture in image superresolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02815</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shamim Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghoneim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1832" to="1842" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment based on visual codebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David S Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3089" to="3092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Single image super-resolution with non-local means and steering kernel regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaibing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An edge-guided image interpolation algorithm via directional filtering and data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2226" to="2238" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Since the VGG-12 can achieve the same accuracy as VGG-16, we apply the VGG-8 and VGG-12 on RankSRGAN. Figure 2 shows the performance of RankSRGAN with different Rankers. The Ranker with higher value of SROCC can achieve better performance when applied on RankSRGAN</title>
		<idno>VGG-8 VGG-12</idno>
		<imprint>
			<publisher>Ours</publisher>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>We train three VGG networks varying from shallow to deep ones: VGG-8, VGG-12 and VGG-16. Table 2 shows the architecture, the number of parameters, and the performance in different models</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">LReLU Architecture Conv4S2-64, BN, LReLU Conv4S2-64, BN, LReLU Conv4S2-64, BN, LReLU Conv3S1-128, BN, LReLU Conv3S1-128, BN, LReLU Conv3S1-128, BN, LReLU Conv4S2-128, BN, LReLU Conv4S2-128, BN, LReLU Conv4S2-128, BN, LReLU Conv3S1-256, BN, LReLU Conv3S1-256, BN, LReLU Conv3S1-256, BN, LReLU Conv4S2-256, BN, LReLU Conv4S2-256, BN, LReLU Conv4S2-256, BN, LReLU Conv3S1-512, BN, LReLU Conv3S1-512, BN, LReLU Conv3S1-512, BN, LReLU Conv4S2-512, BN, LReLU Conv4S2-512, BN, LReLU Conv4S2-512, BN, LReLU Conv3S1-512, BN, LReLU Conv3S1-512, BN, LReLU Conv3S1-512, BN, LReLU Conv4S2-512</title>
		<imprint>
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
	<note>LReLU Conv4S2-512, BN, LReLU Conv4S2-512, BN, LReLU Average pooling FC-100</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The network architecture of Rankers with different depths. The network design draws inspiration from VGG [8] but uses Leaky ReLU activations [5] and strided convolutions instead of pooling layers</title>
	</analytic>
	<monogr>
		<title level="m">Conv3S1-64: Convolutional layer with kernel size 3×3, stride 1 and channel 64. BN: Batch Normalization. LReLU: Leaky ReLU. Figure 2. The convergence curves of RankSRGAN with Ranker-VGG-8 and Ranker-VGG-12 in NIQE and PSNR</title>
		<imprint/>
	</monogr>
	<note>Table 2</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07517</idno>
		<title level="m">pirm challenge on perceptual image super-resolution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Andrew P Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning a no-reference quality metric for single-image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Making a&quot; completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced superresolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
