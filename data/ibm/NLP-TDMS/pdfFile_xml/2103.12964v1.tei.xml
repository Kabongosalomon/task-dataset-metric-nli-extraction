<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Volumetric Propagation Network: Stereo-LiDAR Fusion for Long-Range Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Choe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungdon</forename><surname>Joo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tooba</forename><surname>Imtiaz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
						</author>
						<title level="a" type="main">Volumetric Propagation Network: Stereo-LiDAR Fusion for Long-Range Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Stereo-LiDAR fusion is a promising task in that we can utilize two different types of 3D perceptions for practical usage -dense 3D information (stereo cameras) and highlyaccurate sparse point clouds (LiDAR). However, due to their different modalities and structures, the method of aligning sensor data is the key for successful sensor fusion. To this end, we propose a geometry-aware stereo-LiDAR fusion network for long-range depth estimation, called volumetric propagation network. The key idea of our network is to exploit sparse and accurate point clouds as a cue for guiding correspondences of stereo images in a unified 3D volume space. Unlike existing fusion strategies, we directly embed point clouds into the volume, which enables us to propagate valid information into nearby voxels in the volume, and to reduce the uncertainty of correspondences. Thus, it allows us to fuse two different input modalities seamlessly and regress a long-range depth map. Our fusion is further enhanced by a newly proposed feature extraction layer for point clouds guided by images: FusionConv. FusionConv extracts point cloud features that consider both semantic (2D image domain) and geometric (3D domain) relations and aid fusion at the volume. Our network achieves state-of-the-art performance on the KITTI and the Virtual-KITTI datasets among recent stereo-LiDAR fusion methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Sensor fusion is the process of merging data from multiple sensors, which makes it possible enrich the understanding of 3D environments (i.e., 3D perception) for autonomous driving or robot perception. Each sensor has its unique properties and can complement other sensors' limitations by fusion. In particular, sensor fusion -such as two cameras (stereo matching) or LiDAR and a single camera (depth completion) -allows us to estimate accurate depth information. Several studies have explored fusion-based depth estimation algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. The quality of depth estimated by these traditional methods has been further improved with the advent of deep learning <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.  <ref type="figure" target="#fig_5">Fig. 1</ref>. Pipeline of the proposed stereo-LiDAR fusion. Given (a,b) stereo images and (c) point clouds, we fuse two different input modalities at 3D voxels of (d) fusion volume, which is a unified 3D volumetric space, to infer (e) depth map by volumetric propagation.</p><p>stereo-LiDAR fusion is a novel task where we can utilize two different types of 3D perception: dense 3D information from stereo cameras and sparse 3D point clouds from LiDAR, which can further improve the depth quality. Within this fusion framework, aligning different sensor data into a unified space is an essential step to fully operate depth estimation task. Previous works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> perform fusion on the 2D image domain using the geometric relationship between sensors (i.e., intrinsic and extrinsic parameters). For example, Wang et al. <ref type="bibr" target="#b9">[10]</ref> project point clouds into the 2D image domain to align images with sparse depth maps of the projected point clouds. However, because neighboring pixels in a 2D image space are not necessarily adjacent in the 3D space, this 2D fusion in the image domain may lose depthwise spatial connectivity, thus have difficulty in estimating accurate depth at distant regions. For geometry-aware fusion of stereo images and point clouds, it is necessary to maintain the spatial connectivity in a unified 3D space. In this paper, we propose a geometry-aware stereo-LiDAR fusion network for long-range depth estimation, called volumetric propagation network. To this end, we define 3D volumetric features, called fusion volume, as a unified 3D volumetric space for fusion, where the proposed network computes the correspondences from both stereo images and point clouds, as shown in <ref type="figure" target="#fig_5">Fig. 1</ref>. Specifically, sparse points become seeds of correct correspondence to reduce the uncertainty of matching between the stereo images. Then, our network propagates this valid matching to the overall volume and computes the matching cost of the rest of the volume through stereo images. Furthermore, we facilitate the volumetric propagation by embedding point features into the fusion volume. The point features are extracted by our imageguided feature extraction layer from raw point clouds, called FusionConv. Our FusionConv considers both semantic (2D image domain) and geometric (3D domain) relation for the tight fusion of image features and point features. Finally, our approach achieves state-of-the-art performance among stereo-LiDAR fusion methods for the KITTI dataset <ref type="bibr" target="#b12">[13]</ref> and the Virtual-KITTI dataset <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>We review sensor fusion-based depth estimation methods according to types of sensor systems: stereo cameras, mono-LiDAR fusion and stereo-LiDAR fusion. Stereo cameras. Stereo matching is a task that reconstructs the 3D environment captured from a pair of cameras <ref type="bibr" target="#b0">[1]</ref>. By computing the dense pixel correspondence between a rectified image pair, stereo matching infers the inverse depth i.e., disparity map. Recently, deep-learning techniques have paved the way for more accurate and robust matching by using volume-based deep architectures <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Cost volume is one of the popular volumetric representations that encompass the 3D space in the referential camera view * along the disparity axis. This property is beneficial for computing matching cost between stereo images. Despite the large improvement, stereo matching still lacks accurate depth estimation at distant regions. To address this issue, LiDAR is a prominent sensor to estimate long-range depth. Mono-LiDAR fusion. Depth completion is a task that estimates a depth map using a monocular camera and a LiDAR sensor. By propagating highly accurate but sparse points from a LiDAR sensor, this task aims to densely complete the depth map with the help of image information. Recent deep-learning-based methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> largely increase the quality of depth. These methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref> employ a pre-processing stage that projects point clouds into image domain, and feed this sparse depth map as input to a network (i.e., early fusion). On the other hand, Chen et al. <ref type="bibr" target="#b16">[17]</ref> introduce an intermediate fusion scheme in the feature space. They initially extract features from each modality and implement the fusion in the image feature space by projecting point features. Despite the improvement thus achieved, depth completion task has difficulties in estimating depth at unknown areas not encompassed by point clouds. Stereo-LiDAR fusion. Stereo camera-LiDAR fusion (simply denoted as stereo-LiDAR fusion) has been recently demonstrated to further increase the accuracy of depth estimation by utilizing additional sensory information. Relying on the highly accurate but sparse depth map from point clouds, Park et al. <ref type="bibr" target="#b10">[11]</ref> refine the disparity map from stereo cameras using a sparse depth map. Despite the increased quality of estimated depth, the fusion within this method lies in 2D image domain and is therefore insufficient to maintain metric accuracy of point clouds for long-range depth estimation. Another recent work, Wang et al. <ref type="bibr" target="#b9">[10]</ref>, introduce the idea of input fusion (i.e., stereo RGB-D input) and volumetric normalization conditioned by sparse disparity (i.e., projected point clouds). Typically, this conditional cost volume normalization <ref type="bibr" target="#b9">[10]</ref> mainly affects the 3D volumetric aggregation and looks closer to the idea of our volumetric * We set the left camera as the referential <ref type="bibr" target="#b0">[1]</ref>.</p><p>propagation. However, this method has difficulty in estimating the accurate depth in remote area.</p><p>To address this issue, we introduce the volumetric propagation network that aims to fuse the two input modalities: stereo images and point clouds in a unified 3D volume space, fusion volume. Within the fusion volume, we regard the sparse points as the seed of valid matching between stereo images and propagate the valid matching to the overall volume space. To do so, our network (1) maintains metric accuracy of point clouds during fusion and (2) reduces the uncertainty of stereo matching where point clouds do not exist. We further facilitate fusion at the fusion volume by our feature extraction layer for point clouds, FusionConv. Among recent stereo-LiDAR fusion works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref>, with these two contributions, we achieve remarkable stateof-the-art depth estimation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERVIEW</head><p>We aim to estimate a long-range, highly-accurate, and dense depth map Z in the referential camera viewpoint from rectified stereo images I L and I R , LiDAR point clouds P and their calibration parameters (e.g., camera intrinsic matrix K). The point clouds are presented as a set of N number of 3D points P={p i } N i=1 , where each point p=[x, y, z] lies in the referential camera coordinates. † Under the known K, we can project each point into the image domain, x Kp, where the projected image point x can be located at the pixel location of (u, v). Using this geometric relation in conjunction with interpolation, we fuse the two modalities at the fusion volume (Sec. IV) and FusionConv (Sec. V). The overview of the proposed approach is presented in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. VOLUMETRIC PROPAGATION NETWORK</head><p>In this section, we detail how we construct our fusion volume V to fuse two different input modalities, stereo images and raw point clouds, in a manner of volumetric propagation. The proposed fusion volume V is a unified 3D volumetric space for stereo-LiDAR fusion to estimate long-range depth map Z in the referential view. Unlike the traditional voxel representation -cost volume <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref> which quantizes the 3D data along the disparity axis, our fusion volume V describes the 3D environment as an evenly distributed grid voxel space along the depth range (i.e., metric scale). This is to reduce the quantization loss when we embed sparse points P into the fusion volume. While embedding points into the cost volume dramatically increase the quantization loss in distant regions, our metric-scale fusion volume does not. Specifically, we define the fusion volume V as a 3D volume representation and each voxel v ∈ V contains a feature vector of a certain dimension. That is, V ∈ R W ×H×D×(2C+1) , where W , H and D denote the number of voxels along the width, height, and depth axes, respectively, and C represents the dimension of the image feature vector. Note that the feature dimension of the volume V is set as (2C + 1) to  Overall architecture. Our network consists of three stages: feature extraction stage, volume aggregation stage, and depth regression stage. We initially extract features from input modalities. In particular, we use FusionConv layers to infer point feature vectors F P . These extracted features are embedded into the fusion volume V F P to compute the correspondence with stereo feature maps F L , F R . After volume aggregation through 3D stacked hourglass networks, we finally obtain the depth map Z in the referential camera view.</p><p>embed stereo image features and point cloud features along the different channels but in a unified volume V.</p><p>In the constructed fusion volume, which is empty at the beginning, we first fill in the stereo information. Inspired by the differential cost volume <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, filling the volume with features from sensory data enables the end-to-end learnable voxel representation to estimate depth. From stereo images I L , I R ∈ R 4W ×4H×3 , we extract stereo feature maps F L , F R ∈ R W ×H×C using feature extraction layers from our baseline method <ref type="bibr" target="#b14">[15]</ref>. By projecting the pre-defined location of each voxel v into the image domain, we fill the fusion volume V with stereo features (see <ref type="figure" target="#fig_1">Fig. 2</ref>). A precise description of the fusion volume composition is included in the supplementary material.</p><p>Our fusion volume encapsulates the 3D environments linearly to the metric scale as depth volumes do <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. However, while traditional depth volumes <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> are the results of transformation from the initially-built cost volume (i.e., from disparity to depth), our fusion volume directly built from sensory features. Since the quantization loss happens when points are embedded into the initiallybuilt volume, direct construction of a metric-scale volume can reduce the quantization loss of points, especially at a farther area. For this purpose, with the known camera matrix K and the pre-defined location of each voxel in the volume V, we are able to embed point clouds P into the volume V as binary representation with less quantization loss at farther area. Voxels embedded by points are filled with 1 (occupied) and the others are filled with 0 (non-occupied or empty). To do so, our fusion volume maintains the geometric and spatial relation of stereo images and point clouds within a unified volumetric space.</p><p>So far, we have incorporated stereo feature maps F L , F R and raw point clouds P into the fusion volume V. With this volume V, we can propagate the embedded points to the overall volume for computing the matching cost between stereo features (F L , F R ) or among stereo features and point clouds (F L , F R , P) through the following 3D convolution layers in stacked hourglass networks (see <ref type="figure" target="#fig_1">Fig. 2</ref>).</p><p>Meanwhile, the 3D convolution layers compute both the spatial correspondence and the channel-wise features within the volume V, so that channel-wise information is also an one important factor in our metric-aware fusion. To further facilitate the fusion, we discuss feature extraction from raw point clouds P in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. FUSED CONVOLUTION LAYER</head><p>Recently, many research works <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> have proposed feature extraction layers for point clouds and have demonstrated the potential of leveraging local neighbors in aggregating point feature vectors. Despite the progress in deep architectures for point clouds, previous methods <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> focus purely on utilizing raw point clouds (for classification <ref type="bibr" target="#b26">[27]</ref> or segmentation tasks <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>) instead of fusing them with different sets of sensor information.</p><p>In this section, we introduce an image-guided feature extraction layer for point clouds, called FusionConv, which is specialized in sensor fusion-based depth estimation task. Under the known geometric mapping relation between image and point clouds, we design a FusionConv layer that exploits image guidance in several ways. (1) We adaptively cluster neighbors of each point while considering geometric relation (3D metric domain) as well as the semantic relation (2D image domain), which enables us to determine relevant neighbors. (2) We directly fuse the input point feature with the corresponding image feature via interpolation, which implicitly helps to extract distinctive point features.</p><p>The proposed FusionConv takes the input feature map of the left image F L and the input point feature vectors F in P ∈ R C×N (extracted from raw point clouds P ∈ R 3×N ) and estimates the output point feature vectors F P ∈ R C×N by fusing the two different features while taking account of relevant neighbors (see <ref type="figure" target="#fig_2">Fig. 3</ref>). For simplicity, let p ∈ P and x be a 3D point and its projected image point by K, respectively. We then denote the corresponding 3D feature vector and 2D image feature vector as F P (p) ∈ R C×1 and F L (x) ∈ R C×1 , respectively. </p><formula xml:id="formula_0">N = {p i , p j , p k }).</formula><p>Through the mapping relation, the FusionConv layer clusters the neighboring point feature vectors to aggregate the local response in point feature vectors. While many approaches <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> cluster point clouds P only in a metric space, we need to cluster the neighboring point clouds following the voxel alignment in the volume V, which is also aligned in the referential camera view. Specifically, for each point p in P, we dynamically cluster neighboring point clouds P N that reside within the pre-defined size of the voxel window. For instance in <ref type="figure" target="#fig_2">Fig. 3</ref>, there are three neighboring points p i , p j and p k within the voxel window whose center indicates the point p c . From these three neighbor points and the center point p c itself, FusionConv layer is able to calculate the local response of the point feature vectors at the point p c in alignment with the fusion volume V.</p><p>The following process of the FusionConv layer consists of image-to-points fusion and points-to-points aggregation. For the fusion process, we first interpolate F L into F P following the projection mapping relation to obtain the fused point feature vectors F f use P . These fused features F f use P have the same number of points as N but they have an extended length of channels containing both F L and F in P . After the image-to-points fusion, we convolve the fused point feature vectors F f use P and the geometric distance between the neighboring points P N to aggregate the local response, called points (P N )-to-points (F f use P ) convolution. For instance in <ref type="figure" target="#fig_2">Fig. 3</ref>, the weighted geometric distance G(·) from the center point p c =[x c , y c , z c ] to its neighboring</p><formula xml:id="formula_1">point p i =[x i , y i , z i ] is calculated as: G(p c −p i )=G(∆p)=A 0 + A 1 ·∆x + A 2 ·∆y + A 3 ·∆z,<label>(1)</label></formula><p>where A 0 , A 1 , A 2 and A 3 are learnable weights and we define ∆p= [∆x, ∆y, ∆z] as ∆x=x c −x i , ∆y=y c −y i , and ∆z=z c −z i . This weighted geometric distance G(·) becomes the weight of the convolution with the fused point feature vector F f use P to compute the local response at point p c as:</p><formula xml:id="formula_2">F P (p c ) = 1 |P pc N | p i ∈P pc N F f use P (p i ) · G(p c − p i ),<label>(2)</label></formula><p>where |P pc N | is the number of neighboring points near the point p c . Thus, this is the way of imposing different weights G(·) on the neighboring point features F in P and left feature maps F L when extracting the center point feature vectors F P (p c ). To fully compute the output feature vectors from all point clouds, the FusionConv layer iteratively calculates the output point feature vectors F P as:</p><formula xml:id="formula_3">F P = F P (p 1 ), F P (p 2 ), · · · , F P (p N ) .<label>(3)</label></formula><p>Finally, we extract the output point feature vector F P ∈ R C×N from raw point clouds P and left feature maps F L . This point feature vector becomes embedded into the modified fusion volume V F P ∈ R 3C×D×H×W as in <ref type="figure" target="#fig_1">Fig. 2</ref>. The embedded location of F P is identical to the corresponding spatial locations of raw point clouds P within the extended channel-wise voxels 2C + 1 (V) → 3C (V F P ) to maintain the metric accuracy from raw point clouds P. With this fusion volume V F P , we can fuse the two different modalities to compute the subpixel matching cost by the following 3D convolution layers as in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DEPTH MAP REGRESSION</head><p>After we fuse features from the two modalities at the fusion volume V F P , we propagate the point features to the overall volume and compute the matching cost through the stacked hourglass networks <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b14">[15]</ref> to regress the depth map. Following <ref type="bibr" target="#b14">[15]</ref>, we perform a cost aggregation process by aggregating the fusion volume along the depth dimension as well as the spatial dimension. In our stacked hourglass networks, the networks consist of three encoder-decoder networks that sequentially refine the cost aggregation via intermediate loss <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b18">[19]</ref>. After aggregation, the cost aggregation reduces the channels of V F P into a 3D structure A ∈ R D×H×W . From A, we can estimate the depth valuẽ z u,v at pixel (u, v) as:</p><formula xml:id="formula_4">z u,v = D−1 d=0 d D − 1 · z max · σ(a d u,v ),<label>(4)</label></formula><p>where z max is a hyper-parameter defining the maximum range of depth estimation, σ(·) represents the softmax operation, and a d u,v is the d-th value of the cost aggregation vector a u,v ∈ R D×1 at (u, v). For our experiments, we set the hyper-parameters as D = 48 and z max = 100. Specifically, we compute the depth loss L depth from the estimated depth mapZ and the true depth map Z as follows:</p><formula xml:id="formula_5">L depth = 1 M u v smooth L1 (z u,v −z u,v ),<label>(5)</label></formula><p>where M is the number of valid pixels in Z for the normalizing factor,z u,v is the value of the predicted depth map Z at pixel location (u, v), and smooth L1 (·) is the smooth L1 loss function used to compute the loss <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Finally, the total loss L total of our network is computed from the three different intermediate results of depth maps from the three stacked hourglass networks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b30">[31]</ref> as below: where w i is the weight of the i-th depth loss L i depth (we set the weight to w 1 =0.5, w 2 =0.7 and w 3 =1.0). During the evaluation, we only consider the predicted depth map at the last network, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><formula xml:id="formula_6">L total = 3 i=1 w i · L i depth ,<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTAL EVALUATION</head><p>In this section, we describe the implementation details of our network. Our network is trained on two independent datasets, the KITTI dataset <ref type="bibr" target="#b12">[13]</ref> and the Virtual-KITTI dataset <ref type="bibr" target="#b13">[14]</ref>. We separately evaluate the accuracy of the depth for each dataset against existing techniques. Additionally, we conduct an ablation study to validate each dominant component of our method and to contrast the early fusion <ref type="bibr" target="#b9">[10]</ref> with our intermediate fusion at the fusion volume V F P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture</head><p>Our network consists of three stages: the feature extraction stage, volume aggregation stage, and depth regression stage, as depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>. In the feature extraction stage, we follow the architecture of image feature extraction layers as in Chang and Chen <ref type="bibr" target="#b14">[15]</ref>, but extract the intermediate left feature map to operate image-to-points fusion in FusionConv layers. We use three FusionConv layers to infer the point feature vectors. Then, the extracted features from input modalities are embedded in the fusion volume as explained in Secs. IV and V. We aggregate the volume through the three stacked hourglass networks and finally regress the depth map as described in Sec. VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets and training schemes.</head><p>KITTI dataset. The KITTI Raw benchmark <ref type="bibr" target="#b31">[32]</ref> provides sequential stereo images and LiDAR point clouds under different road environments. Within the benchmark, KITTI completion benchmark <ref type="bibr" target="#b12">[13]</ref> provides the true depth maps and the corresponding sensor data, consisting of 42,949 training samples and 1,000 validation samples. Given the input sensory data, we train our network by setting the learning rate as 0.001 for 5 epochs and as 0.0001 for 30 epochs. We use three NVIDIA 1080-Ti GPUs for training, and the batch size is 9. The entire training scheme takes three days and the inference speed of our network during the test is 0.71 FPS (1.40 sec per frame). We use randomcrop augmentation during the training phase. For training, the size of the cropped images is 256 × 512. We also crop point clouds P that reside within the cropped left images. Usually, there are ∼5K point clouds in the training phase, but it can vary depending on the location of the cropped area within images. For testing, we fully utilize the original shape of images and raw point clouds (∼25K points per image) without any augmentation or filtering. Virtual KITTI 2.0 dataset. Virtual KITTI 2.0 <ref type="bibr" target="#b13">[14]</ref> is a recently published dataset that provides much more realistic images than the previous version of the Virtual-KITTI 1.3.1 <ref type="bibr" target="#b32">[33]</ref>. The merits of the synthetic environment include access to dense ground truth at the farther areas, while the KITTI completion benchmark <ref type="bibr" target="#b12">[13]</ref> provides relatively sparse ground truth. There are five scenes in this dataset and each scene contains ten different scenarios, such as rain, sunset, etc. We set the two scenes (Scene01, Scene02) for training the network and the other scenes are exploited for evaluating the accuracy of depth maps. For each scene, we take the only scenario (15-deg-left) for training and evaluation. In total, there are 680 images in the training set and 1,446 images in the test set. Though the raw point cloud data is not provided in this dataset, we randomly sample the ground truth depth pixels and regard the pixels as the point clouds. We select the same number of selected point clouds as for the KITTI dataset (i.e., 5K points for training and 25K points for testing). Given pre-trained weights from the KITTI dataset, we fine-tune the network for 5K iterations under the identical augmentation methods as we adopt for the KITTI dataset.  <ref type="figure">Fig. 4</ref>. Qualitative results on the KITTI dataset. We visualize depth maps and depth errors from ours and the recent stereo-Lidar method (CCVN <ref type="bibr" target="#b9">[10]</ref>) in three different cases. We also include the depth metric RMSE (lower the better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics.</head><p>We evaluate the quality of the estimated depth by our network. We follow the metric scheme proposed by Eigen et al. <ref type="bibr" target="#b33">[34]</ref> which is identical to the official KITTI depth completion benchmark <ref type="bibr" target="#b12">[13]</ref>, i.e., RMSE, MAE, iRMSE, iMAE, where RMSE and MAE are the target metrics or evaluating the metric distance. These metric formulations are equivalently applied to measure the performance on the Virtual-KITTI dataset <ref type="bibr" target="#b13">[14]</ref>. Since the Virtual-KITTI dataset does not provide raw point clouds data, we evaluate the depth metric by 5 times of repetitive sampling of the ground truth depth pixels. The resulting metric in <ref type="table" target="#tab_1">Table II</ref> is the average over the samples. Note that we re-train the network by accessing the open-source code of the target method and denote the re-evaluated results by a '*' (e.g., PSMnet*) as in <ref type="table" target="#tab_1">Tables I and II.</ref> Comparison. We evaluate our network against existing methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref> for the KITTI completion validation benchmark <ref type="table" target="#tab_1">(Table I</ref>) and the Virtual-KITTI 2.0 <ref type="table" target="#tab_1">(Table II)</ref>. We also include the performance of other depth estimation networks, such as stereo matching methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b4">[5]</ref> and depth completion studies <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Among the existing methods, our method achieves state-of-the-art depth performance in RMSE and MAE as in <ref type="table" target="#tab_1">Tables I and II</ref>. These results suggest that our method shows higher accuracy in the estimation of the depth at a distant area. This is consistent with our intention to create a metric-linear volumetric design. To further verify the strength of our method, we provide the qualitative results for the KITTI dataset and the Virtual-KITTI dataset in <ref type="figure" target="#fig_2">Figs. 3 and 4</ref>. We attribute this improvement to the fusion at the fusion volume that  <ref type="bibr" target="#b12">[13]</ref>. Fusion volume. We compare our fusion volume with other volume representations, such as cost volume <ref type="bibr" target="#b4">[5]</ref> and depth volume <ref type="bibr" target="#b34">[35]</ref> as shown in <ref type="table" target="#tab_1">Table III</ref>. Note that the depth volume <ref type="bibr" target="#b34">[35]</ref> is built via cost volume, but our fusion volume directly encodes point feature vectors F P into the metricaware voxels. For a fair comparison, we evaluate each type of . This synthetic data covers the wide range of depth upto 655m, but we clamp true depth maps and estimated depth maps upto 100m during the evaluation, as in <ref type="table" target="#tab_1">Table II</ref>. For the detailed visualization, we crop and enlarge the part of depth error maps in each frame. Mainly, the cropped images correspond to the farther area to validate our long-range depth estimation. volume under identical conditions, such as deep architectures (e.g., FusionConv) and input sensory data (e.g., stereolidar fusion). In <ref type="table" target="#tab_1">Table III</ref>, our fusion volume shows the highest accuracy among different voxel representations. We deduce the reason that our volume can directly encode the 3D metric point into the volume, while the other voxel representation <ref type="bibr" target="#b34">[35]</ref> is constructed via cost volume, which can lose metric information during the transformation. FusionConv. In <ref type="table" target="#tab_1">Table IV</ref>, we decompose FusionConv into three factors: convolution, cluster, and fusion, to analyze each component in the proposed layers. As a baseline (Method 4 in <ref type="table" target="#tab_1">Table IV</ref>), we embed the raw point clouds P into the volume V P as described in Sec. IV. Moreover, we set another baseline network as a multilayer perceptron layer (i.e., fully-connected layer) for point feature extraction <ref type="bibr" target="#b21">[22]</ref> (Method 5), which does not infer point feature vectors F P via clustering. Though the quality of depth from the two baseline methods outperforms the previous method <ref type="bibr" target="#b9">[10]</ref>, the convolution operation among the neighboring points P N , as in FusionConv, further increases the accuracy of the depth estimation (Method 6). This confirms that our clustering strategy and image-to-point fusion are effective for the sensor fusion-based depth estimation task. Finally, our FusionConv layer (Method 6) shows the highest metric performance. Early fusion. Our method proposes fusion in the feature space, called intermediate fusion, through the fusion volume. However, previous methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref> internally use the pre-processing stage to project raw point clouds P into the image domain to concatenate them with RGB images, called early fusion. With this ablation study, we validate the performance gap between early fusion and our intermediate fusion to prove the effectiveness of fusion in a 3D metric space. For a fair comparison with different levels of fusion, we use similar architectures with different types of volume as listed in <ref type="table" target="#tab_4">Table V</ref>. For early fusion, we do not embed point clouds into the volume but undergo the pre-processing stage as previous works proposed <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Method 9 represents our method. In <ref type="table" target="#tab_4">Table V</ref>, it reveals that intermediate fusion in a 3D voxel space <ref type="bibr">(Methods 8,</ref><ref type="bibr" target="#b8">9)</ref> shows better results than the early fusion approach (Method 7). We deduce that our fusion scheme takes into consideration the spatial connectivity of point clouds and stereo images for seamless fusion and obtains long-range depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>In this paper, we propose a volumetric propagation network for stereo-LiDAR fusion and perform the long-range depth estimation. To this end, we design two dominant modules, fusion volume and FusionConv, to facilitate the fusion in a unified volumetric space. Within the fusion volume, we formulate the fusion as volumetric propagation by considering the spatial connectivity of sparse point features and densely-ordered stereo images features. Our method demonstrates state-of-the-art performance on the KITTI and the Virtual-KITTI datasets and delivers a message about the geometric-aware stereo-LiDAR fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. OVERVIEW</head><p>This document provides additional information that we did not fully cover in the main paper, Volumetric Propagation Network: Stereo-LiDAR Fusion for Long-Range Depth Estimation. All references are consistent with the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. VOLUMETRIC PROPAGATION</head><p>For the fusion of different sets of sensory information, the recent stereo-LiDAR fusion method (CCVN <ref type="bibr" target="#b9">[10]</ref>) introduces input fusion (i.e., stereo RGB-D input) as well as cost volume normalization conditioned by sparse disparity maps. Typically, this conditional cost volume normalization mainly affects the 3D volumetric aggregation and looks closer to the idea of our volumetric propagation.</p><p>On the other hand, the proposed concept of volumetric propagation has a novel and unique geometric insight. The fundamental reasoning of our method is related to triangulation in the 3D space (i.e., our fusion volume). By embedding point clouds directly into the 3D volumetric space, we can provide accurate 3D seeds of pixel correspondences between stereo images. To further elucidate our point, we provide example visualization in <ref type="figure" target="#fig_5">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. QUANTIZATION ERROR</head><p>One of the major differences between cost volume <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref> and our fusion volume is the design choice of depth range, which is a matter of different spacing choices along the referential camera ray.</p><p>Basically, our intention of using the fusion volume is to seamlessly fuse two different sets of 3D information (one from stereo images and the other one from LiDAR) in a unified 3D space for high-quality and long-range depth estimation. In particular, we are interested in improving the depth quality in distant areas, where a pair of stereo images has difficulty in finding the correspondence. To this end, we adopt the metric-scale fusion volume that equally spaces the depth range and has less quantization loss even in the faraway regions.</p><p>For better understanding, we validate our proposed solution by extensive qualitative and quantitative evaluation. We first visualize the quantization loss that comes from embedded points as in <ref type="figure" target="#fig_1">Fig. 2</ref>. We then calculate the performance of depth maps in three different ranges: close area (0m − 20m), middle area (20−40m), and far area (40−80m) as in <ref type="table" target="#tab_1">Table I</ref>. The results are consistent with our intention of creating a volumetric design since our method shows outperforms the recent works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>, especially in the far area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. HYPER-PARAMETERS</head><p>In this section, we describe the details of the hyperparameters that we did not fully describe in the manuscript. Since hyper-parameter tuning can affect the depth performance, we try not to change the specific value but use the proposed values from our baseline method (PSMNet <ref type="bibr" target="#b14">[15]</ref>). For example in the dimension of the channels, given input images with dimension 3 × 4H × 4W , the feature extraction layers infer image feature maps C(=32) × H × W . Note that the feature extraction layer is identical to our baseline network <ref type="bibr" target="#b14">[15]</ref>, as stated in Sec. VII-A of the manuscript. Also, the point feature vectors F P have the same number of channels (C=32). For the composition of the fusion volume, we use D=48, z max =100. Note that the weights are set as w 1 =0.5, w 2 =0.7, w 3 =1.0. The overall hyper-parameters are identical/close to the design choice by our baseline method <ref type="bibr" target="#b14">[15]</ref>. This is for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. INFERENCE TIME</head><p>This section provides the details of the inference time of each specific stage in our volumetric propagation network,  To illustrate the quantization loss for embedded point clouds, we first embed the point clouds into each volume and back-project the points into the 3D metric space. Visually, the fusion volume can generally describe the target road scene while the cost volume concentrates on the closer area, which is consistent with our intention to maintain the metric accuracy of point clouds. Also, we calculate the quantization loss by measuring the absolute distance (i.e., AbsDist) between (d) original point clouds and (c, e) embedded point clouds. Our intention of the usage of the word. quantization loss, is the metric-scale difference.</p><p>as shown in <ref type="table" target="#tab_1">Tables II and III.</ref> As expected, the volume aggregation stage takes the most inference time, while the other stages take less computation. This is because the volumetric data requires large computation powers. On the other hand, the FusionConv shows real-time inference speed in our environment. Because the FusionConv is fully implemented in a GPU-friendly environment using CUDA, the inference time of the FusionConv is less than 27ms. Note that the speed is measured during the evaluation and the number of point clouds are less than 25K. Moreover, we compare the inference speed of our method, recent stereo-LiDAR methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and baseline methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref> as in <ref type="table" target="#tab_1">Table IV</ref>. For fairness, we use a Core i7 processor and an NVIDIA 1080Ti GPU to measure the speed, which are similar to those of the recent method (CCVN <ref type="bibr" target="#b9">[10]</ref>). Though our method requires more computation power than others, this is not from the implementation of FusionConv but from the volumetric aggregation stage in stacked hourglass networks <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. LIDAR DEPENDENCY</head><p>In a real-world scenario, LiDAR provides not only sparse but variable points as ray fails at transparent, highly reflective regions. The stereo-LiDAR system can be one solution since it takes 3D information from both stereo images and point clouds. To validate the dependency of point clouds, we newly conduct an ablation study of the number of point clouds for stereo-LiDAR fusion as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We randomly remove points and the remaining points are used for depth    <ref type="figure" target="#fig_2">Fig. 3</ref>. Illustration of an ablation study for the quality of depth from different numbers of points. We utilize one specific scene in the KITTI depth completion validation benchmark <ref type="bibr" target="#b12">[13]</ref> to measure the depth performance in 4 different cases of (a) input point clouds: 15000 points, 5000 points, 1000 points, and 10 points. In spite of reduced scale of input points, our method can estimate depth maps from stereo images functionally. estimation. Using our trained network, we measure the depth performance in 4 different cases: 15000 points, 5000 points, 1000 points, and 10 points. Obviously, the fewer points we use, the less depth accuracy we have. However, with the visualized quality of inferred depth maps, it is reasonable to say that our method is not fully dependent on the point clouds, and is robust to the different number of point clouds for depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. FUSION VOLUME COMPOSITION</head><p>This section describes the details of the composition of the fusion volume that we explain in Sec. IV of the manuscript. Our fusion volume is the 3D volumetric data that represents the referential camera ray from z min (=0m) to z max (=100m) in a metric scale [u, v, z] as shown in <ref type="figure">Fig. 4</ref>. From the pre-defined location of each voxel [u, v, z] in the fusion volume V F P , we construct the fusion volume using</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overall architecture. Our network consists of three stages: feature extraction stage, volume aggregation stage, and depth regression stage. We initially extract features from input modalities. In particular, we use FusionConv layers to infer point feature vectors F P . These extracted features are embedded into the fusion volume V F P to compute the correspondence with stereo feature maps F L , F R . After volume aggregation through 3D stacked hourglass networks, we finally obtain the depth map Z in the referential camera view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>FusionConv. We illustrate the process of FusionConv layer that extracts output point feature vector F P (p c ) from neighboring point clouds P p c N of a point p c . In (a) image-to-point fusion, interpolation is used to fuse F L and F in P and generate the fused point feature vector F f use P . Then, we operate (b) points-to-points convolution of fused features F f use P and geometric distance G(·) between adjacent points P p c N to infer output point feature vector at the point p c , denoted as F P (p c ). Note that the overall flow is processed after the clustering stage (P p c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results on the Virtual-KITTI 2.0 dataset. We evaluate the estimated depth from both our network and the recent Stereo-LiDAR fusion network by Wang et al.<ref type="bibr" target="#b9">[10]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 .</head><label>1</label><figDesc>Fusion using triangulation and propagation (ours) Cost volume Left RGB-D input Right RGB-D input(c) Early fusion (i.e., input fusion) in CCVN<ref type="bibr" target="#b7">[8]</ref> Examples of fusion mechanism. (a) We provide an example of triangulation in stereo images and colorized point clouds. Note that the orange marks (×) represent the 2D-3D correspondence in stereo images and point clouds. (b) By embedding point clouds into the fusion volume, we apply the idea of propagation into the fusion of stereo images and point clouds. Additionally, (c) we provide conceptual flow of the early fusion (i.e. input fusion) proposed by the recent stereo-LiDAR fusion method (CCVN<ref type="bibr" target="#b9">[10]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 .</head><label>2</label><figDesc>Visualization of quantization loss for embedded point clouds from cost volume and fusion volume. Given (a) a left image and (b) projected point clouds, we back-project (d) colorized point clouds into the 3D metric space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc>QUANTITATIVE RESULTS OF DEPTH ESTIMATION NETWORKS IN KITTI COMPLETION VALIDATION BENCHMARK. REPRESENTS THE REPRODUCED RESULTS. REPRESENTS THE REPRODUCED RESULTS. S, M+L AND S+L REPRESENT STEREO CAMERAS, MONOCULAR CAMERA WITH A LIDAR AND STEREO CAMERAS WITH A LIDAR RESPECTIVELY.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Depth Evaluation</cell></row><row><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell>Modality</cell><cell>(Lower the better)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RMSE (mm) MAE (mm)</cell><cell>iRMSE (1/km) iMAE (1/km)</cell></row><row><cell></cell><cell></cell><cell>PSMnet* [15]</cell><cell></cell><cell></cell><cell>Stereo</cell><cell>884</cell><cell>332</cell><cell>1.649</cell><cell>0.999</cell></row><row><cell></cell><cell></cell><cell cols="2">Sparse2Dense* [6]</cell><cell cols="2">Mono + LiDAR</cell><cell>840.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Guidenet [7]</cell><cell></cell><cell cols="2">Mono + LiDAR</cell><cell>777.78</cell><cell>221.59</cell><cell>2.39</cell><cell>1.00</cell></row><row><cell></cell><cell></cell><cell>NLSPN [18]</cell><cell></cell><cell cols="2">Mono + LiDAR</cell><cell>771.8</cell><cell>197.3</cell><cell>2.0</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell>CSPN++ [30]</cell><cell></cell><cell cols="2">Mono + LiDAR</cell><cell>725.43</cell><cell>207.88</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">Park et al. [11]</cell><cell cols="2">Stereo + LiDAR</cell><cell>2021.2</cell><cell>500.5</cell><cell>3.39</cell><cell>1.38</cell></row><row><cell></cell><cell></cell><cell>LiStereo [12]</cell><cell></cell><cell cols="2">Stereo + LiDAR</cell><cell>832.16</cell><cell>283.91</cell><cell>2.19</cell><cell>1.10</cell></row><row><cell></cell><cell></cell><cell>CCVN [10]</cell><cell></cell><cell cols="2">Stereo + LiDAR</cell><cell>749.3</cell><cell>252.5</cell><cell>1.3968</cell><cell>0.8069</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell cols="2">Stereo + LiDAR</cell><cell>636.2</cell><cell>205.1</cell><cell>1.8721</cell><cell>0.9870</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell></row><row><cell cols="7">QUANTITATIVE RESULTS OF DEPTH ESTIMATION NETWORKS IN THE</cell></row><row><cell></cell><cell cols="5">VIRTUAL-KITTI 2.0 DATASET.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Depth Evaluation</cell></row><row><cell>Method</cell><cell>Modality</cell><cell>RMSE</cell><cell cols="3">(Lower the better) MAE iRMSE</cell><cell>iMAE</cell></row><row><cell></cell><cell></cell><cell>(mm)</cell><cell>(mm)</cell><cell></cell><cell>(1/km)</cell><cell>(1/km)</cell></row><row><cell>PSMnet [15]*</cell><cell>S</cell><cell>5728</cell><cell>2235</cell><cell></cell><cell>9.805</cell><cell>4.380</cell></row><row><cell>Sparse2Dense [6]*</cell><cell>M+L</cell><cell>3357±8</cell><cell cols="2">1336±2.0</cell><cell>12.136±0.045</cell><cell>6.243±0.013</cell></row><row><cell>CCVN [10]*</cell><cell>S+L</cell><cell>3726.83±10.83</cell><cell cols="2">915.6±0.4</cell><cell>8.814±0.019</cell><cell>2.456±0.004</cell></row><row><cell>Ours</cell><cell>S+L</cell><cell>3217.16±1.84</cell><cell cols="2">712±2.0</cell><cell>7.168±0.048</cell><cell>2.694±0.011</cell></row></table><note>**</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDY OF THE TYPE OF VOLUME FOR DEPTH ESTIMATION. WE DIFFERENTIATE THE TYPE OF VOLUME AS COST VOLUME [5] (i.e., COSTV), DEPTH VOLUME (i.e., DEPTHV) BY YOU et al. [35], AND OUR FUSION VOLUME V (i.e., FUSIONV). NOTE THAT DEPTH VOLUME (i.e., DEPTHV) BY YOU et al. [35] IS A TRANSFORMED VOLUME FROM THE COST VOLUME, BUT OUR METHOD DIRECTLY INTERPOLATES SENSOR DATA INTO OUR VOLUME V . WE EVALUATE EACH METHOD ON THE KITTI COMPLETION VALIDATION , we extensively investigate our neural modules: the fusion volume and the FusionConv layer. The following experiments are conducted on the KITTI dataset</figDesc><table><row><cell></cell><cell></cell><cell cols="2">BENCHMARK [13].</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Preserve ( )</cell><cell></cell><cell></cell><cell cols="2">Depth Evaluation</cell><cell></cell></row><row><cell></cell><cell cols="2">Type of Volume</cell><cell cols="3">(Lower the better)</cell><cell></cell></row><row><cell>CostV</cell><cell cols="3">DepthV FusionV RMSE</cell><cell>MAE</cell><cell>iRMSE</cell><cell>iMAE</cell></row><row><cell>[5]</cell><cell>[35]</cell><cell>(ours)</cell><cell>(mm)</cell><cell>(mm)</cell><cell>(1/km)</cell><cell>(1/km)</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>884</cell><cell>332</cell><cell>1.649</cell><cell>0.999</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell>797</cell><cell>286</cell><cell>2.046</cell><cell>1.070</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell>636.2</cell><cell>205.1</cell><cell>1.8721</cell><cell>0.9870</cell></row><row><cell cols="7">simultaneously computes matching cost between features</cell></row><row><cell cols="7">of the two input modalities. More quantitative results and</cell></row><row><cell cols="7">information about the inference time are available in the</cell></row><row><cell cols="3">supplementary material.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">VIII. ABLATION STUDY</cell><cell></cell><cell></cell></row><row><cell cols="2">In this section</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY OF TYPE OF FUSION FOR DEPTH ESTIMATION. EACH METHOD EMBEDS DIFFERENT INFORMATION INTO THE FUSION VOLUME, SUCH AS RAW POINT CLOUD P , POINT FEATURE VECTORS FROM MULTILAYER PERCEPTRON (MLP) BY QI et al. [22] AND POINT FEATURE VECTORS FROM OUR FUSIONCONV LAYER.</figDesc><table><row><cell></cell><cell cols="2">Preserve ( )</cell><cell></cell><cell cols="2">Depth Evaluation</cell><cell></cell></row><row><cell cols="3">Type of point network</cell><cell></cell><cell cols="2">(Lower the better)</cell><cell></cell></row><row><cell>Raw</cell><cell>MLP</cell><cell cols="2">FusionConv RMSE</cell><cell>MAE</cell><cell>iRMSE</cell><cell>iMAE</cell></row><row><cell>points</cell><cell>[22]</cell><cell>(ours)</cell><cell>(mm)</cell><cell>(mm)</cell><cell>(1/km)</cell><cell>(1/km)</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell>669</cell><cell>226</cell><cell>2.169</cell><cell>1.120</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell>652</cell><cell>212</cell><cell>2.099</cell><cell>1.066</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell>636.2</cell><cell>205.1</cell><cell>1.8721</cell><cell>0.9870</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc>ABLATION STUDY OF DIFFERENT LEVELS OF FUSION. EARLY FUSION TAKES PRE-PROCESSING STEPS TO PROJECT POINT CLOUDS INTO IMAGE DOMAIN FOR FUSION, WHILE INTERMEDIATE FUSION PROPOSES FUSION AT FEATURE SPACE, e.g., FUSION VOLUME.</figDesc><table><row><cell></cell><cell>Preserve ( )</cell><cell></cell><cell></cell><cell cols="2">Depth Evaluation</cell><cell></cell></row><row><cell cols="2">Level of fusion</cell><cell></cell><cell></cell><cell cols="2">(Lower the better)</cell><cell></cell></row><row><cell>Early</cell><cell>Intermediate</cell><cell>Volume</cell><cell>RMSE</cell><cell>MAE</cell><cell>iRMSE</cell><cell>iMAE</cell></row><row><cell>fusion</cell><cell>fusion (ours)</cell><cell></cell><cell>(mm)</cell><cell>(mm)</cell><cell>(1/km)</cell><cell>(1/km)</cell></row><row><cell>7</cell><cell></cell><cell>CostV</cell><cell>744</cell><cell>249</cell><cell>2.026</cell><cell>1.022</cell></row><row><cell>8</cell><cell></cell><cell>FusionV</cell><cell>650</cell><cell>215</cell><cell>1.912</cell><cell>0.964</cell></row><row><cell>9</cell><cell></cell><cell>FusionV</cell><cell>636.2</cell><cell>205.1</cell><cell>1.8721</cell><cell>0.9870</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE I</head><label>I</label><figDesc>ABLATION STUDY FOR DEPTH EVALUATION IN DIFFERENT RANGE OF DISTANCE. WE UTILIZE THE KITTI DEPTH COMPLETION VALIDATION BENCHMARK [13] FOR EVALUATION. NOTE THAT ABSREL AND SQREL REPRESENTS ABSOLUTE RELATIVE DIFFERENCE AND SQUARED RELATIVE DIFFERENCE, RESPECTIVELY<ref type="bibr" target="#b33">[34]</ref>.</figDesc><table><row><cell>Evaluation Depth range</cell><cell>Method</cell><cell cols="2">AbsRel (m) SqRel (m)</cell><cell cols="2">Depth Evaluation in KITTI (Lower the better) RMSE (mm) MAE (mm)</cell><cell cols="2">iRMSE (1/km) iMAE (1/km)</cell></row><row><cell>0m − 20m</cell><cell>CCVN [10] Ours</cell><cell>0.0101 0.0106</cell><cell>0.0078 0.0037</cell><cell>280.8 196.8</cell><cell>115.3 115.7</cell><cell>1.9214 1.9237</cell><cell>0.9931 1.1057</cell></row><row><cell>20m − 40m</cell><cell>CCVN [10] Ours</cell><cell>0.0196 0.0112</cell><cell>0.0387 0.0077</cell><cell>1032.3 366.5</cell><cell>557.3 162.2</cell><cell>1.4523 1.8272</cell><cell>0.7240 1.0029</cell></row><row><cell>40m − 80m</cell><cell>CCVN [10] Ours</cell><cell>0.0373 0.0116</cell><cell>0.1810 0.0136</cell><cell>3151.1 631.2</cell><cell>2051.6 205.7</cell><cell>1.4185 1.8018</cell><cell>0.7279 0.9699</cell></row><row><cell></cell><cell cols="2">(a) Left image</cell><cell></cell><cell></cell><cell cols="2">(b) Projected point clouds</cell><cell></cell></row><row><cell cols="2">Back-project</cell><cell></cell><cell cols="2">Embed</cell><cell></cell><cell>Back-project</cell><cell></cell></row><row><cell cols="2">to metric space</cell><cell></cell><cell cols="2">(=quantize)</cell><cell></cell><cell>to metric space</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Fusion volume</cell><cell></cell><cell>Cost volume</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AbsDist</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AbsDist</cell><cell></cell></row><row><cell>0.943 m</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">4.470 m</cell><cell></cell></row><row><cell cols="3">(c) Back-projected point clouds</cell><cell cols="2">(d) Colorized point clouds</cell><cell cols="3">(e) Back-projected point clouds</cell></row><row><cell cols="2">from fusion volume</cell><cell></cell><cell></cell><cell></cell><cell cols="2">from cost volume</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE II INFERENCE</head><label>II</label><figDesc>SPEED OF EACH STAGE OF OUR NETWORK. WE SPLIT THE OUR NETWORK INTO THREE STAGES AS ILLUSTRATED IN FIG. 2 OF THEMANUSCRIPT AND MEASURE THE INFERENCE SPEED OF EACH STAGE DURING THE TEST.</figDesc><table><row><cell></cell><cell>Feature extraction stage</cell><cell>Our overall network Volume aggregation stage</cell><cell>Depth regression stage</cell><cell>Total</cell></row><row><cell>Inference time (sec)</cell><cell>0.027179</cell><cell>1.347444</cell><cell>0.027745</cell><cell>1.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE III INFERENCE</head><label>III</label><figDesc>SPEED OF FEATURE EXTRACTION STAGE. OUR POINT FEATURE EXTRACTION LAYER, FUSIONCONV, SHOWS FAST INFERENCE SPEED IN SPITE OF DYNAMICAL CLUSTERING SCHEME. THE TEST HAS BEEN CONDUCTED UNDER ∼25K POINT CLOUDS P AND 384(H) × 1248(W ) SIZE OF STEREO IMAGES AS INPUT.</figDesc><table><row><cell></cell><cell>Image feature extraction</cell><cell cols="2">Feature extraction 1st Fusion Conv 2nd Fusion Conv</cell><cell>3rd FusionConv</cell><cell>Total</cell></row><row><cell>Inference time (sec)</cell><cell>0.018542</cell><cell>0.00102</cell><cell>0.001312</cell><cell>0.005501</cell><cell>0.027179</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV INFERENCE</head><label>IV</label><figDesc>TIME OF RECENT METHODS<ref type="bibr" target="#b10">[11]</ref>,<ref type="bibr" target="#b9">[10]</ref> BASELINE NETWORKS<ref type="bibr" target="#b4">[5]</ref>,<ref type="bibr" target="#b14">[15]</ref>, AND OURS</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Park et al. [11] GC-Net [5] PSMNet [15]</cell><cell>CCVN [10]</cell><cell>Ours</cell></row><row><cell></cell><cell>Inference time (sec)</cell><cell>0.043</cell><cell>0.962</cell><cell>0.985</cell><cell>1.011</cell><cell>1.40</cell></row><row><cell></cell><cell>Left image</cell><cell>Right image</cell><cell></cell><cell cols="2">Ground truth disparity</cell><cell>Ground truth depth</cell></row><row><cell>depth</cell><cell>RMSE:405mm</cell><cell>RMSE:440mm</cell><cell></cell><cell>RMSE:635mm</cell><cell cols="2">RMSE:667mm</cell></row><row><cell>Inferred</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>clouds</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Point</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a)</cell><cell>15000 points</cell><cell>5000 points</cell><cell></cell><cell cols="2">1000 points</cell><cell>10 points</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">† For simplicity, we transform the point clouds in the LiDAR coordinates into the referential camera coordinates using extrinsic parameters.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 4</ref><p>. Visualization of the composition of the fusion volume. Under the known calibration parameters and the baseline between stereo cameras, there is a geometric relation between the location of the voxel [u, v, z] and the feature data from stereo images and point clouds. This geometric fact is utilized for the composition of the fusion volume. stereo image features (F L , F R ∈ R C×H×W ) and point feature vectors (F P ∈ R C×N ). The pre-defined position of each voxel [u, v, z] can be projected into the right feature maps by the known camera intrinsic matrix K. In addition, conversion of the voxel coordinate [u, v, z] into the lidar coordinate [x, y, z] can be calculated through the given calibration parameters in the KITTI Completion dataset <ref type="bibr" target="#b12">[13]</ref>.</p><p>To do so, our network learns to aggregate the fusion volume to find the correspondence between stereo images and point clouds (i.e., triangulation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. FUSIONCONV</head><p>In this section, we describe the details of FusionConv by comparing it with the recent point-based architecture studies <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Our method can be viewed as in-between the two recent point-based architecture methods, the parametric continuous convolution network (PCCN <ref type="bibr" target="#b22">[23]</ref>) and the interpolated convolution layer (InterpConv <ref type="bibr" target="#b25">[26]</ref>). FusionConv is base on these two pioneering studies <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>. However, the main difference is a different sampling strategy which is related to what we described in Sec. I of the manuscripts, -aligning different sensor data into a unified space is an essential step to fully operate depth estimated task. For a clear answer, here we describe the detailed difference from recent point-based architecture methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref> with respect to algorithmic viewpoint.</p><p>The PCCN <ref type="bibr" target="#b22">[23]</ref> utilizes K-nearest neighbors (K-NN), which is a fixed number of neighboring points, for sampling neighboring point clouds. In a real-world scenario, most of the point clouds are located near a LiDAR sensor, and densely crowded points can have limited representations when K-NN based methods are utilized.</p><p>To overcome this problem, InterpConv <ref type="bibr" target="#b25">[26]</ref> develops an idea of interpolation to apply convolutional operation into dynamic numbers of point clouds in the 3D metric space. This method pre-defines the size of the cube-shaped windows and all points within these windows are considered as neighboring points to be convolved. This window-based sampling concept can consider various numbers of point clouds, which the previous work <ref type="bibr" target="#b22">[23]</ref> colud not do. Despite of the improvement, InterpConv <ref type="bibr" target="#b25">[26]</ref> has an issue in the fusion of images and point clouds.</p><p>This cube-shaped window in InterpConv has different receptive fields in the homogeneous pixel coordinate. As we discussed in Sec. I of the manuscript, aligning different sensor data into a unified space is an essential step to fully operate a depth estimation task from stereo images and point clouds. To align two modalities in a unified volume (fusion volume) whose coordinate obeys [u, v, z], we need to re-design the windows to align point clouds ([x, y, z]) into the fusion volume <ref type="bibr">([u, v, z]</ref>). From the point of view of geometry, FusionConv samples numerous points along the reference camera ray and its neighbor (i.e., frustumshape windows). To do so, the fused point clouds can have receptive fields that are similar to those of image feature maps.</p><p>The details of the FusionConv are close to those of the previous works <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Our FusionConv, as its name implies, is designed for the fusion of point clouds and images in a unified volumetric space [u, v, z], which has not yet been fully studied in point clouds based methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref> and stereo-LiDAR fusion methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>. <ref type="table">In Tables I and II of the manuscript and Table I</ref> of the supplementary material, our method shows higher accuracy for AbsRel, SqRel, RMSE, and MAE, whereas the previous work, CCVN <ref type="bibr" target="#b9">[10]</ref>, shows reasonable inverse depth quality for iRMSE and iMAE. This fact suggests that our method can estimate accurate depth information at the distant area, but the CCVN <ref type="bibr" target="#b9">[10]</ref> shows strength for the closer area. This property comes from our design choice of the depth range in the fusion volume, which can affect the depth performance in close or farther areas. This aspect can be viewed as a deficiency of our method, but this result is consistent with our intention. Our volumetric propagation network is designed to maintain the metric accuracy in point clouds, especially points at a distant region where a pair of stereo images has difficulty in finding the correspondence. Inverse depth metrics (iRMSE and iMAE) and depth quality (AbsRel, SqRel, RMSE, and MAE) are currently a trade-off. We expect a future work to seamlessly describe the 3D scene and show the highest quality in both depth and inverse depth metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. EVALUATION METRICS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Displets: Resolving stereo ambiguities using object knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning guided convolutional network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01238</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stereo object matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Imtiaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Seg-ment2regress: Monocular 3d vehicle localization in two stages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d lidar and stereo fusion using stereo matching network with conditional cost volume normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High-precision depth estimation with the 3d lidar and stereo fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Listereo: Generate dense depth maps from lidar and stereo imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ramanagopalg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Virtual kitti 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Humenberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10773</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dpsnet: End-to-end deep plane sweep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning joint 2d-3d representations for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Non-local spatial propagation network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ga-net: Guided aggregation net for end-to-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
