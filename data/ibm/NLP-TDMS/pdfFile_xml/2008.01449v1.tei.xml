<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prior Guided Feature Enrichment Network for Few-Shot Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Michelle</forename><surname>Shu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zhicheng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
						</author>
						<title level="a" type="main">Prior Guided Feature Enrichment Network for Few-Shot Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Few-shot Segmentation</term>
					<term>Few-shot Learning</term>
					<term>Semantic Segmentation</term>
					<term>Scene Understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art semantic segmentation methods require sufficient labeled data to achieve good results and hardly work on unseen classes without fine-tuning. Few-shot segmentation is thus proposed to tackle this problem by learning a model that quickly adapts to new classes with a few labeled support samples. Theses frameworks still face the challenge of generalization ability reduction on unseen classes due to inappropriate use of high-level semantic information of training classes and spatial inconsistency between query and support targets. To alleviate these issues, we propose the Prior Guided Feature Enrichment Network (PFENet). It consists of novel designs of (1) a training-free prior mask generation method that not only retains generalization power but also improves model performance and (2) Feature Enrichment Module (FEM) that overcomes spatial inconsistency by adaptively enriching query features with support features and prior masks. Extensive experiments on PASCAL-5 i and COCO prove that the proposed prior generation method and FEM both improve the baseline method significantly. Our PFENet also outperforms state-of-the-art methods by a large margin without efficiency loss. It is surprising that our model even generalizes to cases without labeled support samples. Our code is available at https://github.com/Jia-Research-Lab/PFENet/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>R APID development of deep learning has brought significant improvement to semantic segmentation. The iconic frameworks <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b2">[3]</ref> have profited a wide range of applications of automatic driving, robot vision, medical image, etc. The performance of these frameworks, however, worsens quickly without sufficient fully-labeled data or when working on unseen classes. Even if additional data is provided, fine-tuning is still time-and resource-consuming.</p><p>To address this issue, few-shot segmentation was proposed <ref type="bibr" target="#b32">[33]</ref> where data is divided into a support set and a query set. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, images from both support and query sets are first sent to the backbone network to extract features. Feature processing can be accomplished by generating weights for the classifier <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b40">[41]</ref>, cosinesimilarity calculation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b22">[23]</ref>, or convolutions <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b0">[1]</ref> to generate the final prediction.</p><p>The support set provides information about the target class that helps the model to make accurate segmentation prediction on the query images. This process mimics the scenario where a model makes the prediction of unseen classes on testing images (query) with few labeled data (support). Therefore, a few-shot model needs to quickly adapt to the new classes. However, the common problems of existing few-shot segmentation methods include generalization loss due to misuse of high-level features and spatial inconsistency between the query and support samples. In this paper, we mainly tackle these two difficulties. mon semantic segmentation models rely heavily on highlevel features with semantic information. Experiments of CANet <ref type="bibr" target="#b53">[54]</ref> show that simply adding high-level features during feature processing in a few-shot model causes performance drop. Thus the way to utilize semantic information in the few-shot setting is not straightforward. Unlike previous methods, we use ImageNet <ref type="bibr" target="#b31">[32]</ref> pre-trained highlevel features of the query and support images to produce 'priors' for the model. These priors help the model to better identify targets in query images. Since the prior generation process is training-free, the resulting model does not lose the generalization ability to unseen classes, despite the frequent use of high-level information of seen classes during training. Spatial Inconsistency Besides, due to the limited samples, scale and pose of each support object may vary greatly from its query target, which we call spatial inconsistency. To arXiv:2008.01449v1 [cs.CV] 4 Aug 2020 tackle this problem, we propose a new module named Feature Enrichment Module (FEM) to adaptively enrich query features with the support features. Ablation study in Section 4.3 shows that merely incorporating the multi-scale scheme to tackle the spatial inconsistency is sub-optimal by showing that FEM provides conditioned feature selection that helps retain essential information passed across different scales. FEM achieves superior performance than other multi-scale structures, such as HRNet <ref type="bibr" target="#b43">[44]</ref>, PPM <ref type="bibr" target="#b59">[60]</ref>, ASPP <ref type="bibr" target="#b3">[4]</ref> and GAU <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization Reduction &amp; High-level Features Com-</head><p>Finally, based on the proposed prior generation method and Feature Enrichment Module (FEM), we build a new network -Prior Guided Feature Enrichment Network (PFENet). The ResNet-50 based PFENet only contains 10.8 M learnable parameters, and yet achieves new state-of-theart results on both PASCAL-5 i <ref type="bibr" target="#b32">[33]</ref> and COCO <ref type="bibr" target="#b20">[21]</ref> benchmark with 15.9 and 5.1 FPS with 1-shot and 5-shot settings respectively. Moreover, we manifest the effectiveness by applying our model to the zero-shot scenario where no labeled data is available. The result is surprising -PFENet sill achieves decent performance without major structural modification.</p><p>Our contribution in this paper is threefold: <ref type="bibr">•</ref> We leverage high-level features and propose training-free prior generation to greatly improve prediction accuracy and retain high generalization.</p><p>• By incorporating the support feature and prior information, our FEM helps adaptively refine the query feature with the conditioned inter-scale information interaction.</p><p>• PFENet achieves new state-of-the-art results on both PASCAL-5 i and COCO datasets without compromising efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Segmentation</head><p>Semantic segmentation is a fundamental topic to predict the label for each pixel. The Fully Convolutional Network (FCN) <ref type="bibr" target="#b33">[34]</ref> is developed for semantic segmentation by replacing the fully-connected layer in a classification framework with convolutional layers. Following approaches, such as DeepLab <ref type="bibr" target="#b2">[3]</ref>, DPN <ref type="bibr" target="#b23">[24]</ref> and CRF-RNN <ref type="bibr" target="#b61">[62]</ref>, utilize CRF/MRF to help refine coarse prediction. The receptive field is important for semantic segmentation; thus DeepLab <ref type="bibr" target="#b2">[3]</ref> and Dilation <ref type="bibr" target="#b49">[50]</ref> introduce the dilated convolution to enlarge the receptive field. Encoder-decoder structures <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref> are adopted to help reconstruct and refine segmentation in steps. Contextual information is vital for complex scene understanding. ParseNet <ref type="bibr" target="#b21">[22]</ref> applies global pooling for semantic segmentation. PSPNet <ref type="bibr" target="#b59">[60]</ref> utilizes a Pyramid Pooling Module (PPM) for context information aggregation over different regions, which is very effective. DeepLab <ref type="bibr" target="#b2">[3]</ref> develops atrous spatial pyramid pooling (ASPP) with filters in different dilation rates. Attention models are also introduced. PSANet <ref type="bibr" target="#b60">[61]</ref> develops point-wise spatial attention with a bi-directional information propagation paradigm. Channelwise attention <ref type="bibr" target="#b54">[55]</ref> and non-local style attention <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b15">[16]</ref> are also effective for segmentation. These methods work well on large-sample classes. They are not designed to deal with rare and unseen classes. They also cannot be easily adapted without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Few-shot Learning</head><p>Few-shot learning aims at image classification when only a few training examples are available. There are meta-learning based methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b6">[7]</ref> and metric-learning ones <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Data is essential to deep models; therefore, several methods improve performance by synthesizing more training samples <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Different from fewshot learning where prediction is at the image-level, fewshot segmentation makes pixel-level predictions, which is much more challenging.</p><p>Our work closely relates to metric-learning based fewshot learning methods. Prototypical network <ref type="bibr" target="#b36">[37]</ref> is trained to map input data to a metric space where classes are represented as prototypes. During inference, classification is achieved by finding the closest prototype for each input image, because data belonging to the same class should be close to the prototype. Another representative metricbased work is the relation network <ref type="bibr" target="#b39">[40]</ref> that projects query and support images to 1×1 vectors and then performs classification based on the cosine similarity between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Few-shot Segmentation</head><p>Few-shot segmentation places the general semantic segmentation in a few-shot scenario, where models perform dense pixel labeling on new classes with only a few support samples. OSLSM <ref type="bibr" target="#b32">[33]</ref> first tackles few-shot segmentation by learning to generate weights of the classifier for each class. PL <ref type="bibr" target="#b4">[5]</ref> applies prototyping <ref type="bibr" target="#b36">[37]</ref> to the segmentation task. It learns a prototype for each class and calculates the cosine similarity between pixels and prototypes to make the prediction. More recently, CRNet <ref type="bibr" target="#b47">[48]</ref> processes query and support images through a Siamese Network followed by a Cross-Reference Module to mine cooccurrent features in two images. PANet <ref type="bibr" target="#b44">[45]</ref> introduces prototype alignment regularization that encourages the model to learn consistent embedding prototypes for better performance, and CANet <ref type="bibr" target="#b53">[54]</ref> uses the iterative optimization module on the merged query and support feature to iteratively refine results.</p><p>Similar to CANet <ref type="bibr" target="#b53">[54]</ref>, we use convolution to replace the cosine similarity that may not well tackle complex pixelwise classification in the segmentation task. However, different from CANet, our baseline model uses fewer convolution operations and still achieves decent performance.</p><p>As discussed before, these few-shot segmentation methods do not sufficiently consider generalization loss and spatial inconsistency. Unlike PGNet <ref type="bibr" target="#b52">[53]</ref> that uses a graphbased pyramid structure to refine results via Graph Attention Unit (GAU) followed by three residual blocks and an ASPP <ref type="bibr" target="#b3">[4]</ref>, we instead incorporate a few basic convolution operations with the proposed prior masks and FEM in a multi-scale structure to accomplish decent performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR METHOD</head><p>In this section, we first briefly describe the few-shot segmentation task in Section 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Description</head><p>A few-shot semantic segmentation system has two sets, i.e., the query set Q and support set S. Given K samples from support set S, the goal is to segment the area of unseen class C test from each query image I Q in the query set. Models are trained on classes C train (base) and tested on previously unseen classes C test (novel) in episodes (C train ∩ C test = ∅). The episode paradigm was proposed in <ref type="bibr" target="#b42">[43]</ref> and was first applied to few-shot segmentation in <ref type="bibr" target="#b32">[33]</ref>. Each episode is formed by a support set S and a query set Q of the same class c. The support set S consists of K samples S = {S 1 , S 2 , ..., S K } of class c, which we call 'K-shot scenario'. The i-th support sample S i is a pair of {I Si , M Si } where I Si and M Si are the support image and label of c respectively. For the query set, Q = {I Q , M Q } where I Q is the input query image and M Q is the ground truth mask of class c. The query-support pair {I Q , S} = {I Q , I S1 , M S1 , I S2 , M S2 , ..., I S K , M S K } forms the input data batch to the model. The ground truth M Q of the query image is invisible to the model and is used to evaluate the prediction on the query image in each episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prior for Few-Shot Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Important Observations</head><p>CANet <ref type="bibr" target="#b53">[54]</ref> outperforms previous work by a large margin on the benchmark PASCAL-5 i dataset by extracting only middle-level features from the backbone (e.g., conv3 x and conv4 x of ResNet-50). Experiments in CANet also show that the high-level (e.g., conv5 x of ResNet-50) features lead to performance reduction. It is explained in <ref type="bibr" target="#b53">[54]</ref> that the middle-level feature performs better since it constitutes object parts shared by unseen classes, but our alternative explanation is that the semantic information contained in the highlevel feature is more class-specific than the middle-level feature, indicating that the former is more likely to negatively affect model's generalization power to unseen classes. In addition, higher-level feature directly provides semantic information of the training classes C train , contributing more in identifying pixels belonging to C train and reducing the training loss than the middle-level information. Consequently, such behavior results in a preference for C train . The lack of generalization and the preference for the training classes are both harmful for evaluation on unseen test classes C test .</p><p>It is noteworthy that contrary to the finding that highlevel feature adversely affects performance in few-shot segmentation, prior segmentation frameworks <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b30">[31]</ref> exploit these features to provide semantic cues for final prediction. This contradiction motivates us to find a way to make use of high-level information in a training-class-insensitive way to boost performance in few-shot segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Prior Generation</head><p>In our work, we transform the ImageNet <ref type="bibr" target="#b31">[32]</ref> pre-trained high-level feature containing semantic information into a prior mask that tells the probability of pixels belonging to a target class as shown in <ref type="figure">Figure 2</ref>. During training, the backbone parameters are fixed as those in <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b53">[54]</ref>. Therefore, the prior generation process does not bias towards training classes C train and upholds class-insensitivity during the evaluation on unseen test classes C test . Let I Q , I S denote the input query and support images, M S denote the binary support mask, F denote the backbone network, and X Q , X S denote the high-level query and support features. We have</p><formula xml:id="formula_0">X Q = F(I Q ), X S = F(I S ) M S ,<label>(1)</label></formula><p>where is the Hadamard product -the sizes of X Q and X S are both [h, w, c]. Note that the output of F is processed with a ReLU function. So the binary support mask M S removes the background in support feature by setting it to zero. Specifically, we define the prior Y Q of query feature X Q as the mask that reveals the pixel-wise correspondence between X Q and X S . A pixel of query feature X Q with a high value on Y Q means that this pixel has a high correspondence with at least one pixel in support feature. Thus, it is very likely to be in the target area of the query image. By setting the background on support feature to zero, pixels of query feature yield no correspondence with the background on support feature -they only correlate with the foreground target area. To generate Y Q , we first calculate the pixel-wise A#-"4E*.,"# !"#(0#.4*.,"# .')*,"'%4("#3/5'(*%6$2,7' cosine similarity cos(x q , x s ) ∈ R between feature vectors of x q ∈ X Q and x s ∈ X S as</p><formula xml:id="formula_1">cos(x q , x s ) = x T q x s x q x s q, s ∈ {1, 2, ..., hw}<label>(2)</label></formula><p>For each x q ∈ X Q , we take the maximum similarity among all support pixels as the correspondence value c q ∈ R as</p><formula xml:id="formula_2">c q = max s∈{1,2,...,hw} (cos(x q , x s )),<label>(3)</label></formula><formula xml:id="formula_3">C Q = [c 1 , c 2 , ..., c hw ] ∈ R hw×1 .<label>(4)</label></formula><p>Then we produce the prior mask Y Q by reshaping C Q ∈ R hw×1 into Y Q ∈ R h×w×1 . We process Y Q with a min-max normalization (Eq. (5)) to normalize the values to between 0 and 1, as shown in <ref type="figure">Figure 2</ref>. In Eq. (5), is set to 1e − 7 in our experiments.</p><formula xml:id="formula_4">Y Q = Y Q − min(Y Q ) max(Y Q ) − min(Y Q ) + .<label>(5)</label></formula><p>The key point of our proposed prior generation method lies in the use of fixed high-level features to yield the prior mask by taking the maximum value from a similarity matrix of size hw × hw as given in Eqs. <ref type="bibr" target="#b1">(2)</ref> and <ref type="bibr" target="#b2">(3)</ref>, which is rather simple and effective. Ablation study comparing other alternative methods used in <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b57">[58]</ref> in Section 4.4 demonstrates the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Enrichment Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Motivation</head><p>Existing few-shot segmentation frameworks <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b4">[5]</ref> use masked global average pooling for extracting class vectors from support images before further processing. However, global pooling on support images results in spatial information inconsistency since the area of query target may be much larger or smaller than support samples. Therefore, using a global pooled support feature to directly match each pixel of the query feature is not ideal.</p><p>A natural alternative is to add PPM <ref type="bibr" target="#b59">[60]</ref> or ASPP <ref type="bibr" target="#b3">[4]</ref> to provide multi-level spatial information to the feature. PPM and ASPP help the baseline model yield better performance (as demonstrated in our later experiments). However, these two modules are suboptimal in that: 1) they provide spatial information to merged features without specific refinement process within each scale; 2) the hierarchical relations across different scales are ignored.</p><p>To alleviate these issues, we disentangle the multi-scale structure and propose the feature enrichment module (FEM) to 1) horizontally interact the query feature with the support features and prior masks in each scale, and 2) vertically leverage the hierarchical relations to enrich coarse feature maps with essential information extracted from the finer feature via a top-down information path. After horizontal and vertical optimization, features projected into different scales are then collected to form the new query feature. Details of FEM are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Module Structure</head><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the feature enrichment module (FEM) takes the query feature, prior mask and support feature as input. It outputs the refined query feature with enriched information from the support feature. The enrichment process can be divided into three sub-processes of 1) inter-source enrichment that first projects input to different scales and then interacts the query feature with support feature and prior mask in each scale independently; 2) inter-scale interaction that selectively passes essential information between merged query-support features across different scales; and 3) information concentration that merges features in different scales to finally yield the refined query feature. An illustration of FEM with four scales and a top-down path for inter-scale interaction is shown in <ref type="figure" target="#fig_8">Figure 4</ref>.</p><formula xml:id="formula_5">Inter-Source Enrichment In FEM, B = [B 1 , B 2 , ..., B n ]</formula><p>denotes n different spatial sizes for average pooling. They are in the descending order B 1 &gt; B 2 &gt; ... &gt; B n . The input query feature X Q ∈ R h×w×c is first processed with adaptive average pooling to generate n sub-query features</p><formula xml:id="formula_6">X F EM Q = [X 1 Q , X 2 Q , ..., X n Q ] of n different spatial sizes X i Q ∈ R B i ×B i ×c .</formula><p>n spatial sizes make the global-average pooled support feature X S ∈ R 1×1×c be expanded to different n feature maps X F EM</p><formula xml:id="formula_7">S = [X 1 S , X 2 S , ..., X n S ] (X i S ∈ R B i ×B i ×c ), and the prior Y Q ∈ R h×w×1 is accordingly resized to Y F EM Q = [Y 1 Q , Y 2 Q , ..., Y n Q ] (Y i Q ∈ R B i ×B i ×1 ). Then, for i ∈ {1, 2, ..., n}, we concatenate X i Q , X i S</formula><p>and Y i Q , and process each concatenated feature with convolutions to generate the merged query features</p><formula xml:id="formula_8">X i Q,m ∈ R B i ×B i ×c as X i Q,m = F 1×1 (X i Q ⊕ X i S ⊕ Y i Q ),<label>(6)</label></formula><p>where F 1×1 represents the 1×1 convolution that yields the merged feature with c = 256 output channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-Scale Interaction</head><p>It is worth noting that tiny objects may not exist in the down-sampled feature maps. A topdown path adaptively passing information from finer features to the coarse ones is conducive to building a hierarchical relationship within our feature enrichment module. Now the interaction is between not only the query and support features in each scale (horizontal), but also the merged features of different scales (vertical), which is beneficial to the overall performance. The circled M in <ref type="figure" target="#fig_8">Figure 4</ref> represents the inter-scale merging module M that interacts between different scales by selectively passing useful information from the auxiliary feature to the main feature to generate the refined feature X i Q,new . This process can be written as</p><formula xml:id="formula_9">X i Q,new = M(X M ain,i Q,m , X Aux,i Q,m ),<label>(7)</label></formula><p>where X M ain,i Q,m is the main feature and X Aux,i Q,m is the auxiliary feature for the i-th scale B i . For example, in an FEM with a top-down path for inter-scale interaction, finer feature (auxiliary) X i−1 Q,m needs to provide additional information to the coarse feature (main)</p><formula xml:id="formula_10">X i Q,m (B i−1 &gt; B i , i 2). In this case, X Aux,i Q,m = X i−1 Q,m and X M ain,i Q,m = X i Q,m .</formula><p>Other alternatives for inter-scale interaction include the bottomup path that enriches finer features (main) with information coming from the coarse ones (auxiliary), and the bidirectional variants, i.e., a top-down path followed by a bottom-up path, and a bottom-up path followed by a topdown path. The top-down path shows its superiority in Section 4.3.1.</p><p>The specific structure of the inter-scale merging module M is shown in <ref type="figure">Figure 5</ref>. We first resize the auxiliary feature to the same spatial size as the main feature. Then we use a 1×1 convolution α to extract useful information from the auxiliary feature conditioned on the main feature. Two 3×3 convolutions β followed are used to finish the interaction and output the refined feature. The residual link within the inter-scale merging module M is used for keeping the integrity of the main feature in the output feature X i Q,new . For those features that do not have auxiliary features (e.g., the first merged feature X 1 Q,m in the top-down path and the last merged feature X n Q,m in the bottom-up path), we simply ignore the concatenation with the auxiliary feature in Mthe refined feature is produced only by the main feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Concentration</head><p>After inter-scale interaction, n refined feature maps are obtained as X i Q,new , i ∈ {1, 2, ..., n}. Finally, the output query feature X Q,new ∈ R h×w×c is formed by interpolation and concatenation of n refined feature maps X i Q,new ∈ R h×w×c followed by an 1×1 convolution F 1×1 as feature with information coming from the support feature at each location under the guidance of prior mask and supervision of ground-truth. Moreover, the vertical interscale interaction supplements the main feature with the conditioned information provided by the auxiliary feature. Therefore, FEM yields greater performance gain on baseline than other feature enhancement designs (e.g., PPM <ref type="bibr" target="#b59">[60]</ref>, ASPP <ref type="bibr" target="#b3">[4]</ref> and GAU <ref type="bibr" target="#b52">[53]</ref>). Experiments in Section 4.3 provide more details.</p><formula xml:id="formula_11">X Q,new = F 1×1 (X 1 Q,new ⊕ X 2 Q,new ... ⊕ X n Q,new ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prior Guided Feature Enrichment Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Model Description</head><p>Based on the proposed prior generation method and the feature enrichment module (FEM), we propose the Prior Guided Feature Enrichment Network (PFENet) as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The ImageNet <ref type="bibr" target="#b31">[32]</ref> pre-trained CNN is shared by support and query images to extract features. The extracted middle-level support and query features are processed by 1×1 convolution to reduce the channel number to 256. After feature extraction and channel reduction, the feature enrichment module (FEM) enriches the query feature with the support feature and prior mask. On the output feature of FEM, we apply a convolution block ( <ref type="figure">Figure  7</ref>(a)) followed by a classification head to yield the final prediction. Classification head is composed of one 3×3 convolution and 1×1 convolution with Softmax function as shown in <ref type="figure">Figure 7</ref>(b). For all backbone networks, we use the outputs of the last layers of conv3 x and conv4 x as middlelevel features M to generate the query and support features by concatenation, and take the output of the last layer of conv5 x as high-level features H to produce the prior mask.</p><p>In the 5-shot setting, we simply take the average of 5 pooled support features as the new support feature before concatenation with the query feature. Similarly, the final prior mask before the concatenation in FEM is also obtained by averaging five prior masks produced by one query feature with different support features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Loss Function</head><p>We select the cross entropy loss as our loss function. As shown in Section 3.3.2 and <ref type="figure" target="#fig_1">Figure 3</ref>, for a FEM with n different spatial sizes, the intermediate supervision on</p><formula xml:id="formula_12">X i Q,new (i ∈ {1, 2, ..., n}) generates n losses L i 1 (i ∈ {1, 2, ..., n})</formula><p>. The final prediction of PFENet generates the second loss L 2 . The total loss L is the weighted sum of L i 1 and L 2 as</p><formula xml:id="formula_13">L = σ n n i=1 L i 1 + L 2 ,<label>(9)</label></formula><p>where σ is used to balance the effect of intermediate supervision. We empirically set σ to 1.0 in all experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Datasets We use the datasets of PASCAL-5 i <ref type="bibr" target="#b32">[33]</ref> and COCO <ref type="bibr" target="#b20">[21]</ref> in evaluation. PASCAL-5 i is composed of PASCAL VOC 2012 <ref type="bibr" target="#b5">[6]</ref> and extended annotations from SDS <ref type="bibr" target="#b11">[12]</ref> datasets. 20 classes are evenly divided into 4 folds i ∈ {0, 1, 2, 3} and each fold contains 5 classes. Following OSLSM <ref type="bibr" target="#b32">[33]</ref>, we randomly sample 1,000 query-support pairs in each test. Following <ref type="bibr" target="#b27">[28]</ref>, we also evaluate our model on COCO by splitting four folds from 80 classes. Thus each fold has 20 classes. The set of class indexes contained in fold i is written as {4x − 3 + i} where x ∈ {1, 2, ..., 20}, i ∈ {0, 1, 2, 3}. Note that the COCO validation set contains 40,137 images (80 classes), which are much more than the images in PASCAL-5 i . Therefore, 1,000 randomly sampled query-support pairs used in previous work are not enough for producing reliable testing results on 20 test classes. We instead randomly sample 20,000 query-support pairs during the evaluation on each fold, making the results more stable than testing on 1,000 query-support pairs used in previous work. Stability statistics are shown in Section 4.7.</p><p>For both PASCAL-5 i and COCO, when testing the model on one fold, we use the other three folds to train the model for cross-validation. We take the average of five testing results with different random seeds for comparison as shown in <ref type="table" target="#tab_1">Tables 9 and 10</ref>.</p><p>Experimental Setting Our framework is constructed on PyTorch. We select VGG-16 <ref type="bibr" target="#b35">[36]</ref>, ResNet-50 <ref type="bibr" target="#b13">[14]</ref> and ResNet-101 <ref type="bibr" target="#b13">[14]</ref> as our backbones for fair comparison with other methods. The ResNet we use is the dilated version used in previous work <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The VGG we use is the original version <ref type="bibr" target="#b35">[36]</ref>. All backbone networks are initialized with ImageNet <ref type="bibr" target="#b31">[32]</ref> pretrained weights. Other layers are initialized by the default setting of PyTorch. We use SGD as our optimizer. The momemtum and weight decay are set to 0.9 and 0.0001 respectively. We adopt the 'poly' policy <ref type="bibr" target="#b2">[3]</ref> to decay the learning rate by multiplying (1 − currentiter maxiter ) power where power equals to 0.9.</p><p>Our models are trained on PASCAL-5 i for 200 epochs as that of <ref type="bibr" target="#b53">[54]</ref> with learning rate 0.0025 and batch size 4. For experiments on COCO, models are trained for 50 epochs with learning rate 0.005 and batch size 8. Parameters of the backbone network are not updated. During training, samples are processed with mirror operation and random rotation from -10 to 10 degrees. Finally, we randomly crop 473 × 473 patches from the processed images as training samples. During the evaluation, each input sample is resized to the training patch size but with respect to its original aspect ratio by padding zero, then the prediction is resized back to the original label sizes. Finally, we directly output the single-scale results without fine-tuning and any additional post-processing (such as multi-scale testing and DenseCRF <ref type="bibr" target="#b17">[18]</ref>). Our experiments are conducted on an NVIDIA Titan V GPU and Intel Xeon CPU E5-2620 v4 @ 2.10GHz. The code and trained models will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>Following <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we adopt the class mean intersection over union (mIoU) as our major evaluation metric for ablation study since the class mIoU is more reasonable than the foreground-background IoU (FB-IoU) as stated in <ref type="bibr" target="#b53">[54]</ref>. The formulation follows mIoU = 1 C C i=1 IoU i , where C is the number of classes in each fold (e.g., C = 20 for COCO and C = 5 for PASCAL-5 i ) and IoU i is the intersection-over-union of class i. We also report the results of FB-IoU for comparison with other methods. For FB-IoU calculation on each fold, only foreground and background are considered (C = 2). We take average of results on all folds as the final mIoU/FB-IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>As shown in Tables 1, 2 and 3, we build our models on three backbones VGG-16, ResNet-50 and ResNet-101 and report the mIoU/FB-IoU results respectively. By incorporating the proposed prior mask and FEM, our model significantly outperforms previous methods, reaching new state-of-theart on both PASCAL-5 i and COCO datasets. The PFENet can even outperform other methods on COCO with more than 10 points in terms of class mIoU. Our performance advantage on FB-IoU compared to PANet is relatively smaller than class mIoU on COCO, because FB-IoU is biased towards the background and classes that cover a large part of the foreground area. It is worth noting that our PFENet achieves the best performance with the fewest learnable parameters (10.4M for VGG based model and 10.8M for ResNet based models). Qualitative results are shown in <ref type="figure" target="#fig_7">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study of FEM</head><p>The proposed feature enrichment module (FEM) adaptively enriches the query feature by merging with support features in different scales and utilizes an inter-scale path to vertically transfer useful information from the auxiliary features to the main features. To verify the effectiveness of FEM, we first compare different strategies for inter-scale interaction. It shows that the top-down information path brings a decent performance gain to the baseline without compromising the model size much. Then experiments with different designs for inter-source enrichment are presented followed by comparison with the other feature enrichment designs of HRNet <ref type="bibr" target="#b43">[44]</ref>, ASPP <ref type="bibr" target="#b3">[4]</ref> and PPM <ref type="bibr" target="#b59">[60]</ref>. We also compare the Graph Attention Unit (GAU) used in the recent state-of-the-art few-shot segmentation method PGNet <ref type="bibr" target="#b52">[53]</ref> to refine the query feature. In these experiments, since our input images are resized to 473 × 473, the input feature map of the module (e.g., FEM, GAU) has the spatial size 60 × 60.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Inter-Scale Interaction Strategies</head><p>In this section, we show experimental results and analysis on different vertical inter-scale interaction strategies to manifest the rationales behind our designs of FEM.</p><p>As  <ref type="table">Table 4</ref> show that TD and TD+BU help the basic FEM structure without (W/O) the information path accomplish better results than both BU and BU+TD. The model with TD+BU contains more learnable parameters (16.0M) than TD (10.8M), and yet yields comparable performance. We thus choose TD for inter-scale interaction.</p><p>These experiments prove that using the finer feature (auxiliary) to provide additional information to the coarse feature (main) is more effective than using the coarse feature (auxiliary) to refine the finer feature (main). It is because the coarse features are not sufficient for targeting the query classes during the later information concentration stage if the target object disappears in small scales.</p><p>Different from common semantic segmentation where contextual information is the key for good performance, the way of representation and acquisition of query information is more important in few-shot segmentation. Our motivation for designing FEM is to match the query and support features in different scales to tackle the spatial inconsistency between the query and support samples. Thus, a downsampled coarse query feature without target information is less helpful for improving the quality of the final prediction as shown in the experiments comparing TD and BU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison with Other Designs</head><p>PPM <ref type="bibr" target="#b59">[60]</ref> and ASPP <ref type="bibr" target="#b3">[4]</ref> are two popular feature enrichment modules for semantic segmentation by providing multiresolution context, and HRNet <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b37">[38]</ref> provides a new feature enrichment module for the segmentation taskit achieved SOTA results on semantic segmentation benchmarks. In few-shot segmentation, the Graph Attention Unit (GAU) has been used in PGNet <ref type="bibr" target="#b52">[53]</ref> to refine the query feature with contextual information. We note the proposed FEM module yields even better few-shot segmentation performance.</p><p>The improvement brought by FEM stems from: 1) the fusions of query and support features in different spatial sizes (inter-source enrichment) since it encourages the following convolution blocks to process the concatenated features independently in different spatial resolutions, which is beneficial to predicting query targets in various scales; 2) the inter-scale interaction that selectively passes useful information from the auxiliary feature to supplement the main feature. The model without the vertical top-down information path (marked with WO) yields worse results in <ref type="table" target="#tab_5">Table 5</ref>.</p><p>We implement the ASPP with dilation rates {1, 6, 12, 18} and it achieves close results to PPM. The dilated convolution is less effective than adaptive average pooling for fewshot segmentation <ref type="bibr" target="#b52">[53]</ref>. In the following, we mainly make comparisons with PPM and GAU first since they both use the adaptive pooling to provide multi-scale information. Then, we make a discussion with the module proposed by HRNet. <ref type="table" target="#tab_5">Table 5</ref>, the model with spatial sizes {60, 30, 15, 8} achieves better performance than the baseline (original size with spatial size {60}) and models that replace FEM with PPM and ASPP. Experiments of PSPNet <ref type="bibr" target="#b59">[60]</ref> show that the Pyramid Pooling Module (PPM) with spatial sizes {6, 3, 2, 1} yields the best performance. When small spatial sizes are applied to FEM, it still outperforms PPM. But small spatial sizes are not optimal in FEM because the features pooled to spatial sizes like {6, 3, 2, 1} are too coarse for interaction and fusion of query and support features. Similarly, with small spatial size 4, the FEM with {60, 30, 15, 8, 4} yields inferior  Graph Attention Unit (GAU) GAU <ref type="bibr" target="#b52">[53]</ref> uses the graph attention mechanism to establish the element-to-element correspondence between the query and support features in each scale. Pixels of the support feature are weighed by the GAU and the new support feature is the weighted sum of the original support feature. Then the new support feature is concatenated with the query feature for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pyramid Pooling Module (PPM) As shown in</head><p>We directly replace the FEM with GAU on our baseline and keep other settings for a fair comparison. GAU is implemented with the code provided by the authors. Our baseline with GAU achieves class mIoU 55.4 and 56.1 in 1-and 5shot evaluation respectively. Noticing the original feature scales in GAU are {60, 8, 4}, we also implement it with scales {60, 30, 15, 8} (denoted as GAU+) used in our FEM. GAU+ yields smaller mIoU than GAU (54.9 in 1-shot and 55.4 in 5-shot). Though GAU also forms a pyramid structure via adaptive pooling to capture the multi-level semantic information, it is less competitive than the proposed FEM (59.2 in 1-shot and 60.4 in 5-shot) because it misses the hierarchical inter-scale relationship that adaptively provides information extracted from other levels to help refine the merged feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-Resolution Network (HRNet)</head><p>HRNet has shown its superiority on many vision tasks by maintaining a highresolution feature through all the networks and gradually fusing multi-scale features to enrich the high-resolution features. The proposed FEM can be deemed as a variant of HRB to tackle the few-shot segmentation problem. The inter-source enrichment of FEM is analogous to the multiresolution parallel convolution in HRB as shown in <ref type="figure">Figure  9</ref>. But the inter-scale interaction in FEM passes conditioned information from large to small scales rather than dense interaction among all scales without selection in HRB.</p><p>For comparison, we experiment with replacing the FEM in PFENet with HRB and generate feature maps in HRB with the same scales of those in FEM ({60, 30, 15, 8}). Results are listed in <ref type="table" target="#tab_6">Table 6</ref>. Directly applying HRB to the baseline (Baseline + HRB) does yield better results than PPM and ASPP. Densely passing information without selection causes redundancy to the target feature and yields suboptimal results. Our solution is, in the multi-resolution fusion stage of HRB, to apply the proposed inter-scale merging module M to extract essential information from the auxiliary features as shown in <ref type="figure" target="#fig_0">Figure 10</ref>. The model with conditioned feature selection (HRB-Cond) accomplishes better performance. <ref type="table">Table 4</ref>, passing features from coarse to fine levels (in a bottom-up order) adversely affects inter-scale interaction. We accordingly remove all bottom-up paths in HRB and only allow top-down ones (denoted as HRB-TD). It is not surprising that HRB-TD achieves better performance than HRB, and adding conditioned feature selection (HRB-TD-Cond) brings even further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>The best variant of HRB (i.e., HRB-TD-Cond) yields comparable results with FEM, and yet it brings much more learnable parameters (7.5M). Therefore, for few-shot seg-     mentation, the conditioned feature selection mechanism of the proposed inter-scale merging module M is essential for improving the performance of the multi-resolution structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study of the Prior Generation</head><p>Experimental results in <ref type="table" target="#tab_6">Table 6</ref> show that the prior improves models w/ and wo/ FEM. The cosine-similarity is widely used for tackling few-shot segmentation. PANet <ref type="bibr" target="#b44">[45]</ref> uses the cosine-similarity to yield the intermediate and the final prediction masks; SG-One <ref type="bibr" target="#b57">[58]</ref> and <ref type="bibr" target="#b27">[28]</ref> both utilize the cosine-similarity mask from the mask pooled support feature to provide additional guidance. However, these methods overlooked two factors. First, the mask generation process contains trainable components and the generated mask is thus biased towards the base classes during training. Second, the discrimination loss is led by the masked average pooling on support features, since the most relevant information in the support feature may be overwhelmed by the irrelevant ones during the pooling operation. For example, the discriminative regions for "cat &amp; dog" are mainly around their heads. The main bodies share similar characteristics (e.g., tailed quadrupeds), making representation produced by masked global average pooling lose the discriminative information contained in the support samples.</p><p>In the following, we first show the rationale behind our prior generation using the fixed high-level feature and taking the maximum pixel-wise correspondence value from the similarity matrix. Then we make a comparison with other methods to demonstrate the superiority of our strategy. We also include the analysis of the generalization ability on the unseen objects out of the ImageNet <ref type="bibr" target="#b31">[32]</ref> dataset to further manifest the robustness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Feature Selection</head><p>In our design, we select the fixed high-level feature for the prior generation because it can provide sufficient semantic information for accurate segmentation without sacrificing  the generalization ability. The proposed prior generation is independent of the training process. So it does not lead to loss of generalization power. The prior masks provide the bias-free prior information from high-level features for both seen and unseen data during the evaluation, while masks produced by learnable feature maps (e.g., <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b27">[28]</ref>) are affected by parameter learning during training. As a result, the preference for the training classes is inevitable for these later masks during the inference. To show the superiority of our choice, we conduct experiments on different sources of features for generating prior masks. <ref type="table" target="#tab_7">Table 7</ref> shows that the mask generated by either learnable or fixed middle-level features (Prior LM or Prior F M ) is less improved than our Prior F H since the middle-level feature is less effective to reveal the semantic correspondence between the query and support features. However, the results of mask got by learnable highlevel feature (Prior LH ) are even significantly worse than that of our baseline due to the fact that the learnable high-level feature severely overfits to the base classes: the model relies on the accurate prior masks produced by the learnable highlevel feature for locating the target region of base classes during training and therefore it hardly generalizes to the previously unseen classes during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Analysis</head><p>Qualitative Analysis Generated prior masks are shown in <ref type="figure" target="#fig_0">Figure 11</ref>. Masks of unseen classes generated by learnable high-level feature maps (L-H) cannot reveal the potential region-of-interest clearly while using the fixed high-level feature maps (F-H) keeps the general integrity of the target region. Compared to high-level features, prior masks produced by middle-level ones (L-M and F-M) are more biased towards the background region.</p><p>To help explain the quantitative results and those in  Prior-FW: Prior mask got by the feature weighting mechanism proposed in <ref type="bibr" target="#b27">[28]</ref>.  <ref type="figure" target="#fig_0">Figure 11</ref>, embedding visualization is shown in <ref type="figure" target="#fig_0">Figure 12</ref> where 1,000 samples of base classes (gray) and 1,000 samples of novel classes (colored in green, red, purple, blue and orange) are processed by the backbone followed by t-SNE <ref type="bibr" target="#b41">[42]</ref>. Based on the overlapping area between the clusters of the base and novel classes, we draw two conclusions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-Shot 5-Shot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Discrimination Ability</head><p>In our model, the prior mask acts as a pixel-wise indicator for each query image. As given in Eq. (3), taking the maximum correspondence value from the pixel-wise similarity between the query and support features indicates that there exists at least one pixel/area in the support image that has close semantic relation to the query pixel with a high prior value. It is beneficial to reveal most of the potential targets on query images. Other alternatives include using mask pooled support feature to generate the similarity mask as <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b27">[28]</ref>, and taking the average value rather than the maximum value from the pixel-wise similarity.</p><p>To verify the effectiveness of our design, we train two additional models in <ref type="table" target="#tab_7">Table 7</ref>: one with prior masks generated by averaging similarities (Prior-A F H ), and another whose prior masks are obtained by the mask-pooled support feature (Prior-P F H ). They both perform less satisfyingly than the proposed strategy (Prior F H ).</p><p>We note the following fact. Our prior generation method takes the maximum value from a similarity matrix of size hw × hw to generate the prior mask of size h × w (Eq. (3)), in contrast to Prior-P forming the mask from the similarity matrix of size hw × 1, the difference of speed is rather small because computational complexities of the two mask generation methods are much smaller than that of the rest of network. The FPS values of Prior </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Comparison with Other Designs</head><p>Some other methods also use the similarity mask as an intermediate guidance for improving performance (e.g. <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b27">[28]</ref>). Their masks are obtained by the learnable maskpooled support and learnable query feature that is then used for further processing the making final prediction. The strategy of this type of method is similar to Prior-P LM .</p><p>In <ref type="bibr" target="#b27">[28]</ref>, the good discrimination ability of features makes activation high on the foreground and low elsewhere. We follow Eqs. (3)-(6) in <ref type="bibr" target="#b27">[28]</ref> to implement the feature weighting mechanism on both the query and support features used for prior mask generation. In <ref type="bibr" target="#b27">[28]</ref>, the weighting mechanism is directly applied to learnable features, and we offer two choices in our model: the learnable middle-and high-level features. However, it does not perform better for Prior-FW LM and Prior-FW LH . Results of Prior-FW F H demonstrates the effectiveness of our feature selection strategy (with fixed high-level features) for prior generation. Our feature selection strategy is complementary to the weighting mechanism of <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Generalization on Totally Unseen Objects</head><p>Many objects of PASCAL-5 i and COCO have been included in ImageNet <ref type="bibr" target="#b31">[32]</ref> for backbone pre-training. For those previously unseen objects, the backbone still provides strong semantic cues to help identify the target area in query images with the information provided by the support images. The class 'Person' in PASCAL-5 i is not contained in ImageNet, and the baseline with the prior mask achieves 15.81 IoU, better than that without the prior mask <ref type="bibr">(14.38)</ref>. However, the class 'Person' is not rare in ImageNet samples even if their labels are not 'Person'.</p><p>To further demonstrate our generalization ability to totally unseen objects, we conduct experiments on the recently proposed FSS-1000 <ref type="bibr" target="#b18">[19]</ref> dataset where the foreground IoU is used as the evaluation metric. FSS-1000 is composed of 1,000 classes, among which 486 classes are not included in any other existing datasets <ref type="bibr" target="#b18">[19]</ref> 1 . We train our models with ResNet-50 backbone on the seen classes for 100 epochs with batch size 16 and initial learning rate 0.01, and then test them on the unseen classes. The number of query-support pairs sampled for testing is equal to five times the number of unseen samples.</p><p>As shown in <ref type="table" target="#tab_9">Table 8</ref>, the baseline with the prior mask achieves 80.8 and 81.4 foreground IoU in 1-and 5-shot evaluations respectively that outperform the vanilla baseline (79.7 and 80.1) by more than 1.0 foreground IoU in both settings. The visual illustration is given in <ref type="figure" target="#fig_0">Figure 13</ref> where the target regions can still be highlighted in the prior masks even if these objects were not witnessed by the ImageNet pre-trained backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Backbone Training</head><p>In OSLSM <ref type="bibr" target="#b32">[33]</ref>, two backbone networks are trained to achieve few-shot segmentation. However, backbone parameters in recent work <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b44">[45]</ref> are kept to prevent overfitting. There is no experiment to show what effect the backbone training has. To reach a better understanding of how the backbone affects our method, the results of four models trained with all parameters in the backbone are shown in the last four rows of <ref type="table" target="#tab_6">Table 6.</ref> 1. In practice, 420 unseen classes are filtered out. The author of FSS-1000 has clarified in email that they "have made incremental changes to the dataset to improve class balance and label quality so the number may have changed. Please do experiments according to the current version."</p><p>The additional trainable backbone parameters cause significant performance reduction due to the overfitting of training classes. Moreover, the backbone training nearly doubles the training time of each batch because an additional parameter update is required. It does not, however, affect the inference speed. As shown in the results, the improvement that FEM and prior mask bring to models with trainable backbones is less significant than on those with fixed backbones. We note that the prior masks in this section are produced by learnable high-level features because the whole backbone is trainable. The learnable high-level features bring worse performance to the fixed backbone as shown in <ref type="table" target="#tab_7">Table 7</ref>, but they are beneficial to the trainable backbone. On 5-shot evaluation, the prior yields higher performance gain compared to FEM, because the prior is averaged over five support samples, providing a more accurate prior mask than 1-shot for query images to combat overfitting. Finally, the model with both FEM and the prior still outperforms the baseline model, which demonstrates the robustness of our proposed design even with all learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Model Efficiency</head><p>Parameters The parameters of our backbone network are fixed as those in <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Four parts in the baseline model are learnable: two 1×1 convolutions for reducing dimension number of the query and support features, FEM, one convolution block and one classification head. As shown in <ref type="table" target="#tab_6">Table 6</ref>, our best model (Baseline + FEM + Prior) only has 10.8M trainable parameters that are much fewer than other methods shown in <ref type="table" target="#tab_1">Table 1</ref>. The prior generation does not bring additional parameters to the model, and FEM with spatial sizes {60, 30, 15, 8} only brings 6.3M additional learnable parameters to the baseline (4.5M → 10.8M). To prove that the improvement brought by FEM is not due to more learnable parameters, we show results of the model with FEM ‡ that has more parameters (12.9M) but it yields even worse results than FEM (10.8M).</p><p>Speed PFENet based on ResNet-50 yields the best performance with 15.9 and 5.1 FPS in 1-and 5-shot setting respectively on an NVIDIA Titan V GPU. During evaluation, test images are resized to 473 × 473. As shown in <ref type="table" target="#tab_6">Table 6</ref>, FEM does not affect the inference speed much (from 17.7 to 17.3 FPS). Though the proposed prior generation process slows down the baseline from 17.7 to 16.5 FPS, the final model is still efficient with 15+ FPS. Note that we include the processing time of the last block of ResNet in these experiments for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Analysis on Result Stability</head><p>As mentioned in the implementation details, evaluating 1,000 query-support pairs on PASCAL-5 i and COCO may cause instability on results. In this section, we show the analysis of result stability by conducting multiple experiments with different support samples. <ref type="table" target="#tab_10">Table 9</ref> show that the values of standard deviation are lower than 0.5 in both 1-shot and 5-shot setting, which shows the stability of our results on PASCAL-5 i with 1,000 pairs for evaluation.  COCO However, 1,000 pairs are not sufficient to provide reliable results for comparison as shown in <ref type="table" target="#tab_1">Table 10</ref>, since the COCO validation set contains 40,137 images and 1,000 pairs could not even cover the entire 20 test classes. Based on this observation, we instead randomly sample 20,000 querysupport pairs to evaluate our models on four folds, and the results in <ref type="table" target="#tab_1">Table 10</ref> show that 20,000 pairs bring much more stable results than 1,000 pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL-5 i Results in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Extension to Zero-Shot Segmentation</head><p>Zero-shot learning aims at learning a model that is robust even when no labeled data is given. It is an extreme case of few-shot learning. To further demonstrate the robustness of our proposed PFENet in the extreme case, we modify our model by replacing the pooled support features with class label embeddings. Note that our proposed prior generation method requires support features. Therefore the prior is not applicable and we only verify FEM on the baseline with VGG-16 backbone in the zero-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structural Change</head><p>Embeddings of Word2Vec <ref type="bibr" target="#b26">[27]</ref> and FastText <ref type="bibr" target="#b24">[25]</ref> are trained on Google News <ref type="bibr" target="#b45">[46]</ref> and Common Crawl <ref type="bibr" target="#b25">[26]</ref> respectively. The concatenated feature of Word2Vec and FastText embeddings directly replaces the pooled support feature in the original model without normalization. Therefore the structural change on the model structure is the first learnable 1×1 convolution for reducing the support feature channel. Its input channel number 768 (512 + 256) in the original few-shot model (VGG-16 backbone) is updated to 600 (300 + 300) in the zero-shot model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>As shown in <ref type="table" target="#tab_1">Table 11</ref>, our base structure achieves 53.2 class mIoU on unseen classes without support samples, which even outperforms some models with five support samples on PASCAL-5 i in the few-shot setting of OSLSM <ref type="bibr" target="#b32">[33]</ref>. Also, the proposed FEM tackles the spatial inconsistency in the zero-shot setting and brings 1.0 points mIoU improvement (from 53.2 to 54.2) to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have presented the prior guided feature enrichment network (PFENet) with the proposed prior generation method and the feature enrichment module (FEM). The prior generation method boosts the performance by leveraging the cosine-similarity calculation on pre-trained high-level features. The prior mask encourages the model to localize the query target better without losing generalization power. FEM helps solve the spatial inconsistency by adaptively merging the query and support features at multiple scales with intermediate supervision and conditioned feature selection. With these modules, PFENet achieves new state-ofthe-art results on both PASCAL-5 i and COCO datasets without much model size increase and notable efficiency loss. Experiments in the zero-shot scenario further demonstrate the robustness of our work. Possible future work includes extending these two designs to few-shot object detection and few-shot instance segmentation. Analysis on values of mean and std. of five test results (class mIoU) on COCO with different numbers of test query-support pairs (1,000 and 20,000). The model is based on VGG-16 <ref type="bibr" target="#b35">[36]</ref>. 20,000 query-support pairs yield more stable results with a lower standard deviation than 1,000 query-support pairs. . <ref type="table" target="#tab_1">Test-1 Test-2 Test-3 Test-4 Test-5  Mean  Std  Test-1 Test-2 Test-3 Test-4 Test-5  Mean  Std  Fold-</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Folds Pairs 1-Shot 5-Shot</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Summary of recent few-shot segmentation frameworks. The backbone method used to extract support and query features can be either a single shared network or two Siamese networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of our Prior Guided Feature Enrichment Network with the prior generation and Feature Enrichment Module. White blocks marked with H and M represent the high-and middle-level features extracted from backbone respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Visual illustration of FEM (dashed box) with four scales and a top-down path. C, 1x1 and Circled M represent concatenation, 1×1 convolution and inter-scale merging module respectively. Activation functions are ReLU. Visual illustration of the inter-scale merging module M. C is concatenation and + is pixel-wise addition. α means 1×1 convolution and β represents two 3×3 convolutions. Activation functions are ReLU. For features that do not have auxiliary features, there is no concatenation with the auxiliary feature and the refined feature is produced only by the main feature with α and β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Visual illustration of the baseline structure that processes features in the original spatial size of the input features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 8 )Fig. 7 .</head><label>87</label><figDesc>The visual illustration of the baseline model without FEM (B 1 = h = w) is shown inFigure 6. To encourage better feature enrichment, we add intermediate supervision by attaching classification head(Figure 7(b)) to each X i Q,new . In summary, by incorporating the pooled support features and prior masks to query features with different spatial sizes, the model learns to adaptively enrich the query Structures of (a) convolution block and (b) classification head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>mentioned in Section 3.3, there are four alternatives for the inter-scale interaction: top-down path (TD), bottomup path (BU), top-down + bottom-up path (TD+BU), and bottom-up + top-down path (BU+TD). Our experimental results in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative results of the proposed PFENet and the baseline. The left samples are from COCO and the right ones are from PASCAL-5 i . From top to bottom: (a) support images, (b) query images, (c) ground truth of query images, (d) predictions of baseline, (e) predictions of PFENet. performance compared to using the model with spatial sizes {60, 30, 15, 8}. Hence, we select {60, 30, 15, 8} as the feature scales for the inter-source enrichment of FEM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4</head><label>4</label><figDesc>Class mIoU results of different ways for inter-scale interaction on PASCAL-5 i . All models in this table are based on ResNet-50 and are trained and tested with prior masks. W/O: FEM without the information path for inter-scale interaction. TD: FEM with top-down information path. BU: FEM with bottom-up information path. TD+BU: FEM with top-down + bottom-up information path. BU+TD: FEM with bottom-up + top-down information path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Modularized block of HRNet (HRB) that applies dense multiresolution fusions. Comparison between feature fusion strategies of (left) HRB and (right) HRB-Cond. Features from different scales are directly added to the main feature in (left), while in (right), essential information is selected from auxiliary features conditioned on the main features by the interscale merging module M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>(a) Input (b) GT (c) L-M (d) L-H (e) F-M (f) F-H Visual comparison between priors generated by different sources. Prior values are normalized to 0-1, which implies the probability of being the target region. GT: Ground truth. L-M: Learnable middlelevel features. L-H: Learnable high-level features. F-M: Fixed middlelevel features. F-H: Fixed high-level features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Visual comparison between t-SNE results of different feature sources. 1,000 features in gray color are from base classes and 1,000 features in other colors are from novel classes. L-M: Learnable middle-level features. L-H: Learnable high-level features. F-M: Fixed middle-level features. F-H: Fixed high-level features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>First, the middle-level features in Figures 12 (a) &amp; (c) are less discriminative than the high-level features as shown in Figure 12(b) &amp; (d). Second, learnable features lose discrimination ability as shown in (a) &amp; (b) because embeddings of novel classes bias towards that of the base classes, which is detrimental to the generalization on unseen classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>F H , Prior-A F H , Prior-P F H and Prior-FW F H based on VGG-16 baseline are both around 23.1 FPS because the output features only contain 512 channels. The FPS values of Prior F H , Prior-A F H , Prior-P F H and Prior-FW F H based on ResNet-50 baseline whose output features have 2048 channels are 16.5, 16.5, 17.4 and 17.0 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 .</head><label>13</label><figDesc>Visual illustrations of prior masks for totally unseen objects in FSS-1000 dataset. Top: support images with the masked area in the target class. Middle: query images. Bottom: prior masks of query images where the regions of interest are highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Class mIoU results on four folds of PASCAL-5 i . Params: number of learnable parameters. IoU results on PASCAL-5 i . Our results are single-scale ones without additional post-processing like DenseCRF<ref type="bibr" target="#b17">[18]</ref>. As many other methods do not report the specific result of each fold, we present the comparison of the average FB-IoU results in this table.</figDesc><table><row><cell>Methods</cell><cell cols="4">1-Shot Fold-0 Fold-1 Fold-2 Fold-3</cell><cell>Mean</cell><cell cols="4">5-Shot Fold-0 Fold-1 Fold-2 Fold-3</cell><cell>Mean</cell><cell>Params</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">VGG-16 Backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OSLSM 2017 [33]</cell><cell>33.6</cell><cell>55.3</cell><cell>40.9</cell><cell>33.5</cell><cell>40.8</cell><cell>35.9</cell><cell>58.1</cell><cell>42.7</cell><cell>39.1</cell><cell>44.0</cell><cell>276.7M</cell></row><row><cell>co-FCN 2018 [29]</cell><cell>36.7</cell><cell>50.6</cell><cell>44.9</cell><cell>32.4</cell><cell>41.1</cell><cell>37.5</cell><cell>50.0</cell><cell>44.1</cell><cell>33.9</cell><cell>41.4</cell><cell>34.2M</cell></row><row><cell>SG-One 2018 [58]</cell><cell>40.2</cell><cell>58.4</cell><cell>48.4</cell><cell>38.4</cell><cell>46.3</cell><cell>41.9</cell><cell>58.6</cell><cell>48.6</cell><cell>39.4</cell><cell>47.1</cell><cell>19.0M</cell></row><row><cell>AMP 2019 [35]</cell><cell>41.9</cell><cell>50.2</cell><cell>46.7</cell><cell>34.7</cell><cell>43.4</cell><cell>41.8</cell><cell>55.5</cell><cell>50.3</cell><cell>39.9</cell><cell>46.9</cell><cell>34.7M</cell></row><row><cell>PANet 2019 [45]</cell><cell>42.3</cell><cell>58.0</cell><cell>51.1</cell><cell>41.2</cell><cell>48.1</cell><cell>51.8</cell><cell>64.6</cell><cell>59.8</cell><cell>46.5</cell><cell>55.7</cell><cell>14.7M</cell></row><row><cell>FWBF 2019 [28]</cell><cell>47.0</cell><cell>59.6</cell><cell>52.6</cell><cell>48.3</cell><cell>51.9</cell><cell>50.9</cell><cell>62.9</cell><cell>56.5</cell><cell>50.1</cell><cell>55.1</cell><cell>-</cell></row><row><cell>Ours</cell><cell>56.9</cell><cell>68.2</cell><cell>54.4</cell><cell>52.4</cell><cell>58.0</cell><cell>59.0</cell><cell>69.1</cell><cell>54.8</cell><cell>52.9</cell><cell>59.0</cell><cell>10.4M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ResNet-50 Backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CANet 2019 [54]</cell><cell>52.5</cell><cell>65.9</cell><cell>51.3</cell><cell>51.9</cell><cell>55.4</cell><cell>55.5</cell><cell>67.8</cell><cell>51.9</cell><cell>53.2</cell><cell>57.1</cell><cell>19.0M</cell></row><row><cell>PGNet 2019 [53]</cell><cell>56.0</cell><cell>66.9</cell><cell>50.6</cell><cell>50.4</cell><cell>56.0</cell><cell>54.9</cell><cell>67.4</cell><cell>51.8</cell><cell>53.0</cell><cell>56.8</cell><cell>17.2M</cell></row><row><cell>Ours</cell><cell>61.7</cell><cell>69.5</cell><cell>55.4</cell><cell>56.3</cell><cell>60.8</cell><cell>63.1</cell><cell>70.7</cell><cell>55.8</cell><cell>57.9</cell><cell>61.9</cell><cell>10.8M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ResNet-101 Backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FWBF 2019 [28]</cell><cell>51.3</cell><cell>64.5</cell><cell>56.7</cell><cell>52.2</cell><cell>56.2</cell><cell>54.8</cell><cell>67.4</cell><cell>62.2</cell><cell>55.3</cell><cell>59.9</cell><cell>-</cell></row><row><cell>Ours</cell><cell>60.5</cell><cell>69.4</cell><cell>54.4</cell><cell>55.9</cell><cell>60.1</cell><cell>62.8</cell><cell>70.4</cell><cell>54.9</cell><cell>57.6</cell><cell>61.4</cell><cell>10.8M</cell></row><row><cell></cell><cell>TABLE 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FB-Methods</cell><cell>1-Shot</cell><cell>5-Shot</cell><cell>Params</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">VGG-16 Backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OSLSM 2017 [33]</cell><cell>61.3</cell><cell>61.5</cell><cell>272.6M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>co-FCN 2018 [29]</cell><cell>60.1</cell><cell>60.2</cell><cell>34.2M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PL 2018 [5]</cell><cell>61.2</cell><cell>62.3</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SG-One 2018 [58]</cell><cell>63.9</cell><cell>65.9</cell><cell>19.0M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PANet 2019 [45]</cell><cell>66.5</cell><cell>70.7</cell><cell>14.7M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>72.0</cell><cell>72.3</cell><cell>10.4M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ResNet-50 Backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CANet 2019 [54]</cell><cell>66.2</cell><cell>69.6</cell><cell>19.0M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PGNet 2019 [53]</cell><cell>69.9</cell><cell>70.5</cell><cell>17.2M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>73.3</cell><cell>73.9</cell><cell>10.8M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ResNet-101 Backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A-MCG 2019 [15]</cell><cell>61.2</cell><cell>62.2</cell><cell>86.1M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>72.9</cell><cell>73.5</cell><cell>10.8M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Class mIoU / FB-IoU results on COCO. Models with † are evaluated on the labels resized to a fixed training crop size (473 for our models). Models without † are tested on labels with the original sizes.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell cols="4">1-Shot Fold-0 Fold-1 Fold-2 Fold-3</cell><cell>Mean</cell><cell cols="4">5-Shot Fold-0 Fold-1 Fold-2 Fold-3</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Class mIoU Evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FWBF 2019 [28]</cell><cell>VGG-16</cell><cell>18.4</cell><cell>16.7</cell><cell>19.6</cell><cell>25.4</cell><cell>20.0</cell><cell>20.9</cell><cell>19.2</cell><cell>21.9</cell><cell>28.4</cell><cell>22.6</cell></row><row><cell>Ours</cell><cell>VGG-16</cell><cell>33.4</cell><cell>36.0</cell><cell>34.1</cell><cell>32.8</cell><cell>34.1</cell><cell>35.9</cell><cell>40.7</cell><cell>38.1</cell><cell>36.1</cell><cell>37.7</cell></row><row><cell>PANet 2019  † [45]</cell><cell>VGG-16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>20.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.7</cell></row><row><cell>Ours †</cell><cell>VGG-16</cell><cell>35.4</cell><cell>38.1</cell><cell>36.8</cell><cell>34.7</cell><cell>36.3</cell><cell>38.2</cell><cell>42.5</cell><cell>41.8</cell><cell>38.9</cell><cell>40.4</cell></row><row><cell>FWBF 2019 [28]</cell><cell>ResNet-101</cell><cell>19.9</cell><cell>18.0</cell><cell>21.0</cell><cell>28.9</cell><cell>21.2</cell><cell>19.1</cell><cell>21.5</cell><cell>23.9</cell><cell>30.1</cell><cell>23.7</cell></row><row><cell>Ours</cell><cell>ResNet-101</cell><cell>34.3</cell><cell>33.0</cell><cell>32.3</cell><cell>30.1</cell><cell>32.4</cell><cell>38.5</cell><cell>38.6</cell><cell>38.2</cell><cell>34.3</cell><cell>37.4</cell></row><row><cell>Ours †</cell><cell>ResNet-101</cell><cell>36.8</cell><cell>41.8</cell><cell>38.7</cell><cell>36.7</cell><cell>38.5</cell><cell>40.4</cell><cell>46.8</cell><cell>43.2</cell><cell>40.5</cell><cell>42.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FB-IoU Evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PANet 2019  † [45]</cell><cell>VGG-16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.5</cell></row><row><cell>Ours †</cell><cell>VGG-16</cell><cell>53.3</cell><cell>66.1</cell><cell>66.6</cell><cell>67.1</cell><cell>63.3</cell><cell>53.5</cell><cell>68.3</cell><cell>68.2</cell><cell>70.1</cell><cell>65.0</cell></row><row><cell>Ours</cell><cell>VGG-16</cell><cell>50.0</cell><cell>63.1</cell><cell>63.5</cell><cell>63.4</cell><cell>60.0</cell><cell>50.3</cell><cell>65.2</cell><cell>65.2</cell><cell>65.5</cell><cell>61.6</cell></row><row><cell>A-MCG 2019 [15]</cell><cell>ResNet-101</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.7</cell></row><row><cell>Ours</cell><cell>ResNet-101</cell><cell>52.2</cell><cell>59.5</cell><cell>61.5</cell><cell>61.4</cell><cell>58.6</cell><cell>51.5</cell><cell>65.6</cell><cell>65.7</cell><cell>64.7</cell><cell>61.9</cell></row><row><cell>Ours †</cell><cell>ResNet-101</cell><cell>51.6</cell><cell>65.9</cell><cell>66.6</cell><cell>66.0</cell><cell>63.0</cell><cell>52.3</cell><cell>70.0</cell><cell>69.5</cell><cell>71.3</cell><cell>65.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Class mIoU of FEM with different spatial sizes and the comparison with PPM [60] and ASPP [4] on PASCAL-5 i . The backbone is ResNet-50. '{60, 30, 15, 8}': the input query feature is average-pooled into four scales {60, 30, 15, 8} and concatenate with the expanded support features respectively as shown in Figure 4. WO: without inter-scale interaction.</figDesc><table><row><cell>Methods</cell><cell cols="4">1-Shot Fold-0 Fold-1 Fold-2 Fold-3</cell><cell>Mean</cell><cell cols="4">5-Shot Fold-0 Fold-1 Fold-2 Fold-3</cell><cell>Mean</cell></row><row><cell>{60} (Baseline)</cell><cell>54.3</cell><cell>67.3</cell><cell>53.3</cell><cell>50.4</cell><cell>56.3</cell><cell>57.1</cell><cell>68.0</cell><cell>53.8</cell><cell>52.9</cell><cell>58.0</cell></row><row><cell>{60} + PPM [60]</cell><cell>55.4</cell><cell>68.4</cell><cell>53.2</cell><cell>51.4</cell><cell>57.1</cell><cell>58.3</cell><cell>68.9</cell><cell>53.5</cell><cell>50.8</cell><cell>57.9</cell></row><row><cell>{60} + ASPP [4]</cell><cell>57.6</cell><cell>68.4</cell><cell>52.8</cell><cell>49.0</cell><cell>56.9</cell><cell>59.5</cell><cell>69.3</cell><cell>52.6</cell><cell>50.7</cell><cell>58.0</cell></row><row><cell>{60, 6, 3, 2, 1}</cell><cell>58.8</cell><cell>68.0</cell><cell>54.1</cell><cell>51.2</cell><cell>58.0</cell><cell>59.8</cell><cell>68.4</cell><cell>53.8</cell><cell>52.1</cell><cell>58.5</cell></row><row><cell>{60, 30}</cell><cell>55.3</cell><cell>67.8</cell><cell>54.7</cell><cell>51.2</cell><cell>57.3</cell><cell>58.4</cell><cell>68.7</cell><cell>54.5</cell><cell>53.1</cell><cell>58.7</cell></row><row><cell>{60, 30, 15}</cell><cell>56.6</cell><cell>68.0</cell><cell>54.6</cell><cell>52.9</cell><cell>58.0</cell><cell>59.0</cell><cell>68.7</cell><cell>55.0</cell><cell>54.0</cell><cell>59.2</cell></row><row><cell>{60, 30, 15, 8}</cell><cell>59.4</cell><cell>68.9</cell><cell>54.7</cell><cell>53.6</cell><cell>59.2</cell><cell>61.5</cell><cell>69.5</cell><cell>55.4</cell><cell>55.3</cell><cell>60.4</cell></row><row><cell>{60, 30, 15, 8, 4}</cell><cell>58.7</cell><cell>68.5</cell><cell>54.1</cell><cell>54.5</cell><cell>58.9</cell><cell>60.3</cell><cell>69.3</cell><cell>54.9</cell><cell>56.4</cell><cell>60.2</cell></row><row><cell>{60, 30, 15, 8}-WO</cell><cell>57.9</cell><cell>67.4</cell><cell>53.7</cell><cell>53.6</cell><cell>58.2</cell><cell>60.5</cell><cell>68.0</cell><cell>54.2</cell><cell>53.8</cell><cell>59.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>Class mIoU on PASCAL-5 i and efficiency of models with/without the proposed prior and FEM. Models are based on ResNet-50. Params:The number of learnable parameters. Speed: Average frame-per-second (FPS) of 1-shot evaluation. HRB: Modularized block of HRNet<ref type="bibr" target="#b43">[44]</ref>.-TD: Only top-down feature enrichment paths are enabled. -Cond: The inter-scale enrichment modules are implemented to pass the conditioned information. FEM: Feature enrichment module with {60, 30, 15, 8}. FEM ‡ : FEM with spatial sizes {60, 30, 15, 8, 4}. Prior: Prior masks got by fixed high-level features (conv5 x). Baseline † : Models trained with all backbone parameters. Prior † : Prior masks got by learnable high-level features.</figDesc><table><row><cell>Methods</cell><cell>1-Shot</cell><cell>5-Shot</cell><cell>Params</cell><cell>Speed</cell></row><row><cell>Baseline</cell><cell>56.3</cell><cell>58.0</cell><cell>4.5 M</cell><cell>17.7 FPS</cell></row><row><cell>Baseline + PPM [60]</cell><cell>57.1</cell><cell>57.9</cell><cell>5.7 M</cell><cell>17.6 FPS</cell></row><row><cell>Baseline + ASPP [4]</cell><cell>56.9</cell><cell>58.0</cell><cell>7.9 M</cell><cell>17.5 FPS</cell></row><row><cell>Baseline + HRB [44]</cell><cell>58.3</cell><cell>59.4</cell><cell>14.4 M</cell><cell>15.7 FPS</cell></row><row><cell>Baseline + HRB-Cond</cell><cell>59.2</cell><cell>60.0</cell><cell>23.0 M</cell><cell>14.5 FPS</cell></row><row><cell>Baseline + HRB-TD</cell><cell>58.9</cell><cell>60.0</cell><cell>14.0 M</cell><cell>16.1 FPS</cell></row><row><cell>Baseline + HRB-TD-Cond</cell><cell>59.3</cell><cell>60.4</cell><cell>18.3 M</cell><cell>15.6 FPS</cell></row><row><cell>Baseline + FEM</cell><cell>59.2</cell><cell>60.4</cell><cell>10.8 M</cell><cell>17.3 FPS</cell></row><row><cell>Baseline + FEM  ‡</cell><cell>58.9</cell><cell>60.2</cell><cell>12.9 M</cell><cell>16.1 FPS</cell></row><row><cell>Baseline + Prior</cell><cell>58.2</cell><cell>59.6</cell><cell>4.5 M</cell><cell>16.5 FPS</cell></row><row><cell>Baseline + FEM + Prior</cell><cell>60.8</cell><cell>61.9</cell><cell>10.8 M</cell><cell>15.9 FPS</cell></row><row><cell>Baseline  †</cell><cell>48.8</cell><cell>50.1</cell><cell>28.2 M</cell><cell>17.7 FPS</cell></row><row><cell>Baseline  † + FEM</cell><cell>50.2</cell><cell>52.3</cell><cell>34.5 M</cell><cell>16.1 FPS</cell></row><row><cell>Baseline  † + Prior  †</cell><cell>49.7</cell><cell>53.1</cell><cell>28.2 M</cell><cell>16.5 FPS</cell></row><row><cell>Baseline  † + FEM + Prior  †</cell><cell>51.9</cell><cell>55.3</cell><cell>34.5 M</cell><cell>15.9 FPS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc>Class mIoU results of different prior masks on PASCAL-5 i . All models in this table are based on VGG-16. LM: Learnable middle-level features. LH: Learnable high-level features. FM: Fixed middle-level features. FH: Fixed high-level features. Prior: Prior mask got by taking the maximum similarity value. Prior-A: Prior mask got by the average similarity value. Prior-P: Prior mask generated with the mask-pooled support feature.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 Foreground</head><label>8</label><figDesc>IoU results on totally unseen classes of FSS-1000<ref type="bibr" target="#b18">[19]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="2">1-Shot 5-Shot</cell></row><row><cell>Baseline</cell><cell>79.7</cell><cell>80.1</cell></row><row><cell>Baseline + Prior</cell><cell>80.8</cell><cell>81.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>Mean and Std. of five test results (class mIoU) on PASCAL-5 i . 'Fm -Sn' means the n-shot results of Fold-m. Each row shows five test results with the values of mean and standard deviation (Std.).</figDesc><table><row><cell>Fold -Shot</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>Mean</cell><cell>Std.</cell></row><row><cell>F0 -S1</cell><cell cols="5">61.1 61.9 62.2 61.6 61.7</cell><cell>61.7</cell><cell>0.406</cell></row><row><cell>F0 -S5</cell><cell cols="5">63.1 63.2 63.3 63.1 63.3</cell><cell>63.1</cell><cell>0.148</cell></row><row><cell>F1 -S1</cell><cell cols="5">69.5 69.7 69.1 69.5 69.7</cell><cell>69.5</cell><cell>0.245</cell></row><row><cell>F1 -S5</cell><cell cols="5">70.7 70.8 70.9 70.6 70.5</cell><cell>70.7</cell><cell>0.158</cell></row><row><cell>F2 -S1</cell><cell cols="5">55.3 55.2 55.6 55.4 55.1</cell><cell>55.4</cell><cell>0.230</cell></row><row><cell>F2 -S5</cell><cell cols="5">55.2 56.3 55.5 55.9 56.0</cell><cell>55.8</cell><cell>0.432</cell></row><row><cell>F3 -S1</cell><cell cols="5">56.0 56.2 56.2 56.7 56.3</cell><cell>56.3</cell><cell>0.259</cell></row><row><cell>F3 -S5</cell><cell cols="5">57.9 58.1 57.9 58.0 57.6</cell><cell>57.9</cell><cell>0.187</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 10</head><label>10</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 11</head><label>11</label><figDesc>Experimental results in the zero-shot setting. Models shown in this table are based on VGG-16.</figDesc><table><row><cell>Methods</cell><cell>Shot</cell><cell cols="4">Fold-0 Fold-1 Fold-2 Fold-3</cell><cell>Mean</cell></row><row><cell>OSLSM2017 [33]</cell><cell>5</cell><cell>35.9</cell><cell>58.1</cell><cell>42.7</cell><cell>39.1</cell><cell>44.0</cell></row><row><cell>co-FCN2018 [29]</cell><cell>5</cell><cell>37.5</cell><cell>50.0</cell><cell>44.1</cell><cell>33.9</cell><cell>41.4</cell></row><row><cell>SG-One2018 [58]</cell><cell>5</cell><cell>41.9</cell><cell>58.6</cell><cell>48.6</cell><cell>39.4</cell><cell>47.1</cell></row><row><cell>AMP2019 [35]</cell><cell>5</cell><cell>41.8</cell><cell>55.5</cell><cell>50.3</cell><cell>39.9</cell><cell>46.9</cell></row><row><cell>Kato et al.2019 [17]</cell><cell>0</cell><cell>39.6</cell><cell>52.6</cell><cell>41.0</cell><cell>35.6</cell><cell>42.2</cell></row><row><cell>Baseline</cell><cell>0</cell><cell>49.4</cell><cell>67.1</cell><cell>50.3</cell><cell>46.0</cell><cell>53.2</cell></row><row><cell>Baseline + FEM</cell><cell>0</cell><cell>50.0</cell><cell>68.5</cell><cell>51.7</cell><cell>46.6</cell><cell>54.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the texture bias for few-shot CNN segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Fayjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kauffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory matching networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (VOC) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simpropnet: Improved similarity propagation for few-shot image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gairola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention-based multi-context guiding for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Zero-shot semantic segmentation via variational mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fss-1000: A 1000-class dataset for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Part-aware prototype network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conditional networks for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Few-shot segmentation propagation with guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive masked weight imprinting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Bottom-up human pose estimation by ranking heatmap-guided adaptive keypoint estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Differentiable meta-learning model for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Crnet: Crossreference networks for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guosheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fayao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A new local transformation module for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pyramid graph networks with connection attentions for region-based oneshot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Few-shot learning via saliency-guided hallucination of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
