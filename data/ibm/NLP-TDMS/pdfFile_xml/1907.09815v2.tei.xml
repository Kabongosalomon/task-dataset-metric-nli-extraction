<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bilinear Graph Networks for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
							<email>c.xu@</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">FEIT</orgName>
								<orgName type="institution" key="instit3">University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bilinear Graph Networks for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper revisits the bilinear attention networks in the visual question answering task from a graph perspective. The classical bilinear attention networks build a bilinear attention map to extract the joint representation of words in the question and objects in the image but lack fully exploring the relationship between words for complex reasoning. In contrast, we develop bilinear graph networks to model the context of the joint embeddings of words and objects. Two kinds of graphs are investigated, namely image-graph and question-graph. The image-graph transfers features of the detected objects to their related query words, enabling the output nodes to have both semantic and factual information. The question-graph exchanges information between these output nodes from image-graph to amplify the implicit yet important relationship between objects. These two kinds of graphs cooperate with each other, and thus our resulting model can model the relationship and dependency between objects, which leads to the realization of multi-step reasoning. Experimental results on the VQA v2.0 validation dataset demonstrate the ability of our method to handle the complex questions. On the test-std set, our best single model achieves state-of-the-art performance, boosting the overall accuracy to 72.41%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The developments in computer vision and natural language processing enable the machine to deal with complicated tasks that require the integration and understanding of vision and language, e.g. image captioning <ref type="bibr" target="#b0">[1]</ref>, visual grounding <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b7">8]</ref>, visual question answering (VQA) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>, and visual dialog <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>. Compared with image captioning that is to simply describe the topic of an image, VQA needs a complex reasoning process to infer the right answer for a variety of questions. Visual grounding aims to locate the related objects in the image, but VQA takes a further step to convert this information into human language. In addition, VQA is the basic and vital component in visual dialog. Considering the challenges and significance of VQA, increasing research attention has been attracted to it.</p><p>Given an input image and a question, representative VQA models, e.g. Stacked Attention Networks (SAN) <ref type="bibr" target="#b32">[33]</ref>, Multimodal Compact Bilinear Pooling (MCB) <ref type="bibr" target="#b7">[8]</ref>, and Multimodal Low-rank Bilinear Attention Networks (MLB) <ref type="bibr" target="#b16">[17]</ref>, first generate grid image features by ResNet <ref type="bibr" target="#b13">[14]</ref> and represent the question as the last hidden state of Long Short-Term Memory (LSTM) <ref type="bibr" target="#b14">[15]</ref>, and then attend to the image features based on the question vector to ground the target objects; the question vector and the weighted image features are finally projected into a unified embedding for answer prediction. Bilinear Attention Networks (BAN) <ref type="bibr" target="#b15">[16]</ref> notice that these methods neglect the interaction between words in the question and objects in the image and propose to build a bilinear co-attention map considering each pair of multimodal channels. Furthermore, Dynamic Fusion with Intraand Inter-modality (DFAF) <ref type="bibr" target="#b8">[9]</ref> and Deep Modular Co-Attention Networks (MCAN) <ref type="bibr" target="#b33">[34]</ref> consider intra-attention within each modality and inter-attention across different modalities by the scaled dot-product attention from Transformer <ref type="bibr" target="#b28">[29]</ref>.</p><p>However, BAN lacks comprehensive exploitation of the interactions between words in questions for modeling their context. The linear way of using scaled dot-product to calculate the attention within single modality (the queries, keys, and values come from the kind of nodes), such as textual features <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7]</ref> and visual features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32]</ref>, is less expressive to fully capture the complex relationship within the multi-modal inputs.</p><p>In this paper, we develop bilinear graph networks for visual question answering. We first investigate the bilinear attention map between words in the question and objects in the image from a new graph perspective, then we highlight the importance of exploiting the intra-modality relationship between words in the question and exploring the cross-modality relationship between the question and image for complex reasoning. Two graphs are established to formulate these two kinds of relationships. The image-graph focuses on exploring visual features of the image to their related textual features for joint embeddings, which links the semantic information of words with factual information of the image. The question-graph exploits information across different joint embeddings in terms of words, which amplifies the implicit yet important relationships between objects. Given these two graphs cooperating with each other, the resulting VQA model is able to reason complex and compositional questions.</p><p>We conduct experiments on VQA v2.0 dataset <ref type="bibr" target="#b10">[11]</ref>. On the validation dataset, our one-layer graph networks boost the accuracy by 0.62% compared with BAN, and graphs of multiple layers show advantages on multi-step reasoning for long and complex questions, evidenced by a total 1.4% improvement. With the help of pre-trained language model, BERT <ref type="bibr" target="#b6">[7]</ref>, our graphs gain an extra 1.5% increase. On the test-std dataset, our model achieves state-of-the-art performance, increasing the overall accuracy to 72.41%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we will first introduce the related research on VQA and then the graph neural networks on both textbased and visual-based tasks.</p><p>Visual Question Answering (VQA): VQA is a task to answer the given question based on the input image. The question is usually embedded into a vector with LSTM <ref type="bibr" target="#b14">[15]</ref>, and the image is represented by the fixed-size grid features extracted from a pre-trained model, such as ResNet <ref type="bibr" target="#b13">[14]</ref>. Then both of these features are combined by addition or concatenation <ref type="bibr" target="#b1">[2]</ref> before being projected into a unified vector for answer prediction through a multilayer perceptron (MLP). However, not all features of the image are related to the given question, while some of them should be filtered out before generating the unified vector, therefore attention mechanism is introduced to learn the weight of each grid feature. Stack Attention Networks (SAN) <ref type="bibr" target="#b32">[33]</ref> learn the visual attention through multi-steps, trying to answer the question progressively. Dual Attention Networks (DAN) <ref type="bibr" target="#b23">[24]</ref> learn visual and textual attention respectively via the memory vector. Due to the different distributions of question and image features, the outer product of both features has a better explanation and performance compared with the linear combination. But because of its high dimension output, it is hard to be optimized. Multimodal Compact Bilinear Pooling (MCB) <ref type="bibr" target="#b7">[8]</ref> is approaching this process by calculating the count sketch of two features and convolving them in Faster Fourier Transform (FFT) space. Nevertheless, MCB uses sampling features instead of the original ones, which leads to bias and needs a large projected dimension to reduce it. Hadamard Product for Low-rank Bilinear Pooling (MLB) <ref type="bibr" target="#b16">[17]</ref> models the common vector with a lowrank matrix by an element-wise multiplication, and Multimodal Factorized Bilinear Pooling (MFB) <ref type="bibr" target="#b34">[35]</ref> increases the rank from 1 to k to accelerate the convergence rate and improve the model's robustness. Furthermore, Bilinear Atten-tion Networks (BAN) <ref type="bibr" target="#b15">[16]</ref> learn the textual and visual attention simultaneously, which builds a mapping from the detected objects of the image to the words of the question.</p><p>Graph Neural Network (GNN): GNN is used to build the relationship between nodes like social network, citation link <ref type="bibr" target="#b12">[13]</ref>, knowledge graph <ref type="bibr" target="#b18">[19]</ref>, protein-protein interaction <ref type="bibr" target="#b29">[30]</ref>, etc.. It overcomes the limitation of Euclidean distance between each node in the inputs and involves more context information from neighbors. In text-based tasks, such as machine translation and sequence tagging, GNN breaks the sequence restriction between each word and learns the graph weight by attention mechanism, such as Transformer <ref type="bibr" target="#b28">[29]</ref>, which makes it easier to model longer sequence than LSTM and Gated Recurrent Units (GRU) <ref type="bibr" target="#b4">[5]</ref>, since each node is directly linked with others via learned weights instead of through hidden state and gates. Pretraining of Deep Bidirectional Transformers (BERT) <ref type="bibr" target="#b6">[7]</ref>, which is trained on a large corpus with unsupervised learning approaches, can be easily explained and transferred to other tasks. In image-based tasks, GNN gathers information from all the grids <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4]</ref> or proposals <ref type="bibr" target="#b21">[22]</ref> other than surroundings whose size is limited by the receptive fields of Convolution Neural Networks (CNNs), and it aggregates features over coordinate space to improve the performance of object detection and scene generation <ref type="bibr" target="#b31">[32]</ref>. Motivated by these models, DFAF <ref type="bibr" target="#b8">[9]</ref> and MCAN <ref type="bibr" target="#b33">[34]</ref> consider all the relationships between inputs by calculating the attention weight with scaled dot-product, including word and word, object and object, and object and word. Pretraining Taskagnostic Visiolinguistic Representations (Vilbert) <ref type="bibr" target="#b22">[23]</ref> even fine-tune BERT model by reconstructing the image region categories and words as well as predicting the alignment of the image and its caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>The goal of VQA task is to answer the given question T based on the input image I. With the object-detector Faster-RCNN <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b0">1]</ref>, we convert the input image I into</p><formula xml:id="formula_0">object features V = (v 1 , . . . , v n ) with v i ∈ R D ,</formula><p>where n is the number of detected objects, and D is the feature dimension. The question (t 1 , . . . , t m ) is a sequence of m words. It can be encoded using either LSTM <ref type="bibr" target="#b14">[15]</ref> or Transformer <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7]</ref> to Q = (q 1 , . . . , q m ), where Q = LSTM(T ) or Q = Transformer(T ), and Q ∈ R C×m , where C is the dimension of output features. In order to represent the common vector of v ∈ V and q ∈ Q, a weight matrix W i is introduced to calculate the scalar output f i and can be approximated with multiplication of two sub-matrix U i V i following MLB <ref type="bibr" target="#b16">[17]</ref> (bias terms are omitted without loss of generality):</p><formula xml:id="formula_1">f i = q W i v ≈ q U i V i v = 1 (U i q • V i v),<label>(1)</label></formula><formula xml:id="formula_2">where W i ∈ R C×D , U i ∈ R C×d , V i ∈ R D×d , 1 ∈ R d</formula><p>is a vector with all elements equal to 1, and • is Hadamard product (element-wise multiplication). This decomposition makes the rank of matrix W i to be at most d ≤ min(C, D). To obtain the out feature f ∈ R K , two three-dimension tensors, U ∈ R C×d×K and V ∈ R D×d×K , are learned, and empirically d is set to 1, resulting in U ∈ R C×K and V ∈ R D×K for simplicity.</p><p>However, the question features Q and image features V are multiple channels, BAN <ref type="bibr" target="#b15">[16]</ref> reduces both input channels simultaneously and obtains a unified representation of them. It first calculates a bilinear attention map G ∈ R m×n between Q and V , conditioned on which, it then generates the joint embedding z as follows:</p><formula xml:id="formula_3">z = BAN(Q, V ; G).<label>(2)</label></formula><p>The attention map G is defined as:</p><formula xml:id="formula_4">G = softmax ((1 · p ) • σ(Q U ))σ(V V ) , (3) where U ∈ R C×K , V ∈ R D×K , p ∈ R K are vari-</formula><p>ables to be learned, K denotes the shared embedding size, and σ is the ReLU activation function denoted as σ(x) = max(x, 0). Notice that the softmax function works on the rows and columns, i.e.</p><formula xml:id="formula_5">m i=1 n j=1 G i,j = 1.</formula><p>The logit G i,j , element of G before softmax, is the output of lowrank bilinear pooling as:</p><formula xml:id="formula_6">G i,j = p (σ(U q i ) • σ(V v j )).<label>(4)</label></formula><p>The matrix p projects the unified vector of q i and v j into a scalar to represent the relation between them. Then the k-th element value of joint embedding z ∈ R K is given by:</p><formula xml:id="formula_7">z k = m i=1 n j=1 G i,j σ(q i U k )σ(V k v j ),<label>(5)</label></formula><p>where U ∈ R C×K , V ∈ R D×K are the parameters to be optimized. It can also be rewritten as:</p><formula xml:id="formula_8">z k = σ(Q U) k Gσ(V V) k ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">(Q U) k ∈ R m is the k column of Q U, and (V V) k ∈ R n is the k column of V V.</formula><p>After that, we input z to a classifier such as MLP to calculate the score p i for answer a i ∈ A and choose the highest one as the predicted answer, where A is the answer set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Bilinear Graph Networks</head><p>The graph attention network and its variant, Transformer, are efficient in modeling the relationship within single modality, such as textual nodes <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7]</ref>, visual nodes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32]</ref>, and citation nodes <ref type="bibr" target="#b29">[30]</ref>, whose outputs can be calculated as:</p><formula xml:id="formula_10">Tr(Q, K, V) = softmax(QK )V,<label>(7)</label></formula><p>where Q, K, and V denote the queries, keys, and values respectively, and the softmax function only works on the rows. Motivated by Eq. <ref type="formula" target="#formula_10">(7)</ref>, we can easily illustrate BAN from the perspective of graph.</p><p>Given the calculation of z k in Eq. (6), Eq. (2) can be reformulated as:</p><formula xml:id="formula_11">Z = BGN(Q, V ; G) = σ(Q U) • G a σ(V V), (8) z = Z G b ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_12">Z ∈ R K×m , G a ∈ R m×n , G b ∈ R m , G b i = n j=1 G i,j , and G a i,j = Gi,j G b i .</formula><p>The output nodes Z = (z 1 , . . . , z m ) are calculated based on the input nodes {Q ∪ V } and their attention weight G a . The attention map G a in Eq. (8) is equivalent to graph weight softmax(QK ) in Eq. <ref type="bibr" target="#b6">(7)</ref>, and σ(V V ) is the graph value V. Looking into the definition of attention map in Eq. (3), the map G a implies how much information should flow from the nodes V to the nodes Q. (1 · p ) • σ(Q U ) and σ(V V ) correspond to query Q and key K in Eq. <ref type="formula" target="#formula_10">(7)</ref> respectively. Instead of simply using scaled dot-product, low-rank bilinear pooling is utilized to overcome the different distributions of Q and V based on Eq. (4). Moreover, Eq. <ref type="formula" target="#formula_10">(7)</ref> only considers single modality of inputs, while VQA models need to consider the multi-modal inputs (i.e. image and question). An additional</p><formula xml:id="formula_13">Hadamard product of σ(Q U) and G a σ(V V) is thus in- cluded in Eq. (8) to generate the output nodes (z 1 , . . . , z m ), where z i ∈ R K .</formula><p>Finally, the joint embedding z represents the whole graph by summarization of all nodes in Z based on their weight G b in Eq. <ref type="bibr" target="#b8">(9)</ref>.</p><p>Even though Eqs. <ref type="formula">(8)</ref> and <ref type="formula" target="#formula_11">(9)</ref> provide an elegant approach to investigate the relationship between question features Q and image features V , a simply summarization over columns of Z in Eq. (9) cannot fully address the connections between the joint embeddings (z 1 , · · · , z m ) corresponding to words. Given the question and image in <ref type="figure">Figure  1</ref>, BAN (i.e. Eqs. <ref type="bibr" target="#b7">(8)</ref> and <ref type="formula" target="#formula_11">(9)</ref>) can locate a variety of fruits in the image according to the word 'fruit' in the question, but it is unaware of the relative position of each fruit from others by mixing all the information (i.e. the summarization in Eq. (9)), thus we want each joint embeddings z to extract its related information from other items instead of an overall representation. Hence, we are motivated to develop bilinear graph networks, as shown in <ref type="figure">Figure 1</ref>, which has two kinds of graphs, i.e. image-graph and question-graph. The image-graph learns to build the relationship between words and objects and generates their joint embeddings, while the question-graph will update the joint embeddings in terms of words by exploiting their interactions.</p><p>We also find that the right answer may not be decided at once, therefore we stack our graphs to make the words interact with the objects as well as words themselves for multiple times.</p><p>Difference from other graph-based methods. Though we also investigate the VQA problem from a graph view, our model has several differences from existing graph-based methods. Compared with MUREL <ref type="bibr" target="#b2">[3]</ref>, representing the question as a single vector to fuse with the image features What fruit is on the left edge?  <ref type="figure">Figure 1</ref>. Architecture of our model. The image-graph builds the relationship between words and objects, and the question-graph learns the relationship between joint embeddings in terms of words. The two graphs cooperate with each other to predict the answer.  at each step and emphasizing the relationship between objects, our method pays attention to modeling the relationship between words and objects as well as between words and words. Regarding DFAF <ref type="bibr" target="#b8">[9]</ref>, MCAN <ref type="bibr" target="#b33">[34]</ref>, and Vilbert <ref type="bibr" target="#b22">[23]</ref>, all of them use the scaled dot-product <ref type="bibr" target="#b28">[29]</ref> to model the graph weight between image and question as well as linearly combining both features to compute their join embeddings, which is less effective in modeling the representation of multi-modal inputs. We reformulate BAN as a bilinear graph between question and image and reveal its disadvantages, then we propose the image-graph and question-graph to solve it, which has a better explanation.</p><formula xml:id="formula_14">Page 1 w w w + w w • w 1 1 ′ … −1 ′ w w w + s × × × w • • • s • • • Matrix</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image-Graph</head><p>The major target of the image-graph is to locate the objects related to semantic information of each word in the question. Beginning with Eq. (8), we have a multi-glimpse extension as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Consider the graph G = {V, E}, where V and E are the set of nodes and edges respectively. The image-graph has V = {Q ∪ V } and E = G e , where Q ∈ R C×m are tex-tual features of the question and V ∈ R D×n are visual features of the detected objects, and G e are the computed graph weights based on Q and V . To joint model the graph between image and question from different representation subspaces, we extend G e to multiple glimpses following <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>, resulting in G e ∈ R m×n×g e , where g e is the number of glimpse. The j-th graph attention is computed as:</p><formula xml:id="formula_15">G e j = softmax (((1 · p e j ) • σ(Q U e ))σ(V V e ) ,<label>(10)</label></formula><p>where the parameters U e and V e are shared among glimpses except for p e j , which can be seen from the upper part of <ref type="figure" target="#fig_1">Figure 2</ref>. After learning the graph attention, we use Eq. (8) to generate the joint embeddings as:</p><formula xml:id="formula_16">H j = BGN e j (Q, V ; G e j ) = σ(Q U e j ) • G e j σ(V V e j ),<label>(11)</label></formula><p>where H j ∈ R K×m represents the output of image-graph at glimpse j. Instead of concatenation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30]</ref> of joint embeddings from each glimpse, we follow BAN to use the residual form to integrate previous learned joint embeddings as shown in the lower part of <ref type="figure" target="#fig_1">Figure 2</ref>, then Eq. (11) becomes:</p><formula xml:id="formula_17">H j = W e j BGN e j (H j−1 , V, G e j ) + H j−1 ,<label>(12)</label></formula><p>where H 0 = Q, and W e j ∈ R C×K projects the joint embeddings to the same dimension of Q. By convention, we use the output of the last glimpse to represent the whole image-graph, denoted as H = H g e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Question-Graph</head><p>For the question-graph, similarly, we have the graph nodes V = H and graph weight E = G r , where H ∈ R C×m are the output nodes of the image-graph, and G r ∈ R m×m×g r are the self-attention graph weights of multiple glimpses based on H denoted as:</p><formula xml:id="formula_18">G r j = softmax (((1 · p r j ) • σ(H U r ))σ(H V r ) .<label>(13)</label></formula><p>The structure of our question-graph is similar to the imagegraph in <ref type="figure" target="#fig_1">Figure 2</ref>, except that both inputs are H. Different from Eq. (9), which summarizes the outputs from the image-graph based on G b to represent the whole graph, G r in Eq. (13) learns the context of each node for exchanging their information. Based on the graph weight G r , nodes of the question-graph at glimpse j gather information from others and are represented as Eq. <ref type="formula" target="#formula_1">(12)</ref>:</p><formula xml:id="formula_19">O j = W r j BGN r j (O j−1 , H; G r j ) + O j−1 ,<label>(14)</label></formula><p>where W r j ∈ R C×K and O 0 = H. The outputs of questiongraph O, abbreviated version of O g r , can be utilized to answer the question by summarizing all the nodes to represent the whole graph.</p><p>As we mentioned above, the question may be compositional and complex that needs multi-step reasoning, thus we form the basic module of our bilinear graph networks with one image-graph following by one question-graph, and we stack the module for multiple layers to compose our framework shown in <ref type="figure">Figure 1</ref>. The first layer of the imagegraph takes textual nodes Q as query to locate the related visual information in V and outputs their joint nodes H 1 , and the higher layer of it takes the outputs of i − 1 layer of the question-graph, O i−1 , as query to involve more visual information related to the prior knowledge. The layer i of question-graph aims at exchanging the information between nodes of H i to model the context and gets O i for prior knowledge of image-graph or answer prediction.</p><p>After stacking L layers of bilinear graph networks, we summarize all the nodes of O L to represent the whole graph and pass it to a two-layer MLP for classification:</p><formula xml:id="formula_20">p = W a σ(W a O L · 1),<label>(15)</label></formula><p>where W a ∈ R 2C×C , W a ∈ R |A|×2C , and |A| is the size of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate our bilinear graph networks on VQA v2.0 dataset <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>. We first introduce this dataset and then describe our implementation details and results, and finally the qualitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>VQA v2.0 dataset: The dataset was built based on the MSCOCO images <ref type="bibr" target="#b20">[21]</ref>, and it contains 1.1M questions asked by human and each question is annotated by ten people. Compared with v1.0 dataset <ref type="bibr" target="#b1">[2]</ref>, it emphasizes the visual understanding by reducing the text bias. The dataset is split into three parts: training, validation, and test, which have 80k images and 444 questions, 40k images and 214k questions, and 80k images and 448k questions respectively. The answers of the training and validation dataset are published for training model, while those of the test dataset are unknown and should be predicted by the proposed model before being uploaded to the server for performance evaluation. Based on the answer category, the questions can be classified into three types, i.e. yes/no, number, and others. We train our models with different settings on training dataset and evaluate their accuracy on validation dataset by the tools from <ref type="bibr" target="#b1">[2]</ref>, then we pick the settings of the best model for training on the training and validation dataset with extra data from Visual Genome <ref type="bibr" target="#b19">[20]</ref> that has 108k images and 1M questions, reporting results on test-server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We construct the answer vocabulary by restricting to the words that appear in the training and validation dataset more than eight times, resulting in |A| = 3, 129. We then truncate or pad a question's length m to 15 words, and the weight of padding tokens in question-graph G r will be set to −∞ before softmax to reduce its impact. There are two methods to encode the question, one is LSTM, and the other one is Transformer. For the former one, we pass the question through a one-layer LSTM, whose input dimension of each word is 600, 300 of which is learned by our model and another 300 from pre-trained GloVe vector <ref type="bibr" target="#b24">[25]</ref> is fixed, and the output dimension C is 1,024. For the latter one, we encode the words by summing their corresponding token embeddings and position embeddings and project the outputs of the last layer of Transformer into vectors with dimension C following by tanh, where tanh(x) = e x −e −x e x +e −x . We extract object features from a Faster-RCNN model <ref type="bibr" target="#b0">[1]</ref> pretrained on Visual Genome, which has 1,600 object classes. For each image, we obtain top n = 100 objects based on their probabilities with their object features and regions, and each object feature is presented by mean-pooling of their convolutional features with D = 2, 048. The joint embedding size K and K are set to 1,024, and the rank d is set to 3 during computing the graph attention weights in the image-graph and question-graph to increase its capacity. In order to save memory in each layer to make our network go deeper, we reduce the glimpse number from 8 (best performance in BAN) to g e = g r = 4. Weight Normalization <ref type="bibr" target="#b26">[27]</ref> and Dropout <ref type="bibr" target="#b27">[28]</ref> with p = 0.2 are added after each linear mapping to stable the output and prevent from overfitting. Due to the fact that there might exist multiple correct answers for a question, we utilize the binary cross-entropy loss (BCE) as loss function, which is calculated as: <ref type="bibr" target="#b15">(16)</ref> where y i = min( number of people that provided answerai 3 , 1), and φ(x) is the sigmoid function denoted as φ(x) = 1 1+e −x . Adamax <ref type="bibr" target="#b17">[18]</ref>, a variant of Adam, is used to optimize our model. The initial learning rate is 0.001 and grows by 0.001 every epoch until reaching 0.004 for warm start, keeps constant until the eleventh epoch and decays by 1/4 every two epochs to 0.00025. The batch size is 128. </p><formula xml:id="formula_21">L = − |A| i=1 (y i log φ(p i ) + (1 − y i ) log(1 − φ(p i ))),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>We conduct several ablation studies to verify the contribution of each module in our bilinear graph networks (BGNs). The first four lines in <ref type="table">Table 1</ref> show the accuracy of BAN on the VQA v2.0 validation dataset, and BAN-4 and BAN-8 represent the model with 4 and 8 glimpses respectively. It can be seen that simply stacking the module of BAN can improve the accuracy to some extent (0.35% and 0.46% in the two-layer and three-layer model respectively) compared with the one-layer model. Although the multilayer BAN might gain more visual information related to the global representation in Eq. (9) without exchanging context information, it is not clear about the relationship between entities in question. In contrast, our one-layer model, BGNs × 1, gains an accuracy 0.62% and 0.43% higher than BAN-4 × 1 and BAN-8 × 1 respectively, proving the effectiveness of our proposed question-graph even with fewer glimpse. However, if we only stack the question-graph for multiple times (V-graph + Q-graph × 2 and V-graph + Qgraph × 3) with only one-layer image-graph, the performance grows slower than that of the BGNs × 2, this might be caused by that the question-graph can only propagate the information already learned by the image-graph but cannot involve more factual information required in the image to answer the questions. If we replace the proposed bilinear graph network in question-graph with the scaled dot-production (SDP), the accuracy declines (-0.06% and -0.41% than BGNs with the same layer) and grows slightly (0.09%) by stacking the graph. It can be explained that though the inputs of question-graph H are the same type of nodes, the nodes themselves are hybrid and the linear method cannot fully express their relations. By stacking three layers of the BGNs, our model achieves 67.21% on the overall accuracy, which is chosen as the best model.</p><p>Additionally, we investigate the absolute increase of the score of our models compared with the single-layer BAN on questions with varied lengths to show the ability of our   model on multi-step reasoning in <ref type="figure" target="#fig_3">Figure 3</ref>. Our models with different layers outperform the one-layer BAN, especially on long questions. The one-layer model does not perform as well as the other three models for long questions due to its shallow graphs. With more layers, our model becomes better at long questions and achieves a 1.7% increase at word number of nine. What interests us is why the performance drops at four-layer. Comparing the three-layer model and four-layer model, the former one works better in short questions (word number &lt; 8) which take 79% of all questions, while the latter one has a higher score in long questions, this may explain the performance drop. This phenomenon also inspires us to design a network in the future to classify the questions to fit different layers of graphs. Furthermore, we explore the influence of BERT [7] on our method, since it is trained on large text corpus, therefore it has better generalization and representation of textual features. So we replace the LSTM with it when modeling the question and fine-tuning its weight with different strategies. By using the base model of BERT without fine-tuning (BGNs × 1 + Base with lr × 0), the accuracy increases slightly, and by increasing its learning rate, it boosts and achieves the best performance at lr × 0.01. With this learning rate, we switch to the large model that is deeper and wider than the base one, the performance grows and keeps going by stacking our bilinear graph model on it, proving that our model is effective and compatible with BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with State-of-the-Art</head><p>In <ref type="table" target="#tab_4">Table 3</ref>, we evaluate our method on VQA v2.0 testdev dataset, which achieves state-of-the-art. As shown in  The predicted answers are tomato, orange, and apple respectively for one-layer, two-layer, and three-layer models of our bilinear graph networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Overall   <ref type="table" target="#tab_4">Table 3</ref>, the overall accuracy of our BGNs+Glove model is 1.31% higher than BAN+Glove, nearly 3.0% on num-ber metric. It can be explained that the counting task is a kind of relation among objects, which tries to find similar objects in the latter layers with objects grounded by previous layers. And the extra counter module <ref type="bibr" target="#b37">[38]</ref> in our BGNs+Glove+Counter model makes a little gain on overall accuracy since it might increase the counting ability but disturb our reasoning graphs leading to drop in other metrics. Thus, we choose BGNs+Glove and BGNs+BERT as our best models to evaluate them on the test-standard dataset.</p><p>As we mentioned in Section 4, our bilinear graph networks have a better explanation in modeling the relationship within multi-modal inputs, and we also achieve better performance on both test-dev and test-std dataset compared with other methods with and without BERT, proving the effectiveness of our proposed method.  <ref type="figure">Figure 5</ref>. Examples illustrate the answers predicted by BAN and our graph models. BAN, L1, L2, L3 denote the answers predicted by BAN, one-layer, two-layer, and three-layer of our model respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative Analysis</head><p>To visualize the effects of each module in our bilinear graph networks, we present the learned attention maps of the image-graph and the question-graph in each layer to show how the networks work. Given the question 'What fruit is on the left edge?' in <ref type="figure" target="#fig_5">Figure 4</ref>, the image-graph of the first layer attends kinds of objects in the input image, while the question-graph broadcasts the learned fruit information to other words and chooses 'tomato' as the answer, probably because the amount of 'tomato' is the biggest among all detected fruits. The image-graph of the second-layer picks 'orange' that is to the left of 'tomato' and the questiongraph keeps collecting 'fruit' and 'edge' information. In the third layer, the image-graph locates 'apple' that is on the left edge, and every word in the question-graph pays its attention to the 'edge' information to predict the answer.</p><p>In <ref type="figure">Figure 5</ref>, we show the answer predicted by BAN and our models with one layer, two layers, and three layers. In the first image of the top row, BAN cannot correctly answer the question because the entities of 'young girl' and 'bag' learn their positions respectively, but they do not know each other's information, while our proposed question-graph exchanges such positional information to make it possible to compare the relative direction of the two entities. A similar question can also be found in the first image of the bottom row, our model approaches the correct answer step by step as the layer of the graph increases. Moreover, our model can find the implicit relationship between objects, even when the sheep are far away from the dog in the second image of the top row, as well as abstract scenes in the second image (five circles representing Olympics) and third image (many trees composing forest) of the bottom row. Furthermore, our model finely discriminates the highly overlapped objects, such as two sheep in the second image and the rope in the fourth image of the top row, it is possibly because the question-graph undertakes some burden from the original graph of BAN, which makes the image-graph spare more effort on learning details in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Motivated by graph attention networks and Transformer, in this paper, we interpret bilinear attention networks from a new perspective and demonstrate its disadvantages, then we develop bilinear graph networks (BGNs) composed of layers of image-graph and question-graph to overcome them. The image-graph learns the graph between words in the question and objects in the image and generate the joint embeddings of them, while the question-graph models the graph between these joint embeddings in term of words to exchange context information. Our method achieves stateof-the-art performance on VQA v2.0 dataset, and the ablation studies show that our bilinear graph networks significantly outperform the BAN and other graph-based methods on a variety of questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of multiple glimpses of our image-graph. Each glimpse of graph weight G e is computed by utilizing the bilinear attention network between Q and V, then the Hadamard product of question features and weighted image features aim to represent their joint embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Score increase of our models (with layer=1,2,3,4) compared with one-layer BAN model on VQA v2.0 validation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of attention maps for our networks. The attention maps in each graph for multiple glimpses are summed at each layer to briefly show the attended objects and words. The first image at the top shows the bounding boxes of detected objects and others for graph attention weights between words. The images at the bottom show the graph attention weights between words and objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Accuracy of single model on VQA v2.0 test-dev and teststandard dataset, it is trained on training, validation splits and Visual Genome dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Is the young girl to the left or to the right of the bag?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">How many sheep is the dog</cell><cell cols="2">Where is the couch in this</cell><cell>How is the surfer attached to the</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">chasing?</cell><cell></cell><cell></cell><cell>photo?</cell><cell>board?</cell></row><row><cell>BAN: Right</cell><cell cols="2">L2: Left</cell><cell cols="2">BAN: 0</cell><cell cols="2">L2: 1</cell><cell>BAN: Table</cell><cell>L2: Left</cell><cell>BAN: Yes</cell><cell>L2: Balance</cell></row><row><cell>L1: Left</cell><cell cols="2">L3: Left</cell><cell cols="2">L1: 1</cell><cell cols="2">L3: 2</cell><cell>L1: Kitchen</cell><cell>L3: Background</cell><cell>L1: Surfboard</cell><cell>L3: Rope</cell></row><row><cell cols="3">What appliance is to the left of the steel refrigerator?</cell><cell></cell><cell cols="3">What event do the rings represent?</cell><cell cols="2">Where are zebras in the photo?</cell><cell>What is the baby sheep in the foreground doing?</cell></row><row><cell cols="2">BAN: refrigerator</cell><cell cols="2">L2: Microwave</cell><cell cols="2">BAN: Skiing</cell><cell>L2: Skiing</cell><cell>BAN: Field</cell><cell>L2: Field</cell><cell>BAN: Standing</cell><cell>L2: Eating</cell></row><row><cell>L1: refrigerator</cell><cell></cell><cell>L3: Stove</cell><cell></cell><cell>L1: Skiing</cell><cell></cell><cell>L3: Olympics</cell><cell>L1: Field</cell><cell>L3: Forest</cell><cell>L1: Standing</cell><cell>L3: Eating</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Murel: Multimodal relational reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">and Dhruv Batra. Visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic fusion with intra-and inter-modality attention flow for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-modality latent interaction network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04289</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-questionanswer synergistic network for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10434" to="10443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07932</idno>
		<title level="m">Bilinear attention networks</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<title level="m">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structure inference net: Object detection using scene-level context and instance-level relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6985" to="6994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00471</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6281" to="6290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multimodal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1839" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beyond bilinear: generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rethinking diversified and discriminative proposal generation for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03508</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam Prügel-</forename><surname>Bennett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05766</idno>
		<title level="m">Learning to count objects in natural images for visual question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
