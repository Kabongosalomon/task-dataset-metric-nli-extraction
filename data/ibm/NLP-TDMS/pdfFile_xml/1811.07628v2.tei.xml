<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ATOM: Accurate Tracking by Overlap Maximization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Fahad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ATOM: Accurate Tracking by Overlap Maximization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While recent years have witnessed astonishing improvements in visual tracking robustness, the advancements in tracking accuracy have been limited. As the focus has been directed towards the development of powerful classifiers, the problem of accurate target state estimation has been largely overlooked. In fact, most trackers resort to a simple multi-scale search in order to estimate the target bounding box. We argue that this approach is fundamentally limited since target estimation is a complex task, requiring highlevel knowledge about the object.</p><p>We address this problem by proposing a novel tracking architecture, consisting of dedicated target estimation and classification components. High level knowledge is incorporated into the target estimation through extensive offline learning. Our target estimation component is trained to predict the overlap between the target object and an estimated bounding box. By carefully integrating target-specific information, our approach achieves previously unseen bounding box accuracy. We further introduce a classification component that is trained online to guarantee high discriminative power in the presence of distractors. Our final tracking framework sets a new state-of-the-art on five challenging benchmarks. On the new large-scale Track-ingNet dataset, our tracker ATOM achieves a relative gain of 15% over the previous best approach, while running at over 30 FPS. Code and models are available at https: //github.com/visionml/pytracking. * Both authors contributed equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ATOM</head><p>DaSiamRPN UPDT</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generic online visual tracking is a hard and ill-posed problem. The tracking method must learn an appearance model of the target online based on minimal supervision, often a single starting frame in the video. The model then needs to generalize to unseen aspects of the target appearance, including different poses, viewpoints, lightning conditions etc. The tracking problem can be decomposed into <ref type="bibr">Figure 1</ref>. A comparison of our approach with state-of-the-art trackers. UPDT <ref type="bibr" target="#b2">[3]</ref>, based on correlation filters, lacks an explicit target state estimation component, performing a brute-force multiscale search instead. Consequently, it does not handle aspect-ratio changes, which can lead to tracking failure (second row). DaSi-amRPN <ref type="bibr" target="#b41">[42]</ref> employs a bounding box regression strategy to estimate the target state, but still struggles in cases of out-of-plane rotation, deformation, etc. Our approach ATOM, employing an overlap prediction network, successfully handles these challenges and provides accurate bounding box predictions. a classification task and an estimation task. In the former case, the aim is to robustly provide a coarse location of the target in the image by categorizing image regions into foreground and background. The second task is then to estimate the target state, often represented by a bounding box.</p><p>In recent years, the focus of tracking research has been on target classification. Much attention has been invested into constructing robust classifiers, based on e.g. correlation filters <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>, and exploiting powerful deep feature representations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref> for this task. On the other hand, target estimation has seen below expected progress. This trend is clearly observed in the recent VOT2018 challenge <ref type="bibr" target="#b16">[17]</ref>, where older trackers such as KCF <ref type="bibr" target="#b12">[13]</ref> and MEEM <ref type="bibr" target="#b39">[40]</ref> still obtain competitive accuracy while exhibiting vastly inferior robustness. In fact, most current state-of-the-art trackers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33]</ref> still rely on the classification component for target estimation by performing a multi-scale search. How-ever, this strategy is fundamentally limited since bounding box estimation is inherently a challenging task, requiring high-level understanding of the object's pose (see <ref type="bibr">figure 1)</ref>.</p><p>In this work, we set out to bridge the performance gap between target classification and estimation in visual object tracking. We introduce a novel tracking architecture consisting of two components designed exclusively for target estimation and classification. Inspired by the recently proposed IoU-Net <ref type="bibr" target="#b14">[15]</ref>, we train the target estimation component to predict the Intersection over Union (IoU) overlap, i.e. the Jaccard Index <ref type="bibr" target="#b13">[14]</ref>, between the target and an estimated bounding box. Since the original IoU-Net is classspecific, and hence not suitable for generic tracking, we propose a novel architecture for integrating target-specific information into the IoU prediction. We achieve this by introducing a modulation-based network component that incorporates the target appearance in the reference image to obtain target-specific IoU estimates. This further enables our target estimation component to be trained offline on largescale datasets. During tracking, the target bounding box is found by simply maximizing the predicted IoU overlap in each frame.</p><p>To develop a seamless and transparent tracking method, we also revisit the problem of target classification with the aim of avoiding unnecessary complexity. Our target classification component is simple yet powerful, consisting of a two-layer fully convolutional network head. Unlike our target estimation module, the classification component is trained online, providing high robustness against distractor objects in the scene. To ensure real-time performance, we address the problem of efficient online optimization, where gradient descent falls short. Instead, we employ a Conjugate-Gradient-based strategy and demonstrate how it can be easily implemented in modern deep learning frameworks. Our final tracking loop is simple, alternating between target classification, estimation, and model update.</p><p>We perform comprehensive experiments on five challenging benchmarks: NFS <ref type="bibr" target="#b8">[9]</ref>, UAV123 <ref type="bibr" target="#b23">[24]</ref>, TrackingNet <ref type="bibr" target="#b24">[25]</ref>, LaSOT <ref type="bibr" target="#b7">[8]</ref>, and VOT2018 <ref type="bibr" target="#b16">[17]</ref>. Our tracking approach sets a new state-of-the-art on all five datasets, achieving an absolute gain of 10% on the challenging LaSOT dataset. Moreover, we provide an analysis of our tracker, along with different network architectures for overlap prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the context of visual tracking, it often makes sense to distinguish between target classification and target estimation as two separate, but related subtasks. Target classification basically aims at determining the presence of the target object at a certain image location. However, only partial information about the target state is obtained, e.g. its image coordinates. Target estimation then aims at finding the full state. In visual tracking, the target state is often rep-resented by a bounding box, either axis aligned <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37]</ref> or rotated <ref type="bibr" target="#b16">[17]</ref>. State estimation is then reduced to finding the image bounding box that best describes the target in the current frame. In the simplest case, the target is rigid and only moves parallel to the camera plane. In such a scenario, target estimation reduces to finding the 2D image-location of the target, and therefore does not need to be considered separately from target classification. In general, however, objects may undergo radical variations in pose and viewpoint, greatly complicating the task of bounding box estimation.</p><p>In the last few years, the challenge of target classification has been successfully addressed by discriminatively training powerful classifiers online <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. In particular, the correlation-based trackers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref> have gained wide popularity. These methods rely on the diagonalizing transformation of circular convolutions, given by the Discrete Fourier Transform, to perform efficient fully convolutional training and inference. Correlation filter methods often excel at target classification by computing reliable confidence scores in a dense 2D-grid. On the other hand, accurate target estimation has long eluded such approaches. Even finding a one-parameter scale factor has turned out a formidable challenge <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> and most approaches resort to the bruteforce multi-scale detection strategy with its obvious computational impact. As such, the default method is to apply the classifier alone to perform full state estimation. However, target classifiers are not sensitive to all aspects of the target state, e.g. the width and height of the target. In fact, invariance to some aspects of the target state is often considered a valuable property of the discriminative model to improve robustness <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26]</ref>. Instead of relying on the classifier, we learn a dedicated target estimation component.</p><p>Accurate estimation of an object's bounding box is a complex task, requiring high-level a-priori knowledge. The bounding box depends on the pose and viewpoint of the object, which cannot be modeled as a simple image transformation (e.g. uniform image scaling). It is therefore highly challenging, if not impossible, to learn accurate target estimation online from scratch. Many recent methods in the literature have therefore integrated prior knowledge in the form of heavy offline learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42]</ref>. In particular, SiamRPN <ref type="bibr" target="#b17">[18]</ref> and its extension <ref type="bibr" target="#b41">[42]</ref> have been shown capable of bounding box regression thanks to extensive offline training. Yet, these Siamese tracking approaches often struggle at the problem of target classification. Unlike, for instance, correlation-based methods, most Siamese trackers do not explicitly account for distractors, since no online learning is performed. While this problem has been partly addressed using simple template update techniques <ref type="bibr" target="#b41">[42]</ref>, it has yet to reach the level of strong online-learned models. In contrast to Siamese methods, we learn the classification model online, while also utilizing extensive offline training for the target estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this work, we propose a novel tracking approach consisting of two components: 1) A target estimation module that is learned offline; and 2) A target classification module that is learned online. That is, following the modern trend in object detection <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, we separate the subproblems of target classification and estimation. Yet, both of these tasks are integrated in a unified multi-task network architecture, shown in figure 2.</p><p>We employ the same backbone network for both the target classification and estimation tasks. For simplicity, we use a ResNet-18 model that is trained on ImageNet and refrain from fine-tuning the backbone in this work. Target estimation is performed by the IoU-predictor network. This network is trained offline on large-scale video tracking and object detection datasets, and its weights are frozen during online tracking. The IoU-predictor takes four inputs: i) backbone features from current frame, ii) bounding box estimates in the current frame, iii) backbone features from a reference frame, iv) the target bounding box in the reference frame. It then outputs the predicted Intersection over Union (IoU) score for each of the current-frame bounding box estimates. During tracking, the final bounding box is obtained by maximizing the IoU score using gradient ascent. The target estimation component is detailed in section 3.1.</p><p>Target classification is performed by another network head. Unlike the target estimation component, the classification network is entirely learned during online tracking. It is exclusively trained to discriminate the target from other objects in the scene by predicting a target confidence score based on backbone features extracted from the current frame. Both training and prediction are performed in a fully convolutional manner to ensure efficiency and coverage. However, training such a network online with conventional approaches, such as stochastic gradient descent, is suboptimal for our online purpose. We therefore propose to use an optimization strategy, based on Conjugate Gradient and Gauss-Newton, that enables fast online training. Moreover, we demonstrate how this approach can be easily implemented in common deep learning frameworks, such as PyTorch, by exploiting the back-propagation functionality. Our target classification approach is described in section 3.2 and our final tracking framework is detailed in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Target Estimation by Overlap Maximization</head><p>In this section, we detail how the target state estimation is performed. The aim of our state estimation component is to determine the target bounding box given a rough initial estimate. We take inspiration from the IoU-Net <ref type="bibr" target="#b14">[15]</ref>, which was recently proposed for object detection as an alternative to typical anchor-based bounding box regression techniques. In contrast to conventional approaches, the IoU-Net is trained to predict the IoU between an image object and an  <ref type="figure">Figure 2</ref>. Overview of our network architecture for visual tracking. We augment two modules to the pretrained ResNet-18 backbone network (orange). The target estimation module (blue) is trained offline on large-scale datasets to predict the IoU overlap with the target. Using the reference frame and the initial target box, modulation vectors carrying target-specific appearance information are computed. The IoU predictor component then receives features and proposal bounding boxes in the test frame, along with the aforementioned modulation vectors. It estimates the IoU for each input box. The target classification module (green) is trained online to output target confidences in a fully convolutional manner.</p><p>input bounding box candidate. Bounding box estimation is then performed by maximizing the IoU prediction.</p><p>To describe our target estimation component, we first briefly revisit the IoU-Net model. Given a deep feature representation of an image, x ∈ R W ×H×D , and a bounding box estimate B ∈ R 4 of an image object, IoU-Net predicts the IoU between B and the object. Here B is parametrized as B = (c x /w, c y /h, log w, log h), where (c x , c y ) are the image coordinates of the bounding box center. The network uses a Precise ROI Pooling (PrPool) <ref type="bibr" target="#b14">[15]</ref> layer to pool the region in x given by B, resulting in a feature map x B of a pre-determined size. Essentially, PrPool is a continuous variant of adaptive average pooling, with the key advantage of being differentiable w.r.t. the bounding box coordinates B. This allows the bounding box B to be refined by maximizing the IoU w.r.t. B through gradient ascent. Network Architecture: For the task of object detection, independent IoU-Nets are trained in <ref type="bibr" target="#b14">[15]</ref> for each object class. However, in tracking the target class is generally unknown. Further, unlike object detection, the target is not required to belong to any set of pre-defined classes or be represented in any existing training datasets. Class-specific IoU predictors are thus of little use for generic visual tracking. Instead, target-specific IoU predictions are required, by exploiting the target annotation in the first frame. Due to the high-level nature of the IoU prediction task, it is not feasible to train, or even fine-tune the IoU-Net online on a single frame. Thus, we argue that the target estimation network needs to be trained offline to learn a general representation for IoU prediction.</p><p>In the context of visual tracking, where the target object  is unknown beforehand, the challenge is thus to construct an IoU prediction architecture that makes effective use of the reference target appearance given at test-time. Our initial experiments showed that naive approaches for fusing the reference image features with the current-frame features yield poor performance (see section 4.1). We also found Siamese architectures to provide suboptimal results. In this work, we therefore propose a modulation-based network architecture that predicts the IoU for an arbitrary object given only a single reference image. The proposed network is visualized in <ref type="figure" target="#fig_0">figure 3</ref>. Our network has two branches, both of which take backbone features from ResNet-18 Block3 and Block4 as input. The reference branch inputs features x 0 and the target bounding box annotation B 0 in the reference image. It returns a modulation vector c(x 0 , B 0 ), consisting of positive coefficients of size 1 × 1 × D z . As illustrated in <ref type="figure" target="#fig_0">figure 3</ref>, this branch consists of a convolutional layer followed by PrPool and a fully connected layer.</p><p>The current image, in which we want to estimate the target bounding box, is processed through the test branch. It first extracts a deep representation by feeding the backbone features x through two convolutional layers followed by a PrPool with the bounding box estimate B. As the test branch extracts general features for IoU prediction, which constitutes a more complex task, it employs more layers and higher pooling resolution compared to the reference branch (see <ref type="figure" target="#fig_0">figure 3</ref>). The resulting representation z(x, B) is of size K × K × D z , where K is spatial output size of the PrPool layer. The computed feature representation of the test image is then modulated by the coefficient vector c via a channel-wise multiplication. This creates a target-specific representation for IoU prediction, effectively incorporating the reference appearance information. The modulated representation is finally fed to the IoU predictor module g, consisting of three fully connected layers. The predicted IoU of the bounding box B is hence given by</p><formula xml:id="formula_0">IoU(B) = g c(x 0 , B 0 ) · z(x, B) .<label>(1)</label></formula><p>To train the network, we minimize the prediction error of (1), given annotated data. During tracking we maximize (1) w.r.t. B to estimate the target state.</p><p>Training: From (1) it is clear that the entire IoU prediction network can be trained in an end-to-end fashion, using bounding-box-annotated image pairs. We use the training splits of the recently introduced Large-scale Single Object Tracking (LaSOT) dataset <ref type="bibr" target="#b7">[8]</ref> and TrackingNet <ref type="bibr" target="#b24">[25]</ref>. We sample image pairs from the videos with a maximum gap of 50 frames. Similar to <ref type="bibr" target="#b41">[42]</ref>, we augment our training data with synthetic image pairs from the COCO dataset <ref type="bibr" target="#b20">[21]</ref> to have more diverse classes. From the reference image, we sample a square patch centered at the target, with an area of about 5 2 times the target area. From the test image, we sample a similar patch, with some perturbation in the position and scale to simulate the tracking scenario. These cropped regions are then resized to a fixed size. For each image pair we generate 16 candidate bounding boxes by adding Gaussian noise to the ground truth coordinates, while ensuring a minimum IoU of 0.1. We use image flipping and color jittering for data augmentation. As in <ref type="bibr" target="#b14">[15]</ref>, the IoU is nor-</p><formula xml:id="formula_1">malized to [−1, 1].</formula><p>The weights in our head network are initialized using <ref type="bibr" target="#b11">[12]</ref>. For the backbone network, we freeze all weights during training. We use the mean-squared error loss function and train for 40 epochs with 64 image pairs per batch. The ADAM <ref type="bibr" target="#b15">[16]</ref> optimizer is employed with initial learning rate of 10 −3 , and using a factor 0.2 decay every 15 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Target Classification by Fast Online Learning</head><p>While the target estimation module provides accurate bounding box outputs, it lacks the ability to robustly discriminate between the target object and background distractors. We therefore complement the estimation module with a second network head, whose sole purpose is to perform this discrimination. Unlike the estimation component, the target classification module is exclusively trained online, to predict a target confidence score. Since the goal of the target classification module is to provide a rough 2D-location of the object, we wish it to be invariant to the size and scale of the target. Instead, it should emphasize robustness by minimizing false detections. Model: Our target classification module is a 2-layer fully convolutional neural network, formally defined as</p><formula xml:id="formula_2">f (x; w) = φ 2 (w 2 * φ 1 (w 1 * x)) .<label>(2)</label></formula><p>Here, x is the backbone feature map, w = {w 1 , w 2 } are the parameters of the network, φ 1 , φ 2 are activation functions and * denotes standard multi-channel convolution. While our framework is general, allowing more complex models for this purpose, we found such a simple model sufficient and beneficial in terms of computational efficiency. Inspired by the recent success of discriminative correlation filter (DCF) approaches, we formulate a similar learning objective based on the L 2 classification error,</p><formula xml:id="formula_3">L(w) = m j=1 γ j f (x j ; w) − y j 2 + k λ k w k 2 . (3)</formula><p>Each training sample feature map x j is annotated by the classification confidences y j ∈ R W ×H , set to a sampled Gaussian function centered at the target location. The impact of each training sample is controlled by the weight γ j , while the amount of regularization on w k is set by λ k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online Learning:</head><p>A brute-force approach to minimize (3) would be to apply standard gradient descent or its stochastic twin. These approaches are easily implemented in modern deep learning libraries, but are not well suited for online learning due to their slow convergence rates. We therefore develop a more sophisticated optimization strategy that is tailored for such online learning problems, yet requiring only little added implementation complexity. First, we define the residuals of the problem as <ref type="formula">(3)</ref> is then equivalently written as the squared L 2 norm of the residual vector L(w) = r(w) 2 , where r(w) is the concatenation of all residuals r j (w). We utilize the quadratic Gauss-Newton approximationL w (∆w) ≈ L(w +∆w), obtained from a first order Taylor expansion of the residuals r(w + ∆w) ≈ r w + J w ∆w at the current estimate w,</p><formula xml:id="formula_4">r j (w) = √ γ j (f (x j ; w) − y j ) for j ∈ {1, . . . , m} and r m+k (w) = √ λ k w k for k = 1, 2. The loss</formula><formula xml:id="formula_5">L w (∆w) = ∆w T J T w J w ∆w + 2∆w T J T w r w + r T w r w . (4)</formula><p>Here, we have defined r w = r(w) and J w = ∂r ∂w is the Jacobian of r at w. The new variable ∆w represents the increment in the parameters w.</p><p>The Gauss-Newton subproblem (4) forms a positive definite quadratic function, allowing the deployment of specialized machinery such as the Conjugate Gradient (CG) method. While a full description of CG is outside the scope of this paper (see <ref type="bibr" target="#b30">[31]</ref> for a full treatment), intuitively it finds an optimal search direction p and step length α in each iteration. Since the CG algorithm consists of simple vector operations, it can be implemented with only a few lines of python code. The only challenging aspect of CG is the evaluation of the operation J T w J w p for a search direction p. We note that CG has been successfully deployed in some DCF tracking approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33]</ref>. However, these implementations rely on hand-coding all operations in order to implement J T w J w p, requiring much tedious work and derivations even for a simple model <ref type="bibr" target="#b1">(2)</ref>. This approach also lacks flexibility since any minor modification of the architecture <ref type="bibr" target="#b1">(2)</ref>, such as adding a layer or changing a nonlinearity, may require comprehensive re-derivation and implementation work. In this paper, we therefore demonstrate how to implement CG for (4) by exploiting the backpropagation functionality of modern deep learning frameworks, such as PyTorch. Our implementation only requires the user to supply the function r(w) for evaluating the residuals, which is easy to implement. Our algorithm is therefore applicable to any shallow learning problem of the form (3).</p><p>To find a strategy for evaluating J T w J w p, we first consider a vector u of the same size as the residuals r(w). By computing the gradient of their inner product, we obtain</p><formula xml:id="formula_6">∂ ∂w (r(w) T u) = ∂r ∂w T u = J T w u.</formula><p>In fact, this is the standard operation of the backpropagation procedure, namely to apply the transposed Jacobian at each node in the computational graph, starting at the output. We can thus define backpropagation of a scalar function s with respect to a variable v as BackProp(s, v) = ∂s ∂v . Now, as shown above, we have BackProp(r T u, w) = J T w u. However, this only accounts for the second product in J T w J w p. We first have to compute J w p, which involves the application of the Jacobian itself (not its transpose). Thankfully, the Jacobian of the function u → J T w u is trivially J T w , since the function is for n = 1, . . . , N CG do 6: g ← g − αq 2</p><formula xml:id="formula_7">ρ 2 ← ρ 1 , ρ 1 ← g T g , β ← ρ1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>∆w ← ∆w + αp <ref type="bibr">13:</ref> end for <ref type="bibr">14:</ref> w ← w + ∆w 15: end for linear. We can therefore transpose it by applying backpropagation. By letting h := J T w u = BackProp(r T u, w), we get J w p = ∂ ∂u (h T p) = BackProp(h T p, u). Given the above mentioned result, we outline the entire optimization procedure in algorithm 1. It applies N GN Gauss-Newton iterations, each encompassing N CG Conjugate Gradient iterations for minimizing the resulting subproblem <ref type="bibr" target="#b3">(4)</ref>. Each CG iteration requires two BackProp calls for evaluating q 1 = J w p and q 2 = J T w q 1 , respectively. There is a need for computing h = J T w u once in the outer loop. Note that in each call to BackProp in algorithm 1, one of the vectors in the inner product is treated as constant, i.e. gradients are not propagated through it. This is highlighted as comments in algorithm 1 for clarity. It is noteworthy that the optimization algorithm is virtually parameter free, only the number of iterations need to be set. In comparison to gradient descent, the CG-based method adaptively computes the learning rate α and momentum β in each iteration. Observe that g is the negative gradient of (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Online Tracking Approach</head><p>Our tracker ATOM is implemented in Python, using Py-Torch. It runs at over 30 FPS on an Nvidia GT-1080 GPU. Feature extraction: We use ResNet-18 pretrained on Im-ageNet as our backbone network. For target classification, we employ block 4 features, while the target estimation component uses both block 3 and 4 as input. Features are always extracted from patches of size 288×288 from image regions corresponding to 5 times the estimated target size. Note that ResNet-18 feature extraction is shared and only performed on a single image patch every frame. Classification Model: The first layer in our classification head (2) consists of a 1 × 1 convolutional layer w 1 , which reduces the feature dimensionality to 64. As in <ref type="bibr" target="#b3">[4]</ref>, the pur-pose of this layer is to limit memory and computational requirements. The second layer employs a 4 × 4 kernel w 2 with a single output channel. We set φ 1 to identity since we did not observe any benefit of using a non-linearity at this layer. We use a continuously differentiable parametric exponential linear unit (PELU) <ref type="bibr" target="#b34">[35]</ref> as output activation: φ 2 (t) = t, t ≥ 0 and φ 2 (t) = α(e t α − 1), t ≤ 0. Setting α = 0.05 allows us to ignore easy negative examples in the loss <ref type="bibr" target="#b2">(3)</ref>. We found the continuous differentiability of φ 2 to be advantageous for optimization.</p><p>In the first frame, we perform data augmentation by applying varying degrees of translation, rotation, blur, and dropout, similar to <ref type="bibr" target="#b2">[3]</ref>, resulting in 30 initial training samples x j . We then apply algorithm 1 with N GN = 6 and N CG = 10 to optimize the parameters w. Subsequently, we only optimize the final layer w 2 , using N GN = 1 and N CG = 5 every 10th frame. In every frame, we add the extracted feature map x j as a training sample, annotated by a Gaussian y j centered at the estimated target location. The weights γ j in (3) are updated with a learning rate of 0.01. Target Estimation: We first extract features at the previously estimated target location and scale. We then apply the classification model (2) and find the 2D-position with the maximum confidence score. Together with the previously estimated target width and height, this generates the initial bounding box B. While it is possible to perform state estimation using this single proposal, we found that local maxima are better avoided using multiple random initializations. We therefore generate a set of 10 initial proposals by adding uniform random noise to B. The predicted IoU (1) of each box is maximized using 5 gradient ascent iterations with a step length of 1. The final prediction is obtained by taking the mean of the 3 bounding boxes with highest IoU. No further post-processing or filtering, as in e.g. <ref type="bibr" target="#b17">[18]</ref> is performed. This refined state also annotates the training sample (x j , y j ), as described earlier. Note that the modulation vector c(x 0 , B 0 ) in (1) is precomputed in the first frame. Hard Negative Mining: To further robustify our classification component in the presence of distractors, we adopt a hard negative mining strategy, common in many visual trackers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">42]</ref>. If a distractor peak is detected in the classification scores, we double the learning rate of this training sample and instantly run a round of optimization with standard settings (N GN = 1, N CG = 5). We also determine the target as lost if the score falls below 0.25. While the hard negative strategy is not fundamental to our framework, it provides some additional robustness (see section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the proposed tracker ATOM on five benchmarks: Need for Speed (NFS) <ref type="bibr" target="#b8">[9]</ref>, UAV123 <ref type="bibr" target="#b23">[24]</ref>, Track-ingNet <ref type="bibr" target="#b24">[25]</ref>, LaSOT <ref type="bibr" target="#b7">[8]</ref>, and VOT2018 <ref type="bibr" target="#b16">[17]</ref>. Detailed results are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">IoU Prediction Architecture Analysis</head><p>Here, we study the impact of various architectural choices for the IoU prediction module, presented in section 3.1. Our analysis is performed on the combined UAV123 <ref type="bibr" target="#b23">[24]</ref> and NFS (30 FPS version) <ref type="bibr" target="#b8">[9]</ref> datasets, summing to 223 videos. These datasets contain a high variety of videos that are challenging in many aspects, such as deformation, view change, occlusion, fast motion and distractors. We evaluate the trackers based on the overlap precision metric (OP T ), defined as the percentage of frames having bounding box IoU overlap larger than a threshold T with the ground truth. We also report the area-under-the-curve (AUC) score, defined as AUC = 1 0 OP T dT . In all experiments, we report the average result over 5 runs. Reference image: We compare with a baseline approach that excludes target specific information by removing the reference branch in our architecture. That is, the baseline network only uses the test frame to predict the IoU. The results of this investigation are shown in table 1. Excluding the reference frame deteriorates the results by over 5.5% AUC score. This demonstrates the importance of exploiting target-specific appearance information in order to accurately predict the IoU for an arbitrary object. Integration of target appearance: We investigate different network architectures for integrating the reference image features for IoU prediction. We compare our feature modulation based method, presented in section 3.1, with two alternative architectures. Concatenation: Activations from the reference and test branches are concatenated before the final IoU prediction layers. Siamese: Using identical architecture for both branches and performing final IoU prediction as a scalar product of their outputs. All the networks are trained using the same setup, with ResNet18 Block3 and Block4 features as input. For a fair comparison, we ensure that all networks have the same depth and similar number of trainable parameters. Results are shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We perform an ablation study to demonstrate the impact of each component in the proposed method. We use the same dataset and the evaluation criteria as in section 4.1. Target Estimation: We compare our target state estimation component, presented in section 3.1, with a brute-force multi-scale search approach employing only the classification model. This approach mimics the common practice in correlation filter based methods, extracting features at 5 scales with a scale ratio of 1.02. The classification component is then evaluated on all scales, selecting the location and scale with the highest confidence score as the new target state. Results are shown in table 2. Our approach significantly outperforms the multi-scale method by 8.6% in AUC. Further, our approach almost doubles the percentage of highly accurate bounding box predictions, as measured by OP 0.75 . These results highlight the importance of treating target state estimation as a high-level visual task. Target Classification: We investigate the impact of the target classification component (section 3.2) by excluding it from our tracking framework. No Classif in table 2 only employs the target estimation module for tracking, using a larger search region. The resulting method achieves an AUC of 43.0%, almost 20% less than our approach. Online Optimization: We investigate the impact of the optimization strategy presented in algorithm 1, by comparing it with gradient descent. We use carefully tuned learning rate and momentum parameters for the gradient descent approach. In the version termed GD, we run the same number of BackProp operations as in our algorithm, obtaining the same speed as of our tracker. We also compare with GD++, running 5 times as many iterations as in GD, thus running at significantly slower frame rates. In both cases, the proposed Gauss-Newton approach outperforms gradient descent by  (b) UAV123 <ref type="figure">Figure 4</ref>. Success plots on NFS (a) and UAV123 (b). In both cases, our approach improves the state-of-the-art by a large margin.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">State-of-the-art Comparison</head><p>We present the comparison of our tracker with state-ofthe-art methods on five challenging tracking datasets. Need For Speed <ref type="bibr" target="#b8">[9]</ref>: We evaluate on the 30 FPS version of the dataset. <ref type="figure">Figure 4a</ref> shows the success plot over all the 100 videos, reporting AUC scores in the legend. CCOT <ref type="bibr" target="#b6">[7]</ref> and UPDT <ref type="bibr" target="#b2">[3]</ref>, both based on correlation filters, achieve AUC scores of 49.2% and 54.2% respectively. Our tracker significantly outperforms UPDT with a relative gain of 9%. UAV123 <ref type="bibr" target="#b23">[24]</ref>: <ref type="figure">Figure 4b</ref> displays the success plot over all the 123 videos. DaSiamRPN <ref type="bibr" target="#b41">[42]</ref> and its predecessor SiamRPN <ref type="bibr" target="#b17">[18]</ref> employ a target estimation component based on bounding box regression. Compared to other approaches, DaSiamRPN achieves a superior AUC of 58.4%, owing to its accuracy. Our tracker, employing an overlap maximization strategy for target estimation, significantly outperforms DaSiamRPN by achieving an AUC of 65.0%. TrackingNet <ref type="bibr" target="#b24">[25]</ref>: This is a recently introduced largescale dataset consisting of real-world videos sampled from YouTube. The trackers are evaluated using an online evaluation server on a test set of 511 videos. <ref type="table" target="#tab_5">Table 3</ref> shows the results in terms of precision, normalized precision, and success. In terms of precision and success, MDNet <ref type="bibr" target="#b25">[26]</ref> achieves scores of 56.5% and 60.6% respectively. Our tracker outperforms MDNet with relative gains of 14% and   <ref type="table">Table 5</ref>. State-of-the-art comparison on the public VOT2018 dataset in terms of expected average overlap (EAO), robustness (tracking failure), and accuracy. Our tracker outperforms all the previous methods in terms of EAO.</p><p>16% in terms of precision and success respectively.</p><p>LaSOT <ref type="bibr" target="#b7">[8]</ref>: We evaluate our approach on the test split consisting of 280 videos. <ref type="table" target="#tab_7">Table 4</ref> shows the results in terms of normalized precision and success. Among previous approaches, DaSiamRPN achieves the best success scores.</p><p>Our approach significantly outperforms DaSiamRPN with an absolute gain of 10.0% in success.</p><p>VOT2018 <ref type="bibr" target="#b16">[17]</ref>: This dataset consists of 60 videos and the performance is evaluated in terms of robustness (failure rate) and accuracy (average overlap in the course of successful tracking). The two measures are merged in a single metric, Expected Average Overlap (EAO), which provides the overall performance ranking. <ref type="table">Table 5</ref> shows the comparison of our approach with the top-10 trackers in the VOT2018 competition <ref type="bibr" target="#b16">[17]</ref>. Among the top trackers, only DaSiamRPN uses an explicit target state estimation component, achieving higher accuracy compared to its DCF-based counterparts like LADCF <ref type="bibr" target="#b37">[38]</ref> and MFT. Our approach ATOM achieves the best accuracy, while having competitive robustness. Further, our tracker obtains the best EAO score of 0.401, with a relative gain of 3% over LADCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose a novel tracking architecture with explicit components for target estimation and classification. The estimation component is trained offline on large-scale datasets to predict the IoU overlap between the target and a bounding box estimate. Our architecture integrates target-specific knowledge by performing feature modulation. The classification component consists of a two-layer fully convolutional network head and is trained online using a dedicated optimization approach. Comprehensive experiments are performed on four tracking benchmarks. Our approach provides accurate target estimation while being robust against distractor objects in the scene, outperforming previous methods on all four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material we provide additional details and results. Section S1 provides details about the other network architectures evaluated for IoU prediction in section 4.1 of the main paper. Section S2 performs an empirical convergence analysis of the employed optimization procedure and the gradient descent. Detailed results on the LaSOT <ref type="bibr" target="#b7">[8]</ref> dataset are provided in section S3. Section S4 provides results on the OTB-100 <ref type="bibr" target="#b36">[37]</ref> dataset. The impact of the training data on performance of our tracker is analyzed in section S5. Section S6 provides detailed results on the UAV123 dataset. A video showing qualitative results of our tracker can be found at https://youtu. be/T8x8i1KkYGk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1. Network Architectures for IoU Prediction</head><p>Here we describe the different network architectures for integrating the target appearance, investigated in section 4.1 of the main paper. <ref type="figure" target="#fig_1">Figure S1</ref> visualizes the Concatenation architecture. In this architecture, both the reference and test branches have the same network structure. ResNet-18 Block3 and Block4 features that are extracted from the reference and test images are passed through two Conv layers, followed by PrPool and an FC layer. The processed features from both the ResNet blocks and both the images are concatenated and passed through a final FC layer which predicts the IoU. Note that due to the symmetric structure of the network, the weights for the Conv layers before PrPool are shared between the reference branch and the test branch. However the FC layers do not share the weights. <ref type="figure">Figure S2</ref> visualizes the Siamese architecture. Similar to Concatenation, both the reference and test branches have the same network structure. ResNet-18 Block3 and Block4 features that are extracted from the reference and test images are passed through two Conv layers, followed by PrPool and an FC layer. The processed features from both the ResNet blocks are then concatenated. The IoU prediction is obtained as the dot product of the features from the reference and the test branches. The Conv layers before PrPool have shared weights. In the final FC layer however, we found it beneficial not to share the weights between the branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. Convergence Analysis</head><p>We empirically compare of the convergence speed of the employed optimization method (algorithm 1 in the paper) and Gradient Descent (GD). This is performed by comparing the loss for the online learning problem eq. (3), which is minimized in the first frame w.r.t. the filter weights w 1 and w 2 . For our method, we use the settings described in <ref type="figure" target="#fig_1">Figure S1</ref>. Architecture of the Concatenation network for IoU prediction evaluated in section 4.1 in the paper. <ref type="figure">Figure S2</ref>. Architecture of the Siamese network for IoU prediction evaluated in section 4.1 in the paper. the paper. In case of Gradient Descent we employ the same settings used in the ablation study (section 4.2 in the paper).</p><p>In <ref type="figure" target="#fig_0">figure S3</ref> we plot the loss (eq. (3) in the paper) for each method. For a fair comparison, the loss is plotted w.r.t. the number of BackProp calls performed by each method. The loss in <ref type="figure" target="#fig_0">figure S3</ref> is computed as an average of five complete runs over the full NFS dataset <ref type="bibr" target="#b8">[9]</ref>. Our CG-based optimization algorithm exhibits superior convergence speed compared to Gradient Descent. Moreover, the employed optimization methods does not require tuning of the step length and momentum parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3. Detailed results on LaSOT dataset</head><p>In table 4 in the main paper, we provide a state-of-theart comparison on the large-scale LaSOT dataset in terms of normalized precision and success. Here, we provide the success plot for the same. The success plots are obtained using the overlap precision (OP) score, which is computed as the percentage of frames in the dataset for which the intersection-over-union (IoU) overlap between the tracker prediction and the ground truth bounding box is higher than a certain threshold. The OP scores are plotted for a range of thresholds in [0, 1] to obtain the success plot. The area under this plot gives the AUC (success) score, which is reported in the legend. <ref type="figure">Figure S4</ref> shows the success plot over Loss (log scale)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online Classifier Learning Loss</head><p>Our optimization Gradient Descent <ref type="figure" target="#fig_0">Figure S3</ref>. Comparison of convergence speed between our employed online optimization procedure and Gradient Descent. We plot the loss of the online classifier learning (eq. (3) in the paper) w.r.t. the number of performed BackProp iterations. The loss is averaged over five independent runs of the complete NFS dataset. The employed method achieves much faster convergence.  <ref type="figure">Figure S4</ref>. Success plot on the LaSOT dataset. Note that due to the unavailability of raw results for DaSiamRPN, we only report the final AUC score in the legend. Our approach ATOM outperforms all previous methods by a large margin.</p><p>the 280 test videos. Our approach ATOM significantly outperforms the previous best approach DaSiamRPN <ref type="bibr" target="#b41">[42]</ref> with an absolute gain of 10.0% in AUC score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4. Results on OTB-100 dataset</head><p>Here, we compare our approach with the state-of-theart trackers on the OTB-100 <ref type="bibr" target="#b36">[37]</ref> dataset. The success plot over all the 100 videos are shown in <ref type="figure">figure S5</ref>. Our ap-   <ref type="table">Table S2</ref>. Comparision of our approach trained using only ImageNet-VID (denoted ATOM-VID) on the TrackingNet dataset.</p><p>proach achieves results competitive with the state-of-the-art approaches, with an AUC score of 67.1%. Note that the best results are obtained by the correlation filter based methods, ECO <ref type="bibr" target="#b3">[4]</ref> and CCOT <ref type="bibr" target="#b6">[7]</ref>. These methods employ brute-force multi-scale search for target estimation. Since OTB-100 has limited changes in aspect ratio (see <ref type="figure">figure 2</ref> in <ref type="bibr" target="#b23">[24]</ref>), the fixed aspect ratio constraint in multi-scale search strategy helps these methods to obtain a better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5. Impact of training data</head><p>In this section, we investigate the impact of using recent large-scale tracking datasets for offline training of our IoU predictor network. We train our network using only the ImageNet-VID <ref type="bibr" target="#b29">[30]</ref> dataset, that has been commonly used to train trackers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref> in recent years. We compare this version, denoted ATOM-VID, with the state-of-the-art approaches on two recent datasets, namely LaSOT <ref type="bibr" target="#b7">[8]</ref> and TrackingNet <ref type="bibr" target="#b24">[25]</ref>. For comparision, we also include our final version ATOM, trained using the train splits of La-SOT, TrackingNet and COCO <ref type="bibr" target="#b20">[21]</ref>. Results are shown in table S1 for LaSOT and table S2 for TrackingNet, respec-tively. Among previous approaches, DaSiamRPN <ref type="bibr" target="#b41">[42]</ref> uses bounding box regression strategy and achieves the best results on both datasets. Note that DaSiamRPN is trained using the large-scale YoutubeBB <ref type="bibr" target="#b26">[27]</ref>, ImageNet-VID, COCO and ImageNet DET <ref type="bibr" target="#b29">[30]</ref> datasets. Our approach ATOM-VID, trained using only ImageNet-VID, significantly outperforms DaSiamRPN with an absolute gain of 8.0% in AUC score on LaSOT, and 6.0% in AUC score on Track-ingNet. Using the recent tracking datasets for training further improves the results, providing an absolute gain of 2.0% on LaSOT and 0.5% on TrackingNet. While using a larger training set improves the tracking performance as expected, our approach still achieves state-of-the-art results when using less data compared to recent methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6. Additional Results on UAV123</head><p>Here, we provide detailed results on the UAV123 dataset <ref type="bibr" target="#b23">[24]</ref>. In UAV123, each video is annotated with 12 different attributes: aspect ratio change, background clutter, camera motion, fast motion, full occlusion, illumination variation, low resolution, out-of-view, partial occlusion, scale variation, similar objects, and viewpoint change. <ref type="figure">Figure S6</ref> shows the success plots for all the attributes. Our approach obtains the best results on all 12 attributes. Thanks to our target estimation module, our approach excels in case of aspect ratio change, scale variation, and viewpoint change. Furthermore, due to our robust online-learned classifier, our tracker also outperforms previous methods in case of similar objects, illumination variation, partial occlusion, and low resolution.   <ref type="figure">Figure S6</ref>. Attribute analysis on the UAV123 dataset. Our approach ATOM obtains the best performance on all 12 attributes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Full architecture of our target estimation network. ResNet-18 Block3 and Block4 features extracted from the test image are first passed through two Conv layers. Regions defined by the input bounding boxes are then pooled to a fixed size using PrPool layers. The pooled features are modulated by channel-wise multiplication with the coefficient vector returned by the reference branch. The features are then passed through fully-connected layers to predict the IoU. All Conv and FC layers are followed by BatchNorm and ReLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Classification component optimization. Input: Net weights w, residual function r(w), N GN , N CG 1: for i = 1, . . . , N GN do 2: r ← r(w) , u ← r 3: h ← BackProp(r T u, w) # Treat u as constant 4:g ← −h , p ← 0 , ρ 1 ← 1 , ∆w ← 0 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>q 1 ←q 2 ←</head><label>12</label><figDesc>BackProp(h T p, u) # Treat p as constant 9: BackProp(r T q 1 , w) # Treat q 1 as constant 10:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>table 1 .Table 1 .Table 2 .</head><label>112</label><figDesc>Naively concatenating the features from the reference image and the test image achieves an AUC of 56.3%. Our Siamese-based architecture obtains better results, with an AUC of 61.7% and OP 0.50 of 75.1%. Our modulationbased method further improves the results, giving an abso-Analysis of different architectures for IoU prediction on the combined NFS and UAV123 datasets. For each method, we Impact of each component in the proposed approach on the combined NFS and UAV123 datasets. We compare the target estimation component with the brute-force multi-scale approach and analyze the impact of our classification module, online optimization strategy, and hard-negative mining scheme.lute gain of 1.2% in OP 0.50 and achieves an AUC of 62.3%. Backbone feature layers: We evaluate the impact of using different feature blocks from the backbone ResNet-18 (table 1). Using features from only Block3 leads to an AUC of 60.3%, while only Block4 gives an AUC of 58.5%. Fusing features from both the blocks leads to a significant improvement, giving an AUC score of 62.3%. This indicates that Block3 and Block4 features have complementary information useful for predicting the IoU.</figDesc><table><row><cell></cell><cell>Baseline</cell><cell cols="5">Modulation Concatenation Siamese Modulation Modulation</cell></row><row><cell></cell><cell cols="6">(Block 3&amp;4) (Block 3&amp;4) (Block 3&amp;4) (Block 3&amp;4) (Block 3) (Block 4)</cell></row><row><cell>OP0.50(%)</cell><cell>68.3</cell><cell>76.3</cell><cell>67.5</cell><cell>75.1</cell><cell>73.4</cell><cell>73.6</cell></row><row><cell>OP0.75(%)</cell><cell>38.6</cell><cell>48.4</cell><cell>37.9</cell><cell>47.6</cell><cell>44.5</cell><cell>38.9</cell></row><row><cell>AUC (%)</cell><cell>56.7</cell><cell>62.3</cell><cell>56.3</cell><cell>61.7</cell><cell>60.3</cell><cell>58.5</cell></row></table><note>indicate in parenthesis the backbone feature layers that are used as input. The baseline approach, which does not employ a refer- ence branch to integrate target specific information, provides poor results. Among the different architectures, the modulation based approach, using both block 3 and 4, achieves the best results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>State-of-the-art comparison on the TrackingNet test set in terms of precision, normalized precision, and success. Our approach significantly outperforms UPDT, achieving a relative gain of 15% in terms of success.</figDesc><table /><note>more than 1.2% AUC score (Table 2). Note that even a 5- fold increase of iterations does not provide any significant improvement (only 0.2%), indicating slow convergence. Hard Negative Mining: We evaluate our method without the Hard Negative mining component (section 3.3), result- ing in an AUC of 61.9%. This suggests the hard negative mining adds some robustness (0.4% AUC) to our tracker.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>STRCF SINT ECO DSiam StructSiam SiamFC VITAL MDNet DaSiam-ATOM</figDesc><table><row><cell></cell><cell>[19] [34] [4] [10]</cell><cell>[41]</cell><cell>[2]</cell><cell>[32]</cell><cell cols="2">[26] RPN[42]</cell><cell></cell></row><row><cell cols="2">Norm. Prec. (%) 34.0 35.4 33.8 40.5</cell><cell>41.8</cell><cell>42.0</cell><cell>45.3</cell><cell>46.0</cell><cell>49.6</cell><cell>57.6</cell></row><row><cell>Success (%)</cell><cell>30.8 31.4 32.4 33.3</cell><cell>33.5</cell><cell>33.6</cell><cell>39.0</cell><cell>39.7</cell><cell>41.5</cell><cell>51.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>State-of-the-art comparison on the LaSOT dataset in terms of normalized precision and success.</figDesc><table><row><cell></cell><cell cols="7">DLSTpp SASiamR CPT DeepSTRCF DRT RCO UPDT DaSiam-MFT LADCF ATOM</cell></row><row><cell></cell><cell>[17]</cell><cell>[11]</cell><cell>[17]</cell><cell>[19]</cell><cell>[33] [17]</cell><cell>[3] RPN [42] [17]</cell><cell>[38]</cell></row><row><cell>EAO</cell><cell>0.325</cell><cell cols="2">0.337 0.339</cell><cell>0.345</cell><cell cols="3">0.356 0.376 0.378 0.383 0.385 0.389 0.401</cell></row><row><cell cols="2">Robustness 0.224</cell><cell cols="2">0.258 0.239</cell><cell>0.215</cell><cell cols="3">0.201 0.155 0.184 0.276 0.140 0.159 0.204</cell></row><row><cell>Accuracy</cell><cell>0.543</cell><cell cols="2">0.566 0.506</cell><cell>0.523</cell><cell cols="3">0.519 0.507 0.536 0.586 0.505 0.503 0.590</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S1 .</head><label>S1</label><figDesc>Comparision of our approach trained using only ImageNet-VID (denoted ATOM-VID) on the LaSOT dataset. Our approach, trained using considerably less data as compared to the previous best approach DaSiamRPN, significantly outperforms it with an absolute gain of 8.0% in AUC score.</figDesc><table><row><cell>Precision (%)</cell><cell>47.0 47.7</cell><cell>48.0</cell><cell>49.2 53.3</cell><cell>53.3</cell><cell>56.5</cell><cell>55.7</cell><cell>59.1</cell><cell>61.8</cell><cell>64.8</cell></row><row><cell cols="2">Norm. Prec. (%) 60.3 59.8</cell><cell>62.2</cell><cell>61.8 66.6</cell><cell>65.4</cell><cell>70.5</cell><cell>70.2</cell><cell>73.3</cell><cell>74.6</cell><cell>77.1</cell></row><row><cell>Success (%)</cell><cell>52.8 50.4</cell><cell>53.4</cell><cell>55.4 57.1</cell><cell>57.8</cell><cell>60.6</cell><cell>61.1</cell><cell>63.8</cell><cell>69.8</cell><cell>70.3</cell></row></table><note>Staple SAMF CSRDCF ECO SiamFC CFNet MDNet UPDT DaSiamRPN ATOM-VID ATOM</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unveiling the power of deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ECO: efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative scale space tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1561" to="1575" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards a better match in siamese network based visual object tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The distribution of the flora in the alpine zone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Phytologist</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking vot2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pfugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning spatial-temporal regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A scale adaptive kernel correlation filter tracker with feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter tracker with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojír</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="671" to="688" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Trackingnet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Subaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Youtube-boundingboxes: A large high-precision humanannotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An introduction to the conjugate gradient method without the agonizing pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Shewchuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">VITAL: Visual tracking via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Correlation tracking via joint discrimination and reliability learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parametric exponential linear unit for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Trottier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giguère</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chaib-Draa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMLA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning adaptive discriminative correlation filters via temporal consistency preserving spatial feature selection for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<idno>abs/1807.11348</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint representation and truncated inference learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MEEM: robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structured siamese network for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
