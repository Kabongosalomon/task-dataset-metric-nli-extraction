<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepGG: a Deep Graph Generator</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Stier</surname></persName>
							<email>julian.stier@uni-passau.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Passau</orgName>
								<address>
									<addrLine>Innstraße 41</addrLine>
									<postCode>94032</postCode>
									<settlement>Passau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Granitzer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Passau</orgName>
								<address>
									<addrLine>Innstraße 41</addrLine>
									<postCode>94032</postCode>
									<settlement>Passau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepGG: a Deep Graph Generator</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>2020/11/25 ∼ bb6d439</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning distributions of graphs can be used for automatic drug discovery, molecular design, complex network analysis, and much more. We present an improved framework for learning generative models of graphs based on the idea of deep state machines. To learn state transition decisions we use a set of graph and node embedding techniques as memory of the state machine.</p><p>Our analysis is based on learning the distribution of random graph generators for which we provide statistical tests to determine which properties can be learned and how well the original distribution of graphs is represented. We show that the design of the state machine favors specific distributions. Models of graphs of size up to 150 vertices are learned. Code and parameters are publicly available to reproduce our results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We aim to learn generative models of graphs from an empirical set for which we assume a characteristic underlying distribution. These desirable models find applications in the discovery of chemical compounds such as drugs and various other application areas for which the discovery is based on profound expert knowledge, empirical testing and complex underlying rules of how the graphs emerge.</p><p>We present a generalized generative model for graphs based on <ref type="bibr">DGMG Li et al. (2018)</ref> trained on sets of representative graphs and evaluate it on existing probabilistic random graph generators with various graph sizes of up to several hundred vertices. For this, we represent graphs in construction sequences and discuss three variants to obtain such sequences. Construction sequences are possible representations of graphs like the commonly used adjacency matrix of a graph and explained in section 3. Equivalent or similar representations are used by various auto-regressive models as mentioned in section 2. Our contributions comprise 2 Related Work For understanding our work and its embedding in its research field, we differentiate between two families of random models of graphs: imprecisely, we call them probabilistic models of graphs and generative models of graphs although usually the latter are subsets of the former and both terms refer to statistical and thus mathematical models. Here, we consider models as probabilistic if they consist of an algorithmic description and if their draws of random samples are based on simple distributions. Examples for probabilistic models of graphs are the Erdős-Rényi model <ref type="bibr" target="#b5">Erdos (1959)</ref> for random graphs, a special case of exponential random graph models <ref type="bibr" target="#b21">Wasserman and Pattison (1996)</ref>, the Watts-Strogatz model for small-world networks <ref type="bibr" target="#b22">Watts and Strogatz (1998)</ref>, the Kleinberg model <ref type="bibr" target="#b13">Kleinberg (2000)</ref>, the Barabasi-Albert model for scale-free networks Albert and <ref type="bibr">Barabasi (2002)</ref> Probabilistic models of graphs provide a distribution of graphs with a usually small set of parameters controlling a core principle of the model. A major intend to study those models are statistical analyses in network science.</p><p>We consider models as generative models of graphs if they are intended to be able to learn characteristic distributions of graphs based on an observed set of graphs from an underlying unknown distribution of graphs. These generative models are usually of a magnitude higher in terms of their number of parameters. While <ref type="bibr" target="#b18">Liao et al. Liao et al. (2019)</ref> differentiate between auto-regressive and non-autoregressive models, we currently prefer to categorize generative models of graphs further into popular types of deep learning models which we identified as Recurrent Neural Networks (RNN), Variational Auto-Encoder (VAE), Generative Adversarial Networks (GAN), Policy Networks (PI) or a mixture of them. Furthermore, it is important to know of techniques such as Graph Neural Networks <ref type="bibr" target="#b8">Gori et al. (2005)</ref>, Graph Convolution Kipf and Welling (2016) and the Message Passing Framework <ref type="bibr" target="#b7">Gilmer et al. (2017)</ref>, in general, as they provide embedding techniques which are now widely used among above generative models.</p><p>An early work which provides a model that "can generate graphs that obey many of the patterns found in real graphs" is based on kronecker multiplication <ref type="bibr" target="#b15">Leskovec and Faloutsos (2007)</ref> and called KronFit. However, the model is fitted towards a single graph with maximum likelihood. To our understanding the original intend of it was not to learn the hidden distribution of graphs but the exemplary graph itself and despite it can "sample of the real graph" by "using a smaller exponent in the Kronecker exponentiation" it can be assumed that the distribution is close to this particular single objective graph and not an underlying distribution of graphs from which the graph might have been sampled <ref type="bibr" target="#b14">Leskovec et al. (2010)</ref>.</p><p>The most important work related to our model and analysis are Learning Deep Generative Models of Graphs (DGMG) <ref type="bibr" target="#b17">Li et al. (2018)</ref>, Graph Recurrent Neural Networks (GraphRNN) <ref type="bibr" target="#b24">You et al. (2018b)</ref> and Efficient Graph Generation with Graph Recurrent Attention Networks (GRANs) <ref type="bibr" target="#b18">Liao et al. (2019)</ref>. DGMG is the model framework we extended, generalized and provided new experiments for and which computational issues Li et al. have again tackled with the successful GRAN in <ref type="bibr" target="#b18">Liao et al. (2019)</ref>. <ref type="bibr">GraphRNN You et al. (2018b)</ref> is a highly successful auto-regressive model and was experimentally compared on three types of datasets called "grid dataset", "community dataset" and "ego dataset". The model captures a graph distribution in "an autoregressive (recurrent) manner as a sequence of additions of new nodes and edges". Liao et al. identify some major issues of GraphRNN and explain that "handling permutation invariance is vital for generative models on graphs" <ref type="bibr" target="#b18">Liao et al. (2019)</ref>. GRAN is a recent efficient auto-regressive model "generating large graphs of up to 5k vertices" by generating a sequence of vectors representing "one row or one block of rows" of the lower triangular matrix at a time <ref type="bibr" target="#b18">Liao et al. (2019)</ref>.</p><p>Besides those three works, there is also structure2vec <ref type="bibr" target="#b3">Dai et al. (2016)</ref>, <ref type="bibr">Graphite Grover et al. (2018)</ref> and <ref type="bibr">GraphGAN Wang et al. (2018)</ref> for generically learning models of graphs. There exist various generative models which are mostly tailored specifically to applications such as molecular designs. We found Bjerrum and Threlfall (2017) (RNN), <ref type="bibr" target="#b10">Gupta et al. (2018)</ref>  We also want to refer to two frameworks which provide a lot of tools for differentiable computation on graph structures such as the Deep Graph Library , Euler <ref type="bibr" target="#b0">Alibaba (2020)</ref> and StellarGraph Data61 (2018).  <ref type="bibr">(under ismorphism)</ref>. The first graph evolution shows the steps of an Erdős-Rényi model which at first generates five vertices and then adds edges in order of the vertex numbers with an unknown probability p. The second graph evolution shows a traversal with breadth-firstsearch (bfs) through the same graph and the third graph evolution shows a traversal with depth-firstsearch (dfs). Notice, that dfs comes up with a different vertex ordering than bfs at the end (vertices four and five are flipped).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Construction Sequences</head><p>To learn the hidden distribution of graphs given a set of exemplary graphs we need a suitable representation from which we can learn from. Using the flattened adjacency matrix of the graph would be one approach but "suffers from serious drawbacks: it cannot naturally generalize to graphs of varying size, and requires training on all possible vertex permutations or specifying a canonical permutation" <ref type="bibr" target="#b24">You et al. (2018b)</ref>.</p><p>We follow a similar approach as in <ref type="bibr" target="#b24">You et al. (2018b)</ref> and <ref type="bibr" target="#b17">Li et al. (2018)</ref> and learn a distribution of graphs from a sequence we call actionable construction sequence 2 . In other contexts of network science this representation is also viewed under the perspective of graph evolution.</p><p>An action denotes a graph operation on an initially empty graph. Each operation can be parameterized by a fixed number of values in the sequence and thus has at least length one but at most a finite number of values. Each action modifies the current graph by applying the graph operation on it. Many probabilistic graph models can be represented with two graph operations: adding a node and adding an edge. Models such as the Watts-Strogatz model additionally require the operation to remove nodes. All probabilistic graph models examined in our work can be learned by those three operations of adding a node, adding an edge or removing a node. The construction sequence of a graph can be seen as the transition sequence of a (finite) state machine, but with parameterized decisions in each statemaking it a state machine with memory.</p><p>To describe a graph construction sequence we label those operations N (add node), E (add edge) and R (remove node). Technically the labels are encoded as natural numbers in the construction sequence along with all other information which are also natural numbers. One can easily extend the model to also learn to remove edges or perform other graph operations but it has to be considered that this adds more state decisions to the model and thus more learnable parameters. While N needs no further parameters, E is followed by two parameters indicating the source and target vertex and R is followed by one parameter denoting the affected vertex which will be removed from the graph. Those three operations are sufficient for the most well-known probabilistic models of graphs such as the ones of Erdős-Rényi, Barabasi-Albert or Watts-Strogatz.</p><p>The null or empty graph is represented by the empty sequence [ ]. With the introduced labels, a graph with a single node is represented by [N ] and a graph with two connected nodes with [N N E 0 1]. The representation encodes undirected graphs if not otherwise stated. For a directed graph with two edges between two nodes the sequence extends to [N N E 0 1 E 1 0].</p><p>We consider three variants through which we obtain construction sequences of graphs: firstly, through the construction process of the particular probabilistic model, secondly through a breadth-firstsearch traversal of any given graph, and thirdly through a depth-first-search traversal of any given graph.</p><p>Construction Process Given a probabilistic model for graphs -such as the Erdős-Rényi model with a finite number of vertices -one can follow the algorithmic rules to build a construction Figure 2: DeepGG is a sequential generative model based on the notion of a state machine with memory. As we are generating a graph step by step, the memory in our case consists of a graph structure with node and graph embeddings on which message passing updates are performed. The figure shows two states (dark circles) add_node and add_edge with possible state transitions prefixed with "a" between those states. The lighter circles depict sub-states which can read from memory to derive a categorical decision and contribute loss to the overall end-to-end model. Operations (or actions) from the construction sequence control which decision a state has to take given the current memory.</p><p>sequence for each draw from the model. If the model starts with an initial number of n v0 vertices the construction sequence starts with [N ] * n v0 -which we understand as repeating the sequence of contained operations repeated n v0 times. In the appendix we provide peusodcode for generating construction sequences following the rules of the probabilistic model of Barabasi-Albert.</p><p>Graph Traversal In application cases we usually do not know the underlying process used to come up with the observed graphs. Thus, we need to transform graphs into construction sequences and we can do this by using common graph traversal algorithms. The nature of the chosen traversal algorithm has most likely a major impact on our subsequent deep graph generator model (DeepGG) as the decision functions for state transitions in DeepGG are learned based on exactly those exemplary steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deep Graph Generator</head><p>We follow the sequential and auto-regressive generation model "Deep Generative Models of Graphs" (DGMG) of Li et al. <ref type="bibr" target="#b17">Li et al. (2018)</ref> to learn decisions from construction sequences based on a representation of the current graph state. Our generative model -which we will refer to as DeepGGhas three major differences to DGMG: (1) we use a more relaxed notion of a finite state machine in which the model can learn to add edges from a source vertex at any step and not just at the point in which the source vertex was added to the graph, and (2) to avoid exploding losses we reduce the negative log-likelihood loss from choosing a source or target vertex in a growing graph by dividing it with the logarithmic number of vertices at that moment, and (3) we describe the possibility to add another state to also learn to remove vertices from the graph, which is a necessary step to learn probabilistic graph models such as the Watts-Strogatz model.</p><p>The essence of learning categorical decisions for the state transitions lies in the representational power of its memory. Memory refers to a set of real-valued vectors (embeddings) associated with a graph. During inference steps, a graph and associated embeddings are initialized and updated. We use the message passing framework to learn embeddings of vertices and edges to combine them to an graph embedding. This graph embedding and possibly a context embedding of a vertex are used to learn a transition function for states.</p><p>For a graph G = (V, E) with vertices v ∈ V and edges e ∈ E = {(s, t) ∈ V × V | s has edge to t} we have embeddings h v ∈ R Hv and h e ∈ R He for hyperparameters H v , H e ∈ N + for which we chose H v = H e = 16 in accordance with <ref type="bibr" target="#b17">Li et al. (2018)</ref>. The number of vertices is given as n v = |V |. The graph embedding is denoted as h g ∈ R Hg with H g ∈ N + for which we chose H g = 2 · H v . For an empty graph we set h g = (0, . . . , 0) (a zero vector). Like every construction sequence as depicted in <ref type="figure" target="#fig_1">Figure 1</ref> DeepGG starts with an empty graph and iteratively adds edges or vertices.</p><p>Initializing Vertices and Edges Like in DGMG <ref type="bibr">(Li et al., 2018, 4</ref>.1) we use a simple differentiable model such as a Multi-Layer Perceptron (MLP) to learn one initialization function for vertices and one for edges. Each time DeepGG adds a vertex v new or an edge e new an embedding for it is initialized. While the empty graph is initialized with a zero vector, embeddings for v new or e new can be initialized from a learnable function based on the current memory state -the graph embedding h g . We call this initialization functions f init,v and f init,e , respectively:</p><formula xml:id="formula_0">f init,v : R Hg → R Hv</formula><p>In case of a MLP with non-linear activation function σ this could be a function mapping a graph rep-</p><formula xml:id="formula_1">resentation h g ∈ R Hg to σ (W init,v · h g + b init,v ). Equivalently we define f init,e : (R Hv , R Hv ) → R He with (h s , h t ) ∈ R Hv × R Hv → σ (W init,e · (h s ⊕ h t ) + b init,v ).</formula><p>The learnable models f init,v and f init,e are often called sub-modules. Parameters of f init,v and f init,e such as W init,v , b init,v , W init,e and b init,e are then learnable parameters of DeepGG and their dimensions can be easily inferred by definition of our parameter choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep State Decisions</head><p>We identify a state with index κ ∈ N as s κ . Its transition decision function draws from a categorical distribution based on an embedding vector which is either a graph embedding h g , a composition of vertex embeddings h v or a mixture of both. Therefore, its input dimension is denoted with H sκ and for each state it has to be described how to aggregate information from the memory to the particular input embedding from which the decision is derived. The decision function f sκ is given as f sκ : R Hs κ → {1, . . . , n sκ } in which n sκ is the number of possible categorical actions for that state. During learning, the crossentropy loss of this state decision and the observed action from the construction sequence contributes to the overall optimization objective.</p><p>A critical part of sequential state models such as DGMG and DeepGG besides their state transition decisions are additional objectives to learn making a choice from a growing space -such as choosing a target vertex from a growing graph. While DGMG makes only target vertex decisions from the recently added vertex, DeepGG makes two decisions when adding an edge to the graph: one for a source and one for a target vertex. This makes the model more generic and facilitates to more precisely follow a possibly underlying construction rule. For example, in the Erdős-Rényi model first n v vertices are created and then edges are "activated" based on a probability and in order of their enumeration. DGMG would have arguably struggles learning from construction sequences following this process but most likely could learn from this distribution of graphs when traversing them with bfs or dfs. On the other hand, more decisions with growing choices also increases the number of learnable parameters and adds an additional objective. We compensate for exploding losses which we observed in a first version of DeepGG by dividing the cross-entropy loss with the logarithm of possible vertex choices -which will arguably also penalize early wrong choices over wrong choices made later on a large graph. For one vertex choice δ ∈ N as c δ we have a categorical choice based on the current graph state given as:</p><formula xml:id="formula_2">f c δ : R Hc δ → {1, . . . , n v }</formula><p>Note, that with a growing graph as memory during the generation process n v grows linearly. When adding edges, we used two vertex choice functions f c1 and f c2 which can be seen in <ref type="figure">Figure 2</ref> as select source and select target. For the memory readout we chose H c δ = H g + H v + H v for each vertex v under consideration and used the current graph embedding h g , the vertex embedding h v and an additional contextual vertex v c embedding h vc .</p><p>Equivalently to the initialisation functions for vertices and edges above, we chose single-layered MLPs as differentiable and learnable functions for all f sκ and f c δ .</p><p>Memory Update In each step of a construction sequence DeepGG and DGMG compute a graph embedding based on the current vertex embeddings in memory. While DGMG aggregate a graph embedding with a "gated sum R(h v , G)" DeepGG uses a gated vertex embedding in conjunction with a graph convolution <ref type="bibr" target="#b12">Kipf and Welling (2016)</ref> which is then reduced to its mean. For the gated vertex embedding we use a sub-module f reduc :</p><formula xml:id="formula_3">f reduc : R Hv → R Hr , h v → σ (W reduc · h v + b reduc )</formula><p>for which we chose H r = 7 which then specifies the feature dimensions of the subsequent graph convolution. We chose the mean instead of the sum as in DGMG because we observed exploding sums for resulting graph embeddings. Given the adjacency matrix A ∈ R nv×nv of a graph and input features of dimension R din a graph convolution computes f conv : R nv×din → R nv×dout . <ref type="bibr" target="#b12">Kipf and Welling (2016)</ref> for details). We used one single graph convolution f conv with reduced vertex representations for each vertex in graph G, making up H <ref type="bibr">(l)</ref> . A is the graphs adjacency matrix.</p><formula xml:id="formula_4">f conv (H (l) , A) = σ D − 1 2Â D − 1 2 H (l) W (l) withÂ = A + I (see</formula><p>When the current graph G (the memory) is modified, we perform an update step based on the message passing framework <ref type="bibr" target="#b7">Gilmer et al. (2017)</ref> to update vertex representations. The graph propagation (update) step in DeepGG is kept equivalent to DGMG. Two propagation rounds ν rounds of gated recurrent units are used and vertex embeddings are updated as illustrated in <ref type="bibr" target="#b17">(Li et al., 2018,</ref>  <ref type="figure">figure 2)</ref>.</p><p>Training Objective The model draws from categorical distributions to decide on state transitions or choosing vertices as source or target for an edge. We use a cross-entropy loss on f sκ and f c δ and minimize the joint negative log-likelihood with stochastic gradient descent. The learning rate is η = 0.0001, we use ν epochs = 8 number of epochs, ν rounds = 2 rounds of propagations and ν nv_max = 150 as a hard threshold for generating a maximum number of vertices and stopping the generative process. During generative inference a minimum value ν nv_min can be specified to restrain the state machine from transitioning to a halting state before reaching a lower bound in the number of vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Learning distributions of graphs is not only difficult because of the complexity of the combinatorial nature of graphs but also difficult to assess properly. Notably, due to the graph isomorphism problem we possibly require a computational expensive effort to compare two sets of graphs for overlapping isomorphic graphs. Although our interest is not to obtain isomorphic graphs, we are faced with this or related complex problems when formulating a notion of equivalence classes of graphs. Because we are interested in reproducing certain properties of a hidden distribution of graphs we need to investigate on if and which of those properties can statistically significantly be reproduced.</p><p>We computed 69 total DeepGG instances of which 24 instances have been trained on datasets with bfs-traversal, 39 with dfs-traversal and six based on the construction process of the underlying probabilistic model. 24 instances have been computed with datasets based on the Erdős-Rényi model, 26 instances on the Barabasi-Albert model and 19 on the Watts-Strogatz model. Across all computations we used the same objective, the same learning rate, the same number of epochs and other hyperparameters as stated in section 4. For each computation instance we sampled new sets of graphs of size n v = 50 from the ER-model with p = 0.2, from the WS-model with k = 10, p = 0.2 and from the BA-model with m = 3.</p><p>We argue, that it is important to investigate on multiple levels of distributions of graph properties to constitue that a certain hidden process could be reflected. In network science, complex networks are compared based on the degree distribution, clustering coefficient Watts and Strogatz (1998) or on the average path length distribution (or similar aggregated statistics of the underlying properties).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Degree Distribution</head><p>The three considered probabilistic graph models have distinct degree distributions. Barabasi-Albert is known to have a scale-free degree distribution. Erdős-Rényi has a binomial distribution p k ∼ B(n v , ER p ) which in our case gives E(p k ) = 10. The degree distribution of Watts-Strogatz graphs are similar to random graphs in shape. For selected DeepGG instances we find that they are able to reproduce the degree distribution very closely. However, we also encounter instances which follow a scale-free distribution when trained on ER-models. Similar to DGMG we find a close relationship to the degree distribution of scale-free networks but do not observe the same behaviour for WS-graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Path Length Distribution</head><p>We observe a mean of 1.75 for the distribution of average path lengths of DeepGG learned on ER-graphs. This observed mean of generated graphs indeed gets close to the mean in the dataset of 1.91. According to the analytic form given by <ref type="bibr" target="#b6">Fronczak et al. (2004)</ref>    Equivalently, we computed the average path length of each generated graph from the model (on average 171 for the BFS-instances and 170 for the DFS-instances). Seven DeepGG instances have been trained on bfs-traversals, twelve on dfs-traversals. Watts-Strogatz+BFS constitute a superset in which we generated 1,199 graphs, Watts-Strogatz+DFS a superset in which we computed 2,034 graphs. As to no surprise the average path length distributions for both supersets over the datasets are visually equivalent. However, we have to ascertain that the average path length distributions for generated graphs can not only be matched its dataset but also have to point out that the resulting superset distribution for different traversal choices for construction sequences differs significantly.</p><p>l ER = ln(50)−γ ln(0.2·50) + 1 2 ≈ 1.95. For Barabasi-Albert graphs the analytical average path length is l BA = ln(nv)−ln(m/2)−1−γ ln(ln(N ))+ln(m/2) + 3 2 for which we get l BA ≈ 2.59. In our dataset of BA-graphs we observe an averaged average shortest path length of 2.29 while DeepGG resembles 2.61, again with a high standard deviation of 1.50.</p><p>Computation Times Overall the computation time over training datasets of 1,000 graph samples with each large construction sequences has been very high. We used GPU acceleration but did not perform batching. The average computation time was roughly 60 hours with a minimum of 20 hours and a maximum of 120 hours. Instances trained on BA-graphs took only 40 hours on average with a low standard deviation while WS-graphs took 65 hours and ER-graphs over 70 hours on average. We also observed significantly different distribution between computation times when using dfs-or bfs-traversal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Findings in our Experiments</head><p>First of all, we do find DeepGG instances which generate new graphs with property distributions close to the underlying probabilistic models. We even manage to generate larger graphs than reported by Li et al. of up to a few hundred vertices, although the recently published GRAN reportedly achieves to generate several thousand of vertices. Looking at single instances visually one could easily think that DeepGG (or DGMG) instances learn meaningful distributions of graphs. Repeated experiments and reasoning from the nature of both DeepGG and DGMG, however, leads us to the conjecture that the models are biased towards scale-free random graphs and can not repeatedly resemble the behaviour of the targeted probabilistic model -if not supervised carefully.</p><p>The choice of representation of graphs has a significant impact on learning distributions over graphs as already stated in <ref type="bibr" target="#b24">You et al. (2018b)</ref> or <ref type="bibr" target="#b18">Liao et al. (2019)</ref>. In case of our chosen representation with construction sequences we have a huge number of possible permutations over a random ordering of vertices for a single graph. We investigated on three variants of canonical traversal methods over random orderings of vertices and observed different resulting distributions for average shortest path lengths and vertex degrees. For depth-first-search we observe a higher variance in computation times than for breadth-first-search. The difference in the mean of their computation times could be a result to the low number of overall computed instances over a rather high number of hyperparameter choices. We further observe a high computation time for ER-models over WS-models and BA-models.</p><p>Especially the message passing component to obtain vertex and graph embeddings is a computational expensive part of the model and we see possibilities for improvement there. The growing number of vertices further leads to a growing categorical choice space for f c δ which we identify as an essential step towards learning other degree distributions than the distribution of a BA-model. We see a possibility of "almost random" transitions through the state machine which might result in the generation of scale-free random networks and we will investigate further in this decision process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We described a generative model for learning distributions of graph as a generalization of the model given in <ref type="bibr" target="#b17">Li et al. (2018)</ref>. The generalization made it possible to learn distributions of graphs based on the graph construction process of the Watts-Strogatz model and we are confident that DeepGG reduced bias towards scale-free graphs. Our experiments showed, that often an underlying construction process could not be learned although single instances showed promising resuls -visually and based on graph property distributions. We emphasize that generative models of graphs need to be evaluated on multiple levels to reach statistical significant evidence and focussed on providing such experimental setups.</p><p>DeepGG was rendered under the perspective of a deep state machine, which can be seen as a state machine with learnable transitions and a memory. The decision process of such a state machine can be analysed more transparently as compared to other common end-to-end models.</p><p>We hope to contribute valuable insights for learning distributions of graphs and expect to (1) further investigate generative models of graphs and (2) apply the recently developed models in promising domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>This work investigates on learning distributions of graphs from a set of exemplary graphs with the aim to generate new graphs from the same or closely similar hidden distribution. Such graph generators have use cases in molecular design, network analysis, program synthesis and probably many more fields which is why we move in the direction of focussing on evaluating generic methods for learning distributions of graphs. Beneficiaries can be considered everyone who is interested in automating design of complex underlying graph structures. Of course, such methods could then also be used for drug discovery or other designs which at the wrong end can have negative ethical aspects. We argue, however, that this is a conflict which has to be solved on another societal level and is a result of a more general drive towards automation. As the described problem on graph distributions is mostly of theoretical nature, we do not see that the method leverages bias of ethical concern. In general, research on learning distributions of graphs will improve automation and design in various domains. We see mostly societal benefits from this automation process. <ref type="figure">Figure 5</ref>: Distributions of construction sequence lengths for two probabilistic models Erdős-Rényi (left side) and Watts-Strogatz (right side) for the three mentioned variants "construction process", "depth-first search" and "breadth-first search".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(RNN), Jin et al. (2018) (VAE), Bjerrum and Sattarov (2018) (VAE), You et al. (2018a) (PI), Li et al. (2019) worthy to mention here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Variants for obtaining construction sequences representing the same graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Exemplary samples from computed DeepGG instances. The id is a shortened computation timestamp. Layouts are circular to provide a visual impression of the density. The original distribution is based on samples from either an Erdős-Rényi, a Watts-Strogatz or a Barabasi-Albert probabilistic model. Used traversal method are breadth-first-search (bfs), depth-first-search (dfs) or based on the process of the model as described in section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>For multiple instances of DeepGG trained on Watts-Strogatz graphs we computed the average path length of each of the 1,000 graphs used for training for both bfs-and dfs-traversal.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Like with all representations of graphs there is no easy canonical representation of a graph, otherwise the graph isomorphism problem could be solved in polynomial time.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank Jörg Schlötterer for valuable discussions on our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Reka Albert and Albert-Laszlo Barabasi. 2002. Statistical mechanics of complex networks. Reviews of modern physics 74, 1 (2002), 47.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Algorithm 1: Example for generating a construction sequence based on the Barabasi-Albert probabilistic graph generator. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Euler: A distributed graph deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alibaba</forename></persName>
		</author>
		<ptr target="https://github.com/alibaba/euler" />
		<imprint>
			<date type="published" when="2019-05-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving chemical autoencoder latent space and molecular de novo generation diversity with heteroencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannik</forename><surname>Esben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Bjerrum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sattarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomolecules</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">131</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannik</forename><surname>Esben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bjerrum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Threlfall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04612</idno>
		<title level="m">Molecular generation with recurrent neural networks (RNNs)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Csiro&amp;apos;s Data61</surname></persName>
		</author>
		<ptr target="https://github.com/stellargraph/stellargraph." />
		<title level="m">StellarGraph Machine Learning Library</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Erdos</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publicationes mathematicae</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Average path length in random networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Fronczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Fronczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janusz A</forename><surname>Hołyst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">56110</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10459</idno>
		<title level="m">Graphite: Iterative generative modeling of graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative recurrent networks for de novo drug design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anvita</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Berend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><forename type="middle">A</forename><surname>Huisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gisbert</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular informatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">1700111</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Navigation in a small world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">406</biblScope>
			<biblScope unit="page" from="845" to="845" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kronecker graphs: An approach to modeling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="985" to="1042" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable modeling of real graphs using kronecker multiplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DeepChemStable: Chemical Stability Prediction with an Attention-Based Graph Convolution Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1044" to="1049" />
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient graph generation with graph recurrent attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4257" to="4267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graphgan: Graph representation learning with generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.01315" />
		<title level="m">Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs. ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Logit models and logistic regressions for social networks: I. An introduction to Markov graphs andp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippa</forename><surname>Pattison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="401" to="425" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Collective dynamics of &apos;small-world&apos;networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page">440</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6410" to="6421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08773</idno>
		<title level="m">Graphrnn: Generating realistic graphs with deep auto-regressive models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
