<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Make Skeleton-based Action Recognition Model Smaller, Faster and Better</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN</orgName>
								<orgName type="institution">AIP</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN</orgName>
								<orgName type="institution">AIP</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN</orgName>
								<orgName type="institution">AIP</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Make Skeleton-based Action Recognition Model Smaller, Faster and Better</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Skeleton-based Action Recognition</term>
					<term>Body Ac- tions</term>
					<term>Hand Gestures</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although skeleton-based action recognition has achieved great success in recent years, most of the existing methods may suffer from a large model size and slow execution speed. To alleviate this issue, we analyze skeleton sequence properties to propose a Double-feature Double-motion Network (DD-Net) for skeleton-based action recognition. By using a lightweight network structure (i.e., 0.15 million parameters), DD-Net can reach a super fast speed, as 3,500 FPS on one GPU, or, 2,000 FPS on one CPU. By employing robust features, DD-Net achieves the state-of-the-art performance on our experiment datasets: SHREC (i.e., hand actions) and JHMDB (i.e., body actions). Our code is on https://github.com/fandulu/DD-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Skeleton-based action recognition has been widely used in multimedia applications, such as human-computer interaction <ref type="bibr" target="#b0">[1]</ref>, human behavior understanding <ref type="bibr" target="#b1">[2]</ref> and medical assistive applications <ref type="bibr" target="#b2">[3]</ref>. However, most of the existing methods may suffer from a large model size and slow execution speed <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>In real applications, a desirable skeleton-based action recognition model should run efficiently by using a few parameters, and, also be adaptable to various application scenarios (e.g., hand/body, 2D/3D skeleton, and actions related/unrelated to global trajectories). To achieve this goal, we investigate skeleton sequence properties to propose a lightweight Double-feature Double-motion Network (DD-Net), which is equipped with a Joint Collection Distances (JCD) feature and a two-scale global motion feature.</p><p>More specifically, we conduct research on four types of skeleton sequence properties (see <ref type="figure" target="#fig_0">Fig. 1</ref>): (a) locationviewpoint variation, (b) motion scale variation, (c) related/unrelated to global trajectories, (4) uncorrelated joint indices. To address challenges caused by these properties, previous works may prone to propose complicated neural network models, which end up with large model size.</p><p>In contrast, we address these challenges by simplifying both the input feature and the network structure. Our JCD feature contains the location-viewpoint invariant information of skeleton sequences. Compared with other similar features, it can be easily computed and includes fewer elements. Since global motions cannot be incorporated into a locationviewpoint invariant feature, we introduce a two-scale global motion feature to improve the generalization of DD-Net. Besides, its two-scale structure makes it robust to the motion scale variance. Through an embedding process, DD-Net can (a) Location-viewpoint variation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slow Motion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fast Motion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Motion scale variation</head><p>Actions related to global trajectories, e.g., swipe V.</p><p>Actions unrelated to global trajectories, e.g., pinch.  automatically learn the proper correlation of joints, which is hard to be predefined by joint indices. Compared to methods relying on complicated model structures, DD-Net provides higher action recognition accuracy and demonstrates its generalization on our experiential datasets. With its efficiency both in terms of computational complexity and the number of parameters, DD-Net is sufficient to be applied in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c) Related/unrelated to global trajectories</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Nowadays, with the fast advancement of deep learning, skeleton acquisition is not limited to use motion capture systems <ref type="bibr" target="#b9">[10]</ref> and depth cameras <ref type="bibr" target="#b10">[11]</ref>. The RGB data, for instance, can be used to infer 2D skeletons <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> or 3D skeletons <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref> in real time. Moreover, even WiFi signals can be used to estimate skeleton data <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>. Those achievements have made skeleton-based action recognition available on a huge amount of multimedia resources and therefore have stimulated the model's development.</p><p>In general, in order to achieve a better performance for skeleton-based action recognition, previous studies attempt to work on two aspects: introduce new features for skeleton sequences <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b23">[23]</ref>, and, propose novel neural network architectures <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b28">[28]</ref>.</p><p>A good skeleton-sequence representation should contain global motion information and be location-viewpoint invariant. However, it is challenging to satisfy both requirements in one feature. The studies <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b23">[23]</ref> focused on global motions without considering the location-viewpoint variation in their features. Other studies <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b22">[22]</ref>, on the contrary, introduced location-viewpoint invariant features without considering global motions. Our work bridges their gaps by seamlessly integrating a location-viewpoint invariant feature and a two-scale global motion feature together.</p><p>Although Recurrent Neural Networks (RNNs) are commonly used in skeleton-based action recognition <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b22">[22]</ref>, we argue that it is relatively slow and difficult for parallel computing, compared with methods <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b23">[23]</ref> that use Convolutional Neural Networks (CNNs).</p><p>Since we take the model speed as one of our priorities, we utilize 1D CNNs to construct the backbone network of DD-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The network architecture of Double-feature Double-motion Network (DD-Net) is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In the following, we explain our motivation for designing input features and network structures of DD-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modeling Location-viewpoint Invariant Feature by Joint Collection Distances (JCD)</head><p>For skeleton-based action recognition, two types of input features are commonly used: the geometric feature <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b22">[22]</ref> and the Cartesian coordinate feature <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. The Cartesian coordinate feature is variant to locations and viewpoints. As <ref type="figure" target="#fig_0">Fig. 1 (a)</ref> shows, when skeletons are rotated or shifted, the Cartesian coordinate feature can be significantly changed. The geometric feature (e.g., angles/distances), on the other hand, is location-viewpoint invariant, and thereby it has been utilized for skeleton-based action recognition for a while. However, existing geometric features may need to be heavily redesigned from one dataset to another <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b22">[22]</ref>, or, contain redundant elements <ref type="bibr" target="#b33">[33]</ref>. To alleviate these issues, we introduce a Joint Collection Distances (JCD) feature. We calculate the Euclidean distances between a pair of collective joints to obtain a symmetric matrix. To reduce the redundancy, only the lower triangular matrix without the diagonal part is used as the JCD feature (see <ref type="figure" target="#fig_2">Fig. 3</ref>). Hence, the JCD feature is less than half the size of <ref type="bibr" target="#b33">[33]</ref>. In more detail, we assume the total frame number is K (K = 32 as the default setting) and there are totally N joints for one subject. At frame k, the 3D Cartesian coordinates of joint n is represented as J k i = (x, y, z), while the 2D Cartesian coordinates is represented as J k i = (x, y). Put all of joints together, we have a joint collection S k = {J k 1 , J k 2 , ..., J k N }. The formula for calculating the JCD feature of S k is:</p><formula xml:id="formula_0">JCD k =         − −− → J k 2 J k 1 . . . . . . . . . · · · . . . − −− → J k N J k 1 · · · · · · −−−−−→ J k N J k N −1         ; (1) where − −− → J k i J k j (i = j)</formula><p>denotes the Euclidean distance between J k i and J k j . In our processing, the JCD feature is flattened to be a onedimensional vector as our model's input. The dimension of flattened JCD is N 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Modeling Global Scale-invariant Motions by a Two-scale Motion Feature</head><p>Although the JCD feature is location-viewpoint invariant, the same as other geometric features, it does not contain global motion information. When actions are associated with global trajectories (see <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>), solely using the JCD feature is insufficient. Unlike previous works that only utilize either the geometric feature <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b22">[22]</ref> or the Cartesian coordinate feature <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, our DD-Net seamlessly integrates both of them.</p><p>We calculate the temporal differences (i.e., the speed) of the Cartesian coordinate feature to obtain global motions, which is location-invariant. For the same action, however, the scale of global motions may not be exactly identical. Some might be faster, and others might be slower (see <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>). To learn a robust global motion feature, both fast and slow motions should be considered. Conferring this intuition to DD-Net, we employ a fast global motion and a slow global motion to form a two-scale global motion feature. This idea is inspired by the two-scale optical flows proposed for RGB-based action recognition <ref type="bibr" target="#b35">[35]</ref>.</p><p>Technically, the two-scale motions can be generated by the following equation:</p><formula xml:id="formula_1">M k slow = S k+1 − S k , k ∈ {1, 2, 3, ..., K − 1}; M k f ast = S k+2 − S k , k ∈ {1, 3, ..., K − 2};<label>(2)</label></formula><p>where M k slow and M k f ast denote the slow motion and the fast motion at frame k, respectively. S k+1 and S k+2 are behind the S k of one frame and two frames, respectively. Corresponding to S [1,...,K] , we have M Such a process can be done in our DD-Net, and only the Cartesian coordinate feature is needed as the input. <ref type="figure" target="#fig_0">Fig. 1 (d)</ref> shows that the joint indices (i.e., the IDs of the head, left and right hands, etc.) are not locally correlated. Moreover, in different actions, the correlation of joints could be dynamically changed. Hence, the difficulty arises when we try to pre-define the correlation of joints by manually ordering their indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Modeling Joint Correlations by an Embedding</head><p>Since most of neural networks inherently assume that inputs are locally correlated, directly processing the locally uncorrelated joint feature is inappropriate. To tackle this problem, our DD-Net embeds the JCD feature and the two-scale motion feature into latent vectors at each frame. The correlation of joints is automatically learned through the embedding. As another benefit, the embedding process also reduces the effect of skeleton noise.</p><p>More formally, let embedding representations of JCD k , M k slow and M k f ast to be ε k JCD , ε k M slow and ε k M f ast , respectively, the embedding operation is as follows, </p><formula xml:id="formula_2">ε k JCD = Embed 1 (JCD k ); ε k M slow = Embed 1 (M k slow ); ε k M f ast = Embed 2 (M k f ast ).</formula><formula xml:id="formula_3">ε k = ε k JCD ⊕ ε k M slow ⊕ ε k M f ast , w .r .t. ε k ∈ R (K/2)×f ilters ;<label>(4)</label></formula><p>where ⊕ is the concatenation operation. After the embedding process, subsequent processes are not affected by the joint indices, and therefore DD-Net can use the 1D ConvNet to learn the temporal information as <ref type="figure" target="#fig_1">Fig. 2</ref> shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Datasets</head><p>We select two skeleton-based action recognition datasets, as SHREC dataset <ref type="bibr" target="#b3">[4]</ref> and JHMDB dataset <ref type="bibr" target="#b8">[9]</ref>, to evaluate our DD-Net from different perspectives (see <ref type="table" target="#tab_1">Table I</ref>).</p><p>Although other information (e.g., RGB data) is available, only the skeleton information is used in our experiments. 3D skeletons are given by SHREC dataset, which are derived from RGB-D data and contain more spatial information. In JHMDB dataset, 2D skeletons are interpreted from RGB videos, which can be applied in more general cases where inferring the depth information may be hard or impossible. Besides, actions in SHREC dataset are strongly correlated to the subject's global trajectories (e.g., a hand swipes a 'V' shape), while JHMDB dataset may have a weak connection with global trajectories. We show how these properties affect the performance and demonstrate the generalization of DD-Net in our ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Setup</head><p>The SHREC dataset is evaluated in two cases: 14 gestures and 28 gestures. The JHMDB dataset is evaluated by using the manually annotated skeletons, and we average the results from three split training/testing sets.</p><p>In ablation studies, we explore how each DD-Net component contributes to the action recognition performance by removing one component while remaining others unchanged. Furthermore, we also explore how the performance varies with different model size by adjusting the value of f ilters in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>Since the DD-Net is small, it is feasible to put all of the training sets into one batch on a single GTX 1080Ti GPU. We choose Adam (β 1 = 0.9, β 2 = 0.999) <ref type="bibr" target="#b36">[36]</ref> as the optimizer, with an annealing learning rate that drops from 1 −3 to 1 −5 . During the training, DD-Net only takes a temporal augmentation, which randomly selects 0.9 of entire frames.</p><p>To demonstrate the superiority of DD-Net, we do not apply any ensemble strategy or pre-trained weights to boost the performance. To make DD-Net can be easily deployed to real applications, we implement it by Keras <ref type="bibr" target="#b37">[37]</ref> backend in Tensorflow, which is "notorious" for its slow execution speed. Using other neural network frameworks may make DD-Net faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Result Analysis and Discussion</head><p>The action recognition results of SHREC dataset are presented in <ref type="table" target="#tab_1">Table II</ref> and more details are listed in their confusion matrix. The confusion matrix of 14 actions and 28 actions are <ref type="figure" target="#fig_5">Fig. 4 and Fig. 5</ref>, respectively. The action recognition results of JHMDB dataset are presented in <ref type="table" target="#tab_1">Table III</ref>.</p><p>Overall, although DD-Net takes fewer parameters, it can achieve superior results on SHREC dataset and JHMDB   suggests it can accommodate a wide range of skeleton-based action recognition scenarios. From ablation studies, we can inspect that when actions are strongly correlated to global trajectories (e.g., SHREC dataset), just using the JCD feature cannot produce a satisfactory performance. When actions are not strongly correlated to global trajectories (e.g., JHMDB dataset), the global motion feature still helps to improve the performance, but not as significant as the previous case. Such results agree with our assumptions: although the JCD feature is location-viewpoint invariant, it is isolated from global motions. The results also show that using the two-scale motion feature generates higher classification accuracy than only using a one-scale motion feature, which suggests that our proposed two-scale global motion feature is more robust to scale variation of motions. With the same components, DD-Net can adjust its model size by modifying the value of f ilters in CNN layers. We select 64, 32 and 16 as the values of f ilters to perform experiments. When DD-Net reaches the best performance on SHREC and JHMDB datasets, the values of f ilters are 64. It is worth noting that DD-Net can generate comparable results by only using 0.15 million parameters.</p><p>In addition, since DD-net employs one-dimensional CNNs to extract the feature, it is much faster than other models that use RNNs <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b25">[25]</ref> or 2D/3D CNNs <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b28">[28]</ref>. During its inferences, DD-Net's speed can reach around 3,500 FPS on one GPU (i.e., GTX 1080Ti), or, 2,000 FPS on one CPU (i.e., Intel E5-2620). While RNNbased models face great challenges for parallel processing (due to sequential dependency), our DD-Net does not have this issue because CNNs are used. Therefore, whether lowcomputational (e.g., on small devices) or high-computational applications (e.g., on parallel computing stations) are concerned, our DD-Net enjoys significant superiority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>By analyzing the basic properties of skeleton sequence, we propose two features and a DD-Net for efficient skeleton-based action recognition. Although DD-Net only contains a few parameters, it can achieve state-of-the-art performance on our experimental datasets. Due to the simplicity of DD-Net, many possibilities exist to enhance/extend it for broader studies. For instance, online action recognition can be approached by modifying the frame sampling strategies; RGB data or depth data could be used with it to further improve the action recognition performance; it is also possible to extend it for temporal action detection by adding temporal segmentation related modules. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGEMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part of this work was supported by JSPS KAKENHI Grant</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Uncorrelated joint indices (PuppetModel [9]) Examples of skeleton sequence properties. arXiv:1907.09658v8 [cs.CV] 18 Mar 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The network architecture of DD-Net. "2×CNN(3, 2*f ilters), /2" denotes two 1D ConvNet layers (kernel size = 3, channels = 2*f ilters) and a Maxpooling (strides = 2). Other ConvNet layers are defined in the same format. GAP denotes Global Average Pooling. FC denotes Fully Connected Layers (or Dense Layers). We can change the model size by modifying f ilters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>An example of Joint Collection Distances (JCD) feature at frame k, where the number of joints is N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>one-dimensional input at each frame, we reshape M k slow and M k f ast as M k slow ∈ R Dmotion and M k f ast ∈ R Dmotion , respectively, where D motion is the dimension of flattened vector. To match the frame number of the JCD feature, we perform linear interpolation to resize M [1,...,K−1] slow and M [1,...,K/2−1] f ast as M [1,...,K] slow and M [1,...,K/2] f ast , respectively. Consequently, two-scale global motion feature is composed of M [1,...,K] slow ∈ R K×Dmotion and M [1,...,K/2] f ast ∈ R (K/2)×Dmotion .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 3 )</head><label>3</label><figDesc>where the Embed 1 is defined asConv1D(1, 2 * f ilters) → Conv1D(3, f ilters) → Conv1D(1,f ilters), and the Embed 2 is defined as Conv1D(1, 2 * f ilters) → Conv1D(3, f ilters) → Conv1D(1, f ilters) → M axpooling(2), because JCD k and M k slow have double the temporal length of M k f ast . DD-Net futher concatenates embedding features to a representation ε k by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Confusion matrix of SHREC dataset (28 hand actions) obtained by DD-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Numbers JP17H06101 and JP17K00237, the Royal Society under IEC \R3 \ 170013 -International Exchanges 2017 Cost Share (Japan and Taiwan only), and a MSRA Collaborative Research 2019 Grant by Microsoft Research Asia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Adjacent indices but not locally correlated joints: [9, 10], [11, 12], [13, 14], etc. Locally correlated but not adjacent indices joints: [4, 8, 12], [5, 9, 13], [2, 6, 7], etc.</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Properties of experimental datasets</figDesc><table><row><cell></cell><cell>SHREC</cell><cell>JHMDB</cell></row><row><cell></cell><cell>Dataset</cell><cell>Dataset</cell></row><row><cell>Number of samples</cell><cell>2,800</cell><cell>928</cell></row><row><cell>Training/</cell><cell>1 Training Set</cell><cell>3 Split Training/</cell></row><row><cell>Testing Setup</cell><cell>1 Testing Set</cell><cell>Testing Sets</cell></row><row><cell>Dimension of skeletons</cell><cell>3</cell><cell>2</cell></row><row><cell>subject</cell><cell>hand</cell><cell>body</cell></row><row><cell>Number of actions</cell><cell>14 and 28</cell><cell>21</cell></row><row><cell>Actions are strongly</cell><cell></cell><cell></cell></row><row><cell>correlated to</cell><cell></cell><cell></cell></row><row><cell>global trajectories</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Results on SHREC (Using 3D skeletons only)<ref type="bibr" target="#b3">[4]</ref> </figDesc><table><row><cell>Methods</cell><cell>Parameters</cell><cell>14</cell><cell>28</cell><cell>Speed</cell></row><row><cell></cell><cell></cell><cell>Gestures</cell><cell>Gestures</cell><cell>on GPU</cell></row><row><cell>Dynamic hand [19] (CVPRW16)</cell><cell>-</cell><cell>88.2%</cell><cell>81.9%</cell><cell>-</cell></row><row><cell>Key-frame CNN [4] (3DOR17)</cell><cell>7.92 M</cell><cell>82.9%</cell><cell>71.9%</cell><cell>-</cell></row><row><cell>3 Cent [21] (STAG17)</cell><cell>-</cell><cell>77.9%</cell><cell>-</cell><cell>-</cell></row><row><cell>CNN+LSTM[38] (PR18)</cell><cell>8-9 M</cell><cell>89.8%</cell><cell>86.3%</cell><cell>238 FPS</cell></row><row><cell>Parallel CNN [5] (RFIAP18)</cell><cell>13.83 M</cell><cell>91.3%</cell><cell>84.4%</cell><cell>-</cell></row><row><cell>STA-Res-TCN [6] (Gesture18)</cell><cell>5-6 M</cell><cell>93.6%</cell><cell>90.7%</cell><cell>303 FPS</cell></row><row><cell>MFA-Net [23] (Sensor19)</cell><cell>-</cell><cell>91.3%</cell><cell>86.6%</cell><cell>361 FPS</cell></row><row><cell>DD-Net (filters=64, w/o</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>global fast&amp;slow motion)</cell><cell>1.70 M</cell><cell>55.2%</cell><cell>41.6%</cell><cell>-</cell></row><row><cell>DD-Net (filters=64,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o global slow motion)</cell><cell>1.76 M</cell><cell>92.7%</cell><cell>90.2%</cell><cell>-</cell></row><row><cell>DD-Net (filters=64,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o global fast motion)</cell><cell>1.76 M</cell><cell>93.3%</cell><cell>90.5%</cell><cell>-</cell></row><row><cell>DD-Net (filters=64)</cell><cell>1.82 M</cell><cell>94.6%</cell><cell>91.9%</cell><cell>2,200 FPS</cell></row><row><cell>DD-Net (filters=32)</cell><cell>0.50 M</cell><cell>93.5%</cell><cell>90.4%</cell><cell>3,100 FPS</cell></row><row><cell>DD-Net (filters=16)</cell><cell>0.15 M</cell><cell>91.8%</cell><cell>90.0%</cell><cell>3,500 FPS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="4">: Results on JHMDB (Using 2D skeletons only) [9]</cell></row><row><cell>Methods</cell><cell cols="2">Parameters Manually</cell><cell>Speed</cell></row><row><cell></cell><cell></cell><cell>annotated</cell><cell>on GPU</cell></row><row><cell></cell><cell></cell><cell>skeletons</cell><cell></cell></row><row><cell>Chained Net [7] (ICCV17)</cell><cell>17.50 M</cell><cell>56.8%</cell><cell>33 FPS</cell></row><row><cell>EHPI [28] (ITSC19)</cell><cell>1.22 M</cell><cell>65.5%</cell><cell>29 FPS</cell></row><row><cell>PoTion [8] (CVPR18)</cell><cell>4.87 M</cell><cell>67.9%</cell><cell>100 FPS</cell></row><row><cell>DD-Net (filters=64, w/o</cell><cell></cell><cell></cell><cell></cell></row><row><cell>global fast&amp;slow motion)</cell><cell>0.46 M</cell><cell>70.3%</cell><cell>-</cell></row><row><cell>DD-Net (filters=64,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o global slow motion)</cell><cell>0.48 M</cell><cell>72.5%</cell><cell>-</cell></row><row><cell>DD-Net (filters=64,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o global fast motion)</cell><cell>0.48 M</cell><cell>73.1%</cell><cell>-</cell></row><row><cell>DD-Net (filters=64)</cell><cell>1.82 M</cell><cell>77.2%</cell><cell>2,200 FPS</cell></row><row><cell>DD-Net (filters=32)</cell><cell>0.50 M</cell><cell>73.7%</cell><cell>3,100 FPS</cell></row><row><cell>DD-Net (filters=16)</cell><cell>0.15 M</cell><cell>65.7%</cell><cell>3,500 FPS</cell></row><row><cell cols="4">Fig. 4: Confusion matrix of SHREC dataset (14 hand actions)</cell></row><row><cell>obtained by DD-Net.</cell><cell></cell><cell></cell><cell></cell></row></table><note>dataset. The confusion matrix also shows that DD-Net is robust to each action class. Despite the data property divergence existing, DD-Net demonstrates its generalization ability, which</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust hand gesture recognition with kinect sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on Multimedia</title>
		<meeting>the 19th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="759" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Skeleton-augmented human action understanding by learning with progressively refined data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM International Workshop on Human Centered Event Understanding from Multimedia</title>
		<meeting>the 1st ACM International Workshop on Human Centered Event Understanding from Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A kinect-based system for physical rehabilitation: A pilot study for young adults with motor disabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research in developmental disabilities</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2566" to="2570" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shrec&apos;17 track: 3d hand gesture recognition using a depth and skeletal dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vandeborre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Eurographics Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for multivariate time series classification using both interand intra-channel parallel convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Devineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reconnaissance des Formes, Image, Apprentissage et Perception (RFIAP&apos;2018)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Spatialtemporal attention res-tcn for skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">gesture</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Chained multistream networks exploiting pose, motion, and appearance for action classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2923" to="2932" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey of computer vision-based human motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="268" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theobalt</surname></persName>
		</author>
		<ptr target="http://gvv.mpi-inf.mpg.de/projects/VNect/" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time 3d hand pose estimation with 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Through-wall human pose estimation using radio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7356" to="7365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Can wifi estimate person pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00277</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a 3d human pose distance metric from geometric pose descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1676" to="1689" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vandeborre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A 3 cent recognizer: Simple and effective retrieval and classification of mid-air gestures from single 3d traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prebianca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carcangiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Spano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giachetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Smart Tools and Apps for Graphics. Eurographics Association</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fusing geometric features for skeleton-based action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2330" to="2343" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mfa-net: Motion feature augmented network for dynamic hand gesture recognition from skeletal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">239</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint distance maps based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="624" to="628" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition with spatiotemporal visual attention on skeleton image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Simple yet efficient real-time posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ludl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gulde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Curio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09140</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling temporal dynamics and spatial configurations of actions using two-stream recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">e Conference on Computer Vision and Pa ern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition using lstm and cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia &amp; Expo Workshops</title>
		<imprint>
			<publisher>ICMEW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="585" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07455</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional neural networks and long short-term memory for skeleton-based human activity and hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Pantrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Montemayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="80" to="94" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
