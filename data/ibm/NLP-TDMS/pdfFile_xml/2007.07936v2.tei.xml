<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chalmers University of Technology</orgName>
								<address>
									<settlement>Gothenburg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Volvo Cars</orgName>
								<address>
									<settlement>Gothenburg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
							<email>wilhelm.tranheden@volvocars.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Chalmers University of Technology</orgName>
								<address>
									<settlement>Gothenburg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Volvo Cars</orgName>
								<address>
									<settlement>Gothenburg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chalmers University of Technology</orgName>
								<address>
									<settlement>Gothenburg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
							<email>lennart.svensson@chalmers.se</email>
							<affiliation key="aff0">
								<orgName type="institution">Chalmers University of Technology</orgName>
								<address>
									<settlement>Gothenburg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The state of the art in semantic segmentation is steadily increasing in performance, resulting in more precise and reliable segmentations in many different applications. However, progress is limited by the cost of generating labels for training, which sometimes requires hours of manual labor for a single image. Because of this, semi-supervised methods have been applied to this task, with varying degrees of success. A key challenge is that common augmentations used in semi-supervised classification are less effective for semantic segmentation. We propose a novel data augmentation mechanism called ClassMix, which generates augmentations by mixing unlabelled samples, by leveraging on the network's predictions for respecting object boundaries. We evaluate this augmentation technique on two common semi-supervised semantic segmentation benchmarks, showing that it attains state-of-the-art results. Lastly, we also provide extensive ablation studies comparing different design decisions and training regimes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is the task of assigning a semantic label to each pixel of an image. This is an essential part of many applications such as autonomous driving, medical imaging and scene understanding. Significant progress has been made in the area based on fully convolutional network architectures <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b39">39]</ref>. When training deep learning models for semantic segmentation, a common bottleneck is the availability of ground-truth labels. In contrast, unlabelled data is usually abundant, and effectively leveraging it has the potential to increase performance with low cost.</p><p>Semi-supervised learning based on consistency regularization has recently seen remarkable progress for image classification <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b33">33]</ref>, utilizing strong data augmentations to enforce consistent predictions on unlabelled im-* Equal contribution. ages. Augmentation techniques commonly used in classification have however proved ineffective for semi-supervised semantic segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">29]</ref>. Recent works have addressed this issue by either applying perturbations on an encoded state of the network instead of the input <ref type="bibr" target="#b29">[29]</ref>, or by using the augmentation technique CutMix <ref type="bibr" target="#b38">[38]</ref> to enforce consistent predictions over mixed samples <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">21]</ref>.</p><p>We propose a segmentation-based data augmentation strategy, ClassMix, and describe how it can be used for semi-supervised semantic segmentation. The augmentation strategy cuts half of the predicted classes from one image and pastes them onto another image, forming a new sample that better respects semantic boundaries while remaining accessible without ground-truth annotations. This is achieved by exploiting the fact that the network learns to predict a pixel-level semantic map of the original images. The predictions on mixed images are subsequently trained to be consistent with predictions made on the images before mixing. Following a recent trend in state-of-the-art consistency regularization for classification we also integrate entropy minimization <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref>, encouraging the network to generate predictions with low entropy on unlabelled data. We use pseudo-labelling <ref type="bibr" target="#b23">[23]</ref> to accomplish this, and provide further motivations for combining it with ClassMix. Our proposed method is evaluated on established benchmarks for semi-supervised semantic segmentation, and an ablation study is included, analyzing the individual impact of different design and experimental choices.</p><p>Our main contributions can be summarised as: <ref type="bibr" target="#b0">(1)</ref> We introduce an augmentation strategy which is novel for semantic segmentation, which we call ClassMix. <ref type="bibr" target="#b1">(2)</ref> We incorporate ClassMix in a unified framework that makes use of consistency regularization and pseudo-labelling for semantic segmentation. (3) We demonstrate the effectiveness of our method by achieving state-of-the-art results in semi-supervised learning for the Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref>, as well as competitive results for the Pascal VOC dataset <ref type="bibr" target="#b9">[10]</ref>. Code is available at https://github.com/ WilhelmT/ClassMix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>For semantic segmentation, semi-supervised learning has been explored with techniques based on adversarial learning <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b31">31]</ref>, consistency regularization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b30">30]</ref>, and pseudo-labelling <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5]</ref>. Our proposed method primarily incorporates ideas from the latter two approaches, which are expanded upon in subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Consistency Regularization</head><p>The core idea in consistency regularization is that predictions for unlabelled data should be invariant to perturbations. A popular technique for classification is augmentation anchoring <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b0">1]</ref>, where predictions performed on strongly augmented samples are enforced to follow predictions on weakly augmented versions of the same images. Our method utilizes augmentation anchoring in that consistency is enforced from unperturbed images to mixed images. Mixing images will create occlusions and classes in difficult contexts, hence being a strong augmentation.</p><p>Until recently, consistency regularization had been successfully applied for semantic segmentation only in the context of medical imaging <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b30">30]</ref>. Researchers pointed out the difficulties of performing consistency regularization for semantic segmentation, such as the violation of the cluster assumption, as described in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">29]</ref>. Ouali et al. <ref type="bibr" target="#b29">[29]</ref> propose to apply perturbations to the encoder's output, where the cluster assumption is shown to hold. Other approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">21]</ref> instead use a data augmentation technique called CutMix <ref type="bibr" target="#b38">[38]</ref>, which composites new images by mixing two original images, resulting in images with some pixels from one image and some pixels from another image. Our proposed method, ClassMix, builds upon this line of research by using predictions of a segmentation network to construct the mixing. In this way, we can enforce consistency over highly varied mixed samples while a the same time better respecting the semantic boundaries of the original images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pseudo-labelling</head><p>Another technique used for semi-supervised learning is pseudo-labelling, training against targets based on the network class predictions, first introduced in <ref type="bibr" target="#b23">[23]</ref>. Its primary motivation comes from entropy minimization, to encourage the network to perform confident predictions on unlabelled images. Such techniques have shown recent success in semi-supervised semantic segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">20]</ref>. Some methods of semi-supervised learning for classification <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref> integrate entropy minimization in the consistency regularization framework. This is achieved by having consistency targets either sharpened <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref> or pseudo-labelled <ref type="bibr" target="#b33">[33]</ref>. Our proposed method of consistency regularization also naturally incorporates pseudo-labelling, as it prevents predictions close to mixing borders being trained to unreasonable classes, which will be further explained in coming sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Related Augmentation Strategies</head><p>In the CutMix algorithm <ref type="bibr" target="#b38">[38]</ref>, randomized rectangular regions are cut out from one image and pasted onto another. This technique is based on mask-based mixing, where two images are mixed using a binary mask of the same size as the images. Our proposed technique, ClassMix, is based on a similar principle of combining images and makes use of predicted segmentations to generate the binary masks, instead of rectangles.</p><p>ClassMix also shares similarities with other segmentation-based augmentation strategies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b10">11]</ref>, where annotated single instances of objects are cut out of images, and pasted onto new background scenes. Our way of combining two images conditioned on the predicted semantic maps exploits the same idea of compositing images. However, in contrast to several existing techniques our proposed augmentation does not rely on access to ground-truth segmentationmasks, allowing us to learn from unlabelled images in the semi-supervised setting. Additionally, we perform semantic segmentation with multiple classes present in each image rather than single instances, allowing variety by randomizing which classes to transfer. As previously mentioned, ClassMix is formulated as a generalisation of CutMix, using a binary mask to mix two randomly sampled images. This means that we only distinguish between foreground and background when generating the binary mask and not when training on the mixed images. Segmenting both foreground and background images means that semantic objects recognized by the network do not only have to be invariant to their context, but invariant to a diverse set of occlusions as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>This section describes the proposed approach for semisupervised semantic segmentation. It starts by explaining the data augmentation mechanism ClassMix, followed by a description of the loss function used, along with other details about the training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ClassMix: Main Idea</head><p>The proposed method performs semi-supervised semantic segmentation by using a novel data augmentation technique, ClassMix, which uses the unlabelled samples in the dataset to synthesize new images and corresponding artificial labels ("artificial label" in this context is used to refer to the target that is used to train on the augmented image in the augmentation anchoring setup). ClassMix uses two unlabelled images as input and outputs a new augmented image, together with the corresponding artificial label for it. This augmented output is comprised of a mix of the inputs, where half of the semantic classes of one of the images are pasted on top of the other, resulting in an output which is novel and diverse, but still rather similar to the other images in the dataset. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the essence of how ClassMix works. Two unlabelled images, A and B, are sampled from the dataset. Both are fed through the segmentation network, f θ , which outputs the predictions S A and S B . A binary mask M is generated by randomly selecting half of the classes present in the argmaxed prediction S A and setting the pixels from those classes to have value 1 in M , whereas all others will have value 0. This mask is then used to mix images A and B into the augmented image X A , which will contain pixels from A where the mask had 1's and pixels from B elsewhere. The same mixing is also done to the predictions S A and S B , resulting in the artificial label Y A . While artifacts may appear because of the nature of the mixing strategy, as training progresses they become fewer and smaller. Additionally, consistency regulation tends to yield good performance also with imperfect labels and this is further confirmed by our strong results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ClassMix: Details</head><p>Two other techniques were added on top of the version of ClassMix presented in the previous section for improving its performance. This subsection explains those changes and provides a detailed description of the final ClassMix algorithm in pseudocode, in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 ClassMix algorithm</head><p>Require: Two unlabelled samples A and B, segmentation</p><formula xml:id="formula_0">network f θ . 1: S A ← f θ (A) 2: S B ← f θ (B) 3:S A ← arg max c S A (i, j, c ) Take pixel-wise argmax over classes. 4: C ← Set of the different classes present inS A 5: c ← Randomly selected subset of C such that |c| = |C|/2 6: For all i, j: M (i, j) = 1, ifS A (i, j) ∈ c 0, otherwise Create binary mask. 7: X A ← M A + (1 − M ) B Mix images. 8: Y A ← M S A + (1 − M ) S B Mix predictions. 9: return X A , Y A</formula><p>Mean-Teacher Framework. In order to improve stability in the predictions for ClassMix, we follow a trend in stateof-the-art semi-supervised learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b36">36]</ref> and use the Mean Teacher Framework, introduced in <ref type="bibr" target="#b34">[34]</ref>. Instead of using f θ to make predictions for the inputs images A and B in ClassMix, we use f θ , where θ is an exponential moving average of the previous values of θ throughout the optimization. This type of temporal ensembling is cheap and simple to introduce in ClassMix, and results in more stable predictions throughout the training, and consequently more stable artificial labels for the augmented images. The network f θ is then used to make predictions on the mixed images X A , and the parameters θ are subsequently updated using gradient descent.</p><p>Pseudo-labelled Output. Another important detail about ClassMix is that, when generating labels for the augmented image, the artificial label Y A is "argmaxed". That is, the probability mass function over classes for each pixel is changed to a one-hot vector with a one in the class which was assigned the highest probability, zero elsewhere. This forms a pseudo-label to be used in training, and it is a commonly used technique in semi-supervised learning in order to encourage the network to perform confident predictions.</p><p>For ClassMix, pseudo-labelling serves an additional purpose, namely eliminating uncertainty along borders. Since the mask M is generated from the output prediction of A, the edges of the mask will be aligned with the decision boundaries of the semantic map. This comes with the issue that predictions are especially uncertain close to the mixing borders, as the segmentation task is hardest close to class boundaries <ref type="bibr" target="#b24">[24]</ref>. This results in a problem we denote label contamination, illustrated in figure 2. When the classes chosen by M are pasted on top of image B, their adjacent context will often change, resulting in poor artificial labels. Pseudo-labelling effectively mitigates this issue, since the probability mass function for each pixel is changed to a onehot vector for the most likely class, therefore "sharpening" the artificial labels, resulting in no contamination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss and Training</head><p>For all the experiments in this paper, we train the parameters of the semantic segmentation network f θ by minimizing the following loss:</p><formula xml:id="formula_1">L(θ) = E f θ (X L ), Y L + λ f θ (X A ), Y A .<label>(1)</label></formula><p>In this expectation, X L is an image sampled uniformly at random from the dataset of labelled images, and Y L is its corresponding ground-truth semantic map. The random variables X A and Y A are respectively the augmented image and its artificial label, produced by the ClassMix augmentation method (as described in algorithm 1), where the input images A and B are sampled uniformly at random from the unlabelled dataset. (in practice the augmentations are computed by mixing all the images within a batch, for efficiency reasons; we refer interested readers to our code for further details). Lastly, λ is a hyper-parameter that controls the balance between the supervised and unsupervised terms, and  is the cross-entropy loss, averaged over all pixel positions in the semantic maps, i.e.</p><formula xml:id="formula_2">(S, Y ) = − 1 W · H W i=1 H j=1 C c=1 Y (i, j, c) · log S(i, j, c) ,<label>(2)</label></formula><p>where W and H are the width and height of the images, and S(i, j, c) and Y (i, j, c) are the probabilities that the pixel in coordinates i, j belongs to class c, according to the prediction S and target Y , respectively. We train θ by stochastic gradient descent on this loss, imposing batches with 50% labelled data and 50% augmented data.</p><p>It is beneficial to the training progress that the unsupervised weight λ starts close to zero, because initially the network predictions are of low quality and therefore the pseudo-labels generated will not be reasonable targets for training on the augmented images. As the predictions of the network improve, this weight can then be increased. This was accomplished by setting the value of λ for an augmented sample as the proportion of pixels in its artificial label where the probability of the most likely class is above a predetermined threshold τ . This results in a value between 0 and 1, which we empirically found to serve as an adequate proxy for the quality of the predictions, roughly in line with <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In order to evaluate the proposed method, we perform experiments on two common semi-supervised semantic segmentation datasets, and this section presents the results obtained. Additionally, an extensive ablation study for motivating our design decisions is also provided, where we further investigate the properties of ClassMix and its components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details and Datasets</head><p>Our method is implemented using the PyTorch framework and training was performed on two Tesla V100 GPUs. We adopt the DeepLab-v2 framework <ref type="bibr" target="#b3">[4]</ref> with a ResNet101  backbone <ref type="bibr" target="#b18">[18]</ref> pretrained on ImageNet <ref type="bibr" target="#b6">[7]</ref> and MSCOCO <ref type="bibr" target="#b26">[26]</ref>, identical to the ones used in <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">28]</ref>. As optimizer, we use Stochastic Gradient Descent with Nesterov acceleration, and a base learning rate of 2.5 × 10 −4 , decreased with polynomial decay with power 0.9 as used in <ref type="bibr" target="#b3">[4]</ref>. Momentum is set to 0.9 and weight decay to 5 × 10 −4 .</p><p>We present results for two semantic segmentation datasets, Cityscapes <ref type="bibr" target="#b5">[6]</ref> and Pascal VOC 2012 <ref type="bibr" target="#b9">[10]</ref>. The Cityscapes urban scenery dataset contains 2,975 training images and 500 validations images. We resize images to 512 × 1024 with no random cropping, scaling or flipping, use batches with 2 labelled and 2 unlabelled samples and train for 40k iterations, all in line with <ref type="bibr" target="#b19">[19]</ref>. For the Pascal VOC 2012 dataset we use the original images along with the extra annotated images from the Semantic Boundaries dataset <ref type="bibr" target="#b16">[16]</ref>, resulting in 10,582 training images and 1,449 validation images. Images are randomly scaled between 0.5 and 1.5 as well as randomly horizontally flipped and after that cropped to a size of 321 × 321 pixels, also in line with <ref type="bibr" target="#b19">[19]</ref>. We train for 40k iterations using batches with 10 labelled and 10 unlabelled samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figures 3 and 4 show example images of both datasets along with their corresponding ground truth semantic maps.</head><p>It is clear that the images in Cityscapes contain a lot more classes in each image than the Pascal images do. At the same time the semantic maps are more consistent throughout the images in the Cityscapes dataset than between Pascal images, for example the road and sky classes are almost always present and in approximately the same place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Cityscapes. In <ref type="table" target="#tab_1">Table 1</ref> we present our results for the Cityscapes dataset, given as mean Intersection over Union (mIoU) scores. We have performed experiments for four proportions of labelled samples, which are given along with baselines that are trained in a supervised fashion on the corresponding data amounts. In the table we also provide results from four other papers, all using the same DeepLab-v2 framework. <ref type="bibr">Hung</ref>   <ref type="bibr" target="#b11">[12]</ref>. We note that our results are higher for three out of four data amounts and that our improvement from the baseline result to the SSL result is higher for all amounts of training data.</p><p>The fact that we achieve, to the best of our knowledge, the best SSL-results on the Cityscapes dataset further supports that consistency regularization can be successfully applied to semi-supervised semantic segmentation. French et al. use a method similar to ours <ref type="bibr" target="#b12">[13]</ref>, where they enforce consistency with CutMix as their mixing algorithm, instead of our ClassMix. We believe that one reason for the higher performance of ClassMix is the diversity of the masks created. This diversity stems from the fact that each image includes many classes and that each class often contains several objects. Since there are many classes in each image, an image rarely has the exact same classes being selected for mask generation several times, meaning that the masks based on a given image will be varied over the course of training. Furthermore, since each class often contains several objects, the masks will naturally become very irregular, and hence very different between images; when using Cut-Mix, the masks will not be nearly as varied.</p><p>We believe that another reason that ClassMix works well is that the masks are based on the semantics of the images, as discussed previously. This minimizes the occurrence of partial objects in the mixed image, which are difficult to predict and make learning unnecessarily hard. It also means that mixing borders will be close to being aligned with boundaries of objects. This creates mixed images that better respect the semantic boundaries of the original images. They are consequently more realistic looking than images created using, e.g., CutMix, and lie closer to the underlying data distribution.</p><p>A third reason for ClassMix performing well for Cityscapes may be that images are similar within the dataset. All images have the road class in the bottom and sky at the top, and whenever there are cars or people, for example, they are roughly in the same place. Another way to put this is that classes are not uniformly distributed across the image area, but instead clustered in smaller regions of the image, as is shown by the spatial distribution of classes in <ref type="figure">Figure 5</ref>. Because of this, objects that are pasted from one image to another are likely to end up in a reasonable Pascal VOC 2012. In <ref type="table" target="#tab_3">Table 2</ref>, we present the results from using our method on the Pascal VOC 2012 dataset. We compare our results to the same four papers as for Cityscapes. We note that our results are competitive, and that we have the strongest performance for two data amounts. There is, however, a significant difference in baselines between the different works, complicating the comparison of the results. In particular, French et al. have a significantly lower baseline, largely because their network is not pre-trained on MSCOCO, resulting in a bigger room for improvement, as shown in the table. We use a network pre-trained on MSCOCO because that is what most existing work is using. Our results can also be compared to those from Ouali et al., who obtain 69.4% mIoU on Pascal VOC 2012 for 1.5k samples using DeepLabv3 <ref type="bibr" target="#b29">[29]</ref>. Our results for 1/8 (or 1323) labelled samples is higher, despite us using fewer labelled samples and a less sophisticated network architecture.</p><p>Our results are not as strong for the Pascal dataset as they are for Cityscapes. We believe that this is largely because Pascal contains very few classes in each image, usually only a background class and one or two foreground classes. This means that the diversity of masks in ClassMix will be very small, with the same class or classes frequently being selected for mask generation for any given image. This is in contrast to Cityscapes as discussed above. The images in  <ref type="figure">Figure 5</ref>: Spatial distribution of all classes in the Cityscapes training dataset. Dark pixels correspond to more frequent appearance. the Pascal dataset are also not that similar to each other.</p><p>There is no pattern to where in the images certain classes appear, or in what context, unlike Cityscapes. Therefore, pasted objects often end up in unreasonable contexts, which we believe is detrimental for performance. The patterns of where classes appear are made obvious by calculating the spatial distribution of classes, which is visualised for Pascal in <ref type="figure" target="#fig_4">Figure 6</ref>, and can be compared to Cityscapes in <ref type="figure">Figure  5</ref>. In these figures it is clear that the spatial distributions are much less uniform for Cityscapes than for Pascal. We note that in spite of these challenges for the Pascal VOC dataset, ClassMix still performs competitively with previous state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We investigate our method further by training models with some components changed or removed, in order to see how much those specific components contribute to the performance of the overall algorithm. Additionally, we also experiment with additions to the algorithm, namely adding more augmentations and training for a longer time. Although such additions increase the final performance, they also make comparisons with other existing approaches unfair, which is why these results are not presented in subsection 4.2. The ablation results are presented in <ref type="table" target="#tab_5">Table 3</ref>. All figures are from training a model with 1/8 labelled samples on the Cityscapes dataset, with the same settings as used for the main results except for the part being examined. <ref type="bibr" target="#b1">2</ref> As reported by <ref type="bibr" target="#b28">[28]</ref>. <ref type="bibr" target="#b2">3</ref>   First, we examine the effect of using different mixed sample data augmentations. Apart from ClassMix we try CutMix <ref type="bibr" target="#b38">[38]</ref>, as used for semi-supervised semantic segmentation in <ref type="bibr" target="#b12">[13]</ref>, and CowMix, introduced by French et al. <ref type="bibr" target="#b15">[15]</ref>. We note that CowMix is very similar to the concur-  <ref type="bibr" target="#b17">[17]</ref>. As can be seen in <ref type="table" target="#tab_5">Table 3</ref>, ClassMix performs significantly better than both other mixes. That CutMix performs the worst, we attribute to CutMix's masks being less varied than for the other methods. Both ClassMix and CowMix yield flexible masks and we speculate that ClassMix achieves higher results because the masks will follow semantic boundaries to a high degree, giving more natural borders between the two mixed images. We try three different ways of weighting the unsupervised loss, additional to our default way of weighting it against the proportion of pixels that have a maximum predicted value above a threshold 0.968, as used in <ref type="bibr" target="#b12">[13]</ref>. In contrast to this is pixel-wise threshold, where instead all pixels with predicted certainties below the threshold are masked and ignored in the loss. As can be seen in <ref type="table" target="#tab_5">Table  3</ref>, using the pixel-wise threshold significantly lowers the results. We have found that this strategy masks almost all pixels close to class boundaries, as well as some small objects, such that no unsupervised training is ever performed on this kind of pixels, as also noted in <ref type="bibr" target="#b12">[13]</ref>. Sigmoid ramp up increases the unsupervised weight λ from 0 to 1 over the course of the first 10k iterations, similarly to what was done in, e.g., <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b22">22]</ref>. This yields results somewhat lower than in our default solution, and exactly the same results as when keeping the unsupervised weight at a constant 1.</p><p>We investigate adjusting the unsupervised loss by changing our default cross-entropy loss with a squared error loss. The loss is summed over the class probability dimension and averaged over batch and spatial dimensions, in keeping with <ref type="bibr" target="#b12">[13]</ref>. This yields results considerably lower than when using cross-entropy. We also try training with cross-entropy without using pseudo-labels, and instead merely softmax outputs as targets, which also lowers the results. This is likely because entropy minimization, here in the form of pseudo-labelling, helps the network generalize better, as seen in previous works <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b23">23]</ref>. When not using pseudolabels we also fail to avoid the problem of "label contamination", described in <ref type="figure" target="#fig_1">Figure 2</ref>, causing the network to be trained against unreasonable targets near object boundaries.</p><p>In our results for Cityscapes, the images are not cropped. Here, however, we investigate randomly cropping both labelled and unlabelled images to 512 × 512. This increases the performance of both baseline and SSL. The reason for this is likely that cropping adds a regularizing effect. The increase from cropping is larger for the baseline than for SSL, which is believed to be because the SSL solution already receives a regularizing effect from ClassMix, leaving less room for improvement.</p><p>Adding color jittering (adjusting brightness, contrast, hue and saturation) and Gaussian blurring also improves the results. These extra augmentations are applied as part of the strong augmentation scheme after ClassMix. This introduces more variation in the data, likely increasing the network's ability to generalize. It also makes the strong augmentation policy more difficult, in line with the use of augmentation anchoring.</p><p>Training for 80k iterations instead of 40k improves the results significantly, as can be seen in <ref type="table" target="#tab_5">Table 3</ref>. It improves the results more for SSL than for the baseline, which is likely because there is more training data in SSL training, meaning that overfitting is a smaller issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we have proposed an algorithm for semisupervised semantic segmentation that uses ClassMix, a novel data augmentation technique. ClassMix generates augmented images and artificial labels by mixing unlabelled samples together, leveraging on the network's semantic predictions in order to better respect object boundaries. We evaluated the performance of the algorithm on two commonly used datasets, and showed that it improves the state of the art. It is also worth noting that since many semantic segmentation algorithms rely heavily on data augmentations, the ClassMix augmentation strategy may become a useful component also in future methods. Finally, additional motivation for the design choices was presented through an extensive ablation study, where different configurations and training regimes were compared.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>ClassMix augmentation technique. Two images A and B are sampled from the unlabelled dataset. Based on the prediction SA of image A, a binary mask M is created. The mask is then used to mix the images A and B and their respective predictions into an augmented image XA and the corresponding artificial label YA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Toy example of label contamination with 3 different classes. Left: Ground-truth labels; red is class 1, blue is class 2. Middle: Prediction made by network; regions where the network is uncertain between classes 1 and 2 have a mix of red and blue colors. The decision boundary is marked with a white line. Right: The red class is pasted on top of a new image, which is comprised entirely of the third class. Note how the pasted class still brings some uncertainty of class 2 (blue) to the new image. This results in problematic artificial labels for training, since the context around the pasted object now changed to class 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Images and corresponding semantic maps from the Cityscapes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Images and corresponding semantic maps from the Pascal VOC 2012 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Spatial distribution of all classes in the Pascal training dataset. Dark pixels correspond to more frequent appearance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>et al. and Mittal et al. use an adversarial approach [19, 28], French et al. use consistency regularization [13] and Feng et al. use a self-training scheme</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance (mIoU) on Cityscapes validation set averaged over three runs. Results from four previous papers are provided for comparison, all using the same DeepLab-v2 network with ResNet-101 backbone.</figDesc><table><row><cell>Labelled samples</cell><cell>1/30</cell><cell>1/8</cell><cell>1/4</cell><cell>1/2</cell><cell>Full (2975)</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>55.5%</cell><cell>59.9%</cell><cell>64.1%</cell><cell>66.4%</cell></row><row><cell>Adversarial [19]</cell><cell>-</cell><cell>58.8%</cell><cell>62.3%</cell><cell>65.7%</cell><cell>-</cell></row><row><cell>Improvement</cell><cell>-</cell><cell>3.3</cell><cell>2.4</cell><cell>1.6</cell><cell>-</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>56.2%</cell><cell>60.2%</cell><cell>-</cell><cell>66.0%</cell></row><row><cell>s4GAN [28] 1</cell><cell>-</cell><cell>59.3%</cell><cell>61.9%</cell><cell>-</cell><cell>65.8%</cell></row><row><cell>Improvement</cell><cell>-</cell><cell>3.1</cell><cell>1.7</cell><cell>-</cell><cell>-0.2</cell></row><row><cell>Baseline</cell><cell>44.41%</cell><cell>55.25%</cell><cell>60.57%</cell><cell>-</cell><cell>67.53%</cell></row><row><cell cols="2">French et al. [13] 1 51.20%</cell><cell>60.34%</cell><cell>63.87%</cell><cell>-</cell><cell>-</cell></row><row><cell>Improvement</cell><cell>6.79</cell><cell>5.09</cell><cell>3.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline</cell><cell>45.5 %</cell><cell>56.7%</cell><cell>61.1%</cell><cell>-</cell><cell>66.9%</cell></row><row><cell>DST-CBC [12]</cell><cell>48.7 %</cell><cell>60.5%</cell><cell>64.4%</cell><cell>-</cell><cell>-</cell></row><row><cell>Improvement</cell><cell>3.2</cell><cell>3.8</cell><cell>3.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline</cell><cell cols="5">43.84%±0.71 54.84%±1.14 60.08%±0.62 63.02%±0.14 66.19%±0.11</cell></row><row><cell>Ours</cell><cell cols="5">54.07%±1.61 61.35%±0.62 63.63%±0.33 66.29%±0.47 -</cell></row><row><cell>Improvement</cell><cell>10.23</cell><cell>6.51</cell><cell>3.55</cell><cell>3.27</cell><cell>-</cell></row><row><cell cols="3">context, which may be important in the cases where objects</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">are transferred without any surrounding context from the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>original image.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance (mIoU) on the Pascal VOC 2012 validation set, results are given from a single run. Results from four previous papers are provided for comparison, all using the same DeepLab-v2 network with a ResNet-101 backbone.</figDesc><table><row><cell>Labelled samples</cell><cell>1/100</cell><cell>1/50</cell><cell>1/20</cell><cell>1/8</cell><cell>1/4</cell><cell>Full (10582)</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>53.2% 2</cell><cell cols="2">58.7% 2 66.0%</cell><cell>68.3%</cell><cell>73.6%</cell></row><row><cell>Adversarial [19]</cell><cell>-</cell><cell>57.2% 2</cell><cell cols="2">64.7% 2 69.5%</cell><cell>72.1%</cell><cell>-</cell></row><row><cell>Improvement</cell><cell>-</cell><cell>4.0</cell><cell>6.0</cell><cell>3.5</cell><cell>3.8</cell><cell>-</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>53.2%</cell><cell>58.7%</cell><cell>66.0%</cell><cell>-</cell><cell>73.6%</cell></row><row><cell>s4GAN [28]</cell><cell>-</cell><cell>63.3%</cell><cell>67.2%</cell><cell>71.4%</cell><cell>-</cell><cell>75.6%</cell></row><row><cell>Improvement</cell><cell>-</cell><cell>10.1</cell><cell>8.5</cell><cell>5.4</cell><cell>-</cell><cell>2.0</cell></row><row><cell>Baseline</cell><cell cols="5">33.09% 43.15% 52.05% 60.56% -</cell><cell>72.59%</cell></row><row><cell cols="6">French et al. [13] 3 53.79% 64.81% 66.48% 67.60% -</cell><cell>-</cell></row><row><cell>Improvement</cell><cell>20.70</cell><cell>21.66</cell><cell>14.48</cell><cell>7.04</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline</cell><cell cols="2">45.7% 4 55.4%</cell><cell>62.2%</cell><cell>66.2%</cell><cell>68.7%</cell><cell>73.5%</cell></row><row><cell>DST-CBC [12]</cell><cell cols="2">61.6% 4 65.5%</cell><cell>69.3%</cell><cell>70.7%</cell><cell>71.8%</cell><cell>-</cell></row><row><cell>Improvement</cell><cell>15.9</cell><cell>10.1</cell><cell>7.1</cell><cell>4.5</cell><cell>3.1</cell><cell>-</cell></row><row><cell>Baseline</cell><cell cols="6">42.47% 55.69% 61.36% 67.14% 70.20% 74.13%</cell></row><row><cell>Ours</cell><cell cols="6">54.18% 66.15% 67.77% 71.00% 72.45% -</cell></row><row><cell>Improvement</cell><cell>11.71</cell><cell>10.46</cell><cell>6.41</cell><cell>3.86</cell><cell>2.25</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of the proposed method on the Cityscapes dataset. All results are mIoU scores averaged over three runs.</figDesc><table><row><cell>Settings</cell><cell>mIoU</cell></row><row><cell>Baseline</cell><cell>54.84%</cell></row><row><cell>Default SSL</cell><cell>61.35%</cell></row><row><cell>CowMix</cell><cell>60.37%</cell></row><row><cell>CutMix</cell><cell>59.12%</cell></row><row><cell>Pixel-wise threshold</cell><cell>58.61%</cell></row><row><cell>Sigmoid ramp up</cell><cell>60.58%</cell></row><row><cell cols="2">Constant unsupervised weight 60.58%</cell></row><row><cell>Squared error loss</cell><cell>58.74%</cell></row><row><cell>No Pseudo-label</cell><cell>60.15%</cell></row><row><cell>Random crop Baseline</cell><cell>56.42%</cell></row><row><cell>Random crop</cell><cell>62.16%</cell></row><row><cell>Extra augmentations</cell><cell>61.85%</cell></row><row><cell>80k iterations Baseline</cell><cell>55.05%</cell></row><row><cell>80k iterations</cell><cell>62.92%</cell></row></table><note>rent FMix, introduced by Harris et al.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Same DeepLab-v2 network but with only ImageNet pre-training and not MSCOCO.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno>abs/1911.09785</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alché-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="5050" to="5060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semi-supervised learning in video sequences for urban scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10266</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling visual context is key to augmenting object detection datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="375" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cut, paste and learn: Surprisingly easy synthesis for instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="1310" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Instaboost: Boosting instance segmentation via probability map guided copypasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation via dynamic self-training and classbalanced curriculum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08514</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation needs strong, varied perturbations</title>
	</analytic>
	<monogr>
		<title level="m">29th British Machine Vision Conference, BMVC 2020</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">H</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Milking cowmask for semi-supervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12022</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Understanding and enhancing mixed sample data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesan</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Prügel-Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial learning for semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Ting</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-09-03" />
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Universal semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarun</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5259" to="5270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Structured consistency loss for semi-supervised semantic segmentation. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooyoung</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2013 Workshop : Challenges in Representation Learning (WREPL)</title>
		<imprint>
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised skin lesion segmentation via transformation consistent self-ensembling model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-09-03" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<editor>David J. Fleet, Tomás Pajdla, Bernt Schiele, and Tinne Tuytelaars</editor>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with high-and lowlevel consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semisupervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Céline</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12674" to="12684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep semisupervised segmentation with weight-averaged consistency targets. In Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen-Adad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kegan: Knowledge embedded generative adversarial networks for semi-supervised scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengshi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5237" to="5246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to segment via cut-and-paste</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to generate synthetic data via compositing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visesh</forename><surname>Chari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3635" to="3641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised data augmentation for consistency training</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
