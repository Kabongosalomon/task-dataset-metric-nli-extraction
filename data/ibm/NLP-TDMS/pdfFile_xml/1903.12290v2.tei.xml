<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Wollongong</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot learning in image classification aims to learn a classifier to classify images when only few training examples are available for each class. Recent work has achieved promising classification performance, where an image-level feature based measure is usually used. In this paper, we argue that a measure at such a level may not be effective enough in light of the scarcity of examples in few-shot learning. Instead, we think a local descriptor based image-to-class measure should be taken, inspired by its surprising success in the heydays of local invariant features. Specifically, building upon the recent episodic training mechanism, we propose a Deep Nearest Neighbor Neural Network (DN4 in short) and train it in an end-to-end manner. Its key difference from the literature is the replacement of the image-level feature based measure in the final layer by a local descriptor based image-to-class measure. This measure is conducted online via a k-nearest neighbor search over the deep local descriptors of convolutional feature maps. The proposed DN4 not only learns the optimal deep local descriptors for the image-to-class measure, but also utilizes the higher efficiency of such a measure in the case of example scarcity, thanks to the exchangeability of visual patterns across the images in the same class. Our work leads to a simple, effective, and computationally efficient framework for few-shot learning. Experimental study on benchmark datasets consistently shows its superiority over the related stateof-the-art, with the largest absolute improvement of 17% over the next best. The source code can be available from https://github.com/WenbinLee/DN4.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Few-shot learning aims to learn a model with good generalization capability such that it can be readily adapted to new unseen classes (concepts) by accessing only one or few examples. However, the extremely limited number of ex-amples per class can hardly represent the class distribution effectively, making this task truly challenging.</p><p>To tackle the few-shot learning task, a variety of methods have been proposed, which can be roughly divided into two types, i.e., meta-learning based <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref> and metriclearning based <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref>. The former type introduces a meta-learning paradigm <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref> to learn an across-task meta-learner for generalizing to new unseen tasks. They usually resort to recurrent neural networks or long short term memory networks to learn a memory network <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12]</ref> to store knowledge. The latter type adopts a relatively simpler architecture to learn a deep embedding space to transfer representation (knowledge). This type of methods usually relies on the metric learning and episodic training mechanism <ref type="bibr" target="#b21">[22]</ref>. Both types of methods have greatly advanced the development of few-shot learning.</p><p>These existing methods mainly focus on making knowledge transfer <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref>, concept representation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b3">4]</ref> or relation measure <ref type="bibr" target="#b24">[25]</ref>, but have not paid sufficient attention to the way of the final classification. They generally take the common practice, i.e., using the image-level pooled features or fully connected layers designed for larger-scale image classification, for the few-shot case. Considering the unique characteristic of few-shot learning (i.e., the scarcity of examples for each training class), such a common practice may not be appropriate any more.</p><p>In this paper, we revisit the Naive-Bayes Nearest-Neighbor (NBNN) approach <ref type="bibr" target="#b0">[1]</ref> published a decade ago, and investigate its effectiveness in the context of the latest few-shot learning research. The NBNN approach demonstrated a surprising success when the bag-of-features model with local invariant features (i.e., SIFT) was popular. That work provides two key insights. First, summarizing the local features of an image into a compact image-level representation could lose considerable discriminative information. It will not be recoverable when the number of training examples is small. Second, in this case, directly using these local features for classification will not work if an image-toimage measure is used. Instead, an image-to-class measure should be taken, by exploiting the fact that a new image can be roughly "composed" using the pieces of other images in the same class. The above two insights inspire us to review the way of the final classification in the existing methods for few-shot learning and reconsider the NBNN approach for this task with deep learning.</p><p>Specifically, we develop a novel Deep Nearest Neighbor Neural Network (DN4 in short) for few-shot learning. It follows the recent episodic training mechanism and is fully end-to-end trainable. Its key difference from the related existing methods lies in that it replaces the image-level feature based measure in the final layer with a local descriptor based image-to-class measure. Similar to NBNN <ref type="bibr" target="#b0">[1]</ref>, this measure is computed via a k-nearest neighbor search over local descriptors, with the difference that these descriptors are now trained deeply via convolutional neural networks. Once trained, applying the proposed network to new fewshot learning tasks is straightforward, consisting of local descriptor extraction and then a nearest neighbor search. Interestingly, in terms of computation, the scarcity of examples per class now turns out to be an "advantage" making NBNN more appealing for few-shot learning. It mitigates the computation of searching for the nearest neighbors from a huge set of local descriptors, which is one factor of the lower popularity of NBNN in large-scale image classification.</p><p>Experiments are conducted on multiple benchmark datasets to compare the proposed DN4 with the original NBNN and the related state-of-the-art methods for the task of few-shot learning. The proposed method again demonstrates a surprising success. It improves the 1-shot and 5shot accuracy on miniImageNet from 50.44% to 51.24% and from 66.53% to 71.02%, respectively. Particularly, on fine-grained datasets it achieves the largest absolute improvement over the next best method by 17%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Among the recent literature of few-shot learning, the transfer learning based methods are most relevant to the proposed method. Therefore, we briefly review two main branches of this kind of methods as follows.</p><p>Meta-learning based methods. As shown by the representative work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5]</ref>, the meta-learning based methods train a meta-learner with the meta-learning or the learning-to-learn paradigm <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> for few-shot learning. This is beneficial for identifying how to update the parameters of the learner's model. For instance, Santoro et al. <ref type="bibr" target="#b15">[16]</ref> trained an LSTM as a controller to interact with an external memory module. And the work <ref type="bibr" target="#b13">[14]</ref> adopted an LSTM-based meta-learner as an optimizer to train another classifier as well as learning a task-common initialization for this classifier. The work of MM-Net <ref type="bibr" target="#b1">[2]</ref> constructed a contextual learner to predict the parameters of an embedding network for unlabeled images by using memory slots.</p><p>Although the meta-learning based methods can achieve excellent results for few-shot classification, it is difficult to train their complicated memory-addressing architecture because of the temporally-linear hidden state dependency <ref type="bibr" target="#b12">[13]</ref>. Compared with the methods in this branch, the proposed framework DN4 can be trained more easily in an end-to-end manner from scratch, e.g., by only using a common single convolutional neural networks (CNN), and could provide quite competitive results. Metric-learning based methods. The metric-learning based methods mainly depend on learning an informative similarity metric, as demonstrated by the representative work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b10">11]</ref>. Specifically, to introduce the metric-based method into few-shot learning, Koch et al. <ref type="bibr" target="#b8">[9]</ref> originally utilized a Siamese Neural Network to learn powerful discriminative representations and then generalized them to unseen classes. And then, Vinyals et al. <ref type="bibr" target="#b21">[22]</ref> introduced the episodic training mechanism into few-shot learning and proposed the Matching Nets by combining attention and memory together. In <ref type="bibr" target="#b16">[17]</ref>, a Prototypical Network was proposed by taking the mean of each class as its corresponding prototype representation to learn a metric space. Recently, Sung et al. considered the relation between query images and class images, and presented a Relation Network <ref type="bibr" target="#b24">[25]</ref> to learn a deep non-linear measure.</p><p>The proposed framework DN4 belongs to the metriclearning based methods. However, a key difference from them is that the above methods mainly adopt the imagelevel features for classification, while the proposed DN4 exploits deep local descriptors and the image-to-class measure for classification, as inspired by the NBNN approach <ref type="bibr" target="#b0">[1]</ref>. As will be shown in the experimental part, the proposed DN4 can clearly outperform the several state-of-the-art metric-learning based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Let S denote a support set, which contains C different image classes and K labeled samples per class. Given a query set Q, few-shot learning aims to classify each unlabeled sample in Q according to the set S. This setting is also called C-way K-shot classification. Unfortunately, when S only has few samples per class, it will be hard to effectively learn a model to classify the samples in Q. Usually, the literature resorts to an auxiliary set A to learn transferable knowledge to improve the classification on Q. Note that the set A can contain a large number of classes and labeled samples, but it has a disjoint class label space with respect to the set S.</p><p>The episodic training mechanism <ref type="bibr" target="#b21">[22]</ref> has been demonstrated in the literature as an effective approach to learning the transferable knowledge from A, and it will also be adopted in this work. Specifically, at each iteration, an episode is constructed to train the classification model by  <ref type="figure">Figure 1</ref>. Illustration of the proposed Deep Nearest Neighbor Neural Network (DN4 in short) for a few-shot learning task in the 5-way and 1-shot setting. As shown, this framework consists of a CNN-based embedding module Ψ(·) for learning deep local descriptors and an image-to-class module Φ(·) for measuring the similarity between a given query image X and each of the classes, ci (i = 1, 2, · · · , 5). simulating a few-shot learning task. The episode consists of a support set A S and a query set A Q that are randomly sampled from the auxiliary set A. Generally, A S has the same numbers of ways (i.e., classes) and shots as S. In other words, there are exactly C classes and K samples per class in A S . During training, tens of thousands of episodes will be constructed to train the classification model, namely the episodic training. In the test stage, with the support set S, the learned model can be directly used to classify each image in Q.</p><formula xml:id="formula_0">Embedding Ψ Φ (Ψ( ), 2 ) Φ (Ψ( ), 3 ) Φ (Ψ( ), 4 ) Φ (Ψ( ), 5 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motivation from the NBNN Approach</head><p>This work is largely inspired by the Naive-Bayes Nearest-Neighbor (NBNN) method in <ref type="bibr" target="#b0">[1]</ref>. The two key observations of NBNN are described as follows, and we show that they apply squarely to few-shot learning.</p><p>First, for the (then-popular) bag-of-features model in image classification, local invariant features are usually quantized into visual words to generate the distribution of words (e.g., a histogram obtained by sum-pooling) in an image. It is observed in <ref type="bibr" target="#b0">[1]</ref> that due to quantization error, such an image-level representation could significantly lose discriminative information. If there are sufficient training samples, the subsequent learning process (e.g., via support vector machines) can somehow recover from such a loss, still showing satisfactory classification performance. Nevertheless, when training samples are insufficient, this loss is unrecoverable and leads to poor classification.</p><p>Few-shot learning is impacted more significantly by the issue of example scarcity than NBNN. And the existing methods usually pool the last convolutional feature maps (e.g., via the global average pooling or fully connected layer) to an image-level representation for the final classification. In this case, such an information loss will also occur and is unrecoverable.</p><p>Second, as further observed in <ref type="bibr" target="#b0">[1]</ref>, using the local invariant features of two images, instead of their image-level representations, to measure an image-to-image similarity for classification will still incur a poor result. This is because such an image-to-image similarity does not generalize beyond training samples. When the number of training samples is small, a query image could be different from any training samples of the same class due to intra-class variation or background clutter. Instead, an image-to-class measure should be used. Specifically, the local invariant features from all training samples in the same class are collected into one pool. This measure evaluates the proximity (e.g., via nearest-neighbor search) of the local features of a query image to the pool of each class for classification.</p><p>Again, this observation applies to few-shot learning. Essentially, the above image-to-class measure breaks the boundaries of training images in the same class, and uses their local features collectively to provide a richer and more flexible representation for a class. As indicated in <ref type="bibr" target="#b0">[1]</ref>, this setting can be justified by a fact that a new image can be roughly "composed" by using the pieces of other images in the same class (i.e., the exchangeability of visual patterns across the images in the same class).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Proposed DN4 Framework</head><p>The above analysis motivates us to review the way of the final classification in few-shot learning and reconsider the NBNN approach. This leads to the proposed framework Deep Nearest Neighbor Neural Network (DN4 in short). <ref type="figure">Figure 1</ref>, DN4 mainly consists of two components: a deep embedding module Ψ and an imageto-class measure module Φ. The former learns deep local descriptors for all images. With the learned descriptors, the latter calculates the aforementioned image-to-class measure. Importantly, these two modules are integrated into a unified network and trained in an end-to-end manner from scratch. Also, note that the designed image-to-class module can readily work with any deep embedding module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As illustrated in</head><p>Deep embedding module. The module Ψ routinely learns the feature representations for query and support images. Any proper CNN can be used. Note that Ψ only contains convolutional layers but has no fully connected layer, since we just need deep local descriptors to compute the image-to-class measure. In short, given an image X, Ψ(X) will be an h×w×d tensor, which can be viewed as a set of m (m = hw) d-dimensional local descriptors as</p><formula xml:id="formula_1">Ψ(X) = [x 1 , . . . , x m ] ∈ R d×m ,<label>(1)</label></formula><p>where x i is the i-th deep local descriptor. In our experiments, given an image with a resolution of 84 × 84, we can get h = w = 21 and d = 64. It means that each image has 441 deep local descriptors in total. Image-to-Class module. The module Φ uses the deep local descriptors from all training images in a class to construct a local descriptor space for this class. In this space, we calculate the image-to-class similarity (or distance) between a query image and this class via k-NN, as in <ref type="bibr" target="#b0">[1]</ref>.</p><p>Specifically, through the module Ψ, a given query image q will be embedded as</p><formula xml:id="formula_2">Ψ(q) = [x 1 , . . . , x m ] ∈ R d×m . For each descriptor x i , we find its k-nearest neighborsx j i | k j=1</formula><p>in a class c. Then we calculate the similarity between x i and eachx i , and sum the mk similarities as the image-toclass similarity between q and the class c. Mathematically, the image-to-class measure can be easily expressed as</p><formula xml:id="formula_3">Φ Ψ(q), c = m i=1 k j=1 cos(x i ,x j i ) cos(x i ,x i ) = x ix i x i · x i ,<label>(2)</label></formula><p>where cos(·) indicates the cosine similarity. Other similarity or distance functions can certainly be employed. Note that in terms of computational efficiency, the image-to-class measure seems more suitable for few-shot classification than the generic image classification focused in <ref type="bibr" target="#b0">[1]</ref>. The major computational issue in NBNN caused by searching for k-nearest neighbors from a huge pool of local descriptors has now been substantially weakened due to the much smaller number of training samples in few-shot setting. This makes the proposed framework computationally efficient. Furthermore, compared with NBNN, it will be more promising, by benefiting from the deep feature representations that are much more powerful than the handcrafted features used in NBNN.</p><p>Finally, it is worth mentioning that the image-to-class module in DN4 is non-parametric. So the entire classification model is non-parametric if not considering the embedding module Ψ. Since a non-parametric model does not involve parameter learning, the over-fitting issue in parametric few-shot learning methods (e.g., learning a fully connected layer over image-level representation) can also be mitigated to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Architecture</head><p>For fair comparison with the state-of-the-art methods, we take a commonly used four-layer convolutional neural network as the embedding module. It contains four convolutional blocks, each of which consists of a convolutional layer, a batch normalization layer and a Leaky ReLU layer. Besides, for the first two convolutional blocks, an additional 2×2 max-pooling layer is also appended, respectively. This embedding network is named Conv-64F, since there are 64 filters of size 3 × 3 in each convolutional layer. As for the image-to-class module, the only hyper-parameter is the parameter k, which will be discussed in the experiment.</p><p>At each iteration of the episodic training, we feed a support set S and a query image q into our model. Through the embedding module Ψ, we obtain all the deep local representations for all these images. Then via the module Φ, we calculate the image-to-class similarity between q and each class by Eq. <ref type="bibr" target="#b1">(2)</ref>. For a C-way K-shot task, we can get a similarity vector z ∈ R C . The class corresponding to the largest component of z will be the prediction for q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>The main goal of this section is to investigate two interesting questions: (1) How does the pre-trained deep features based NBNN without episodic training perform on the fewshot learning? (2) How does our proposed DN4 framework, i.e., a CNN based NBNN in an end-to-end episodic training manner, perform on the few-shot learning?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>task on this dataset, and take 70, 20 and 30 classes for training (auxiliary), validation and test, respectively.</p><p>Stanford Cars. This dataset <ref type="bibr" target="#b9">[10]</ref> is also a benchmark dataset for fine-grained classification task, which consists of 196 classes of cars with a total number of 16, 185 images. Similarly, 130, 17 and 49 classes in this dataset are split for training (auxiliary), validation and test.</p><p>CUB-200. This dataset <ref type="bibr" target="#b22">[23]</ref> contains 6033 images from 200 bird species. In a similar way, we select 130, 20 and 50 classes for training (auxiliary), validation and test.</p><p>For the last three fine-grained datasets, all the images in these datasets are resized to 84 × 84 as miniImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setting</head><p>All experiments are conducted around the C-way K-shot classification task on the above datasets. To be specific, 5-way 1-shot and 5-shot classification tasks will be conducted on all these datasets. During training, we randomly sample and construct 300, 000 episodes to train all of our models by employing the episodic training mechanism. In each episode, besides the K support images (shots) in each class, 15 and 10 query images will also be selected from each class for the 1-shot and 5-shot settings, respectively. In other words, for a 5-way 1-shot task, there will be 5 support images and 75 query images in one training episode. To train our model, we adopt the Adam algorithm <ref type="bibr" target="#b7">[8]</ref> with an initial learning rate of 1×10 −3 and reduce it by half of every 100, 000 episodes.</p><p>During test, we randomly sample 600 episodes from the test set, and take the top-1 mean accuracy as the evaluation criterion. This process will be repeated five times, and the final mean accuracy will be reported. Moreover, the 95% confidence intervals are also reported. Notably, all of our models are trained from scratch in an end-to-end manner, and do not need fine-tuning in the test stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison Methods</head><p>Baseline methods. To illustrate the basic classification performance on the above datasets, we implement a baseline method k-NN (Deep global features). Particularly, we adopt the basic embedding network Conv-64F and append three additional FC layers to train a classification network on the corresponding training (auxiliary) dataset. During test, we use this pre-trained network to extract features from the last FC layer and use a k-NN classifier to get the final classification results. Also, to answer the first question at the beginning of Section 4, we re-implement the NBNN algorithm <ref type="bibr" target="#b0">[1]</ref> by using the pre-trained Conv-64F truncated from the above k-NN (Deep global features) method. This new NBNN algorithm employing the deep local descriptors instead of the hand-crafted descriptors (i.e., SIFT), is called NBNN (Deep local features).</p><p>Metric-learning based methods. As our method belongs to the metric-learning branch, we mainly compare our model with four state-of-the-art metric-learning based models, including Matching Nets FCE <ref type="bibr" target="#b21">[22]</ref>, Prototypical Nets <ref type="bibr" target="#b16">[17]</ref>, Relation Net <ref type="bibr" target="#b24">[25]</ref> and Graph Neural Network (GNN) <ref type="bibr" target="#b3">[4]</ref>. Note that we re-run the GNN model by using the Conv-64F as its embedding module because the original GNN adopts a different embedding module Conv-256F, which also has four convolutional layers but with 64, 96, 128 and 256 filters for the corresponding layers, respectively. Also, we re-run the Prototypical Nets via the same 5-way training setting instead of the 20-way training setting in the original work for a fair comparison.</p><p>Meta-learning based methods. Besides the metriclearning based models, five state-of-the-art meta-learning based models are also picked for reference. These models include Meta-Learner LSTM <ref type="bibr" target="#b13">[14]</ref>, Model-agnostic Metalearning (MAML) <ref type="bibr" target="#b2">[3]</ref>, Simple Neural AttentIve Learner (SNAIL) <ref type="bibr" target="#b12">[13]</ref>, MM-Net <ref type="bibr" target="#b1">[2]</ref> and Dynamic-Net <ref type="bibr" target="#b4">[5]</ref>. As SNAIL adopts a much more complicated ResNet-256F (a smaller version of ResNet <ref type="bibr" target="#b5">[6]</ref>) as its embedding module, we will additionally report its results based on the Conv-32F provided in its appendix for a fair comparison. Note that Conv-32F has the same architecture with Conv-64F, but with 32 filters per convolutional layer, which has also been employed by Meta-Learner LSTM and MAML to reduce over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Few-shot Classification</head><p>The generic few-shot classification task is conducted on miniImageNet. The results are reported in <ref type="table">Table 1</ref>, where the hyper-parameter k is set as 3. From <ref type="table">Table 1</ref>, it is amazing to see that NBNN (Deep local features) can achieve much better results than k-NN (Deep global features), and it is even better than Matching Nets FCE, Meta-Learner LSTM and SNAIL (Conv-32F). This not only verifies that the local descriptors can perform better than the image-level features (i.e., FC layer features used by k-NN), but also shows that the image-to-class measure is truly promising. However, NBNN (Deep local features) still has a large performance gap compared with the state-of-the-art Prototypical Nets, Relation Net and GNN. The reason is that, as a lazy learning algorithm, NBNN (Deep local features) does not have a training stage and also lacks the episodic training. So far, the first question has been answered.</p><p>On the contrary, our proposed DN4 embeds the imageto-class measure into a deep neural network, and can learn the deep local descriptors jointly by employing the episodic training, which indeed obtains superior results. Compared with the metric-learning based models, our DN4 (Conv-64F) gains 7.68%, 2.22%, 2.79% and 0.8% improvements over Matching Nets FCE, GNN ‡ (Conv-64F), Prototypical Nets ‡ (i.e., via 5-way training setting) and Relation Net on the 5-way 1-shot classification task, respectively. On the 5-way 5-shot classification task, we can even get 15.71%, 7.52%, 4.49% and 5.7% significant improvements over these models. The reason is that these methods usually <ref type="table">Table 1</ref>. The mean accuracies of the 5-way 1-shot and 5-shot tasks on the miniImageNet dataset, with 95% confidence intervals. The second column refers to which kind of embedding module is employed, e.g., Conv-32F and Conv-64F etc. The third column denotes the type of this method, i.e., meta-learning based or metric-learning based. * Results reported by the original work. ‡ Results re-implemented in the same setting for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Embedding Type 5-Way Accuracy (%) use image-level features whose number is too small, while our DN4 adopts learnable deep local descriptors which are more abundant especially in the 5-shot setting. On the other hand, local descriptors enjoy the exchangeability characteristic, making the distribution of each class built upon the local descriptors more effective than the one built upon the image-level features. Therefore, the second question can also be answered.</p><p>To take a whole picture of the few-shot learning area, we also report the results of the state-of-the-art meta-learning based methods. We can see that our DN4 is still competitive with these methods. Especially in the 5-way 5-shot setting, our DN4 gains 15.82%, 10.42%, 7.91% and 4.05% improvements over SNAIL (Conv-32F), Meta-Learner LSTM, MAML and MM-Net, respectively. As for the Dynamic-Net, a two-stage model, it pre-trains its model with all classes together before conducting the few-shot training, while our DN4 does not. More importantly, our DN4 only has one single unified network, which is much simpler than these meta-learning based methods with additional complicated memory-addressing architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Fine-grained Few-shot Classification</head><p>Besides the generic few-shot classification, we also conduct fine-grained few-shot classification tasks on three finegrained datasets, i.e., Stanford Dogs, Stanford Cars and CUB-200. Two baseline models and three state-of-theart models are implemented on these three datasets, i.e., k-NN (Deep global features), NBNN (Deep local features), Matching Nets FCE <ref type="bibr" target="#b21">[22]</ref>, Prototypical Nets <ref type="bibr" target="#b16">[17]</ref> and GNN <ref type="bibr" target="#b3">[4]</ref>. The results are shown in <ref type="table" target="#tab_0">Table 2</ref>. In general, the fine-grained few-shot classification task is more challenging than the generic one due to the smaller inter-class and larger intra-class variations of the fine-grained datasets. It can be seen by comparing the performance of the same methods between Tables 1 and 2. The performance of the k-NN (Deep global features), NBNN (Deep local features) and Prototypical Nets on the fine-grained datasets is worse than that on miniImageNet. It can also be observed that NBNN (Deep local features) performs consistently better than k-NN (Deep global features).</p><p>Due to the small inter-class variation of the fine-grained task, we choose k = 1 for our DN4 to avoid introducing noisy visual patterns. From <ref type="table" target="#tab_0">Table 2</ref>, we can see that our DN4 performs surprisingly well on these datasets under the 5-shot setting. Especially on the Stanford Cars, our DN4 gains the largest absolute improvement over the second best method, i.e., GNN, by 17%. Under the 1-shot setting, our DN4 does not perform as well as in the 5-shot setting. The key reason is that our model relies on the k-nearest neighbor algorithm, which is a lazy learning algorithm and its performance depends largely on the number of samples. This characteristic has been shown in <ref type="table" target="#tab_2">Table 5</ref>, i.e., the performance of DN4 gets better and better as the number of shots increases. Another reason is that these fine-grained datasets are not sufficiently large (e.g., CUB-200 only has 6033 images), resulting in over-fitting when training deep networks.</p><p>To avoid over-fitting, we perform data augmentation on the training (auxiliary) sets by cropping and horizontally flipping randomly. Then, we re-train our model, i.e., DN4-DA, on these augmented datasets but test on the original test sets. It can be observed that our DN4-DA can obtain nearly the best results for both 1-shot and 5-shot tasks. The finegrained recognition largely relies on the subtle local visual patterns, and they can be naturally captured by the learnable deep local descriptors emphasized in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Discussion</head><p>Ablation study. To further verify that the image-to-class measure is more effective than the image-to-image measure, we perform an ablation study by developing two image-toimage (IoI for short) variants of DN4. Specifically, the first variant named DN4-IoI-1 concatenates all local descriptors of an image as a high-dimensional (h × w × d) feature vector and uses the image-to-image measure. As for the second variant (DN4-IoI-2 for short), it keeps the local descriptors like DN4 without concatenation. The only difference between DN4-IoI-2 and DN4 is that DN4-IoI-2 restricts the search for the k-NN of a query's local descriptor within each individual support image, while DN4 can search from one entire support class. Under the 1-shot setting, DN4-IoI-2 is identical with DN4. Both variants still adopt the k-NN search, and use k = 1 and k = 3 for 1-shot setting and 5-shot setting, respectively. The results on miniImageNet are reported in <ref type="table" target="#tab_1">Table 3</ref>. As seen, DN4-IoI-1 performs clearly the worst by using the concatenated global features with the image-to-image measure. In contrast, DN4-IoI-2 performs excellently on both 1-shot and 5-shot tasks, which verifies the importance of local descriptors and the exchangeability (within one image). Notably, DN4 is superior to DN4-IoI-2 on the 5-shot task, which shows that utilizing the exchangeability of visual patterns within a class indeed helps to gain performance.</p><p>Influence of backbone networks. Besides the commonly used Conv-64F, we also evaluate our model by using another deeper embedding module, i.e., ResNet-256F used by SNAIL <ref type="bibr" target="#b12">[13]</ref> and Dynamic-Net <ref type="bibr" target="#b4">[5]</ref>. The details of ResNet-256F can refer to SNAIL <ref type="bibr" target="#b12">[13]</ref>. When using ResNet-256F as the embedding module, the accuracy of DN4 reaches 54.37 ± 0.36% for the 5-way 1-shot task and 74.44 ± 0.29% for the 5-shot task. As seen, with a deeper backbone network, DN4 can perform better than the case of using the shallow Conv-64F. Moreover, when using the same ResNet-256F as the embedding module, our DN4 (ResNet-256F) can gain 4.31% improvements over Dynamic-Net (ResNet-256F) (i.e., 70.13 ± 0.68%) under the 5-shot setting (see <ref type="table">Table 1</ref>).</p><p>Influence of neighbors. In the image-to-class module, we need to find the k-nearest neighbors in one support class for each local descriptor of a query image. Next, we measure the image-to-class similarity between a query image and a specific class. How to choose a suitable hyperparameter k is thus a key. For this purpose, we perform a 5-way 5-shot task on miniImageNet by varying the value of k ∈ {1, 3, 5, 7}, and show the results in <ref type="table">Table 4</ref>. It can be seen that the value of k has a mild impact on the performance. Therefore, in our model, k should be selected according to the specific task.</p><p>Influence of shots. The episodic training mechanism is popular in current few-shot learning methods. The basic In other words, if we want to perform a 5-way 1-shot task, the same 5-way 1-shot setting should be maintained in the training stage. However, in the real training stage, we still want to know the influence of mismatching conditions, i.e., under-matching condition and over-matching condition. We find that the over-matching condition can achieve better performance than the matching condition, and much better than the under-matching condition. Basically, for the under-matching condition, we use a smaller number of shots in the training stage, and conversely, use a larger number of shots for the over-matching condition. We fix the number of ways but vary the number of shots during training to learn several different models. Then we test these models under different shot settings, where the number of shots is changed but the number of ways is fixed. A 5-way K-shot (K = 1, 2, 3, 4, 5) task is conducted on miniImageNet by using our DN4. The results are presented in <ref type="table" target="#tab_2">Table 5</ref>, where the entries on the diagonal are the results of the matching condition. The results in the upper triangle are the results of the under-matching condition. Also, the lower triangle contains the results of the over-matching condition. It can be seen that the results in the lower triangle are better than those on the diagonal, and the results on the diagonal are better than those in the upper triangle. This exactly verifies our statement made above. It is also worth mentioning that if we use a 5-shot trained model and test it on the 1-shot task, we can obtain an accuracy of 53.85%. This result is quite high in this task, and much better than 51.24% obtained by the 1-shot trained model using our DN4 under a matching condition.</p><p>Visualization. We visualize the similarity matrices learned by NBNN (Deep local features) and our DN4 under the 5-way 5-shot setting on miniImageNet. Both of them are image-to-class measure based models. We select 20 query images from each class (i.e., 100 query images in total), calculate the similarity between each query image and each class, and visualize the 5×100 similarity matrices. From <ref type="figure" target="#fig_1">Figure 2</ref>, it can be seen that the results of DN4 are much closer to the ground truth than those of NBNN, which demonstrates that the end-to-end manner is more effective. Runtime. Although NBNN performs successfully in the literature <ref type="bibr" target="#b0">[1]</ref>, it did not become popular. One key reason is the high computational complexity of the nearest-neighbor search, especially in large-scale image classification tasks. Fortunately, under the few-shot setting our framework can enjoy the excellent performance of NBNN without being significantly affected by its computational issue. Generally, during training for a 5-way 1-shot or 5-shot task, one episode (batch) time is 0.31s or 0.38s with 75 or 50 query images on a single Nvidia GTX 1080Ti GPU and a single Intel i7-3820 CPU. During test, it will be more efficient, and only takes 0.18s for one episode. Moreover, the efficiency of our model can be further improved with optimized parallel implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we revisit the local descriptor based imageto-class measure and propose a simple and effective Deep Nearest Neighbor Neural Network (DN4) for few-shot learning. We emphasize and verify the importance and value of the learnable deep local descriptors, which are more suitable than image-level features for the few-shot problem and can well boost the classification performance. We also verify that the image-to-class measure is superior to the image-to-image measure, owing to the exchangeability of visual patterns within a class.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Similarity matrices of NBNN (Deep local Features), our DN4 and the ground truth on miniImageNet under the 5-way 5shot setting. Vertical axis denotes the five classes in the support set. Horizontal axis denotes 20 query images per class. The warmer colors indicate higher similarities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>The mean accuracies of the 5-way 1-shot and 5-shot tasks on three fine-grained datasets, i.e., Stanford Dogs, Stanford Cars and CUB-200, with 95% confidence intervals. For each setting, the best and the second best methods are highlighted. 64F 45.41±0.76 63.51±0.62 59.84±0.80 88.65±0.44 46.84±0.81 74.92±0.64 Our DN4-DA (k=1) Conv-64F 45.73±0.76 66.33±0.66 61.51±0.85 89.60±0.44 53.15±0.84 81.90±0.60</figDesc><table><row><cell>1-shot</cell><cell>5-shot</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>The results of the ablation study on miniImageNet.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">5-Way Accuracy (%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1-shot</cell><cell cols="2">5-shot</cell></row><row><cell>DN4-IoI-1</cell><cell></cell><cell>37.39±0.82</cell><cell cols="2">50.47±0.66</cell></row><row><cell>DN4-IoI-2</cell><cell></cell><cell>51.14±0.79</cell><cell cols="2">69.52±0.62</cell></row><row><cell>DN4</cell><cell cols="2">51.24±0.74</cell><cell cols="2">71.02±0.64</cell></row><row><cell cols="5">Table 4. The 5-way 5-shot mean accuracy (%) of our DN4 by vary-</cell></row><row><cell cols="5">ing the value of k ∈ {1, 3, 5, 7} during training on miniImageNet.</cell></row><row><cell>Model</cell><cell cols="3">5-way 5-shot Accuracy (%)</cell><cell></cell></row><row><cell></cell><cell>k = 1</cell><cell>k = 3</cell><cell>k = 5</cell><cell>k = 7</cell></row><row><cell>DN4</cell><cell>71.95</cell><cell>71.02</cell><cell>70.20</cell><cell>68.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>The 5-way K-shot mean accuracy (%) of our DN4 by varying the number of shots (K = 1, 2, 3, 4, 5) during training on miniImageNet. For each test setting, the best result is highlighted.</figDesc><table><row><cell>Train</cell><cell></cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell>1-shot</cell><cell>2-shot</cell><cell>3-shot</cell><cell>4-shot</cell><cell>5-shot</cell></row><row><cell cols="5">rule is the matching condition between training and test. It</cell></row><row><cell cols="5">means that, in the training stage, the numbers of ways and</cell></row><row><cell cols="5">shots should keep consistent with those adopted in the test</cell></row><row><cell>stage.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">We conduct all the experiments on four benchmark datasets as follows.miniImageNet. As a mini-version of ImageNet<ref type="bibr" target="#b14">[15]</ref>, this dataset<ref type="bibr" target="#b21">[22]</ref> contains 100 classes with 600 images per class, and has a resolution of 84 × 84 for each image. Following the splits used in<ref type="bibr" target="#b13">[14]</ref>, we take 64, 16 and 20 classes for training (auxiliary), validation and test, respectively.Stanford Dogs. This dataset<ref type="bibr" target="#b6">[7]</ref> is originally used for the task of fine-grained image classification, including 120 breeds (classes) of dogs with a total number of 20, 580 images. Here, we conduct fine-grained few-shot classification</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is partially supported by the NSF awards </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">In defense of nearest-neighbor based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory matching networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4080" to="4088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distribution consistency based covariance metric networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-H</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to learn: Introduction and overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Few-shot learning through an information retrieval lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2255" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A perspective view and survey of meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vilalta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Drissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="95" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno>1410.3916</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning to compare: Relation network for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
