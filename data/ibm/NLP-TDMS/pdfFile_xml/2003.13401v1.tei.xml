<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Context Based Emotion Recognition using EMOTIC Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronak</forename><surname>Kosti</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Recasens</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Context Based Emotion Recognition using EMOTIC Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Emotion recognition</term>
					<term>Affective computing</term>
					<term>Pattern recognition !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In our everyday lives and social interactions we often try to perceive the emotional states of people. There has been a lot of research in providing machines with a similar capacity of recognizing emotions. From a computer vision perspective, most of the previous efforts have been focusing in analyzing the facial expressions and, in some cases, also the body pose. Some of these methods work remarkably well in specific settings. However, their performance is limited in natural, unconstrained environments. Psychological studies show that the scene context, in addition to facial expression and body pose, provides important information to our perception of people's emotions. However, the processing of the context for automatic emotion recognition has not been explored in depth, partly due to the lack of proper data. In this paper we present EMOTIC, a dataset of images of people in a diverse set of natural situations, annotated with their apparent emotion. The EMOTIC dataset combines two different types of emotion representation: (1) a set of 26 discrete categories, and (2) the continuous dimensions Valence, Arousal, and Dominance. We also present a detailed statistical and algorithmic analysis of the dataset along with annotators' agreement analysis. Using the EMOTIC dataset we train different CNN models for emotion recognition, combining the information of the bounding box containing the person with the contextual information extracted from the scene. Our results show how scene context provides important information to automatically recognize emotional states and motivate further research in this direction. Dataset and Code is open-sourced and available on https://github.com/rkosti/emotic. Link for the published article https://ieeexplore.ieee.org/document/8713881.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>O Ver the past years, the interest in developing automatic systems for recognizing emotional states has grown rapidly. We can find several recent works showing how emotions can be inferred from cues like text <ref type="bibr" target="#b0">[1]</ref>, voice <ref type="bibr" target="#b1">[2]</ref>, or visual information <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The automatic recognition of emotions has a lot of applications in environments where machines need to interact or monitor humans. For instance, automatic tutors in an online learning platform would provide better feedback to a student according to her level of motivation or frustration. Also, a car with the capacity of assisting a driver can intervene or give an alarm if it detects the driver is tired or nervous.</p><p>In this paper we focus on the problem of emotion recognition from visual information. Concretely, we want to recognize the apparent emotional state of a person in a given image. This problem has been broadly studied in computer vision mainly from two perspectives: (1) facial expression analysis, and (2) body posture and gesture analysis. Section 2 gives an overview of related work on these perspectives and also on some of the common public datasets for emotion recognition.</p><p>Although face and body pose give lot of information on the affective state of a person, our claim in this work is that scene context information is also a key component for understanding emotional states. <ref type="bibr">Scene</ref>   surroundings of the person, like the place category, the place attributes, the objects, or the actions occurring around the person. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the importance of scene context for emotion recognition. When we just see the kid it is difficult to recognize his emotion (from his facial expression it seems he is feeling Surprise). However, when we see the context ( <ref type="figure" target="#fig_1">Fig. 2</ref>.a) we see the kid is celebrating his birthday, blowing the candles, probably with his family or friends at home. With this additional information we can interpret much better his face and posture and recognize that he probably feels engaged, happy and excited.</p><p>The importance of context in emotion perception is well supported by different studies in psychology <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. In general situations, facial expression is not sufficient to determine the emotional state of a person, since the perception of the emotion is heavily influenced by different types of context, including the scene context <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this work, we present two main contributions. Our first contribution is the creation and publication of the EMOTIC (from EMOTions In Context) Dataset. The EMOTIC database is a collection of images of people annotated according to their apparent emotional states. Images are spontaneous and unconstrained, showing people doing different things in different environments. <ref type="figure" target="#fig_1">Fig. 2</ref> shows some examples of images in the EMOTIC database along with their corresponding annotations. As shown, annotations combine 2 different types of emotion representation: Discrete Emotion Categories and 3 Continuous Emotion Dimensions Valence, Arousal, and Dominance <ref type="bibr" target="#b6">[7]</ref>. The EMOTIC dataset is now publicly available for download at the EMOTIC website <ref type="bibr" target="#b0">1</ref> . Details of the dataset construction process and dataset statistics can be found in section 3.</p><p>Our second contribution is the creation of a baseline system for the task of emotion recognition in context. In particular, we present and test a Convolutional Neural Network (CNN) model that jointly processes the window of the person and the whole image to predict the apparent emotional state of the person. Section 4 describes the CNN model and the implementation details while section 5 presents our experiments and discussion on the results. All the trained models resulting from this work are also publicly available at the EMOTIC website 1 .</p><p>This paper is an extension of the conference paper "Emotion Recognition in Context", presented at the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR) 2017 <ref type="bibr" target="#b7">[8]</ref>. We present here an extended version of the EMOTIC dataset, with further statistical dataset analysis, an analysis of scene-centric algorithms on the data, and a study on the annotation consistency among different annotators. This new release of the EMOTIC database contains 44.4% more annotated people as compared to its previous smaller version. With the new extended dataset we retrained all the proposed baseline CNN models with additional loss 1. http://sunai.uoc.edu/emotic/ functions. We also present comparative analysis of two different scene context features, showing how the context is contributing to recognize emotions in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Emotion recognition has been broadly studied by the Computer Vision community. Most of the existing work has focused on the analysis of facial expression to predict emotions <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. The base of these methods is the Facial Action Coding System <ref type="bibr" target="#b10">[11]</ref>, which encodes the facial expression using a set of specific localized movements of the face, called Action Units. These facial-based approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> usually use facial-geometry based features or appearance features to describe the face. Afterwards, the extracted features are used to recognize Action Units and the basic emotions proposed by Ekman and Friesen <ref type="bibr" target="#b11">[12]</ref>: anger, disgust, fear, happiness, sadness, and surprise. Currently, state-of-the-art systems for emotion recognition from facial expression analysis use CNNs to recognize emotions or Action Units <ref type="bibr" target="#b12">[13]</ref>.</p><p>In terms of emotion representation, some recent works based on facial expression <ref type="bibr" target="#b13">[14]</ref> use the continuous dimensions of the V AD Emotional State Model <ref type="bibr" target="#b6">[7]</ref>. The VAD model describes emotions using 3 numerical dimensions: Valence (V), that measures how positive or pleasant an emotion is, ranging from negative to positive; Arousal (A), that measures the agitation level of the person, ranging from non-active / in calm to agitated / ready to act; and Dominance (D) that measures the level of control a person feels of the situation, ranging from submissive / non-control to dominant / in-control. On the other hand, Du et al. <ref type="bibr" target="#b14">[15]</ref> proposed a set of 21 facial emotion categories, defined as different combinations of the basic emotions, like 'happily surprised' or 'happily disgusted'. With this categorization the authors can give a fine-grained detail about the expressed emotion.</p><p>Although the research in emotion recognition from a computer vision perspective is mainly focused in the analysis of the face, there are some works that also consider other additional visual cues or multimodal approaches. For instance, in <ref type="bibr" target="#b15">[16]</ref> the location of shoulders is used as additional information to the face features to recognize basic emotions. More generally, Schindler et al. <ref type="bibr" target="#b16">[17]</ref> used the body pose to recognize 6 basic emotions, performing experiments on a small dataset of non-spontaneous poses acquired under controlled conditions. Mou et al. <ref type="bibr" target="#b17">[18]</ref> presented a system of affect analysis in still images of groups of people, recognizing group-level arousal and valence from combining face, body and contextual information.</p><p>Emotion Recognition in Scene Context and Image Sentiment Analysis are different problems that share some characteristics. Emotion Recognition aims to identify the emotions of a person depicted in an image. Image Sentiment Analysis consists of predicting what a person will feel when observing a picture. This picture does not necessarily contain a person. When an image contains a person, there can be a difference between the emotions experienced by the person in the image and the emotions felt by observers of the image. For example, in the image of <ref type="figure" target="#fig_1">Figure 2</ref>.b, we see a kid who seems to be annoyed for having an apple instead of chocolate and another who seems happy to have chocolate. However, as observers, we might not have any of those sentiments when looking at the photo. Instead, we might think the situation is not fair and feel disapproval. Also, if we see an image of an athlete that has lost a match, we can recognize the athlete feels sad. However, an observer of the image may feel happy if the observer is a fan of the team that won the match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Emotion Recognition Datasets</head><p>Most of the existing datasets for emotion recognition using computer vision are centered in facial expression analysis. For example, the GENKI database <ref type="bibr" target="#b18">[19]</ref> contains frontal face images of a single person with different illumination, geographic, personal and ethnic settings. Images in this dataset are labelled as smiling or non-smiling. Another common facial expression analysis dataset is the ICML Face-Expression Recognition dataset <ref type="bibr" target="#b19">[20]</ref>, that contains 28, 000 images annotated with 6 basic emotions and a neutral category. On the other hand, the UCDSEE dataset <ref type="bibr" target="#b20">[21]</ref> has a set of 9 emotion expressions acted by 4 persons. The lab setting is strictly kept the same in order to focus mainly on the facial expression of the person.</p><p>The dynamic body movement is also an essential source for estimating emotion. Studies such as <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> establish the relationship between affect and body posture using as ground truth the base-rate of human observers. The data consist of a spontaneous set of images acquired under a restrictive setting (people playing Wii games). The GEMEP database <ref type="bibr" target="#b23">[24]</ref> is multi-modal (audio and video) and has 10 actors playing 18 affective states. The dataset has videos of actors showing emotions through acting. Body pose and facial expression are combined.</p><p>The Looking at People (LAP) challenges and competitions <ref type="bibr" target="#b24">[25]</ref> involve specialized datasets containing images, sequences of images and multi-modal data. The main focus of these datasets is the complexity and variability of human body configuration which include data related to personality traits (spontaneous), gesture recognition (acted), apparent age recognition (spontaneous), cultural event recognition (spontaneous), action/interaction recognition and human pose recognition (spontaneous).</p><p>The Emotion Recognition in the Wild (EmotiW) challenges <ref type="bibr" target="#b25">[26]</ref> host 3 databases: (1) The AFEW database <ref type="bibr" target="#b26">[27]</ref> focuses on emotion recognition from video frames taken from movies and TV shows, where the actions are annotated with attributes like name, age of actor, age of character, pose, gender, expression of person, the overall clip expression and the basic 6 emotions and a neutral category; <ref type="bibr" target="#b1">(2)</ref> The SFEW, which is a subset of AFEW database containing images of face-frames annotated specifically with the 6 basic emotions and a neutral category; and (3) the HAPPEI database <ref type="bibr" target="#b27">[28]</ref>, which addresses the problem of group level emotion estimation. Thus, <ref type="bibr" target="#b27">[28]</ref> offers a first attempt to use context for the problem of predicting happiness in groups of people.</p><p>Finally, the COCO dataset has been recently annotated with object attributes <ref type="bibr" target="#b28">[29]</ref>, including some emotion categories for people, such as happy and curious. These attributes show some overlap with the categories that we define in this paper. However, COCO attributes are not intended to be exhaustive for emotion recognition, and not all the people in the dataset are annotated with affect attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EMOTIC DATASET</head><p>The EMOTIC dataset is a collection of images of people in unconstrained environments annotated according to their apparent emotional states. The dataset contains 23, 571 images and 34, 320 annotated people. Some of the images were manually collected from the Internet by Google search engine. For that we used a combination of queries containing various places, social environments, different activities and a variety of keywords on emotional states. The rest of images belong to 2 public benchmark datasets: COCO <ref type="bibr" target="#b29">[30]</ref> and Ade20k <ref type="bibr" target="#b30">[31]</ref>. Overall, the images show a wide diversity of contexts, containing people in different places, social settings, and doing different activities. <ref type="figure" target="#fig_1">Fig. 2</ref> shows three examples of annotated images in the EMOTIC dataset. Images were annotated using Amazon Mechanical Turk (AMT). Annotators were asked to label each image according to what they think people in the images are feeling. Notice that we have the capacity of making reasonable guesses about other people's emotional state due to our capacity of being empathetic, putting ourselves into another's situation, and also because of our common sense knowledge and our ability for reasoning about visual information. For example, in <ref type="figure" target="#fig_1">Fig. 2</ref>.b, the person is performing an activity that requires Anticipation to adapt to the trajectory. Since he is doing a thrilling activity, he seems excited about it and he is engaged or focused in this activity. In <ref type="figure" target="#fig_1">Fig. 2</ref>.c, the kid feels a strong desire (yearning) for eating the chocolate instead of the apple. Because of his situation we can interpret his facial expression as disquietness and annoyance. Notice that images are also annotated according to the continuous dimensions V alence, Arousal, and Dominance. We describe the emotion annotation modalities of EMOTIC dataset and the annotation process in sections 3.1 and 3.2, respectively.</p><p>After the first round of annotations (1 annotator per image), we divided the images into three sets: Training (70%), Validation (10%), and Testing (20%) maintaining a similar affective category distribution across the different sets. After that, Validation and Testing were annotated by 4 and 2 extra annotators respectively. As a consequence, images in the Validation set are annotated by a total of 5 annotators, while images in the Testing set are annotated by 3 annotators (these numbers can slightly vary for some images since we removed noisy annotations).</p><p>We used the annotations from the Validation to study the consistency of the annotations across different annotators. This study is shown in section 3.3. The data statistics and algorithmic analysis on the EMOTIC dataset are detailed in sections 3.4 and 3.5 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Emotion representation</head><p>The EMOTIC dataset combines two different types of emotion representation:</p><p>Continuous Dimensions: images are annotated according to the V AD model <ref type="bibr" target="#b6">[7]</ref>, which represents emotions by a combination of 3 continuous dimensions: Valence, Arousal and Dominance. In our representation each dimension takes an integer value that lies in the range <ref type="bibr">[1 − 10]</ref>. <ref type="figure" target="#fig_3">Fig. 4</ref> shows examples of people annotated by different values of the given dimension.  <ref type="table" target="#tab_2">Table 1</ref>). The person in the red bounding box is annotated by the corresponding category. Emotion Categories: in addition to VAD we also established a list of 26 emotion categories that represent various state of emotions. The list of the 26 emotional categories and their corresponding definitions can be found in <ref type="table" target="#tab_2">Table  1</ref>. Also, <ref type="figure">Fig</ref> The list of emotion categories has been created as follows. We manually collected an affective vocabulary from dictionaries and books on psychology <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. This vocabulary consists of a list of approximately 400 words representing a wide variety of emotional states. After a careful study of the definitions and the similarities amongst these definitions, we formed cluster of words with similar meanings. The clusters were formalized into 26 categories such that they were distinguishable in a single image of a person with her context. We created the final list of 26 emotion categories taking into account the Visual Separability criterion: words that have a close meaning could not be visually separable. For instance, Anger is defined by 1. Affection: fond feelings; love; tenderness 2. Anger: intense displeasure or rage; furious; resentful 3. Annoyance: bothered by something or someone; irritated; impatient; frustrated 4. Anticipation: state of looking forward; hoping on or getting prepared for possible future events 5. Aversion: feeling disgust, dislike, repulsion; feeling hate 6. Confidence: feeling of being certain; conviction that an outcome will be favorable; encouraged; proud 7. Disapproval: feeling that something is wrong or reprehensible;  the words rage, furious and resentful. These affective states are different, but it is not always possible to separate them</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emotion Category</head><p>Continuous Dimension "Consider each emotion category separately and, if it is applicable to the person in the given context, select that emotion category" "Consider each emotion dimension separately, observe what level is applicable to the person in the given context, and select that level" Notice that the final list of affective categories also includes the 6 basic emotions (categories 2, 5, 16, 17, 21, 24), but we used the more general term Aversion for the category Disgust. Thus, the category Aversion also includes the subcategories dislike, repulsion, and hate apart from disgust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Collecting Annotations</head><p>We used Amazon Mechanical Turk (AMT) crowd-sourcing platform to collect the annotations of the EMOTIC dataset. We designed two Human Intelligence Tasks (HITs), one for each of the 2 formats of emotion representation. The two annotation interfaces are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. Each annotator is shown a person-in-context enclosed in a red bounding-box along with the annotation format next to it. <ref type="figure" target="#fig_5">Fig. 5</ref>.a shows the interface for discrete category annotation while <ref type="figure" target="#fig_5">Fig. 5</ref>.b displays the interface for continuous dimension annotation. Notice that, in the last box of the continuous dimension interface, we also ask AMT workers to annotate the gender and estimate the age (range) of the person enclosed in red bounding-box. The designing of the annotation interface has two main focuses: i) the task is easy to understand and ii) the interface fits the HIT in one screen which avoids scrolling.</p><p>To make sure annotators understand the task, we showed them how to annotate the images step-wise, by explaining two examples in detail. Also, instructions and examples were attached at the bottom on each page as a quick reference to the annotator. Finally, a summary of the detailed instructions was shown at the top of each page ( <ref type="table" target="#tab_3">Table 2)</ref>. We adopted two strategies to avoid noisy annotations in the EMOTIC dataset. First, we conduct a qualification task to annotator candidates. This qualification task has two parts: (i) an Emotional Quotient HIT (based on standard EQ task <ref type="bibr" target="#b35">[36]</ref>) and (ii) 2 sample image annotation tasks -one for each of our 2 emotion representations (discrete categories and continuous dimensions). For the sample annotations, we had a set of acceptable labels. The responses of the annotator candidates to this qualification task were evaluated and those who responded satisfactorily were allowed to annotate the images from the EMOTIC dataset. The second strategy to avoid noisy annotations was to insert, randomly, 2 control images in every annotation batch of 20 images; the correct assortment of labels for the control images was know beforehand. Annotators selecting incorrect labels on these control images were not allowed to annotate further and their annotations were discarded.</p><formula xml:id="formula_0">a) b) Back</formula><p>Go to Next Image (Image 1 of 20)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Peace</head><p>(fond feelings/tenderness/love/compassion) Expectation (state of anticipating/hoping on something or someone) Esteem (favorable opinion or judgment/gratefulness/admiration/respect) <ref type="bibr">(</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Agreement Level Among Different Annotators</head><p>Since emotion perception is a subjective task, different people can perceive different emotions after seeing the same image. For example in both <ref type="figure" target="#fig_7">Fig. 6</ref>.a and 6.b, the person in the red box seems to feel Affection, Happiness and Pleasure and the annotators have annotated with these categories with consistency. However, not everyone has selected all these emotions. Also, we see that annotators do not agree in the emotions Excitement and Engagement. We consider, however, that these categories are reasonable in this situation. Another example is that of Roger Federer hitting a tennis ball in <ref type="figure" target="#fig_7">Fig. 6</ref>.c. He is seen predicting the ball (or Anticipating) and clearly looks Engaged in the activity. He also seems Confident in getting the ball. After these observations we conducted different quantitative analysis on the annotation agreement. We focused first on analyzing the agreement level in the category annotation. Given a category assigned to a person in an image, we consider as an agreement measure the number of annotators agreeing for that particular category. Accordingly, we calculated, for each category and for each annotation in the validation set, the agreement amongst the annotators and sorted those values across categories. <ref type="figure" target="#fig_8">Fig. 7</ref> shows the distribution on the percentage of annotators agreeing for an annotated category across the validation set.</p><p>We also computed the agreement between all the annotators for a given person using Fleiss' Kappa (κ). Fleiss' Kappa is a common measure to evaluate the agreement level among a fixed number of annotators when assigning categories to data. In our case, given a person to annotate, there is a subset of 26 categories. If we have N annotators    per image, that means that each of the 26 categories can be selected by n annotators, where 0 ≤ n ≤ N . Given an image we compute the Fleiss' Kappa per each emotion category first, and then the general agreement level on this image is computed as the average of these Fleiss' Kappa values across the different emotion categories. We obtained that more than 50% of the images have κ &gt; 0.30. <ref type="figure" target="#fig_9">Fig. 8</ref>.a shows the distribution of kappa values across the validation set for all the annotated people in the validation set, sorted in decreasing order. Random annotations or total disagreement produces κ ∼ 0, however for our case, κ ∼ 0.3 (on average) suggesting significant agreement level even though the task of emotion recognition is subjective. For continuous dimensions, the agreement is measured by the standard deviation (SD) of the different annotations. The average SD across the Validation set is 1.04, 1.57 and 1.84 for Valence, Arousal and Dominance respectively -indicating that Dominance has higher (±1.84) dispersion than the other dimensions. It reflects that annotators disagree more often for Dominance than for the other dimensions which is understandable since Dominance is more difficult to interpret than Valence or Arousal <ref type="bibr" target="#b6">[7]</ref>. As a summary, <ref type="figure" target="#fig_9">Fig.  8</ref>.b shows the standard deviations of all the images in the   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dataset Statistics</head><p>EMOTIC dataset contains 34, 320 annotated people, where 66% of them are males and 34% of them are females. There are 10% children, 7% teenagers and 83% adults amongst them. <ref type="figure" target="#fig_10">Fig. 9</ref>.a shows the number of annotated people for each of the 26 emotion categories, sorted by decreasing order. Notice that the data is unbalanced, which makes the dataset particularly challenging. An interesting observation is that there are more examples for categories associated to positive emotions, like Happiness or Pleasure, than for categories associated with negative emotions, like Pain or Embarrassment. The category with most examples is Engagement. This is because in most of the images people are doing something or are involved in some activity, showing some degree of engagement. <ref type="figure" target="#fig_10">Figs. 9.b, 9</ref>.c and 9.d show the number of annotated people for each value of the 3 continuous dimensions. In this case we also observe unbalanced data but fairly distributed across the 3 dimensions which is good for modelling.    <ref type="figure" target="#fig_0">Fig. 10</ref>: Co-variance between 26 emotion categories. Each row represents the occurrence probability of every other category given the category of that particular row. <ref type="figure" target="#fig_0">Fig. 10</ref> shows the co-occurrence rates of any two categories. Every value in the matrix (r, c) (r represents the row category and c column category) is a co-occurrence probability (in %) of category r if the annotation also contains the category c, that is, P (r|c). We observe, for instance, that when a person is labelled with the category Annoyance, then there is 46.05% probability that this person is also annotated by the category Anger. This means that when a person seems to be feeling Annoyance it is likely (by 46.05%) that this person might also be feeling Anger. We also used a K-Means clustering on the category annotations to find groups of categories that occur frequently. We found, for example, that these category groups are common in the EMOTIC annotations: {Anticipation, Engagement, Confidence}, {Affection, Happiness, Pleasure}, {Doubt/Confusion, Disapproval, Annoyance}, {Yearning, Annoyance, Disquietment}. <ref type="figure" target="#fig_0">Fig. 11</ref> shows the distribution of each continuous dimension across the different emotion categories. For every plot, categories are arranged in increasing order of their average values of the given dimension (calculated for all the instances containing that particular category). Thus, we observe from <ref type="figure" target="#fig_0">Fig. 11</ref>.a that emotion categories like Suffering, Annoyance, Pain correlate with low Valence values (feeling less positive) in average whereas emotion categories like Pleasure, Happiness, Affection correlate with higher Valence values (feeling more positive). Also interesting is to note that a category like Disconnection lies in the mid-range of Valence value which makes sense. When we observe <ref type="figure" target="#fig_0">Fig. 11</ref>.b, it is easy to understand that emotional categories like Disconnection, Fatigue, Sadness show low Arousal values and we see high activeness for emotion categories like Anticipation, Confidence, Excitement. Finally, <ref type="figure" target="#fig_0">Fig. 11</ref>.c shows that people are not in control when they show emotion categories like Suffering, Pain, Sadness whereas when the Dominance is high, emotion categories like Esteem, Excitement, Confidence occur more often.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cooccurence of Categories</head><p>An important remark about the EMOTIC dataset is that there are people whose faces are not visible. More than 25% of the people in EMOTIC have their faces partially occluded or with very low resolution, so we can not rely on facial expression analysis for recognizing their emotional state. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Algorithmic Scene Context Analysis</head><p>This section illustrates how current scene-centric systems can be used to extract contextual information that can be potentially useful for emotion recognition. In particular, we illustrate this idea with a CNN trained on Places dataset <ref type="bibr" target="#b36">[37]</ref> and with the Sentibanks Adjective-Noun Pair (ANP) detectors <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, a Visual Sentiment Ontology for Image Sentiment Analysis. As a reference, <ref type="figure" target="#fig_0">Fig. 12</ref> shows Places and ANP outputs for sample images of the EMOTIC dataset. We used AlexNet Places CNN <ref type="bibr" target="#b36">[37]</ref> to predict the scene category and scene attributes for the images in EMOTIC. This information helps to divide the analysis into place category and place attribute. We observed that the distribution of emotions varies significantly among different place categories. For example, we found that people in the 'ski slope' frequently experience Anticipation or Excitement, which are associated to the activities that usually happen in this place category. Comparing sport-related and workingenvironment related images, we find that people in sport-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Places CNN output</head><p>Sentibanks NAP (score </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Place Category: land ll</head><p>Attributes: natural_light, open_area, dirt, sunny, no_horizon, rugged scene, dry, foliage, trees. <ref type="figure" target="#fig_0">Fig. 12</ref>: Illustration of 2 current scene-centric methods for extracting contextual features from the scene: AlexNet Places CNN outputs (place categories and attributes) and Sentibanks ANP outputs for three example images of the EMOTIC dataset. related images usually show Excitement, Anticipation and Confidence, however they show Sadness or Annoyance less frequently. Interestingly, Sadness and Annoyance appear with higher frequency in working environments. We also observe interesting patterns when correlating continuous dimensions with place attributes and categories. For instance, places where people usually show high Dominance are sport-related places and sport-related attributes. On the contrary, low Dominance is shown in 'jail cell' or attributes like 'enclosed area' or 'working', where the freedom of movement is restricted. In <ref type="figure" target="#fig_0">Fig. 12</ref>, the predictions by Places CNN describe the scene in general, like in the top image there is a girl sitting in a 'kindergarten classroom' (places category) which usually is situated in enclosed areas with 'no horizon' (attributes).</p><p>We also find interesting patterns when we compute the correlation between detected ANPs and emotions labelled in the image. For example, in images with people labelled with Affection, the most frequent ANP is 'young couple', while in images with people labelled with Excitement we found frequently the ANPs 'last game' and 'playing field'. Also, we observe a high correlation between images with Peace and ANP like 'old couple' and 'domestic scenes', and between Happiness and the ANPs 'outdoor wedding', 'outdoor activities', 'happy family' or 'happy couple'.</p><p>Overall, these observations suggest that some common sense knowledge patterns related with emotions and context could be potentially extracted, automatically, from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CNN MODEL FOR EMOTION RECOGNITION IN SCENE CONTEXT</head><p>We propose a baseline CNN model for the problem of emotion recognition in context. The pipeline of the model is shown in <ref type="figure" target="#fig_0">Fig. 13</ref> and it is divided in three modules: body feature extraction, image (context) feature extraction and fusion network. The first module takes the whole image as input and generates scene-related features. The second module takes the visible body of the person and generates bodyrelated features. Finally, the third module combines these features to do a fine-grained regression of the two types of emotion representations (section 3.1). <ref type="figure" target="#fig_0">Fig. 13</ref>: Proposed end-to-end model for emotion recognition in context. The model consists of two feature extraction modules and a fusion network for jointly estimating the discrete categories and the continuous dimensions.</p><p>The body feature extraction module takes the visible part of the body of the target person as input and generates body-related features. These features include important cues like face and head aspects and pose or body appearance. In order to capture these aspects, this module is pre-trained with ImageNet <ref type="bibr" target="#b39">[40]</ref>, which is an object centric dataset that includes the category person.</p><p>The image feature extraction module takes the whole image as input and generates scene-context features. These contextual features can be interpreted as an encoding of the scene category, its attributes and objects present in the scene, or the dynamics between other people present in the scene. To capture these aspects, we pre-train this module with the scene-centric Places dataset <ref type="bibr" target="#b36">[37]</ref>.</p><p>The fusion module combines features of the two feature extraction modules and estimates the discrete emotion categories and the continuous emotion dimensions.</p><p>Both feature extraction modules are based on the onedimensional filter CNN proposed in <ref type="bibr" target="#b40">[41]</ref>. These CNN networks provide competitive performance while the number of parameters is low. Each network consists of 16 convolutional layers with 1-dimensional kernels alternating between horizontal and vertical orientations, effectively modeling 8 layers using 2-dimensional kernels. Then, to maintain the location of different parts of the image, we use a global average pooling layer to reduce the features of the last convolutional layer. To avoid internal-covariant-shift we add a batch normalizing layer <ref type="bibr" target="#b41">[42]</ref> after each convolutional layer and rectifier linear units to speed up the training.</p><p>The fusion network module consists of two fully connected (FC) layers. The first FC layer is used to reduce the dimensionality of the features to 256 and then, a second fully connected layer is used to learn independent representations for each task <ref type="bibr" target="#b42">[43]</ref>. The output of this second FC layer branches off into 2 separate representations, one with 26 units representing the discrete emotion categories, and second with 3 units representing the 3 continuous dimensions (section 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Loss Function and Training Setup</head><p>We define the loss function as a weighted combination of two separate losses. A predictionŷ is composed by the prediction of each of the 26 discrete categories and the 3 continuous dimensions,ŷ = (ŷ disc ,ŷ cont ). In particular,  . Given a predictionŷ, the loss in this prediction is defined by L = λ disc L disc + λ cont L cont , where L disc and L cont represent the loss corresponding to learning the discrete categories and the continuous dimensions respectively. The parameters λ (disc,cont) weight the contribution of each loss and are set empirically using the validation set.</p><p>Criterion for Discrete categories (L disc ): The discrete category estimation is a multilabel problem with an inherent class imbalance issue, as the number of training examples is not the same for each class (see <ref type="figure" target="#fig_10">Fig 9.a)</ref>.</p><p>In our experiments, we use a weighted Euclidean loss for the discrete categories. Empirically, we found the Euclidean loss to be more effective than using Kullback−Leibler divergence or a multi-class multi-classification hinge loss. More precisely, given a predictionŷ disc , the weighted Euclidean loss is defined as follows</p><formula xml:id="formula_1">L 2disc (ŷ disc ) = 26 i=1 w i (ŷ disc i − y disc i ) 2<label>(1)</label></formula><p>whereŷ disc i is the prediction for the i-th category and y disc i is the ground-truth label. The parameter w i is the weight assigned to each category. Weight values are defined as w i = 1 ln(c+pi) , where p i is the probability of the i-th category and c is a parameter to control the range of valid values for w i . Using this weighting scheme the values of w i are bounded as the number of instances of a category approach to 0. This is particularly relevant in our case as we set the weights based on the occurrence of each category for each batch. Experimentally, we obtained better results using this approach compared to setting the global weights based on the entire dataset.</p><p>Criterion for Continuous dimensions (L cont ): We model the estimation of the continuous dimensions as a regression problem. Due to multiple annotators annotating the data based on subjective evaluation, we compare the performance when using two different robust losses: (1) a margin Euclidean loss L 2cont , and (2) the Smooth L 1 SL 1cont . The former defines a margin of error (v k ) when computing the loss for which the error is not considered. The margin Euclidean loss for continuous dimension is defined as:</p><formula xml:id="formula_2">L 2cont (ŷ cont ) = 3 k=1 v k (ŷ cont k − y cont k ) 2 ,<label>(2)</label></formula><p>whereŷ cont k and y cont k are the prediction and the groundtruth for the k-th dimension, respectively, and v k ∈ {0, 1} is a binary weight to represent the error margin</p><formula xml:id="formula_3">. v k = 0 if |ŷ cont k − y cont k | &lt; θ. Otherwise, v k = 1.</formula><p>If the predictions are within the error margin, i.e. error is smaller than θ, then these predictions do not contribute to update the weights of the network.</p><p>The Smooth L 1 loss refers to the absolute error using the squared error if the error is less than a threshold (set to 1 in our experiments). This loss has been widely used for object detection <ref type="bibr" target="#b43">[44]</ref> and, in our experiments, has been shown to be less sensitive to outliers. Precisely, the Smooth L 1 loss is defined as follows</p><formula xml:id="formula_4">SL 1cont (ŷ cont ) = 3 k=1 v k 0.5x 2 , if |x k | &lt; 1 |x k | − 0.5, otherwise<label>(3)</label></formula><p>where x k = (ŷ cont k − y cont k ), and v k is a weight assigned to each of the continuous dimensions and it is set to 1 in our experiments.</p><p>We train our recognition system end-to-end, learning the parameters jointly using stochastic gradient descent with momentum. The first two modules are initialized using pretrained models from Places <ref type="bibr" target="#b36">[37]</ref> and Imagenet <ref type="bibr" target="#b44">[45]</ref> while the fusion network is trained from scratch. The batch size is set to 52 -twice the size of the discrete emotion categories. We found empirically after testing multiple batch sizes (including multiples of 26 like 26, 52, 78, 108) that batchsize of 52 gives the best performance (on the validation set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We trained four different instances of our CNN model, which are the combination of two different input types and the two different continuous loss functions described in section 4.1. The input types are body (i.e., upper branch in <ref type="figure" target="#fig_0">Fig. 13</ref>), denoted by B, and body plus image (i.e., both branches shown in <ref type="figure" target="#fig_0">Fig. 13</ref>), denoted by B+I. The continuous loss types are denoted in the experiments by L 2 for Euclidean loss (equation 2) and SL 1 for the Smooth L 1 (equation 3).</p><p>Results for discrete categories in the form of Average Precision per category (the higher, the better) are summarized in <ref type="table" target="#tab_12">Table 3</ref>. Notice that the B+I model outperforms the B model in all categories except 1. The combination of body and image features (B+I(SL 1 ) model) is better than the B model.</p><p>Results for continuous dimensions in the form of Average Absolute Error per dimension, AAE (the lower, the better) are summarized in <ref type="table" target="#tab_13">Table 4</ref>. In this case, all the models provide similar results where differences are not significant. <ref type="figure" target="#fig_0">Fig. 14</ref> shows the summary of the results obtained per each instance in the testing set. Specifically, <ref type="figure" target="#fig_0">Fig. 14.</ref>a shows Jaccard coefficient (J C) for all the samples in the test set. The JC coefficient is computed as follows: per each category we use as threshold for the detection of the category the value where Precision = Recall. Then, the JC coefficient is computed as the number of categories detected that are also present in the ground truth (number of categories in the intersection of detections and ground truth) divided by the total number of categories that are in the ground truth or detected (union over detected categories and categories in the ground truth). The higher this JC is the better, with a maximum value of 1, where the detected categories and the ground truth categories are exactly the same. In the graphic, examples are sorted in decreasing order of the JC coefficient. Notice that these results also support that the B+I model outperforms the B model.</p><p>For the case of continuous dimensions, <ref type="figure" target="#fig_0">Fig. 14.b</ref> shows the Average Absolute Error (AAE) obtained per each sample in the testing set. Samples are sorted by increasing order (best performances on the left). Consistent with the results shown in <ref type="table" target="#tab_13">Table 4</ref>, we do not observe a significant difference among the different models.</p><p>Finally, <ref type="figure" target="#fig_0">Fig. 15</ref> shows qualitative predictions for the best B and B+I models. These examples were randomly selected among samples with high JC in B+I (a-b) and samples with    <ref type="figure" target="#fig_0">Fig. 15</ref>.c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Context Features Comparison</head><p>The goal of this section is to compare different context features for the problem of emotion recognition in context.</p><p>A key aspect for incorporating the context in an emotion recognition model is to be able to obtain information from the context that is actually relevant for emotion recognition.</p><p>Since the context information extraction is a scene-centric task, the information extracted from the context should be based in a scene-centric feature extraction system. That is why our baseline model uses a Places CNN for the context feature extraction module. However, recent works in sentiment analysis (detecting the emotion of a person when he/she observes an image) also provide a system for scene feature extraction that can be used for encoding the relevant contextual information for emotion recognition.</p><p>To compute body features, denoted by B f , we fine tune an AlexNet ImageNet CNN with EMOTIC database, and use the average pooling of the last convolutional layer as features. For the context (image), we compare two different feature types, which are denoted by I f and I S . I f are obtained by fine tunning an AlexNet Places CNN with EMOTIC database, and taking the average pooling of the last convolutional layer as features (similar to B f ), while I S is a feature vector composed of the sentiment scores for the ANP detectors from the implementation of <ref type="bibr" target="#b38">[39]</ref>.</p><p>To fairly compare the contribution of the different context features, we train Logistic Regressors for the following features and combination of features: (1) B f , (2) B f +I f , and (3) B f +I S . For the discrete categories we obtain mean APs AP = 23.00, AP = 27.70, and AP = 29.45, respectively. For the continuous dimensions, we obtain AAE 0.0704, 0.0643, and 0.0713 respectively. We observe that, for the discrete categories, both I f and I S contribute relevant information to the emotion recognition in context. Interestingly, I S performs better than I f , even though these features have not been trained using EMOTIC. However, these features are smartly designed for sentiment analysis, which is a problem closely related to extracting relevant contextual information for emotion recognition, and are trained with a large dataset of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper we pointed out the importance of considering the person scene context in the problem of automatic emotion recognition in the wild. We presented the EMOTIC database, a dataset of 23, 571 natural unconstrained images with 34, 320 people labeled according to their apparent emotions. The images in the dataset are annotated using two different emotion representations: 26 discrete categories, and the 3 continuous dimensions V alence, Arousal and Dominance. We described in depth the annotation process and analyzed the annotation consistency of different annotators. We also provided different statistics and algorithmic analysis on the data, showing the characteristics of the EMOTIC database. In addition, we proposed a baseline B (L2) Ground Truth B+I (SL1)  CNN model for emotion recognition in scene context that combines the information of the person (body bounding box) with the scene context information (whole image). We also compare two different feature types for encoding the contextual information. Our results show the relevance of using contextual information to recognize emotions and, in conjunction with the EMOTIC dataset, motivate further research in this direction. All the data and trained models are publicly available for the research community in the website of the project.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>How is this kid feeling? Try to recognize his emotional states from the person bounding box, without scene context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Sample images in the EMOTIC dataset along with their annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Examples of annotated people in EMOTIC dataset for each of the 26 emotion categories (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>ValenceFig. 4 :</head><label>4</label><figDesc>Examples of annotated images in EMOTIC dataset for each of the 3 continuous dimensions Valence, Arousal &amp; Dominance. The person in the red bounding box has the corresponding value of the given dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. 3 shows (per category) examples of people showing different emotional categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>AMT interface designs (a) For Discrete Categories' annotations &amp; (b) For Continuous Dimensions' annotations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Annotations of five different annotators for 3 images in EMOTIC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Representation of agreement between multiple annotators. Categories sorted in decreasing order according to the average number of annotators who agreed for that category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>(a) Kappa values (sorted) and (b) Standard deviation (sorted), for each annotated person in validation set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 :</head><label>9</label><figDesc>Dataset Statistics. (a) Number of people annotated for each emotion category; (b), (c) &amp; (d) Number of people annotated for every value of the three continuous dimensions viz.Valence, Arousal &amp; Dominance validation set for all the 3 dimensions, sorted in decreasing order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 :</head><label>11</label><figDesc>Distribution of continuous dimension values across emotion categories. Average value of a dimension is calculated for every category and then plotted in increasing order for every distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>y disc = (ŷ disc 1 , 1 ,ŷ cont 2 ,ŷ cont 3</head><label>1123</label><figDesc>...,ŷ disc 26 ) andŷ cont = (ŷ cont</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 :</head><label>14</label><figDesc>Results per each sample (Test Set, sorted): (a) Jaccard Coefficient (J C) of the recognized discrete categories (b) Average Absolute Error (AAE) in the estimation of the three continuous dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 :</head><label>15</label><figDesc>Ground truth and results on images randomly selected with different JC scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>context includes the</figDesc><table /><note>• Ronak Kosti &amp; Agata Lapedriza are with Universitat Oberta de Catalunya, Spain. Email: rkosti@uoc.edu, alapedriza@uoc.edu.• Adria Recasens is with the Computer Science and Artificial Intelli- gence Laboratory, Massachusetts Institute of Technology, USA. Email: recasens@mit.edu.• Jose M. Alvarez is with NVIDIA, USA. Email: jal- varez.research@gmail.com• Project Page: http://sunai.uoc.edu/emotic/</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>11. Embarrassment: feeling ashamed or guilty 12. Engagement: paying</head><label></label><figDesc>well being and relaxed; no worry; having positive thoughts or sensations; satisfied 20. Pleasure: feeling of delight in the senses 21. Sadness: feeling unhappy, sorrow, disappointed, or discouraged 22. Sensitivity: feeling of being physically or emotionally wounded; feeling delicate or vulnerable 23. Suffering: psychological or emotional pain; distressed; anguished 24. Surprise: sudden discovery of something unexpected 25. Sympathy: state of sharing others emotions, goals or troubles;</figDesc><table><row><cell>contempt; hostile</cell></row><row><cell>8. Disconnection: feeling not interested in the main event of the</cell></row><row><cell>surrounding; indifferent; bored; distracted</cell></row><row><cell>9. Disquietment: nervous; worried; upset; anxious; tense; pres-</cell></row><row><cell>sured; alarmed</cell></row><row><cell>10. Doubt/Confusion: difficulty to understand or decide; thinking</cell></row><row><cell>about different options</cell></row></table><note>attention to something; absorbed into something; curious; interested 13. Esteem: feelings of favourable opinion or judgement; respect; admiration; gratefulness 14. Excitement: feeling enthusiasm; stimulated; energetic 15. Fatigue: weariness; tiredness; sleepy 16. Fear: feeling suspicious or afraid of danger, threat, evil or pain; horror 17. Happiness: feeling delighted; feeling enjoyment or amusement 18. Pain: physical suffering 19. Peace:supportive; compassionate 26. Yearning: strong desire to have something; jealous; envious; lust</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc></figDesc><table /><note>Proposed emotion categories with definitions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc>Instruction summary for each HIT visually in a single image. Thus, our list of affective categories can be seen as a first level of a hierarchy, where each category has associated subcategories.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Calm vs. Ready to act Dominance: Dominated vs. In control Gender and age of the person in the yellow box</head><label></label><figDesc>feeling of being certain/proud/encouraged/optimistic) Engagement (occupied/absorbed/interested/paying attention to something) Pleasure (feeling of delight in the senses) Happiness (feeling delighted/enjoyment/amusement) Excitement (pleasant and excited state/stimulated/energetic/enthusiastic) Surprise (sudden discovery of something unexpected) (distressed/perturbed/anguished) Disapproval (think that something is wrong or reprehensible/contempt/hostile) Yearning (strong desire to have something/jealous/envious) Fatigue (weariness/tiredness/sleepy)</figDesc><table><row><cell></cell><cell></cell><cell>Pain</cell></row><row><cell></cell><cell></cell><cell>Doubt/Confusion</cell></row><row><cell></cell><cell></cell><cell cols="2">Fear (feeling afraid of danger/evil/pain/horror)</cell></row><row><cell></cell><cell></cell><cell cols="2">Vulnerability (feeling of being physically or emotionally wounded)</cell></row><row><cell></cell><cell></cell><cell cols="2">Disquitement (unpleasant restlessness/tense/worried/upset/stressed)</cell></row><row><cell></cell><cell></cell><cell cols="2">Annoyance (bothered/iritated/impatient/troubled/frustrated)</cell></row><row><cell></cell><cell></cell><cell cols="2">Anger (intense displeasure or rage/furious/resentful)</cell></row><row><cell></cell><cell></cell><cell cols="2">Disgust (feeling dislike or repulsion/feeling hateful)</cell></row><row><cell></cell><cell></cell><cell cols="2">Sadness (feeling unhappy/grief/disappointed/discouraged)</cell></row><row><cell></cell><cell></cell><cell>Disconnection</cell></row><row><cell></cell><cell></cell><cell cols="2">Embarrassment (feeling ashamed or guilty)</cell></row><row><cell></cell><cell></cell><cell cols="2">Valence: Negative vs. Positive</cell></row><row><cell></cell><cell></cell><cell cols="2">Negative Arousal (awakeness): Positive (pleasant) (unpleasant)</cell></row><row><cell></cell><cell></cell><cell>Calm</cell><cell>Ready to act</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(active)</cell></row><row><cell></cell><cell></cell><cell>Dominated</cell><cell>In</cell></row><row><cell></cell><cell></cell><cell>(no</cell><cell>control</cell></row><row><cell></cell><cell></cell><cell>control)</cell></row><row><cell></cell><cell></cell><cell>Male</cell><cell>Female</cell></row><row><cell></cell><cell></cell><cell>Kid (0-12)</cell><cell>Teenager (13-20)</cell><cell>Adult (more than 20)</cell></row><row><cell>Back</cell><cell>(Image 1 of 20)</cell><cell>Go to Next Image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 3 :</head><label>3</label><figDesc>Average Precision (AP) obtained on test set per category. Results for models where the input is just the body B, and models where the input are both the body and the whole image B+I. The type of L cont used is indicated in parenthesis (L 2 refers to equation 2 and SL 1 refers to equation 3).</figDesc><table><row><cell>Continuous</cell><cell></cell><cell cols="3">CNN Inputs and Lcont type</cell></row><row><cell>Dimensions</cell><cell>B (L 2 )</cell><cell>B (SL 1 )</cell><cell>B+I (L 2 )</cell><cell>B+I (SL 1 )</cell></row><row><cell>Valence</cell><cell>0.0537</cell><cell>0.0545</cell><cell>0.0546</cell><cell>0.0528</cell></row><row><cell>Arousal</cell><cell>0.0600</cell><cell>0.0630</cell><cell>0.0648</cell><cell>0.0611</cell></row><row><cell>Dominance</cell><cell>0.0570</cell><cell>0.0567</cell><cell>0.0573</cell><cell>0.0579</cell></row><row><cell>Mean</cell><cell>0.0569</cell><cell>0.0581</cell><cell>0.0589</cell><cell>0.0573</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 4 :</head><label>4</label><figDesc>Average Absolute Error (AAE) obtained on test set per each continuous dimension. Results for models where the input is just the body B, and models where the input are both the body and the whole image B+I. The type of L cont used is indicated in parenthesis (L 2 refers to equation 2 and SL 1 refers to equation 3).low JC in B+I (g-h). Incorrect category recognition is indicated in red. As shown, in general, B+I model outperforms B, although there are some exceptions, like</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work has been partially supported by the Ministerio de Economia, Industria y Competitividad (Spain), under the Grants Ref. TIN2015-66951-C2-2-R and RTI2018-095232-B-C22, and by Innovation and Universities (FEDER funds). The authors also thank NVIDIA for their generous hardware donations.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Angry, disgusted, or afraid? studies on the malleability of emotion perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aviezer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Hassin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moscovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bentin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="724" to="732" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rapid influence of emotional scenes on encoding of facial expressions: an erp study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Righart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Gelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social cognitive and affective neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="278" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Placing the face in context: cultural differences in the perception of facial emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van De Veerdonk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">365</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Context in emotion perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gendron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="286" to="290" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">How emotions are made: The secret life of the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Houghton Mifflin Harcourt</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Framework for a comprehensive description and measurement of emotional states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>Genetic, social, and general psychology monographs</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emotion recognition in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Expert system for automatic analysis of facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="881" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facial-component-based bag of words and phog descriptor for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaneko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SMC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1353" to="1358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Facial action coding system: a technique for the measurement of facial movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<pubPlace>Palo Alto</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Constants across cultures in the face and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR16)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR16)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis of eeg signals and facial expressions for continuous emotion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Asghari-Esfeden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="28" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compound facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1454" to="1462" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="92" to="105" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognizing emotions expressed by body pose: A biologically inspired neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Gelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1238" to="1246" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Group-level arousal and valence recognition in static images: Face, body and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Celiktutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Genki Database</surname></persName>
		</author>
		<ptr target="http://mplab.ucsd.edu/wordpress/?page-id=398" />
		<imprint>
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">ICML face expression recognition dataset</title>
		<ptr target="https://goo.gl/nn9w4R" />
		<imprint>
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Development of a facs-verified set of basic and self-conscious emotion expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Tracy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Schriber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">554</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing affective dimensions from body posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-74889-25</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-540-74889-25" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2Nd International Conference on Affective Computing and Intelligent Interaction, ser. ACII &apos;07</title>
		<meeting>the 2Nd International Conference on Affective Computing and Intelligent Interaction, ser. ACII &apos;07<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="48" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic recognition of non-acted affective postures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1027" to="1038" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gemep-geneva multimodal emotion portrayals: A corpus for the study of multimodal emotional expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bänziger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="15" to="019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chalearn looking at people: Events and resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<idno>abs/1701.02664</idno>
		<ptr target="http://arxiv.org/abs/1701.02664" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Emotiw 2016: Video and group-level emotion recognition challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
		<idno type="DOI">10.1145/2993148.2997638</idno>
		<ptr target="http://doi.acm.org/10.1145/2993148.2997638" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimodal Interaction, ser. ICMI 2016</title>
		<meeting>the 18th ACM International Conference on Multimodal Interaction, ser. ICMI 2016<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="427" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Finding happiest moments in a social context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="613" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coco attributes: Attributes for people, animals, and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<ptr target="http://arxiv.org/abs/1405.0312" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Semantic understanding of scenes through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1608.05442" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Oxford english dictionary</title>
		<ptr target="http://http://www.oed.com" />
		<imprint>
			<biblScope unit="page" from="2017" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Merriam-Webster Online English Dictionary</surname></persName>
		</author>
		<ptr target="https://www.merriam-webster.com" />
		<imprint>
			<biblScope unit="page" from="2017" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Fernández-Abascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Domínguez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Psicología de la emoción. Editorial Universitaria Ramón Areces</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Affective computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">252</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The empathy and systemizing quotient: The psychometric properties of the dutch version and a review of the cross-cultural stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Groen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B M</forename><surname>Fuermaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Heijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Althaus</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10803-015-2448-z</idno>
		<ptr target="http://dx.doi.org/10.1007/s10803-015-2448-z" />
	</analytic>
	<monogr>
		<title level="j">Journal of Autism and Developmental Disorders</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2848" to="2864" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Places: An image database for deep scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno>abs/1610.02055</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deepsentibank: Visual sentiment concept classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8586</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Decomposeme: Simplifying convnets for end-to-end learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<idno>abs/1606.05426</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<title level="m">A Dozen Tricks with Multitask Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="163" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
