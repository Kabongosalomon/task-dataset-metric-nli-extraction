<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DenseImage Network: Video Spatial-Temporal Evolution Encoding and Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokai</forename><surname>Chen</surname></persName>
							<email>chenxiaokai@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gao</surname></persName>
							<email>kegao@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DenseImage Network: Video Spatial-Temporal Evolution Encoding and Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many of the leading approaches for video understanding are data-hungry and time-consuming, failing to capture the gist of spatial-temporal evolution in an efficient manner. The latest research shows that CNN network can reason about static relation of entities in images. To further exploit its capacity in dynamic evolution reasoning, we introduce a novel network module called Den-seImage Network(DIN) with two main contributions. 1) A novel compact representation of video which distills its significant spatial-temporal evolution into a matrix called DenseImage, primed for efficient video encoding. 2) A simple yet powerful learning strategy based on DenseImage and a temporal-order-preserving CNN network is proposed for video understanding, which contains a local temporal correlation constraint capturing temporal evolution at multiple time scales with different filter widths. Extensive experiments on two recent challenging benchmarks demonstrate that our DenseImage Network can accurately capture the common spatial-temporal evolution between similar actions, even with enormous visual variations or different time scales. Moreover, we obtain the state-of-the-art results in action and gesture recognition with much less time-and-memory cost, indicating its immense potential in video representing and understanding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The spatial-temporal evolution of entities is of vital importance for video understanding tasks such as action recognition. Similar actions are often performed by various entities at different speeds. Due to the large diversity and complexity, modeling the gist of dynamic evolution in videos is very challenging.</p><p>Although deep convolutional neural networks have made significant progress in image understanding tasks <ref type="bibr" target="#b0">[Badrinarayanan et al., 2015;</ref><ref type="bibr" target="#b10">Vinyals et al., 2015]</ref>, their impact on video analysis has been somewhat limited, and how to optimally represent videos remains unclear <ref type="bibr">[Sigurdsson et al., 2017]</ref>. On one hand, many top performing approaches <ref type="bibr" target="#b8">[Simonyan and Zisserman, 2014;</ref><ref type="bibr" target="#b10">Wang et al., 2016]</ref> rely heavily on optical flow to model temporal dynamics in videos, however, <ref type="bibr" target="#b7">[Sevilla-Lara et al., 2017]</ref> found that the invariance to appearance is responsible for the success of optical flow, rather than the temporal trajectories. 3D CNNs with warm-start are proposed to learn both appearance and motion features simultaneously, but they're data-hungry and computationally expensive based on dense sampled frames <ref type="bibr" target="#b1">[Carreira and Zisserman, 2017]</ref>. Moreover, <ref type="bibr" target="#b11">[Xie et al., 2017]</ref> has shown that it makes no difference in accuracy whether or not temporal order is reversed for the current state-of-the-art approach I3D.</p><p>On the other hand, traditional datasets for video analysis such as UCF101 <ref type="bibr">[Soomro et al., 2012]</ref>, Sport1M <ref type="bibr" target="#b6">[Karpathy et al., 2014]</ref>, and THUMOS <ref type="bibr" target="#b4">[Jiang et al., 2014]</ref>, contain many actions that can be identified without temporal reasoning, which leads to approaches only good at modeling appearance invariance and short-term motion.</p><p>From the above points, we can see that most existing methods such as 3D CNNs  or two-stream networks <ref type="bibr" target="#b8">[Simonyan and Zisserman, 2014]</ref> are learned directly from raw pixels, which makes it difficult to recognize highlevel actions from low-level trivial details efficiently.</p><p>In our opinion, a good approach for video understanding should encode the appearance and motion evolution in a certain temporal sequence effectively and efficiently. In this work, we seek to address two questions:</p><p>• How to encode spatial-temporal evolution in videos effectively and efficiently ?</p><p>• Can we capture the spatial-temporal evolution correctly with the help of 2D CNNs?</p><p>Trying to answer the first question, our first contribution is to introduce a novel video representation called DenseImage which distills spatial-temporal evolution of a video into a matrix, where the spatial information is encoded in each column of the matrix and the temporal information is preserved in the row sequence.</p><p>To answer the second question, our second contribution is to propose a temporal-order-preserving CNN network to capture the core common spatial-temporal properties in Den-seImage. The local temporal correlation constraint contained in the learning process makes the temporal evolution stable, and also captures temporal evolution at multiple time scales with different filter widths.</p><p>To sum up, a novel approach called DenseImage Network is proposed for video spatial-temporal encoding and understanding. Experiments on two challenging benchmarks and visualization analysis demonstrate that our DIN can accurately and efficiently capture the common spatial-temporal evolution between similar actions, with much less time-andmemory cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Early works for video understanding usually use hand-crafted features and SVM classifier, here we focus on recently proposed approaches based on convolutional neural networks.</p><p>How to encode spatial-temporal evolution in videos effectively and efficiently ? The success of static image classification with CNNs has driven the development of video recognition, but how to represent spatial-temporal evolution in videos using CNNs is still a problem. <ref type="bibr" target="#b6">[Karpathy et al., 2014]</ref> studied approaches for fusing information over temporal dimension via 2D CNNs. <ref type="bibr" target="#b8">[Simonyan and Zisserman, 2014]</ref> proposed a two-stream CNNs, one stream extract spatial information from RGB, and the other extract temporal information from optical flow, and finally the prediction scores from each stream are fused. <ref type="bibr" target="#b10">[Wang et al., 2016]</ref> proposed a temporal segment networks(TSN) for long-range temporal structure modeling, they extend the traditional two-stream method with a sparse temporal sampling strategy.  proposed 3D CNNs to capture both appearance and motion features simultaneously. <ref type="bibr" target="#b1">[Carreira and Zisserman, 2017]</ref> found that optical flow is also useful in 3D CNNs, they took the strength of both two-stream and 3D CNNs achieved state-of-the-art performance on UCF101.</p><p>Recently, <ref type="bibr" target="#b7">[Sevilla-Lara et al., 2017]</ref> pointed that the invariance to appearance is responsible for the success of optical flow, rather than the temporal trajectories.They shuffled flow fields, but the accuracy descrised slightly from 86.85% to 78.64%, they further shuffled the images to compute opti-cal flow, and the accuracy is still upto 59.55%. Their experiments illustrate that relay on optical flow to model temporal stucture is not enough yet. <ref type="bibr" target="#b11">[Xie et al., 2017]</ref> has shown that it makes no difference in accuracy whether or not temporal order is reversed with the state-of-the-art approach I3D on Full-Kinetics dataset <ref type="bibr" target="#b1">[Carreira and Zisserman, 2017]</ref>, one reason is that many kinds of actions in classical datasets can be identified by a single frame, which leads the existing approaches pay more attention to appearance and short-term motion. On the other hand, optical flow needs to be pre-computed before training which lowers the efficiency of the online recognition system and also requires lots of storage. Our DenseImage Network do not need optical flow to capture motion information, instead it further exploits the strength of 2D CNN to capture spatial-temporal evolution in video, which is dataefficient.</p><p>Can we capture the spatial-temporal evolution accurately with the help of 2D CNNs? As is known, CNNs perform well on the task of image classification, and it can also be used to other static image understanding tasks, such as: image caption, image semantic segmentation, however when it comes to video understanding, one may doubt that details are lost when using pretrained CNNs to distill spatial information in frame level, however, recently <ref type="bibr" target="#b7">[Santoro et al., 2017]</ref> shows that relational network module using the features exracted from ImageNet pretrained CNNs such as ResNet101 <ref type="bibr">[He et al., 2016]</ref>, can gain the ability of spatial relational reasoning and even outperforms average human performance on dataset CLEVR <ref type="bibr" target="#b5">[Johnson et al., 2017]</ref>, which is a visual question answering dataset designed to address relational reasoning in images. Inspried by that, <ref type="bibr" target="#b12">[Zhou et al., 2017]</ref> successfully extend relational networks to temporal relational reasoning by using multiple time scale MLPs to model temporal relation between frames. Their works demonstrate that pretrained CNN is still a powerful feature extractor for relational reasoning, which is a building block of our proposed video representation: DenseImage.</p><p>The approaches similar to ours. Dynamic image networks <ref type="bibr" target="#b0">[Bilen et al., 2016]</ref> and TRN <ref type="bibr" target="#b12">[Zhou et al., 2017]</ref> are the two most related works to ours. Dynamic image using rank pooling to compress a video into an image, it's good at modeling long term motion patterns, but loses details, while our method apply CNNs to distill spatial-temporal evolution of a video into a matrix called DenseImage. TRN apply MLPs to model temporal relational reasoning, however in order to calculate efficiently, they have to down sample from n-frame combinations in each time scale, in our opinion, this operation could cause unstableness of temporal evolution as the time interval in each scale between frames is randomly decided, not fixed, which makes their model converge slower, what's more, down sampling has the risk of losing frame which is important for action recognition. Instead, we propose a simple yet powerful temporal convolution based on DenseImage, it incorporates a local temporal correlation constraint and can effectively and efficiently capture temporal evolution at multiple time scales with different filter widths.</p><p>In brief, we propose an effective and efficient method which consists a novel video representation and a simple yet powerful learning strategy for video understanding tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial evolution Temporal evolution</head><p>Figure 2: Framework of DenseImage Network, which consists of a video encoding method called DenseImage and a simple yet powerful learning strategy to capture the gist of spatial-temporal evolution. Here is an example with two different time scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section,we give detailed descriptions of DenseImage Network module. As shown in <ref type="figure">Figure 2</ref>, it consists of a compact structure distilling the spatial-temporal evolution of a video into a matrix called DenseImage, and a temporal-orderpreserving CNN network for video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video Encoding with DenseImage</head><p>Suppose that we sample n frames from a video: {I 1 , · · · , I n }, the form of encoding function ψ is an open question, in this work we use ImageNet pretrained CNNs to form it. Let x t = ψ(I t ) ∈ R k be a feature vector extracted from each individual frame I t . We then stack them in temporal order as below:</p><formula xml:id="formula_0">X = x 1 + x 2 + · · · + x n .<label>(1)</label></formula><p>where + is the concatenation operator, the matrix X ∈ R nk called DenseImage because each row in X represents a frame. This encoding algorithm distills spatiotemporal evolution in a video into a DenseImage X, where the spatial information is encoded in each column of X and the temporal information is preserved in row sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Video Understanding with DenseImage Networks</head><p>Given a DenseImage X for a video, let x i:i+j refer to the concatenation of frames x i , x i+1 , · · · , x i+j . We then conduct convolution as below:</p><formula xml:id="formula_1">c h i,m = f (w T m,h x i:i+h−1 + b m ).<label>(2)</label></formula><p>Where m is the channel index of feature map, w m,h is the filter that captures temporal evolution between h frames, b m ∈ R is a bias term and f is a non-linear function such  as the rectifier. Filter w m,h is applied to each possible window of frames in X to produce a feature map or a vector:</p><formula xml:id="formula_2">c h m = [c h 1,m , c h 2,m , · · · , c h n−h+1,m ],<label>(3)</label></formula><p>Note that each element in c h m represents a local temporal evolution in its position, and the whole vector c h m represents an abstract h-frame temporal evolution for X with one channel m and one filter size h. We then apply a max pooling operation , o h m = max(c h m ) , to get the most important local temproal structure.</p><p>Therefore, if the number of channels is M , then for filter size h, we'll get a h-frame spatial-temporal evolution representation based on DenseImage X,</p><formula xml:id="formula_3">c h = [o h 1 , o h 2 , · · · , o h M ].<label>(4)</label></formula><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, our approach is simple yet powerful at:</p><p>• Local temporal correlation constraint. Only adjacent h frames will be convouted in filter w m,h , this constrain makes temporal evolution much more stable, therefore the model is prone to be trained.  Given multi-scale temporal features for a DenseImage, the classification can be formed as:</p><formula xml:id="formula_4">score = sof tmax( h∈H f φ(h) (c h )),<label>(5)</label></formula><p>Each timescale feature c h is passed to a fully connected layer f φ(h) with parameters φ(h), and the probability distributions are sumed and then normalized by a softmax function to get the final classification score. Note that our approach is differentiable throughout each module: DenseImage ecoding, convolution on DenseImage and classification, so they can all be trained together with back propagation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate this work on two challenging new benchmarks, in which the spatial-temporal evolution between frames is critical to recognize actions correctly.</p><p>Something-Something.The dataset <ref type="bibr" target="#b2">[Goyal et al., 2017]</ref> is collected for generic human-object interaction. It comprises of 174 categories, like "Dropping something into something" and even "Pretending to open something without actually opening it". It contains 108,499 videos in total, 86,017 videos for training, 11,522 for validation and 10,960 for testing.</p><p>Jester.The dataset <ref type="bibr">[Twentybn, 2017]</ref> is collected for gesture recognition. It comprises of 27 categories, like "Swiping Left" and "Pulling Two Fingers In". It contains 148,092 videos in total, 118,562 videos for training, 14,787 for validation and 14,743 for testing.Each video is represented as a set of images that are extracted from the orginal videos at 12 frames per second for both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>Features:As it known that ImageNet pretrained CNNs are powerful for image representation and superior CNNs such as <ref type="bibr">ResNet [He et al., 2016]</ref> usually perform better. Here in order to verify the effectiveness of our method, we fix the ImageNet pretrained CNNs to be the same in this work. We adopt Inception with Batch Normalization(BN-Inception) <ref type="bibr" target="#b3">[Ioffe and Szegedy, 2015]</ref> because of its balance between accuracy and efficiency. Specifically, image features are extracted from the global pool layer with dimension 1024, following a fully connected layer to reduce dimension from 1024 to 256.</p><p>DenseImage:Following TSN [Wang et al., 2016], we apply the same sampling strategy to sample eight frames from each video. Therefore DenseImage X ∈ R 8 * 256 (Section 3.1 for details).</p><p>Training settings:We empirically set the timescale h = {2, 3, · · · , 6}, filter numbers of each time scale is 256. We follow the strategy of partial BN and dropout after global pooling as used in TSN. We add an extra dropout layer before fully connected layer to further reduce the effect of overfitting. Mini-batch SGD algorithm is applied to optimize our model. We use mini-batches of 32 videos, momentum of 0.9 and weight decay of 5e −4 . All models are initialized with learning rate 5e −4 and this value is further reduced to its 1 10 whenever the validation error stops decreasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Accuracy</head><p>We show the leaderboard on Something-Something 1 and Jester 2 datasets. As shown in <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref>, we tops the leaderboard of Something-Something and Jester at the time of submission. Notice that we only use single modal RGB features and without ensembling of multiple models.</p><p>For intuitive explanation, we show four examples in <ref type="figure" target="#fig_3">Figure  4</ref>. As can be seen, one can hardly identify the four examples with only one single frame, because spatial-temporal evolution in them is essential for a successful recognition. Our  <ref type="figure">Figure 5</ref>: Number of parameters and video-level computation complexity of each method. Our method is 6.5x more efficient than the state-of-the-art two-stream I3D, and has much less parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Efficiency Analysis</head><p>Our method is efficient because there is no extra data, such as optical flow, needed to be pre-computed and the temporal convolution based on DenseImage is an efficient 2D style CNN which contains only one single convolutional layer. <ref type="figure">Figure 5</ref> compares the number of parameters and the videolevel computation complexity of our approach with state-of-the-art method: Two-Stream <ref type="bibr" target="#b8">[Simonyan and Zisserman, 2014]</ref>, <ref type="bibr">TSN [Wang et al., 2016]</ref>, C3D , Res3D <ref type="bibr" target="#b10">[Tran et al., 2017]</ref>, Two-Stream I3D <ref type="bibr" target="#b1">[Carreira and Zisserman, 2017]</ref>. As can be seen, our approach has 1.9x much less parameters than the current state-of-the-art Two-Stream I3D, and 12.9x less for the classical Two-Stream, TSN use BN-Inception as base model which is the same with us, however due to using 3 modalities: RGB, optical flow and warped flow it has to fine-tune 3 base model to capture feature in different modalities which leads 3x much more parameters than us.</p><p>As for computation complexity, state-of-the-art methods are usually applied to multiple video clips and the recognition results are averaged during test time, for fair comparison here we compare the video-level computation complexity of them. Our approach is 6.5x efficient than Two-Stream I3D, this is mainly due to Two-Stream I3D is trained on 4x longer videos , and the optical flow stream further increases the computation complexity.</p><p>To sum up, the optical flow computation is the bottleneck for two-stream networks, and 3D CNNs are computationally expensive based on dense sampled frames. In contrast, there is only RGB discrete frames used in our approach, and all of the convolutional layers in our DIN is 2D, which makes DIN efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualization Analysis</head><p>This section we conduct several experiments to visualize the spatial-temporal evolution captured by our DIN.</p><p>Temporal convolution based on DenseImage can accurately capture the common spatial-temporal evolution between similar actions at multiple time scales. As shown in <ref type="figure">Figure 6</ref>, we use t-SNE algorithm to visualize the highlevel video features with different filter widths, features without temporal modeling are also included. These videos come from the 10 most frequent action classes in the Jester validation set.</p><p>As described in section3, our DIN consists a 2D temporal convolutional layer which can capture spatial-temporal evolution at multiple time scales with different filter widths. Here we visualize the response of different width of filters at Pulling Two Fingers In <ref type="figure">Figure 6</ref>: t-SNE plot showing that complex actions can be better classified at different time scales. As can be seen in (a), "Shaking Hand" is highly overlapped with "Pulling Two Fingers In", indicating that temporal evolution is essential for a successful recognition. As shown in (b) and (c), samples from different classes are clearly separated, indicating that filter widths with 2 and 4 can capture the tiny difference between similar classes. As shown in (d), "No gesture" and "Pulling Hand In" is clustered together, while other classes such as:"Rolling Hand Backward" and "Rolling Hand Forward" are more distinguishable, indicating that the cooperation of different time scales is important to recognize these samples.  ever possible location in DenseImage. As shown in <ref type="figure" target="#fig_5">Figure 7</ref>, which indicating that our DIN can discover frames that are important for action recognition at multiple time scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, a novel approach called DenseImage Network is proposed for video spatial-temporal encoding and understanding. Experiments on two challenging benchmarks and visualization analysis demonstrate that our DIN can accurately and efficiently capture the common spatial-temporal evolution between similar actions, with much less time-andmemory cost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Ground truth: Plugging something into something Plugging something into something Plugging something into something but pulling it right out as you remove your hand Pulling something out of something Pretending to put something behind something Ground truth:Zooming In With Full Action recognition results from two challenging new benchmarks Something-Something and Jester. Bars with different lengths indicating the recognition scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Spatial-temporal convolution on DenseImage with local temporal correlation constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Action recognition results from two challenging new benchmarks Something-Something and Jester. Bars with different lengths indicating the recognition scores.• Temporal order preserving. As shown in Figure 3, Suppose three ordered frames [A, B, C] are sampled from a video, then a 2-frame filter works on it, getting a vector c = [c 1 , c 2 ]. Notice that c 1 represents temporal information for the ordered pair &lt; A, B &gt; and c 2 for &lt; B, C &gt;. • Efficient multi-scale temporal structure modeling.Multi-scale temporal evolution can be captured by varying the value of h. For example, if h = {2, 3, 4}, it can model 2-frame, 3-frame, 4-frame temporal structure in videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization the response of filters with different widths in videos. Showing that the common spatial-temporal evolution in videos can be correctly captured at different time scales. The height of each bin represents the response intensity at corresponding position, h refers to the width of filter, and the bounding boxs in different colors correspond to positions where the response is greatest. Eight frames sampled from each video, so there exists 7 bins with h = 2, 6 bins with h = 3, and 5 bins with h = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Ground truth: Poking something so that it falls over Poking something so that it falls over Poking something so it slightly moves Poking something so lightly that it doesn't or almost doesn't move Moving something and something closer to each other Ground truth: Throwing something in the air and catching it</figDesc><table><row><cell cols="4">Throwing something in the air and catching it</cell><cell>0.841</cell><cell>0.607</cell></row><row><cell cols="2">Throwing something</cell><cell>0.081</cell><cell></cell><cell>0.246</cell></row><row><cell>Twisting something</cell><cell cols="2">0.012</cell><cell></cell><cell>Poking something so that it spins around 0.021</cell></row><row><cell cols="4">Throwing something in the air and letting it fal</cell><cell>0.003</cell><cell>0.002</cell></row><row><cell cols="3">Throwing something against something</cell><cell>0.001</cell><cell>0.002</cell></row><row><cell cols="4">Ground truth: Turning Hand Counterclockwise</cell><cell>Ground truth: Swiping Down</cell></row><row><cell cols="3">Turning Hand Counterclockwise</cell><cell></cell><cell>0.994</cell><cell>Swiping Down</cell><cell>0.955</cell></row><row><cell cols="3">Turning Hand Clockwise Zooming Out With Full Hand 0.006 0.004</cell><cell></cell><cell>Swiping Up Pushing Hand Away 0.021</cell><cell>0.005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">: Top-1 accuracy on Something-Something test set.</cell></row><row><cell>Model</cell><cell>Top 1 acc.(%)</cell></row><row><cell>Peter CV</cell><cell>19.68</cell></row><row><cell>Valentin(esc)</cell><cell>24.55</cell></row><row><cell>Harrison.AI</cell><cell>26.38</cell></row><row><cell>I3D</cell><cell>27.23</cell></row><row><cell>Besnet</cell><cell>31.66</cell></row><row><cell>TRN v2</cell><cell>33.60</cell></row><row><cell>DIN(ours)</cell><cell>34.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Top-1 accuracy on Jester test set.</figDesc><table><row><cell>Model</cell><cell>Top 1 acc.(%)</cell></row><row><cell>3D CNN</cell><cell>77.85</cell></row><row><cell>20BN's Jester System</cell><cell>82.34</cell></row><row><cell>ConvLSTM</cell><cell>82.76</cell></row><row><cell>VideoLSTM</cell><cell>85.86</cell></row><row><cell>Ford's Gesture Recognition System</cell><cell>94.11</cell></row><row><cell>Besnet</cell><cell>94.23</cell></row><row><cell>TRN</cell><cell>94.78</cell></row><row><cell>DIN(ours)</cell><cell>95.31</cell></row><row><cell cols="2">DIN method correctly identifies these examples indicating</cell></row><row><cell cols="2">that it captures the spatiotemporal evolution between frames.</cell></row><row><cell cols="2">It further demonstrate that the cooperation of proposed video</cell></row><row><cell cols="2">representation:DenseImage and temporal convolution is ef-</cell></row><row><cell>fective to recognize these actions.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Recognition result:Humb Up Recognition result:Turing something upside down Recognition result:Putting something next to something</head><label></label><figDesc></figDesc><table><row><cell>h=2</cell><cell>h=3</cell><cell>h=4</cell><cell>h=2</cell><cell>h=3</cell><cell>h=4</cell></row><row><cell>Recognition result:Swiping Left</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>h=2</cell><cell>h=3</cell><cell>h=4</cell><cell>h=2</cell><cell>h=3</cell><cell>h=4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.twentybn.com/datasets/something-something 2 https://www.twentybn.com/datasets/jester</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Badrinarayanan</surname></persName>
		</author>
		<idno>abs/1511.00561</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3034" to="3042" />
		</imprint>
	</monogr>
	<note>Dynamic image networks for action recognition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Venice, Italy; Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>Ioffe and Szegedy</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Karpathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Olga Russakovsky, and Abhinav Gupta. What actions are needed for understanding human actions in videos?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Santoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA; Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2156" to="2165" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisserman ; Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<editor>Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
	<note>Soomro et al., 2012. UCF101: A dataset of 101 human actions classes from videos in the wild. CoRR, abs/1212.0402</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<idno>abs/1708.05038</idno>
		<ptr target="https://www.twentybn.com/datasets/jester" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA; Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<idno>abs/1712.04851</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Temporal relational reasoning in videos</title>
		<idno>abs/1711.08496</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
