<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Image-to-Image Translation Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
							<email>mingyul@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
							<email>tbreuel@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<email>jkautz@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Image-to-Image Translation Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many computer visions problems can be posed as an image-to-image translation problem, mapping an image in one domain to a corresponding image in another domain. For example, super-resolution can be considered as a problem of mapping a low-resolution image to a corresponding high-resolution image; colorization can be considered as a problem of mapping a gray-scale image to a corresponding color image. The problem can be studied in supervised and unsupervised learning settings. In the supervised setting, paired of corresponding images in different domains are available <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. In the unsupervised setting, we only have two independent sets of images where one consists of images in one domain and the other consists of images in another domain-there exist no paired examples showing how an image could be translated to a corresponding image in another domain. Due to lack of corresponding images, the UNsupervised Image-to-image Translation (UNIT) problem is considered harder, but it is more applicable since training data collection is easier.</p><p>When analyzing the image translation problem from a probabilistic modeling perspective, the key challenge is to learn a joint distribution of images in different domains. In the unsupervised setting, the two sets consist of images from two marginal distributions in two different domains, and the task is to infer the joint distribution using these images. The coupling theory <ref type="bibr" target="#b15">[16]</ref> states there exist an infinite set of joint distributions that can arrive the given marginal distributions in general. Hence, inferring the joint distribution from the marginal distributions is a highly ill-posed problem. To address the ill-posed problem, we need additional assumptions on the structure of the joint distribution.</p><p>To this end we make a shared-latent space assumption, which assumes a pair of corresponding images in different domains can be mapped to a same latent representation in a shared-latent space. Based on the assumption, we propose a UNIT framework that are based on generative adversarial networks (GANs) and variational autoencoders (VAEs). We model each image domain using a VAE-GAN. The adversarial training objective interacts with a weight-sharing constraint, which enforces a sharedlatent space, to generate corresponding images in two domains, while the variational autoencoders relate translated images with input images in the respective domains. We applied the proposed  <ref type="figure">Figure 1</ref>: (a) The shared latent space assumption. We assume a pair of corresponding images (x1, x2) in two different domains X1 and X2 can be mapped to a same latent code z in a shared-latent space Z. E1 and E2 are two encoding functions, mapping images to latent codes. G1 and G2 are two generation functions, mapping latent codes to images. (b) The proposed UNIT framework. We represent E1 E2 G1 and G2 using CNNs and implement the shared-latent space assumption using a weight sharing constraint where the connection weights of the last few layers (high-level layers) in E1 and E2 are tied (illustrated using dashed lines) and the connection weights of the first few layers (high-level layers) in G1 and G2 are tied. Here,x 1→1</p><formula xml:id="formula_0">E 1 E 2 G 1 G 2 X 2 X 1 x 1 x 2 zx 1!1 1 x 2!1 2 x 1!2 1 x 2!2 2 E 1 E 2 G 2 G 1 (a) (b) T/F T/F D 1 D 2</formula><formula xml:id="formula_1">1 andx 2→2 2 are self-reconstructed images, andx 1→2 1 andx 2→1 2</formula><p>are domain-translated images. D1 and D2 are adversarial discriminators for the respective domains, in charge of evaluating whether the translated images are realistic. <ref type="table">Table 1</ref>: Interpretation of the roles of the subnetworks in the proposed framework.</p><formula xml:id="formula_2">Networks {E1, G1} {E1, G2} {G1, D1} {E1, G1, D1} {G1, G2, D1, D2}</formula><p>Roles VAE for X1 Image Translator X1 → X2 GAN for X1 VAE-GAN <ref type="bibr" target="#b13">[14]</ref> CoGAN <ref type="bibr" target="#b16">[17]</ref> framework to various unsupervised image-to-image translation problems and achieved high quality image translation results. We also applied it to the domain adaptation problem and achieved state-ofthe-art accuracies on benchmark datasets. The shared-latent space assumption was used in Coupled GAN <ref type="bibr" target="#b16">[17]</ref> for joint distribution learning. Here, we extend the Coupled GAN work for the UNIT problem. We also note that several contemporary works propose the cycle-consistency constraint assumption <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref>, which hypothesizes the existence of a cycle-consistency mapping so that an image in the source domain can be mapped to an image in the target domain and this translated image in the target domain can be mapped back to the original image in the source domain. In the paper, we show that the shared-latent space constraint implies the cycle-consistency constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Assumptions</head><p>Let X 1 and X 2 be two image domains. In supervised image-to-image translation, we are given samples (x 1 , x 2 ) drawn from a joint distribution P X1,X2 (x 1 , x 2 ). In unsupervised image-to-image translation, we are given samples drawn from the marginal distributions P X1 (x 1 ) and P X2 (x 2 ). Since an infinite set of possible joint distributions can yield the given marginal distributions, we could infer nothing about the joint distribution from the marginal samples without additional assumptions.</p><p>We make the shared-latent space assumption. As shown <ref type="figure">Figure 1</ref>, we assume for any given pair of images x 1 and x 2 , there exists a shared latent code z in a shared-latent space, such that we can recover both images from this code, and we can compute this code from each of the two images. That is, we postulate there exist functions E * 1 , E * 2 , G * 1 , and G * 2 such that, given a pair of corresponding images (x 1 , x 2 ) from the joint distribution, we have z = E * 1 (x 1 ) = E * 2 (x 2 ) and conversely x 1 = G * 1 (z) and x 2 = G * 2 (z). Within this model, the function x 2 = F * 1→2 (x 1 ) that maps from X 1 to X 2 can be represented by the composition F * 1→2 (x 1 ) = G * 2 (E * 1 (x 1 )). Similarly,</p><formula xml:id="formula_3">x 1 = F * 2→1 (x 2 ) = G * 1 (E * 2 (x 2 )</formula><p>). The UNIT problem then becomes a problem of learning F *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1→2</head><p>and F * 2→1 . We note that a necessary condition for F * 1→2 and F * 2→1 to exist is the cycle-consistency constraint <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref>:</p><formula xml:id="formula_4">x 1 = F * 2→1 (F * 1→2 (x 1 )</formula><p>) and x 2 = F * 1→2 (F * 2→1 (x 2 )). We can reconstruct the input image from translating back the translated input image. In other words, the proposed shared-latent space assumption implies the cycle-consistency assumption (but not vice versa).</p><p>To implement the shared-latent space assumption, we further assume a shared intermediate representation h such that the process of generating a pair of corresponding images admits a form of</p><formula xml:id="formula_5">z → h x 1 x 2 .<label>(1)</label></formula><p>Consequently, we have G * 1 ≡ G * L,1 • G * H and G * 2 ≡ G * L,2 • G * H where G * H is a common high-level generation function that maps z to h and G * L,1 and G * L,2 are low-level generation functions that map h to x 1 and x 2 , respectively. In the case of multi-domain image translation (e.g., sunny and rainy image translation), z can be regarded as the compact, high-level representation of a scene ("car in front, trees in back"), and h can be considered a particular realization of z through G * H ("car/tree occupy the following pixels"), and G * L,1 and G * L,2 would be the actual image formation functions in each modality ("tree is lush green in the sunny domain, but dark green in the rainy domain"). Assuming h also allow us to represent E * 1 and E * 2 by E *</p><formula xml:id="formula_6">1 ≡ E * H • E * L,1 and E * 2 ≡ E * H • E * L,2 .</formula><p>In the next section, we discuss how we realize the above ideas in the proposed UNIT framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Framework</head><p>Our framework, as illustrated in <ref type="figure">Figure 1</ref>, is based on variational autoencoders (VAEs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14]</ref> and generative adversarial networks (GANs) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>. It consists of 6 subnetworks: including two domain image encoders E 1 and E 2 , two domain image generators G 1 and G 2 , and two domain adversarial discriminators D 1 and D 2 . Several ways exist to interpret the roles of the subnetworks, which we summarize in <ref type="table">Table 1</ref>. Our framework learns translation in both directions in one shot.</p><p>VAE. The encoder-generator pair {E 1 , G 1 } constitutes a VAE for the X 1 domain, termed VAE 1 . For an input image x 1 ∈ X 1 , the VAE 1 first maps x 1 to a code in a latent space Z via the encoder E 1 and then decodes a random-perturbed version of the code to reconstruct the input image via the generator G 1 . We assume the components in the latent space Z are conditionally independent and Gaussian with unit variance. In our formulation, the encoder outputs a mean vector E µ,1 (x 1 ) and the distribution of the latent code z 1 is given by</p><formula xml:id="formula_7">q 1 (z 1 |x 1 ) ≡ N (z 1 |E µ,1 (x 1 ), I) where I is an identity matrix. The reconstructed image isx 1→1 1 = G 1 (z 1 ∼ q 1 (z 1 |x 1 )</formula><p>). Note that here we abused the notation since we treated the distribution of q 1 (z 1 |x 1 ) as a random vector of N (E µ,1 (x 1 ), I) and sampled from it. Similarly, {E 2 , G 2 } constitutes a VAE for X 2 : VAE 2 where the encoder E 2 outputs a mean vector E µ,2 (x 2 ) and the distribution of the latent code z 2 is given by</p><formula xml:id="formula_8">q 2 (z 2 |x 2 ) ≡ N (z 2 |E µ,2 (x 2 ), I). The reconstructed image isx 2→2 2 = G 2 (z 2 ∼ q 2 (z 2 |x 2 )).</formula><p>Utilizing the reparameterization trick <ref type="bibr" target="#b12">[13]</ref>, the non-differentiable sampling operation can be reparameterized as a differentiable operation using auxiliary random variables. This reparameterization trick allows us to train VAEs using back-prop. Let η be a random vector with a multi-variate Gaussian distribution: η ∼ N (η|0, I). The sampling operations of z 1 ∼ q 1 (z 1 |x 1 ) and z 2 ∼ q 2 (z 2 |x 2 ) can be implemented via z 1 = E µ,1 (x 1 ) + η and z 2 = E µ,2 (x 2 ) + η, respectively.</p><p>Weight-sharing. Based on the shared-latent space assumption discussed in Section 2, we enforce a weight-sharing constraint to relate the two VAEs. Specifically, we share the weights of the last few layers of E 1 and E 2 that are responsible for extracting high-level representations of the input images in the two domains. Similarly, we share the weights of the first few layers of G 1 and G 2 responsible for decoding high-level representations for reconstructing the input images.</p><p>Note that the weight-sharing constraint alone does not guarantee that corresponding images in two domains will have the same latent code. In the unsupervised setting, no pair of corresponding images in the two domains exists to train the network to output a same latent code. The extracted latent codes for a pair of corresponding images are different in general. Even if they are the same, the same latent component may have different semantic meanings in different domains. Hence, the same latent code could still be decoded to output two unrelated images. However, we will show that through adversarial training, a pair of corresponding images in the two domains can be mapped to a common latent code by E 1 and E 2 , respectively, and a latent code will be mapped to a pair of corresponding images in the two domains by G 1 and G 2 , respectively.</p><p>The shared-latent space assumption allows us to perform image-to-image translation. We can translate an image x 1 in X 1 to an image in X 2 through applying G 2 (z 1 ∼ q 1 (z 1 |x 1 )). We term such an information processing stream as the image translation stream. Two image translation streams exist in the proposed framework: X 1 → X 2 and X 2 → X 1 . The two streams are trained jointly with the two image reconstruction streams from the VAEs. Once we could ensure that a pair of corresponding images are mapped to a same latent code and a same latent code is decoded to a pair of corresponding images, (x 1 , G 2 (z 1 ∼ q 1 (z 1 |x 1 ))) would form a pair of corresponding images. In other words, the composition of E 1 and G 2 functions approximates F * 1→2 for unsupervised image-to-image translation discussed in Section 2, and the composition of E 2 and G 1 function approximates F * 2→1 . GANs. Our framework has two generative adversarial networks:</p><formula xml:id="formula_9">GAN 1 = {D 1 , G 1 } and GAN 2 = {D 2 , G 2 }.</formula><p>In GAN 1 , for real images sampled from the first domain, D 1 should output true, while for images generated by G 1 , it should output false. G 1 can generate two types of images: 1) images from the reconstruction streamx 1→1 1 = G 1 (z 1 ∼ q 1 (z 1 |x 1 )) and 2) images from the translation streamx 2→1 2 = G 1 (z 2 ∼ q 2 (z 2 |x 2 )). Since the reconstruction stream can be supervisedly trained, it is suffice that we only apply adversarial training to images from the translation stream,x 2→1 2 . We apply a similar processing to GAN 2 where D 2 is trained to output true for real images sampled from the second domain dataset and false for images generated from G 2 .</p><p>Cycle-consistency (CC). Since the shared-latent space assumption implies the cycle-consistency constraint (See Section 2), we could also enforce the cycle-consistency constraint in the proposed framework to further regularize the ill-posed unsupervised image-to-image translation problem. The resulting information processing stream is called the cycle-reconstruction stream.</p><p>Learning. We jointly solve the learning problems of the VAE 1 , VAE 2 , GAN 1 and GAN 2 for the image reconstruction streams, the image translation streams, and the cycle-reconstruction streams:</p><formula xml:id="formula_10">min E1,E2,G1,G2 max D1,D2 L VAE1 (E 1 , G 1 ) + L GAN1 (E 2 , G 1 , D 1 ) + L CC1 (E 1 , G 1 , E 2 , G 2 ) L VAE2 (E 2 , G 2 ) + L GAN2 (E 1 , G 2 , D 2 ) + L CC2 (E 2 , G 2 , E 1 , G 1 ).<label>(2)</label></formula><p>VAE training aims for minimizing a variational upper bound In <ref type="formula" target="#formula_10">(2)</ref>, the VAE objects are</p><formula xml:id="formula_11">L VAE1 (E 1 , G 1 ) =λ 1 KL(q 1 (z 1 |x 1 )||p η (z)) − λ 2 E z1∼q1(z1|x1) [log p G1 (x 1 |z 1 )] (3) L VAE2 (E 2 , G 2 ) =λ 1 KL(q 2 (z 2 |x 2 )||p η (z)) − λ 2 E z2∼q2(z2|x2) [log p G2 (x 2 |z 2 )].<label>(4)</label></formula><p>where the hyper-parameters λ 1 and λ 2 control the weights of the objective terms and the KL divergence terms penalize deviation of the distribution of the latent code from the prior distribution. The regularization allows an easy way to sample from the latent space <ref type="bibr" target="#b12">[13]</ref>. We model p G1 and p G2 using Laplacian distributions, respectively. Hence, minimizing the negative log-likelihood term is equivalent to minimizing the absolute distance between the image and the reconstructed image. The prior distribution is a zero mean Gaussian p η (z) = N (z|0, I).</p><p>In <ref type="formula" target="#formula_10">(2)</ref>, the GAN objective functions are given by</p><formula xml:id="formula_12">L GAN1 (E 2 , G 1 , D 1 ) = λ 0 E x1∼P X 1 [log D 1 (x 1 )] + λ 0 E z2∼q2(z2|x2) [log(1 − D 1 (G 1 (z 2 )))] (5) L GAN2 (E 1 , G 2 , D 2 ) = λ 0 E x2∼P X 2 [log D 2 (x 2 )] + λ 0 E z1∼q1(z1|x1) [log(1 − D 2 (G 2 (z 1 )))]. (6)</formula><p>The objective functions in <ref type="bibr" target="#b4">(5)</ref> and <ref type="formula">(6)</ref> are conditional GAN objective functions. They are used to ensure the translated images resembling images in the target domains, respectively. The hyperparameter λ 0 controls the impact of the GAN objective functions.</p><p>We use a VAE-like objective function to model the cycle-consistency constraint, which is given by</p><formula xml:id="formula_13">L CC1 (E 1 , G 1 , E 2 , G 2 ) =λ 3 KL(q 1 (z 1 |x 1 )||p η (z)) + λ 3 KL(q 2 (z 2 |x 1→2 1 ))||p η (z))− λ 4 E z2∼q2(z2|x 1→2 1 ) [log p G1 (x 1 |z 2 )]<label>(7)</label></formula><formula xml:id="formula_14">L CC2 (E 2 , G 2 , E 1 , G 1 ) =λ 3 KL(q 2 (z 2 |x 2 )||p η (z)) + λ 3 KL(q 1 (z 1 |x 2→1 2 ))||p η (z))− λ 4 E z1∼q1(z1|x 2→1 2 ) [log p G2 (x 2 |z 1 )].<label>(8)</label></formula><p>where the negative log-likelihood objective term ensures a twice translated image resembles the input one and the KL terms penalize the latent codes deviating from the prior distribution in the cycle-reconstruction stream (Therefore, there are two KL terms). The hyper-parameters λ 3 and λ 4 control the weights of the two different objective terms.</p><p>Inheriting from GAN, training of the proposed framework results in solving a mini-max problem where the optimization aims to find a saddle point. It can be seen as a two player zero-sum game. The first player is a team consisting of the encoders and generators. The second player is a team consisting of the adversarial discriminators. In addition to defeating the second player, the first player has to minimize the VAE losses and the cycle-consistency losses. We apply an alternating gradient  update scheme similar to the one described in <ref type="bibr" target="#b5">[6]</ref> to solve <ref type="bibr" target="#b1">(2)</ref>. Specifically, we first apply a gradient ascent step to update D 1 and D 2 with E 1 , E 2 , G 1 , and G 2 fixed. We then apply a gradient descent step to update E 1 , E 2 , G 1 , and G 2 with D 1 and D 2 fixed.</p><p>Translation: After learning, we obtain two image translation functions by assembling a subset of the subnetworks. We have F 1→2 (x 1 ) = G 2 (z 1 ∼ q 1 (z 1 |x 1 )) for translating images from X 1 to X 2 and F 2→1 (x 2 ) = G 1 (z 2 ∼ q 2 (z 2 |x 2 )) for translating images from X 2 to X 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first analyze various components of the proposed framework. We then present visual results on challenging translation tasks. Finally, we apply our framework to the domain adaptation tasks.</p><p>Performance Analysis. We used ADAM <ref type="bibr" target="#b10">[11]</ref> for training where the learning rate was set to 0.0001 and momentums were set to 0.5 and 0.999. Each mini-batch consisted of one image from the first domain and one image from the second domain. Our framework had several hyper-parameters. The default values were λ 0 = 10, λ 3 = λ 1 = 0.1 and λ 4 = λ 2 = 100. For the network architecture, our encoders consisted of 3 convolutional layers as the front-end and 4 basic residual blocks <ref type="bibr" target="#b6">[7]</ref> as the back-end. The generators consisted of 4 basic residual blocks as the front-end and 3 transposed convolutional layers as the back-end. The discriminators consisted of stacks of convolutional layers. We used LeakyReLU for nonlinearity. The details of the networks are given in Appendix A.</p><p>We used the map dataset <ref type="bibr" target="#b7">[8]</ref> (visualized in <ref type="figure">Figure 2</ref>), which contained corresponding pairs of images in two domains (satellite image and map) useful for quantitative evaluation. Here, the goal was to learn to translate between satellite images and maps. We operated in an unsupervised setting where we used the 1096 satellite images from the training set as the first domain and 1098 maps from the validation set as the second domain. We trained for 100K iterations and used the final model to translate 1098 satellite images in the test set. We then compared the difference between a translated satellite image (supposed to be maps) and the corresponding ground truth maps pixel-wisely. A pixel translation was counted correct if the color difference was within 16 of the ground truth color value. We used the average pixel accuracy over the images in the test set as the performance metric. We could use color difference for measuring translation accuracy since the target translation function was unimodal. We did not evaluate the translation from maps to images since the translation was multi-modal, which was difficult to construct a proper evaluation metric.</p><p>In one experiment, we varied the number of weight-sharing layers in the VAEs and paired each configuration with discriminator architectures of different depths during training. We changed the number of weight-sharing layers from 1 to 4. (Sharing 1 layer in VAEs means sharing 1 layer for E 1 and E 2 and, at the same time, sharing 1 layer for G 1 and G 2 .) The results were reported in <ref type="figure">Figure 2</ref>(b). Each curve corresponded to a discriminator architecture of a different depth. The x-axis denoted the number of weigh-sharing layers in the VAEs. We found that the shallowest discriminator architecture led to the worst performance. We also found that the number of weight-sharing layer had little impact. This was due to the use of the residual blocks. As tying the weight of one layer, it effectively constrained the other layers since the residual blocks only updated the residual information.</p><p>In the rest of the experiments, we used VAEs with 1 sharing layer and discriminators of 5 layers.</p><p>We analyzed impact of the hyper-parameter values to the translation accuracy. For different weight values on the negative log likelihood terms (i.e., λ 2 , λ 4 ), we computed the achieved translation accuracy over different weight values on the KL terms (i.e., λ 1 , λ 3 ). The results were reported in <ref type="figure">Figure 2</ref>(c). We found that, in general, a larger weight value on the negative log likelihood terms yielded a better translation accuracy. We also found setting the weights of the KL terms to 0.1 resulted in consistently good performance. We hence set λ 1 = λ 3 = 0.1 and λ 2 = λ 4 = 100.</p><p>We performed an ablation study measuring impact of the weight-sharing and cycle-consistency constraints to the translation performance and showed the results in <ref type="figure">Figure 2(d)</ref>. We reported average accuracy over 5 trials (trained with different initialized weights.). We note that when we removed the weight-sharing constraint (as a consequence, we also removed the reconstruction streams in the framework), the framework was reduced to the CycleGAN architecture <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref>. We found the model achieved an average pixel accuracy of 0.569. When we removed the cycle-consistency constraint and only used the weight-sharing constraint 1 , it achieved 0.568 average pixel accuracy. But when we used the full model, it achieved the best performance of 0.600 average pixel accuracy. This echoed our point that for the ill-posed joint distribution recovery problem, more constraints are beneficial.</p><p>Qualitative results. <ref type="figure">Figure 3</ref> to 6 showed results of the proposed framework on various UNIT tasks.</p><p>Street images. We applied the proposed framework to several unsupervised street scene image translation tasks including sunny to rainy, day to night, summery to snowy, and vice versa. For each task, we used a set of images extracted from driving videos recorded at different days and cities. The numbers of the images in the sunny/day, rainy, night, summery, and snowy sets are 86165, 28915, 36280, 6838, and 6044. We trained the network to translate street scene image of size 640×480. In <ref type="figure">Figure 3</ref>, we showed several example translation results . We found that our method could generate realistic translated images. We also found that one translation was usually harder than the other. Specifically, the translation that required adding more details to the image was usually harder (e.g. night to day). Additional results are available in https://github.com/mingyuliutw/unit. Synthetic to real. In <ref type="figure">Figure 3</ref>, we showed several example results achieved by applying the proposed framework to translate images between the synthetic images in the SYNTHIA dataset <ref type="bibr" target="#b22">[23]</ref> and the real images in the Cityscape dataset <ref type="bibr" target="#b1">[2]</ref>. For the real to synthetic translation, we found our method made the cityscape images cartoon like. For the synthetic to real translation, our method achieved better results in the building, sky, road, and car regions than in the human regions.</p><p>Dog breed conversion. We used the images of Husky, German Shepherd, Corgi, Samoyed, and Old English Sheep dogs in the ImageNet dataset to learn to translate dog images between different breeds. We only used the head regions, which were extracted by a template matching algorithm. Several example results were shown in <ref type="figure">Figure 4</ref>. We found our method translated a dog to a different breed.</p><p>Cat species conversion. We also used the images of house cat, tiger, lion, cougar, leopard, jaguar, and cheetah in the ImageNet dataset to learn to translate cat images between different species. We only used the head regions, which again were extracted by a template matching algorithm. Several example results were shown in <ref type="figure" target="#fig_2">Figure 5</ref>. We found our method translated a cat to a different specie.</p><p>Face attribute. We used the CelebA dataset <ref type="bibr" target="#b17">[18]</ref> for attribute-based face images translation. Each face image in the dataset had several attributes, including blond hair, smiling, goatee, and eyeglasses. The face images with an attribute constituted the 1st domain, while those without the attribute constituted the 2nd domain. In <ref type="figure" target="#fig_3">Figure 6</ref>, we visualized the results where we translated several images that do not have blond hair, eye glasses, goatee, and smiling to corresponding images with each of the individual attributes. We found that the translated face images were realistic.</p><p>Domain Adaptation. We applied the proposed framework to the problem for adapting a classifier trained using labeled samples in one domain (source domain) to classify samples in a new domain (target domain) where labeled samples in the new domain are unavailable during training. Early works have explored ideas from subspace learning <ref type="bibr" target="#b3">[4]</ref> to deep feature learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>We performed multi-task learning where we trained the framework to 1) translate images between the source and target domains and 2) classify samples in the source domain using the features extracted by the discriminator in the source domain. Here, we tied the weights of the high-level layers of D 1 and D 2 . This allows us to adapt a classifier trained in the source domain to the target domain. Also, for a pair of generated images in different domains, we minimized the L1 distance    between the features extracted by the highest layer of the discriminators, which further encouraged D 1 and D 2 to interpret a pair of corresponding images in the same way. We applied the approach to several tasks including adapting from the Street View House Number (SVHN) dataset <ref type="bibr" target="#b19">[20]</ref> to the MNIST dataset and adapting between the MNIST and USPS datasets. <ref type="table" target="#tab_2">Table 2</ref> reported the achieved performance with comparison to the competing approaches. We found that our method achieved a 0.9053 accuracy for the SVHN→MNIST task, which was much better than 0.8488 achieved by the previous state-of-the-art method <ref type="bibr" target="#b25">[26]</ref>. We also achieved better performance for the MNIST↔SVHN task than the Coupled GAN approach, which was the state-of-the-art. The digit images had a small resolution. Hence, we used a small network. We also found that the cycle-consistency constraint was not necessary for this task. More details about the experiments are available in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Several deep generative models were recently proposed for image generation including GANs <ref type="bibr" target="#b5">[6]</ref>, VAEs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>, and PixelCNN <ref type="bibr" target="#b26">[27]</ref>. The proposed framework was based on GANs and VAEs but it was designed for the unsupervised image-to-image translation task, which could be considered as a conditional image generation model. In the following, we first review several recent GAN and VAE works and then discuss related image translation works.</p><p>GAN learning is via staging a zero-sum game between the generator and discriminator. The quality of GAN-generated images had improved dramatically since the introduction. LapGAN <ref type="bibr" target="#b2">[3]</ref> proposed a Laplacian pyramid implementation of GANs. DCGAN <ref type="bibr" target="#b20">[21]</ref> used a deeper convolutional network. Several GAN training tricks were proposed in <ref type="bibr" target="#b23">[24]</ref>. WGAN <ref type="bibr" target="#b0">[1]</ref> used the Wasserstein distance.</p><p>VAEs optimize a variational bound. By improving the variational approximation, better image generation results were achieved <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12]</ref>. In <ref type="bibr" target="#b13">[14]</ref>, a VAE-GAN architecture was proposed to improve image generation quality of VAEs. VAEs were applied to translate face image attribute in <ref type="bibr" target="#b27">[28]</ref>.</p><p>Conditional generative model is a popular approach for mapping an image from one domain to another. Most of the existing works were based on supervised learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Our work differed to the previous works in that we do not need corresponding images. Recently, <ref type="bibr" target="#b25">[26]</ref> proposed the domain transformation network (DTN) and achieved promising results on translating small resolution face and digit images. In addition to faces and digits, we demonstrated that the proposed framework can translate large resolution natural images. It also achieved a better performance in the unsupervised domain adaptation task. In <ref type="bibr" target="#b24">[25]</ref>, a conditional generative adversarial network-based approach was proposed to translate a rendering images to a real image for gaze estimation. In order to ensure the generated real image was similar to the original rendering image, the L1 distance between the generated and original image was minimized. We note that two contemporary papers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref> independently introduced the cycle-consistency constraint for the unsupervised image translation. We showed that that the cycle-consistency constraint is a natural consequence of the proposed shared-latent space assumption. From our experiment, we found that cycle-consistency and the weight-sharing (a realization of the shared-latent space assumption) constraints rendered comparable performance. When the two constraints were jointed used, the best performance was achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We presented a general framework for unsupervised image-to-image translation. We showed it learned to translate an image from one domain to another without any corresponding images in two domains in the training dataset. The current framework has two limitations. First, the translation model is unimodal due to the Gaussian latent space assumption. Second, training could be unstable due to the saddle point searching problem. We plan to address these issues in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Network Architecture</head><p>The network architecture used for the unsupervised image-to-image translation experiments is given in <ref type="table" target="#tab_3">Table 3</ref>. We use the following abbreviation for ease of presentation: N=Neurons, K=Kernel size, S=Stride size. The transposed convolutional layer is denoted by DCONV. The residual basic block is denoted as RESBLK. The encoder and generator architecture was given in <ref type="table" target="#tab_4">Table 4</ref>. For the discriminators and classifiers, we used an architecture akin to the one used in the Coupled GAN paper, which is given in <ref type="table" target="#tab_5">Table 5</ref>.</p><p>SVHN→MNIST. For the SVHN and MNIST domain adaptation experiment, we used the extra training set (consisting of 531131 images) in the SVHN dataset for the source domain and the training set (consisting of 60000 images) in the MNIST dataset for the target domain as in the DTN work <ref type="bibr" target="#b25">[26]</ref>. The test set consists of 10000 images in the MNIST test dataset. The MNIST images were in gray-scale. We converted them to RGB images and performed a data augmentation where we also used the inversions of the original MNIST images for training. All the images were resized to 32×32 for facilitating the experiment. We also found spatial context information was useful. For each input image, we created a 5-channel variant where the first three channels were the original RGB images and the last two channels were the normalized x and y coordinates.</p><p>The encoder and generator architecture was the same as the one used in the MNIST↔USPS domain adaptation. For the discriminators and the classifier, we used a network architecture akin to the one used in the DTN paper. The details are given in <ref type="table" target="#tab_6">Table 6</ref>. We used dropout at every layer in the classifier for avoiding over-fitting. DCONV-(N3,K1,S1), TanH No  1 CONV-(N64,K5,S1), MaxPooling-(K2,S2) No 2 CONV-(N128,K5,S1), MaxPooling-(K2,S2) Yes 3</p><p>CONV-(N256,K5,S1), MaxPooling-(K2,S2) Yes 4</p><p>CONV-(N512,K5,S1), MaxPooling-(K2,S2) Yes 5a</p><p>FC-(N1), Sigmoid Yes 5b</p><p>FC-(N10), Softmax Yes</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Figure 2 :</head><label>22</label><figDesc>(a) Illustration of the Map dataset. Left: satellite image. Right: map. We translate holdout satellite images to maps and measure the accuracy achieved by various configurations of the proposed framework. (b) Translation accuracy versus different network architectures. (c) Translation accuracy versus different hyper-parameter values. (d) Impact of weight-sharing and cycle-consistency constraints on translation accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Street scene image translation results. For each pair, left is input and right is the translated image. Dog breed translation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Cat species translation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Attribute-based face translation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. arXiv:1703.00848v6 [cs.CV] 23 Jul 2018</figDesc><table><row><cell>Z : shared latent space z</cell><cell></cell></row><row><cell>x 1</cell><cell>x 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Unsupervised domain adaptation performance. The reported numbers are classification accuracies.</figDesc><table><row><cell>Method</cell><cell>SA [4]</cell><cell>DANN [5]</cell><cell>DTN [26]</cell><cell>CoGAN</cell><cell>UNIT (proposed)</cell></row><row><cell>SVHN→ MNIST</cell><cell>0.5932</cell><cell>0.7385</cell><cell>0.8488</cell><cell>-</cell><cell>0.9053</cell></row><row><cell>MNIST→ USPS</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.9565</cell><cell>0.9597</cell></row><row><cell>USPS→ MNIST</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.9315</cell><cell>0.9358</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Network architecture for the unsupervised image translation experiments. For the MNIST and USPS adaptation experiments, we used the entire training sets in the two domains (60000 training images for MNIST and 7291 training images for USPS) for learning and reported the classification performance on the test sets (10000 test images for MNIST and 2007 test images for USPS). The MNIST and USPS images had different sizes, we resized them to 28×28 for facilitating the experiments. We trained the Coupled GAN algorithm<ref type="bibr" target="#b16">[17]</ref> using the same setting for a fair comparison.</figDesc><table><row><cell cols="2">Layer Encoders</cell><cell>Shared?</cell></row><row><cell>1</cell><cell>CONV-(N64,K7,S1), LeakyReLU</cell><cell>No</cell></row><row><cell>2</cell><cell>CONV-(N128,K3,S2), LeakyReLU</cell><cell>No</cell></row><row><cell>3</cell><cell>CONV-(N256,K3,S2), LeakyReLU</cell><cell>No</cell></row><row><cell>4</cell><cell>RESBLK-(N512,K3,S1)</cell><cell>No</cell></row><row><cell>5</cell><cell>RESBLK-(N512,K3,S1)</cell><cell>No</cell></row><row><cell>6</cell><cell>RESBLK-(N512,K3,S1)</cell><cell>No</cell></row><row><cell>µ</cell><cell>RESBLK-(N512,K3,S1)</cell><cell>Yes</cell></row><row><cell cols="2">Layer Generators</cell><cell>Shared?</cell></row><row><cell>1</cell><cell>RESBLK-(N512,K3,S1)</cell><cell>Yes</cell></row><row><cell>2</cell><cell>RESBLK-(N512,K3,S1)</cell><cell>No</cell></row><row><cell>3</cell><cell>RESBLK-(N512,K3,S1)</cell><cell>No</cell></row><row><cell>4</cell><cell>RESBLK-(N512,K3,S1)</cell><cell>No</cell></row><row><cell>5</cell><cell>DCONV-(N256,K3,S2), LeakyReLU</cell><cell>No</cell></row><row><cell>6</cell><cell>DCONV-(N128,K3,S2), LeakyReLU</cell><cell>No</cell></row><row><cell>7</cell><cell>DCONV-(N3,K1,S1), TanH</cell><cell>No</cell></row><row><cell cols="2">Layer Discriminators</cell><cell>Shared?</cell></row><row><cell>1</cell><cell>CONV-(N64,K3,S2), LeakyReLU</cell><cell>No</cell></row><row><cell>2</cell><cell>CONV-(N128,K3,S2), LeakyReLU</cell><cell>No</cell></row><row><cell>3</cell><cell>CONV-(N256,K3,S2), LeakyReLU</cell><cell>No</cell></row><row><cell>4</cell><cell>CONV-(N512,K3,S2), LeakyReLU</cell><cell>No</cell></row><row><cell>5</cell><cell>CONV-(N1024,K3,S2), LeakyReLU</cell><cell>No</cell></row><row><cell>6</cell><cell>CONV-(N1,K2,S1), Sigmoid</cell><cell>No</cell></row><row><cell cols="2">B Domain Adaptation</cell><cell></cell></row><row><cell>MNIST↔USPS.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Encoder and generator architecture for MNIST↔USPS domain adaptation.</figDesc><table><row><cell cols="2">Layer Encoders</cell><cell>Shared?</cell></row><row><cell>1</cell><cell>CONV-(N64,K5,S2), BatchNorm, LeakyReLU</cell><cell>No</cell></row><row><cell>2</cell><cell>CONV-(N128,K5,S2), BatchNorm, LeakyReLU</cell><cell>Yes</cell></row><row><cell>3</cell><cell>CONV-(N256,K8,S1), BatchNorm, LeakyReLU</cell><cell>Yes</cell></row><row><cell>4</cell><cell>CONV-(N512,K1,S1), BatchNorm, LeakyReLU</cell><cell>Yes</cell></row><row><cell>µ</cell><cell>CONV-(N1024,K1,S1)</cell><cell>Yes</cell></row><row><cell cols="2">Layer Generators</cell><cell>Shared?</cell></row><row><cell>1</cell><cell>DCONV-(N512,K4,S2), BatchNorm, LeakyReLU</cell><cell>Yes</cell></row><row><cell>2</cell><cell>DCONV-(N256,K4,S2), BatchNorm, LeakyReLU</cell><cell>Yes</cell></row><row><cell>3</cell><cell>DCONV-(N128,K4,S2), BatchNorm, LeakyReLU</cell><cell>Yes</cell></row><row><cell>4</cell><cell>DCONV-(N64,K4,S2), BatchNorm, LeakyReLU</cell><cell>No</cell></row><row><cell>5</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Discriminator architecture for MNIST↔USPS domain adaptation.</figDesc><table><row><cell cols="2">Layer Discriminators</cell><cell>Shared?</cell></row><row><cell>1</cell><cell>CONV-(N20,K5,S1), MaxPooling-(K2,S2)</cell><cell>No</cell></row><row><cell>2</cell><cell>CONV-(N50,K5,S1), MaxPooling-(K2,S2)</cell><cell>Yes</cell></row><row><cell>3</cell><cell>FC-(N500), ReLU, Dropout</cell><cell>Yes</cell></row><row><cell>4a</cell><cell>FC-(N1), Sigmoid</cell><cell>Yes</cell></row><row><cell>4b</cell><cell>FC-(N10), Softmax</cell><cell>Yes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Discriminator architecture for SVHN→MNIST domain adaptation.</figDesc><table><row><cell>Layer Discriminators</cell><cell>Shared?</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We used this architecture in an earlier version of the paper.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scharwächter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<title level="m">The cityscapes dataset. Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference in Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Auto-encoding variational bayes. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Lectures on the coupling method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Courier Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Deep learning face attributes in the wild. International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<title level="m">Auxiliary deep generative models. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and variational inference in deep latent gaussian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference in Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
