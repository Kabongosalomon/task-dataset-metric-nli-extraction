<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DADA: Depth-Aware Domain Adaptation in Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>1 valeo.ai</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DADA: Depth-Aware Domain Adaptation in Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: We propose a novel depth-aware domain adaptation framework (DADA) to efficiently leverage depth as privileged information in the unsupervised domain adaptation setting. This example shows how semantic segmentation of a scene from the target domain benefits from the proposed approach, in comparison to state-of-the-art domain adaptation with no use of depth. In figure's top, we use different background colors (blue and red) to represent source and target information that are available during training. Here, annotated source domain data come from the synthetic SYNTHIA dataset and un-annotated target domain images are real scenes from Cityscapes. The cyclist highlighted by the yellow box is a good qualitative illustration of the improvement we obtain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Unsupervised domain adaptation (UDA) is important for applications where large scale annotation of representative data is challenging. For semantic segmentation in particular, it helps deploy, on real "target domain" data, models that are trained on annotated images from a different "source domain", notably a virtual environment. To this end, most previous works consider semantic segmentation as the only mode of supervision for source domain data, while ignoring other, possibly available, information like depth. In this work, we aim at exploiting at best such a privileged information while training the UDA model. We propose a unified depth-aware UDA framework that leverages in several complementary ways the knowledge of dense depth in the source domain. As a result, the performance of the trained semantic segmentation model on the target domain is boosted. Our novel approach indeed achieves state-of-the-art performance on different challenging synthetic-2-real benchmarks. Code and models are available at https://github.com/ valeoai/DADA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Advances in deep convolutional neural networks (CNNs) brought significant leaps forward in many recognition tasks including semantic segmentation. Still, predicting semantic labels for all imagery pixels is a challenging problem, especially when models are trained on one domain, named as source, yet evaluated on another domain, named as target. The so-called domain gap between source and target distributions often causes drastic drops in target performance. Instead, autonomous critical systems such as self-driving cars require robust performance under diverse testing conditions, despite the lack of ubiquitous training data. For such systems, semantic segmentation models trained on sunny urban images taken in Roma should yield good results even on foggy scenes in London.</p><p>Techniques addressing the domain gap problem are usually classified as domain adaptation (DA) <ref type="bibr" target="#b6">[7]</ref>. In previous works, most DA settings are unsupervised on the target side, i.e., only un-annotated target samples are available during the supervised training on source domain. This is referred to as unsupervised domain adaptation (UDA). In recent years, considerable progress has been made in UDA with approaches such as distribution discrepancy minimization with MMD <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref> or adversarial training <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref>, generative approaches <ref type="bibr" target="#b12">[13]</ref> and others.</p><p>In this UDA context, the synthetic-2-real scenario, where source and target samples are synthetic and real images respectively, is especially appealing thanks to the zero-cost source label acquisition. Moreover, recent virtual engines can simulate other sensory outputs like dense/sparse depth or radar; for example, the SYNTHIA dataset <ref type="bibr" target="#b28">[29]</ref> provides corresponding virtual depth maps of synthesized urban scenes. Most previous UDA works ignore such extra information, except for Lee et al. <ref type="bibr" target="#b17">[18]</ref> who propose to use depth for regularizing a style-transfer network.</p><p>In this work, we propose a new scheme to leverage depth information available in source domain for UDA. We hypothesize that introducing additional depth-specific adaptation brings complementary effects to further bridge the performance gap between source and target at test time. Toward this end, we transform the segmentation backbone such that the depth information is embedded into a dedicated deep architecture by means of an auxiliary depth regression task. Depth, operating as an additional sourcedomain supervision in our framework (only available while training), will be considered as a privileged information.</p><p>Another challenge is to incorporate efficiently depth signals into the UDA learning. Addressing this concern, we introduce a new depth-aware adversarial training protocol based on the fusion of the network outputs. Such a late fusion was inspired by our intuition that visual information at different depth levels should be treated differently.</p><p>The proposed approach is illustrated in <ref type="figure">Figure 1</ref>, where the benefit of depth-aware adaptation on key object categories like 'human' and 'vehicle' is visible. The contributions of this approach are the following:</p><p>• Depth-aware UDA learning strategy: we introduce a novel depth-aware adaptation scheme, coined DADA learning, which simultaneously aligns segmentationbased and depth-based information of source and target while being aware of scene geometry. • Depth-aware deep architecture: we propose a novel depth-aware segmentation pipeline, named DADA architecture, in which depth-specific and standard CNN appearance features are fused before being forwarded through the segmentation classifiers. • State-of-the-art performance: evaluations show that our framework achieves SotA results on challenging synthetic-2-real benchmarks. We also report ablation studies to provide insights into the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Unsupervised domain adaption has received a lot of attention in last few years <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref>. It is usually approached with domain discrepancy minimization, generative approaches or using some privileged information to guide the learning. Since we are only concerned with visual semantic segmentation in this work, we limit our review of UDA to approaches that aim at this task as well.</p><p>Various approaches of UDA for segmentation employ adversarial training to minimize cross-domain discrepancy. The main idea, stemming from generative adversarial networks (GANs) <ref type="bibr" target="#b9">[10]</ref>, is to train a discriminator for predicting the domain of the data (source or target) while the segmentation network tries to fool it (along with the supervised segmentation task on the source). Under the competition with the discriminator, the segmentation network tries to map its input to domain-agnostic intermediate or final representation (which is the input to the discriminator), before accomplishing its task. This alignment with adversarial training is usually done in the feature space. In <ref type="bibr" target="#b13">[14]</ref>, the feature alignment is done not only with adversarial training but also by transferring the label statistics of the source domain by category specific adaptation. <ref type="bibr" target="#b4">[5]</ref> uses adversarial training for class-level alignment on grid-wise soft pseudo-labels. In <ref type="bibr" target="#b3">[4]</ref>, spatial-aware adaptation is conducted and a distillation loss is used to address specifically synthetic-to-real adaptation by enforcing the segmentation network's output to be similar to a reference network trained on real data. While most of these methods do alignment on the features space, recently <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39]</ref> propose alignment on the output space. <ref type="bibr" target="#b34">[35]</ref> does the alignment on the prediction of the segmentation network and <ref type="bibr" target="#b38">[39]</ref> proposes to do it on the weighted self-information of the prediction probability. <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b22">[23]</ref> extend the approach of <ref type="bibr" target="#b34">[35]</ref> by patch-level alignment and category-level adversarial loss respectively. Another use of adversarial training for UDA is proposed in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, where the discrepancy between two instances of the same input from target domain is minimized while the classification layer tries to maximize it. The approach we propose also uses adversarial training but it takes this further by introducing the depth.</p><p>Another strategy that received much attention is the use of generative networks to turn source domain samples into target-like images. CyCADA <ref type="bibr" target="#b12">[13]</ref> uses Cycle-GAN <ref type="bibr" target="#b44">[45]</ref> to generate target-like images conditioned on the source images, i.e., generated images contain the structure or semantic content of the source with the "style" (colors and textures) of the target domain. These generated images inherit the ground truth semantic segmentation of the conditioning source images and can then be used for supervised training of the segmentation network. <ref type="bibr" target="#b45">[46]</ref> aligns the source and target embeddings using GAN <ref type="bibr" target="#b9">[10]</ref> and replaces the crossentropy loss by a conservative loss (CL) which penalizes the easy and hard source examples. In DCAN <ref type="bibr" target="#b40">[41]</ref>, a similar generative approach is used with channel-wise feature alignment in the generator and segmentation networks. Some other interesting works have investigated combination of adversarial and generative approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref>, self-training <ref type="bibr" target="#b46">[47]</ref> and curriculum style learning <ref type="bibr" target="#b42">[43]</ref>.</p><p>In order to help domain adaptation in various tasks, several works explore the use of privileged information on the source data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref>. The idea of using privileged information (PI) for learning was first formulated by <ref type="bibr" target="#b37">Vapnik &amp; Vashist (2009)</ref>  <ref type="bibr" target="#b37">[38]</ref>. PI is an additional information available only at training time. This is conceptually similar to humans learning new notions or concepts with the help of teacher's comment or explanation. Following <ref type="bibr" target="#b37">[38]</ref> many works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref> investigated PI for various tasks. Recently, SPIGAN <ref type="bibr" target="#b17">[18]</ref> has proposed such a UDA approach for semantic segmentation. SPIGAN first uses a generative network to convert the source images into target-type images. These new images are then used to train the segmentation network along with a depth regression network (privileged information network) in a supervised manner, thanks to the ground truth of the source images. This work shows that the additional depth regression task helps in pixel level adaptation or, in other words, better captures the content of source images in the generated target-like images.</p><p>In the present work, we also leverage a depth regression task to aid domain adaptation. However, this is accomplished in a way that notably differs form SPIGAN (which uses depth only as a regularization for the generator).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Depth-aware domain adaptation</head><p>In this section, we describe our proposed UDA method for semantic segmentation using depth. Our goal is to use depth as the privileged information in the UDA setting to improve segmentation performance on the target domain.</p><p>To this end, we modify a network of semantic segmentation by including a monocular depth regression part. More specifically, we design a deep architecture to embed the depth in a dedicated residual block as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> ('DADA architecture' in top part). In Section 3.1, we detail DADA network architecture and the supervised learning used on source dataset. The second part concerns the learning scheme for such a UDA approach. To get the full benefit of the geometry information, we propose a depthaware adversarial learning scheme. We argue that for domain adaptation, particularly in the urban settings, objects closer to the ego-camera should be emphasized more. Our framework, illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> ('DADA learning' in bottom part), is detailed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DADA Network Architecture.</head><p>Starting from an existing semantic segmentation architecture, we insert additional modules (1) to predict monocular depth as additional output and (2) to feed the information exploited by this auxiliary task back to the main stream. More specifically, we adopt the residual auxiliary block that was recently introduced in <ref type="bibr" target="#b23">[24]</ref> for detection. It amounts to grafting a new branch to the backbone CNN. In this branch, the backbone CNN features are consecutively fed into three encoding convolutional layers, followed by an average pooling layer to output depth map predictions. On the residual path back to the main branch, the encoded features (before the depth pooling) are decoded by a convolutional layer and fused with the backbone features. The top-part of <ref type="figure" target="#fig_0">Figure 2</ref> shows the proposed hybrid architecture, mixing the auxiliary block architecture with the backbone one.</p><p>Importantly, for the feature-level fusion, we adopt an element-wise product, indicated as "Feat. Fusion" in the top part of <ref type="figure" target="#fig_0">Figure 2</ref>. To produce segmentation predictions, we feed-forward the fused features through the remaining classification modules.</p><p>Source domain supervised training. Our model is trained with supervised segmentation and depth losses on source domain.</p><p>We consider a training set T s ⊂ R H×W ×3 × (1, C) H×W × R H×W + of source color images of size H × W along with pixel-level C-class segmentation and depth annotations.</p><p>Let DADA be the network which takes an image x and jointly predicts a C-dimensional "soft-segmentation map"</p><formula xml:id="formula_0">DADA seg (x) = P x = P (h,w,c) x h,w,c 1 and a depth map DADA depth (x) = Z (h,w) x</formula><p>h,w . Similar to <ref type="bibr" target="#b14">[15]</ref>, we adopt the inverse depth representation, i.e., depth attenuates when moving away from the camera. The parameters θ DADA of DADA are learned to minimize the segmentation and depth losses on source samples (x s , y s , z s ) ∈ T s :</p><formula xml:id="formula_1">L seg (x s , y s ) = − H h=1 W w=1 C c=1 y (h,w,c) s log P (h,w,c) xs , (1) L dep (x s , z s ) = − H h=1 W w=1 berHu Z (h,w) xs −z (h,w) s ,<label>(2)</label></formula><p>with the reverse Huber loss defined as <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_2">berHu(e z ) = |e z | , if |e z | ≤ c, e 2 z +c 2 2c otherwise,<label>(3)</label></formula><p>where c is a positive threshold that we fix in practice to <ref type="bibr">1 5</ref> of the maximum depth residual. Empirically, the berHu loss is favorable for the depth regression task: samples with larger residuals are penalized more by the 2 term, while gradients of small-residual samples are more underlined with 1 .</p><p>Finally, our DADA optimization problem on source domain is formulated as:</p><formula xml:id="formula_3">min θDADA 1 |T s | Ts L seg (x s , y s ) + λ dep L dep (x s , z s ),<label>(4)</label></formula><p>with λ dep a weighting factor for depth regression. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">DADA Adversarial Learning Scheme</head><p>For UDA in semantic segmentation, the key idea is to align the source and target domains so that a discriminator network cannot distinguish between the domains. We follow here the recent strategies that align features at the output level <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39]</ref>, i.e., the soft segmentation map P x produced by the segmentation network DADA seg on input image x.</p><p>We hereby question the plausibility of adapting such a methodology for an auxiliary space, i.e., depth prediction Z x in the present work, in the hope that the performance of the main task is improved. We hypothesize that aligning source and target distributions also in the depth space implicitly bridges the domain gaps of the shared lower-level CNN representations and should bring improvements to the main task on target domain.</p><p>In order to carry out such a strategy, we propose a joint alignment: we first merge both signals, then we feedforward the fused features as input to a discriminator.</p><p>We illustrate DADA adversarial learning scheme in the lower right part of <ref type="figure" target="#fig_0">Figure 2</ref>. More precisely:</p><p>• We compute weighted self-information ("surprisal") maps <ref type="bibr" target="#b38">[39]</ref> I x ∈ [0, 1] H×W ×C defined as:</p><formula xml:id="formula_4">I (h,w,c) x = −P (h,w,c) x · log P (h,w,c) x .<label>(5)</label></formula><p>• We fuse the weighted self-information I x with the depth prediction Z x to produce a depth-aware mapÎ x . The fusion of Z x and I x , which we refer to as DADA fusion, is the element-wise product of I x with Z x . As here the inverse depth is used, such a multiplication implies stronger attention toward scene elements that are closer to the ego-camera. • Then, we do the adversarial adaptation onÎ x . The depth-aware mapÎ x carries 3D-structural and geometrical information which should be consistent across the domains, thus alignment on this space is beneficial to adaptation as we will see in the next section.</p><p>Formally, given X t the set of un-annotated images in the target domain, the discriminator D is trained to distinguish source vs. target outputs (labeled as '1' and '0' respectively) with the following classification objective minimization: <ref type="bibr" target="#b5">(6)</ref> and the DADA network is updated using the "fooling" objective minimization:</p><formula xml:id="formula_5">min θ D 1 |T s | Ts L D (Î xs , 1) + 1 |X t | Xt L D (Î xt , 0),</formula><formula xml:id="formula_6">min θDADA 1 |X t | Xt L D (Î xt , 1).<label>(7)</label></formula><p>At each training iteration, we feed the network a minibatch of two samples coming from source and target domains. The two objectives in (4) and <ref type="bibr" target="#b6">(7)</ref> are jointly optimized with a weighting factor λ adv used for the adversarial part. Gradients of all losses are accumulated and then backpropagated to update the network.</p><p>Discussion. The only method using depth as privileged information in the same task as ours is SPIGAN <ref type="bibr" target="#b17">[18]</ref>. While in DADA, with feature fusion and DADA fusion, we exploit depth to enhance the appearance features and improve the source-target alignment, SPIGAN leverages depth as a regularization for the pixel-level alignment generator. We argue that our way of using depth is more explicit, which, in return, gets more benefit from the privileged information. Moreover, thanks to the residual fusion in the auxiliary block presented in Section 3.1, depth signals are directly taken into account while deriving main task predictions. Such a residual fusion spreads beneficial effects of the depth-specific adaptation to the main segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section presents quantitative and qualitative results. We introduce in Section 4.1 the synthetic-2-real benchmarks used in this work. We then analyze the performance of the proposed model in Section 4.2 and report ablation studies in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental details</head><p>Datasets. In this work, we use SYNTHIA dataset <ref type="bibr" target="#b28">[29]</ref> as the source domain. It is composed of 9, 400 synthetic images annotated with pixel-wise semantic labels and depth. Similar to previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>, we adopt the split SYNTHIA-RAND-CITYSCAPES using Cityscapesstyle annotations. For the target domain, we use either Cityscapes <ref type="bibr" target="#b5">[6]</ref> or Mapillary Vistas <ref type="bibr" target="#b25">[26]</ref> datasets. What follows are our experimental set-ups in detail:</p><p>• SYNTHIA→Cityscapes (16 classes): This is a standard evaluation protocol used in previous works. The models are trained on the 16 classes common to SYNTHIA and Cityscapes. Similar to <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39]</ref>, we also report performance on the 13-class subset.</p><p>• SYNTHIA→Cityscapes / Vistas (7 classes): Following <ref type="bibr" target="#b17">[18]</ref>, we conduct experiments on the 7 categories that are common to SYNTHIA, Cityscapes and Vistas.</p><p>Network architecture. In our experiments, we adopt Deeplab-V2 <ref type="bibr" target="#b2">[3]</ref> based on ResNet-101 <ref type="bibr" target="#b10">[11]</ref> as the backbone segmentation architecture. Like <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39]</ref>, we apply Atrous Spatial Pyramid Pooling (ASPP) with sampling rates of {6, 12, 18, 24}. Segmentation prediction is only done on the conv5 features. For the adversarial training, we use DC-GAN's discriminator <ref type="bibr" target="#b27">[28]</ref> composed of 4 sequential convolutional layers with leaky-ReLUs as activation functions. The encoding module used for depth regression has three consecutive convolutional layers: the first and last ones have kernel size of 1; the middle layer has kernel size of 3 with a suitable zero-padding to ensure the same input and output resolutions. Each layer uses 4 times fewer channels than the previous one. In the decoding part, we feed the encoded features through a 1 × 1 convolutional layer. The decoding layer has the same number of output channels as the channel size of the ResNet-101 backbone feature.</p><p>Implementation details. Implementations are done with the PyTorch deep learning framework <ref type="bibr" target="#b26">[27]</ref>. To train and validate our models, we use a single NVIDIA 1080TI GPU with 11GB memory. We initialize our models with the ResNet-101 <ref type="bibr" target="#b10">[11]</ref> pre-trained on the ImageNet dataset <ref type="bibr" target="#b7">[8]</ref>. Segmentation and depth regression networks are trained by a standard Stochastic Gradient Descent optimizer <ref type="bibr" target="#b0">[1]</ref> with learning rate 2.5 × 10 −4 , momentum 0.9 and weight decay 10 −4 . For discriminator training, we adopt Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with learning rate 10 −4 . In all experiments, we fixed λ dep as 10 −3 for the depth regression task and used λ adv = 10 −3 to weight the adversarial loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We present results of the proposed DADA approach in comparison to different baselines. On the three benchmarks, our models achieve state-of-the-art performance. Our extensive study shows the benefit of leveraging depth as privileged information with our DADA framework for UDA in semantic segmentation. <ref type="table" target="#tab_0">Table 1</ref>, we report semantic segmentation performance in term of "mean Intersection over Union" (mIoU in %) on the 16 classes of the Cityscapes validation set. DADA achieves state-of-the-art performance on the benchmark. To the best of our knowledge, SPIGAN <ref type="bibr" target="#b17">[18]</ref> is the only published work targeting the same problem that also considers depth as privileged information. DADA achieves a D-Gain of 1.8%, almost double of SPIGAN's. Analyzing per-class results, we observe that the improvement over AdvEnt <ref type="bibr" target="#b38">[39]</ref> primarily comes from the 'vehicle' category, i.e., 'car' (+7%), 'bus' (+8.1%) and 'bike' (+5.1%). On 'object' classes like 'light' and 'pole', DADA introduces moderate gains. <ref type="figure">Figure 3</ref>  Top and bottom sub-tables correspond to VGG-16-based and ResNet-101-based models respectively. For methods making use of depth, we report the absolute depth-driven mIoU gain (D-Gain). We also show the mIoU (%) of the 13 classes (mIoU*) excluding classes with *.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYNTHIA→Cityscapes: In</head><p>(a) SYNTHIA → Cityscapes <ref type="formula" target="#formula_6">(7 classes</ref>  <ref type="table">Table 2</ref>: Semantic segmentation performance mIoU (%) in 7-classes setups. (a) Cityscapes and (b) Vistas validation set. We report results produced at different resolutions. AdvEnt* is the adaptation of AdvEnt published code to 7-classes set-ups. <ref type="table">Table 2</ref>-a shows results in the same experimental set-up, except that training and validation were done on 7 categories. To be comparable with <ref type="bibr" target="#b17">[18]</ref>, we report additional results produced at the 320 × 640 resolution. Our DADA framework outperforms the state-of-the-art on this benchmark by a large margin. Over the AdvEnt baseline, we achieve +3.2% mIoU improvement. Similar to the 16-class results, an important gain on the 'vehicle' category (+3.7%) is observed. In addition, we contrast a +3% IoU on the 'human' category to the negative IoU drops on classes 'person' and 'rider' reported in <ref type="table" target="#tab_0">Table 1</ref>. We conjecture that this drop stems from the intra-category confusion, i.e., 'pedestrian' and 'rider' are easily confused. A significant improvement is pointed out in the lower resolution set-up where using depth adds +14.0% to the 'human' category IoU. These re-sults demonstrate the merit of DADA for UDA, especially on crucial categories like 'human' and 'vehicle' -the vulnerable road users (VRUs).</p><p>An interesting UDA metric introduced in <ref type="bibr" target="#b17">[18]</ref> is the negative transfer rate (lower is better) -the percentage of afteradaptation test cases having per-image mIoUs lower than ones coming from the model trained only on source (without adaptation). On SYNTHIA→Cityscapes (7 classes), 320 × 640 resolution DADA model has only 5% negative transfer rate, compared to 9% for SPIGAN. It is worth noting that our only-on-source mIoU in this case is 50%, much larger than the one reported in <ref type="bibr" target="#b17">[18]</ref> (36.3%). The AdvEnt baseline, without using depth, suffers from a negative transfer rate of 11% -more than double of DADA's.   DADA (trained and tested on 320 × 640 images) has 30% negative transfer rate compared to the 42% of SPI-GAN. As discussed in <ref type="bibr" target="#b17">[18]</ref>, the challenging domain gap between SYNTHIA and Vistas might cause these high negative rates. In addition to this explanation, we also question the annotation quality of the Vistas dataset, visually inspection of results having revealed inconsistencies. Interestingly, when we evaluate the DADA model trained with the current set-up (SYNTHIA→Vistas) on the Cityscapes validation set with arguably cleaner annotations, the obtained negative transfer rate reduces to 6%.</p><p>In <ref type="figure">Figure 4</ref>, we show some qualitative results comparing our best model with SPIGAN. As mentioned above, we note that the Vistas segmentation annotations are noisy. For example, some construction areas slightly covered by tree branches are annotated as 'vegetation'. DADA provides reasonable predictions on these areas -sometimes even better than human ground-truths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation studies</head><p>Effect of depth-aware adversarial adaptation. We report in <ref type="table" target="#tab_3">Table 3</ref> performance with seven training setups, S1 to S7: S1 is the source-only baseline (no adaption at all), S2 amounts to AdvEnt (no use of depth) and S7 is DADA. Intermediate setups S3 to S6 amount to using or not the AdvEnt's "surprisal adaptation", the auxiliary depth adaptation ("depth adaptation"), the point-wise feature fusion mechanism ("feature fusion") and the output-based fusion for DADA adversarial training ("DADA fusion"). First, we remark that adversarial adaptation on the auxiliary depthspace (S4 and S5) does help improve performance of the main task. Improvement of S5 over S4 demonstrates the advantage of depth fusion at the feature-level. Comparable performance of S2 and S3 indicates that depth supervision on the source domain is not effective in absence of depthspecific adaptation. Indeed, S6, with two separate adversar-% of SYNTHIA 10% 30% 50% 70% 100% Cityscapes mIoU 32.6 35.1 40.9 41.0 42.6 ial adaptations on the surprisal and depth spaces, works better than S2 and S3. Still, in S6, the coupling between spaces remains loose as the adversarial losses are separately optimized. Our depth-aware adaptation framework S7 employing both feature fusion and DADA fusion performs best: paying more attention to closer objects during adversarial training is beneficial.</p><p>Annotation effort advantage. <ref type="table" target="#tab_4">Table 4</ref> reports DADA's performance when trained on different fractions of the source dataset. Using only 50% of the SYNTHIA images with segmentation and depth annotations, DADA achieves comparable performance to AdvEnt trained on all images with segmentation annotations (40.9% vs. 40.8%). This finding is of practical importance for real-world setups where the source domain is also composed of real scenes: while dense depth annotation remains automatic in this case (through stereo matching as in Cityscapes or densification of sparse LiDAR measurements), semantic annotation must be manual, which incurs high costs and quality problems. Annotating fewer scenes can thus be beneficial even if depth is additionally required.</p><p>Limitations. We observe a few failure cases where different objects are indistinguishable due to blurry depth outputs. Improving depth quality may help in such cases. However, in our framework depth regression is only an auxiliary task which helps leveraging geometry-specific information to enrich visual representation and thus to improve on the main task. As in <ref type="bibr" target="#b23">[24]</ref>, paying too much attention to the auxiliary task actually hurts the performance on the main task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a novel UDA framework coined DADA -Depth-Aware Domain Adaptation -which leverages depth in source data as privileged information to help semantic segmentation on the target domain. This additional information is exploited through an auxiliary depthprediction task that allows in turn a feature enrichment via fusion as well as a depth-aware modification of the original adaptation loss. Our experimental evaluations show that DADA consistently outperforms other UDA methods on different synthetic-2-real semantic segmentation benchmarks. As a direction of future work, we envisage the extension to real-world scenarios where depth information in the source domain is only sparsely available, e.g., as provided by automotive laser scanners (LiDARs).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>DADA architecture (top) and DADA learning scheme (bottom). In the top part, the dark-blue stack shows the backbone CNN network; light-blue boxes symbolize the network modules; and green blocks stand for output features. In the lower part, the arrows drawn in blue and red differentiate network flows of source and target samples respectively. For convenient reference, over the learning blocks -illustrated by dashed boxes -we indicate the corresponding equation numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Qualitative results in the SYNTHIA→Cityscapes (16 classes) set-up. The four columns plot (a) RGB input images, (b) ground-truths, (c) AdvEnt baseline outputs and (d) DADA predictions. DADA shows good performance on 'bus', 'car', 'bicycle' classes. Best viewed in color. Qualitative results in the SYNTHIA→Vistas (7 classes) set-up. All models were trained and tested at the resolution of 320 × 640. From left to right, we show (a) RGB input images, (b) corresponding segmentation ground-truths, (c) SPIGAN's and (d) our DADA's segmentation predictions. Not only DADA performs visually better than SPIGAN, but it also produces correct predictions on wrongly annotated construction areas. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>illustrates some qualitative examples comparing DADA and theAdvEnt baseline. Our model shows better results on 'vehicle' classes while AdvEnt sometimes makes severe mistakes of predicting 'car' as 'road' or 'sidewalk'. DADA also outperforms significantly other baseline methods that report results on a 13-class subset. Semantic segmentation performance mIoU (%) on Cityscapes validation set of different models trained on SYNTHIA.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">SYNTHIA → Cityscapes (16 classes)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell>Depth</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall*</cell><cell>fence*</cell><cell>pole*</cell><cell>light</cell><cell>sign</cell><cell>veg</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>mbike</cell><cell>bike</cell><cell>mIoU</cell><cell>D-Gain</cell><cell>mIoU*</cell></row><row><cell>SPIGAN-no-PI [18]</cell><cell></cell><cell cols="19">69.5 29.4 68.7 4.4 0.3 32.4 5.8 15.0 81.0 78.7 52.2 13.1 72.8 23.6 7.9 18.7 35.8 -41.2</cell></row><row><cell>SPIGAN [18]</cell><cell></cell><cell cols="19">71.1 29.8 71.4 3.7 0.3 33.2 6.4 15.6 81.2 78.9 52.7 13.1 75.9 25.5 10.0 20.5 36.8 1.0 42.4</cell></row><row><cell>AdaptSegnet [35]</cell><cell></cell><cell cols="4">79.2 37.2 78.8 -</cell><cell>-</cell><cell cols="12">-9.9 10.5 78.2 80.5 53.5 19.6 67.0 29.5 21.6 31.3 -</cell><cell cols="2">-45.9</cell></row><row><cell>AdaptPatch [36]</cell><cell></cell><cell cols="4">82.2 39.4 79.4 -</cell><cell>-</cell><cell cols="12">-6.5 10.8 77.8 82.0 54.9 21.1 67.7 30.7 17.8 32.2 -</cell><cell cols="2">-46.3</cell></row><row><cell>CLAN [23]</cell><cell></cell><cell cols="4">81.3 37.0 80.1 -</cell><cell>-</cell><cell cols="12">-16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9 22.6 30.7 -</cell><cell cols="2">-47.8</cell></row><row><cell>AdvEnt [39]</cell><cell></cell><cell cols="19">87.0 44.1 79.7 9.6 0.6 24.3 4.8 7.2 80.1 83.6 56.4 23.7 72.7 32.6 12.8 33.7 40.8 -47.6</cell></row><row><cell>DADA</cell><cell></cell><cell cols="19">89.2 44.8 81.4 6.8 0.3 26.2 8.6 11.1 81.8 84.0 54.7 19.3 79.7 40.7 14.0 38.8 42.6 1.8 49.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Segmentation performance (mIoU) on the Cityscapes validation set of 7 ablation experiments. Setup S1, with no check-marks, indicates source-only training. In this experiment, the Mapillary Vistas<ref type="bibr" target="#b25">[26]</ref> is used as the target domain. In SPIGAN<ref type="bibr" target="#b17">[18]</ref>, the authors report unfavorable UDA behaviors on Vistas compared to Cityscapes. This seems caused by the artifacts that the source-target image translation introduces when trying to close the larger gap between SYNTHIA and Vistas. In such a case, leveraging depth information demonstrates important adaptation improvement (+17.3%). On the other hand, our UDA framework does not undergo such a difficulty. Indeed, as shown inTable 2(a-b), the AdvEnt baseline performs much better than SPIGAN-no-PI, with no significant difference in absolute mIoU on the two target datasets (59.4% vs. 54.0%). Over such a stronger baseline, DADA still achieves an overall improvement of +2.4% mIoU. We also obtain best per-class IoUs on the benchmark.</figDesc><table /><note>SYNTHIA→Vistas:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>DADA performance when trained on fractions of SYNTHIA. Performance on Cityscapes as a function of the used percentage of training set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Seen as empirical probabilities that sum to one over the C classes.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno>COMPSTAT. 2010. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognizing RGB images by learning from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<title level="m">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. PAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Cheng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain adaptation for visual applications: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning with side information through modality hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">FCNs in the wild: Pixel-level adversarial and constraintbased adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Privileged adversarial learning from simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spi-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting privileged information from web data for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Taking a closer look at domain shift: Categorylevel adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09478</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting multi-task learning with rock: a deep residual auxiliary block for visual detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive SVM+: Learning with privileged information for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vrigkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to rank using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktoriia</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Novi</forename><surname>Quadrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05427</idno>
		<title level="m">Domain adaptation for structured output via discriminative representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A new learning paradigm: Learning using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Vashist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">ADVENT: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12833</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Classifier learning with hidden information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">DCAN: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><forename type="middle">Gokhan</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Penalizing top performers: Conservative loss for semantic segmentation adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
