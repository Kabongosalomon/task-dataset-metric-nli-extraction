<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer Learning with Dynamic Distribution Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-01-01">Article 1.. 2019. January 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
							<email>han.yu@ntu.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Huang</surname></persName>
							<email>meiyuhuang@qxslab.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology, Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Academy of Space Technology</orgName>
								<orgName type="department" key="dep3">QIANG YANG</orgName>
								<orgName type="department" key="dep4">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">MEIYU HUANG, Qian Xuesen Laboratory of Space Technology</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country>Singapore, China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>No. 6 Kexueyuan South Road</addrLine>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Academy of Space Technology</orgName>
								<orgName type="laboratory">Qian Xuesen Laboratory of Space Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>Qiang Yang</region>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transfer Learning with Dynamic Distribution Adaptation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Trans. Intell. Syst. Technol. 1, 1, Article</title>
						<imprint>
							<biblScope unit="volume">1</biblScope>
							<biblScope unit="issue">1</biblScope>
							<date type="published" when="2019-01-01">Article 1.. 2019. January 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3360309</idno>
					<note type="submission">Publication date: January 2019.</note>
					<note>1 2157-6904/2019/1-ART1 $15.00 1:2 Wang, et al. ACM Reference Format:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: • Computing methodologies → Transfer learning</term>
					<term>Learning under covariate shift</term>
					<term>Dimensionality reduction and manifold learning Additional Key Words and Phrases: Transfer Learning, Domain Adaptation, Distribution Alignment, Deep Learning, Subspace Learning, Kernel Method * Corresponding author: Yiqiang Chen Authors&apos; addresses: Jindong Wang, jindongwang@microsoftcom, Microsoft Research Asia, No 5 Danling Street, Beijing, 100080, China</term>
					<term>Yiqiang Chen, yqchen@ictaccn</term>
					<term>Wenjie Feng, fengwenjie@ictaccn, Institute of Computing Technology,</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transfer learning aims to learn robust classifiers for the target domain by leveraging knowledge from a source domain. Since the source and the target domains are usually from different distributions, existing methods mainly focus on adapting the cross-domain marginal or conditional distributions. However, in real applications, the marginal and conditional distributions usually have different contributions to the domain discrepancy. Existing methods fail to quantitatively evaluate the different importance of these two distributions, which will result in unsatisfactory transfer performance. In this paper, we propose a novel concept called Dynamic Distribution Adaptation (DDA), which is capable of quantitatively evaluating the relative importance of each distribution. DDA can be easily incorporated into the framework of structural risk minimization to solve transfer learning problems. On the basis of DDA, we propose two novel learning algorithms: (1) Manifold Dynamic Distribution Adaptation (MDDA) for traditional transfer learning, and (2) Dynamic Distribution Adaptation Network (DDAN) for deep transfer learning. Extensive experiments demonstrate that MDDA and DDAN significantly improve the transfer learning performance and setup a strong baseline over the latest deep and adversarial methods on digits recognition, sentiment analysis, and image classification. More importantly, it is shown that marginal and conditional distributions have different contributions to the domain divergence, and our DDA is able to provide good quantitative evaluation of their relative importance which leads to better performance. We believe this observation can be helpful for future research in transfer learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Supervised learning is perhaps the most popular and well-studied paradigm in machine learning during the past years. Significant advances have been achieved in supervised learning by exploiting a large amount of labeled training data to build powerful models. For instance, in computer vision, large-scale labeled datasets such as ImageNet <ref type="bibr" target="#b17">[18]</ref> for image classification and MS COCO <ref type="bibr" target="#b35">[36]</ref> for object detection and semantic segmentation have played an instrumental role to help train computer vision models with superior performance. In sentiment analysis, a lot of reviews for all kinds of products are available to train sentiment classification models. Unfortunately, it is often expensive and time-consuming to acquire sufficient labeled data to train these models. Furthermore, there is often dataset bias in newly emerging data, i.e. the existing model is often trained on a particular dataset and will generalize poorly in a new domain. For example, the images of an online product can be very different from those taken at home. The product review for electronic devices is likely to be different from that of clothes. Under this circumstance, it is necessary and important to design algorithms that can handle both the label scarcity and dataset bias challenges.</p><p>Domain adaptation, or transfer learning <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b60">61]</ref> has been a promising approach to solve such problems. The main idea of transfer learning is to leverage the abundant labeled samples in some existing domains to facilitate learning in a new target domain by reducing the dataset bias. The domain with abundant labeled samples is often called the source domain, while the domain for which a new model is to be trained is the target domain. However, due to the dataset bias, the data distributions on different domains are usually different. In such circumstance, traditional machine learning algorithms cannot be applied directly since they assume that training and testing data are under the same distributions. Transfer learning is able to reduce the distribution divergence <ref type="bibr" target="#b45">[46]</ref> such that the models on the target domain can be learned.</p><p>To cope with the difference in distributions between domains, existing works can be summarized into two main categories: (a) instance reweighting <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b67">68]</ref>, which reuses samples from the source domain according to some weighting technique; and (b) feature matching, which either performs subspace learning by exploiting the subspace geometrical structure <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b63">64]</ref>, or distribution alignment to reduce the marginal or conditional distribution divergence between domains <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b73">74]</ref>. Recently, the success of deep learning has dramatically increased the performance of transfer learning either via deep representation learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b77">78]</ref> or adversarial learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b74">75]</ref>. The key idea behind these works is to learn more transferable representations using deep neural networks. Then, the learned feature distributions can be aligned such that their domain discrepancy can be reduced.</p><p>However, despite the great success achieved by traditional and deep transfer learning methods, there is still a challenge ahead. Existing works only attempt to align the marginal <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58]</ref> or the conditional distributions <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48]</ref>. Although recent advance has suggested that aligning both distributions will lead to better performance <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b61">62]</ref>, they only give the two distributions equal weights, which fails to evaluate the relative importance of these two distributions. In their assumptions, both the marginal and conditional distributions are contributing equally to the domain divergence. However, in this paper, we argue that this assumption is not practical in real applications. For example, when two domains are very dissimilar (e.g., transfer learning between <ref type="bibr" target="#b0">(1)</ref> and <ref type="bibr" target="#b2">(3)</ref> in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>), the marginal distribution is more important. When the marginal distributions are close (transfer learning between <ref type="bibr" target="#b0">(1)</ref> and <ref type="bibr" target="#b3">(4)</ref> in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>), the conditional distribution of each class  should be given more weight. Ignoring this fact will likely to result in unsatisfactory transfer performance. There is no method which can quantitatively account for the relative importance of these two distributions in conjunction.</p><p>In this paper, we propose a novel concept of Dynamic Distribution Adaptation (DDA) to dynamically and quantitatively adapt the marginal and conditional distributions in transfer learning. To be specific, DDA is able to dynamically learn the distribution weights through calculating the H ∆H divergence <ref type="bibr" target="#b4">[5]</ref> between domains when learning representations. Then, the relative importance of marginal and conditional distributions can be obtained, which in turn can be utilized to learn more transferable feature representations. This dynamic importance learning and feature learning are being optimized iteratively to learn a domain-invariant transfer classifier eventually. To the best of our knowledge, DDA is the first work to dynamically and quantitatively evaluate the importance of both distributions. The significant improvements of DDA over the latest method on different kinds of tasks are shown in <ref type="figure" target="#fig_1">Fig. 1</ref></p><formula xml:id="formula_0">(b).</formula><p>To enable good representation learning, we propose two novel learning methods based on DDA with the principle of Structural Risk Minimization (SRM) <ref type="bibr" target="#b58">[59]</ref>. For traditional transfer learning, we develop Manifold Dynamic Distribution Adaptation (MDDA) method to utilize the Grassmann manifold <ref type="bibr" target="#b29">[30]</ref> in learning non-distorted feature representations. For deep transfer learning, we develop Dynamic Distribution Adaptation Network (DDAN) to use the deep neural work in learning end-to-end transfer classifier. We also develop respective learning algorithms for MDDA and DDAN.</p><p>To sum up, this work makes the following contributions: 1) We propose the DDA concept for domain adaptation. DDA is the first quantitative evaluation framework for the relative importance of marginal and conditional distributions in domain adaptation. This is useful for a wide range of future research on transfer learning.</p><p>2) On top of DDA, we propose two novel methods: MDDA for traditional transfer learning and DDAN for deep transfer learning. Both methods can be efficiently formulated and finally learn the domain-invariant transfer classifier.</p><p>3) We conduct extensive experiments on digit classification, object recognition, and sentiment classification datasets. Experimental results demonstrate that both MDDA and DDAN are significantly better than many state-of-the-art traditional and deep methods. More importantly, empirical results have also demonstrated that the different effect of marginal and conditional distributions do exist, and our DDA is able to give them quantitative weights, which facilitates the performance of transfer learning. This paper is an extension of our previous oral paper at ACM Multimedia conference <ref type="bibr" target="#b64">[65]</ref>. Our extensions include: (1) A more general and clear concept of dynamic distribution adaptation and its calculation. <ref type="bibr" target="#b1">(2)</ref> We extend DDA in both manifold learning and deep learning methods, then we formulate these algorithms and propose respective learning algorithms. (3) We extend the experiments in digit classification, sentiment analysis, and image classification, which have shown the effectiveness of our methods. And (4) We extensively analyze our calculation of DDA in new experiments.</p><p>The remainder of this paper is structured as follows. We review the related work in Section 2. In Section 3, we introduce some previous knowledge before introducing the proposed method. Section 4 thoroughly presents our proposed DDA concept and its two extensions: MDDA and DDAN. Extensive experiments are shown in Section 5, where we extensively evaluate the performance of MDDA and DDAN. Finally, Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Domain adaptation, or transfer learning, is an active research area in machine learning. Apart from the popular survey by Pan and Yang <ref type="bibr" target="#b45">[46]</ref>, several recent survey papers have extensively investigated specific research topics in transfer learning including: visual domain adaptation <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>, heterogeneous transfer learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>, multi-task learning <ref type="bibr" target="#b75">[76]</ref>, and cross-dataset recognition <ref type="bibr" target="#b72">[73]</ref>. There are also several successful applications using transfer learning for: activity recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b65">66]</ref>, object recognition <ref type="bibr" target="#b23">[24]</ref>, face recognition <ref type="bibr" target="#b48">[49]</ref>, speech recognition <ref type="bibr" target="#b68">[69]</ref>, speech synthesis <ref type="bibr" target="#b32">[33]</ref>, and text classification <ref type="bibr" target="#b18">[19]</ref>. Interested readers are recommended to refer to http://transferlearning. xyz to find out more related works and applications.</p><p>From the perspective of transfer learning methods, there are three main categories: (1) Instance re-weighting, which reuses samples according to some weighting technique <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b55">56]</ref>; (2) Feature transformation, which performs representation learning to transform the source and target domains into the same subspace <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b73">74]</ref>; (3) Transfer metric learning <ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>, which learns transferable metric between domains. Since our proposed methods are mainly related to feature-based transfer learning, we will extensively introduce the related work in the following aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Subspace and Manifold Learning</head><p>One category of feature-based transfer learning is subspace and manifold learning. The goal is to learn representative subspace or manifold representations that are invariant across domains. Along this line, subspace alignment (SA) <ref type="bibr" target="#b19">[20]</ref> aligned the base vectors of both domains, but failed to adapt feature distributions. Subspace distribution alignment (SDA) <ref type="bibr" target="#b52">[53]</ref> extended SA by adding subspace variance adaptation. However, SDA did not consider the local property of subspaces and ignored conditional distribution alignment. CORAL <ref type="bibr" target="#b51">[52]</ref> aligned subspaces in second-order statistics, but it did not consider the distribution alignment. Scatter component analysis (SCA) <ref type="bibr" target="#b23">[24]</ref> converted the samples into a set of subspaces (i.e. scatters) and then minimized the divergence between them.</p><p>On the other hand, some work used the property of manifold to further learn tight representations. Geodesic flow kernel (GFK) <ref type="bibr" target="#b24">[25]</ref> extended the idea of sampled points in manifold <ref type="bibr" target="#b26">[27]</ref> and proposed to learn the geodesic flow kernel between domains. The work of <ref type="bibr" target="#b2">[3]</ref> used a Hellinger distance to approximate the geodesic distance in Riemann space. <ref type="bibr" target="#b1">[2]</ref> proposed to use Grassmann for domain adaptation, but they ignored the conditional distribution alignment. Different from these approaches, DDA can learn a domain-invariant classifier in the manifold and align both marginal and conditional distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distribution Alignment</head><p>Another category of feature-based transfer learning is distribution alignment. The work of this category is pretty straight forward: find some feature transformations that can minimize the distribution divergence. Along this line, existing work can be classified into three subcategories: marginal distribution alignment, conditional distribution alignment, and joint distribution alignment.</p><p>DDA substantially differs from existing work that only aligns marginal or conditional distribution <ref type="bibr" target="#b44">[45]</ref>. Joint distribution adaptation (JDA) <ref type="bibr" target="#b39">[40]</ref> matched both distributions with equal weights. Others extended JDA by adding regularization <ref type="bibr" target="#b38">[39]</ref>, sparse representation <ref type="bibr" target="#b66">[67]</ref>, structural consistency <ref type="bibr" target="#b31">[32]</ref>, domain invariant clustering <ref type="bibr" target="#b54">[55]</ref>, and label propagation <ref type="bibr" target="#b73">[74]</ref>. The work of Balanced Distribution Adaptation (BDA) <ref type="bibr" target="#b61">[62]</ref> firstly proposed to manually weight the two distributions. The main differences between DDA (MDDA) and these methods are: 1) These work treats the two distributions equally. However, when there is a greater discrepancy between both distributions, they cannot evaluate their relative importance and thus lead to undermined performance. Our work is capable of evaluating the quantitative importance of each distribution via considering their different effects. 2) These methods are designed only for the original space, where feature distortion will negatively affect the performance. DDA (MDDA) can align the distributions in the manifold to overcome the feature distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Domain-invariant Classifier Learning</head><p>Different from the above two types of work that further need to learn a classifier for the target domain, some research is able to learn the domain-invariant classifier while simultaneously performing subspace learning or distribution alignment. Recent work such as adaptation regularization for transfer learning (ARTL) <ref type="bibr" target="#b38">[39]</ref>, domain-invariant projection (DIP) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, and distribution matching machines (DMM) <ref type="bibr" target="#b9">[10]</ref> also aimed to build a domain-invariant classifier. However, ARTL and DMM cannot effectively handle feature distortion in the original space. Nor can they account for the different importance of distributions. DIP mainly focused on feature transformation and only aligned marginal distributions. DDA (MDDA) is able to mitigate feature distortion and quantitatively evaluate the importance of marginal and conditional distribution adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Deep and Adversarial Transfer Learning</head><p>Recent years have witnessed the advance of deep transfer learning. Compared to traditional shallow learning, deep neural networks are capable of learning better representations <ref type="bibr" target="#b69">[70]</ref>. Deep domain confusion (DDC) <ref type="bibr" target="#b57">[58]</ref> firstly added the MMD loss to a deep network to adapt the network. Similar to DDC, deep adaptation networks (DAN) adopted the multiple-kernel MMD <ref type="bibr" target="#b28">[29]</ref> to the network. Instead, Deep CORAL <ref type="bibr" target="#b53">[54]</ref> added CORAL loss <ref type="bibr" target="#b51">[52]</ref> to the network. CORAL is a second-order loss compared to MMD, which is a first-order loss. Furthermore, Zellinger et al. introduce the central moment discrepancy (CMD) <ref type="bibr" target="#b70">[71]</ref> to the network, which is a higher-order distance. Different from the above deep transfer learning methods, adversarial learning <ref type="bibr" target="#b25">[26]</ref> also helps to learn more transferable and discriminative representations. Domain-adversarial neural network (DANN) was first introduced by Ganin et al. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. The core idea is to add a domain-adversarial loss to the network instead of the predefined distance function such as MMD. This has dramatically enabled the network to learn more discriminative information. Following the idea of DANN, a lot of work adopted domain-adversarial training <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b74">75]</ref>.</p><p>The above-discussed work all ignore the different effect of marginal and conditional distributions in transfer learning, while our proposed DDA (DDAN) is fully capable of dynamically evaluating the importance of each distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>Let Ω ∈ R d be an input measurable space of dimension d and C the set of possible labels. We use P(Ω) to denote the set of all probability measures over Ω. In standard transfer learning setting, there is a source domain Ω s = {x s i , y s i } n s i=1 with known labels y s i ∈ C and a target domain Ω t = {x t j } n t j=1 with unknown labels. Here, x s ∼ P(Ω s ) and x t ∼ P(Ω t ) are samples from either source or the target domain. Different from existing work that either assume the marginal or conditional distributions of two domains are different, in this work, we tackle a more general case that both distributions are different, i.e. P(x s ) P(x t ), P(y s |x s ) P(y t |x t ). The goal is to learn a transferable classifier f such that the risk on the target domain can be minimized: ϵ t = min P (x,y)∼Ω t (f (x) y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Structural Risk Minimization</head><p>From a statistical machine learning perspective, the above problem can be formulated and solved by the structural risk minimization (SRM) principle <ref type="bibr" target="#b58">[59]</ref>. In SRM, the prediction function f can be formulated as f = arg min</p><formula xml:id="formula_1">f ∈H K ,(x,y)∼Ω l J (f (x), y) + λR(f ),<label>(1)</label></formula><p>where the first term indicates the loss on data samples with J (·, ·) is the loss function, the second term denotes the regularization term, and H K is the Hilbert space induced by kernel function K(·, ·). λ is the trade-off parameter. The symbol Ω l denotes the domain that has labels. In our problem, we have Ω l = Ω s since there are no labels in the target domain. Specifically, in order to effectively handle the different distributions between Ω s and Ω t , we can further divide the regularization term as</p><formula xml:id="formula_2">R(f ) = λD f (Ω s , Ω t ) + ρR f (Ω s , Ω t ),<label>(2)</label></formula><p>where D f (·, ·) represents the distribution divergence between Ω s and Ω t with λ, ρ the trade-off parameters and R f (·, ·) denotes other regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Maximum Mean Discrepancy</head><p>There are a variety of means to measure the distribution divergence between two domains such as Kullback−Leibler divergence and cross-entropy. With respect to efficiency, we adopt the maximum mean discrepancy (MMD) <ref type="bibr" target="#b4">[5]</ref> to empirically calculate the distribution divergence between domains. As a non-parametric measurement, MMD has been widely adopted by many existing methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b73">74]</ref>, and its effectiveness has been proven analytically in <ref type="bibr" target="#b27">[28]</ref>. Formally, the MMD distance between distributions P and Q is defined as <ref type="bibr" target="#b27">[28]</ref> MMD(H k , P, Q) := sup</p><formula xml:id="formula_3">| |f | | H k ≤1 E X ∼P f (X ) − E Y ∼Q f (Y ),<label>(3)</label></formula><p>where H k is the Reproduced kernel Hilbert space (RKHS) with Mercer kernel K(·, ·), || f || H k ≤ 1 is its unit norm ball, and E[·] denotes the mean of the embedded samples. This is known as an integral probability metric in the statistics literature. To compute this divergence, a biased empirical estimate of MMD is obtained by replacing the population expectations with empirical expectations computed on the samples X and Y ,</p><formula xml:id="formula_4">MMD b (H k , P, Q) = sup | |f | | H k ≤1 1 m m i=1 f (X i ) − 1 n n i=1 f (Y i ) ,<label>(4)</label></formula><p>where m, n are sample numbers of P and Q, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DYNAMIC DISTRIBUTION ADAPTATION</head><p>In this section, we present the general dynamic distribution adaptation framework and its two learning algorithms in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The General Framework</head><p>Transfer learning is to learn transferable representations which can generalize well across different domains. The key idea of Dynamic Distribution Adaptation (DDA) is to dynamically learn the relative importance of marginal and conditional distributions in transfer learning. Therefore, the dynamic importance learning and transfer feature learning are not independently, but quite involved. Accordingly, DDA first performs feature learning to learn more transferable representations. Then, it can perform dynamic distribution adaptation to quantitatively account for the relative importance of marginal and conditional distributions to address the challenge of unevaluated distribution alignment. These two steps are iteratively optimized via several iterations. Eventually, a domaininvariant classifier f can be learned by combining these two steps based on the principle of SRM.</p><p>Recall the principle of SRM in Eq. (1). If we use д(·) to denote the feature learning function, then f can be represented as</p><formula xml:id="formula_5">f = arg min f ∈ n i =1 H K J (f (д(x i )), y i ) + η|| f || 2 K + λD f (Ω s , Ω t ) + ρR f (Ω s , Ω t )<label>(5)</label></formula><p>where || f || 2 K is the squared norm of f . The term D f (·, ·) represents the proposed dynamic distribution alignment. Additionally, we introduce R f (·, ·) as a Laplacian regularization to further exploit the similar geometrical property of nearest points in manifold G <ref type="bibr" target="#b3">[4]</ref>. η, λ, and ρ are the regularization parameters.</p><p>In the next sections, we first introduce the learning of dynamic distribution adaptation. Then, we show how to learn the feature learning function д(·) either through manifold learning (i.e. Manifold Dynamic Distribution Adaptation, or MDDA in <ref type="figure" target="#fig_2">Fig. 2(a)</ref>) and deep learning (i.e. Dynamic Distribution Network, or DDAN in <ref type="figure" target="#fig_2">Fig. 2(b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dynamic Distribution Adaptation</head><p>The purpose of dynamic distribution adaptation is to quantitatively evaluate the importance of aligning marginal (P) and conditional (Q) distributions in domain adaptation. Existing methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b73">74]</ref> assume that both distributions are equally important. However, this assumption may not be valid in real-world applications. For instance, when transferring from (1) to (3) in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>, there is a large difference between datasets. Therefore, the divergence between P s and P t is more dominant.</p><p>In contrast, from (1) to (4) in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>, the datasets are similar. Therefore, the distribution divergence in each class (Q s and Q t ) is more dominant.</p><p>In view of this phenomenon, we introduce an adaptive factor to dynamically adjust the importance of these two distributions. Formally, the dynamic distribution alignment D f is defined as</p><formula xml:id="formula_6">D f (Ω s , Ω t ) = (1 − µ)D f (P s , P t ) + µ C c=1 D (c) f (Q s , Q t )<label>(6)</label></formula><p>where µ ∈ [0, 1] is the adaptive factor and c ∈ {1, · · · , C} is the class indicator. D f (P s , P t ) denotes the marginal distribution alignment, and D (c) f (Q s , Q t ) denotes the conditional distribution alignment for class c.</p><p>When µ → 0, it means that the distribution distance between the source and the target domains is large. Thus, marginal distribution alignment is more important ((1) → (3) in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>). When µ → 1, it means that feature distribution between domains is relatively small, such that the distribution of each class is dominant. Thus, the conditional distribution alignment is more important ((1) → (4) in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>). When µ = 0.5, both distributions are treated equally as in existing methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b73">74]</ref>. Hence, the existing methods can be regarded as special cases of the dynamic distribution alignment. By learning the optimal adaptive factor µ opt (which we will discuss later), MDDA can be applied to different domain adaptation problems.</p><p>We use the maximum mean discrepancy (MMD) <ref type="bibr" target="#b4">[5]</ref> introduced in the last section to empirically calculate the distribution divergence between domains. To be specific, the marginal and conditional distribution distances can be respectively computed as</p><formula xml:id="formula_7">D f (P s , P t ) = ∥E[f (z s )] − E[f (z t )]∥ 2 H K (7) D (c) f (Q s , Q t ) = ∥E[f (z (c) s )] − E[f (z (c) t )]∥ 2 H K<label>(8)</label></formula><p>Then, DDA can be expressed as</p><formula xml:id="formula_8">D f (Ω s , Ω t ) = (1 − µ)∥E[f (z s )) − E[f (z t )]∥ 2 H K + µ C c=1 ∥E[f (z (c) s )] − E[f (z (c) t )]∥ 2 H K .<label>(9)</label></formula><p>Note that since Ω t has no labels, it is not feasible to evaluate the conditional distribution Q t = Q t (y t |z t ). Instead, we follow the idea in <ref type="bibr" target="#b61">[62]</ref> and use the class conditional distribution Q t (z t |y t ) to approximate Q t . In order to evaluate Q t (z t |y t ), we apply prediction to Ω t using a base classifier trained on Ω s to obtain soft labels for Ω t . The soft labels may be less reliable, so we iteratively refine the prediction. Note that we only use the base classifier in the first iteration. After that, MDDA can automatically refine the labels for Ω t using results from previous iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Quantitative Evaluation of Adaptive Factor µ.</head><p>We can treat µ as a parameter and tune its value by cross-validation techniques. However, there are no labels for the target domain in unsupervised transfer learning problems. There are two indirect solutions to apply the value of µ in DDA rather than estimating its value: by Random guessing and by Max-min averaging. Random guessing is technically very intuitive. We can randomly pick a value of µ in [0, 1], then perform MDDA using the corresponding µ r and to get the transfer learning result. If we repeat this process t times and denote the t-th transfer learning result as r t , then the final result can be calculated as <ref type="bibr" target="#b0">1</ref> Although the random guessing and max-min averaging are both feasible and simple solutions to estimate µ, they are computationally prohibitive. More importantly, there is no guarantee of their results. It is extremely challenging to calculate the value of µ.</p><p>In this work, we make the first attempt towards calculating µ (i.e.μ) by exploiting the global and local structure of domains. We adopt the A-distance <ref type="bibr" target="#b4">[5]</ref> as the basic measurement. The Adistance is defined as the error of building a linear classifier to distinguish two domains (i.e. a binary classification). Formally, we denote ϵ(h) the error of a linear classifier h discriminating the two domains Ω s and Ω t . Then, the A-distance can be defined as</p><formula xml:id="formula_9">d A (Ω s , Ω t ) = 2(1 − 2ϵ(h)).<label>(10)</label></formula><p>We can directly compute the marginal A-distance using the above equation, which is denoted as d M . For the A-distance between conditional distributions, we use d c to denote the A-distance for the cth class. It can be calculated as</p><formula xml:id="formula_10">d c = d A (D (c) s , D (c) t ),</formula><p>where D (c) s and D (c) t denote samples from class c in Ω s and Ω t , respectively. Note that d M denotes the marginal difference, while sum C c=1 d c denotes the conditional difference on all classes. In this paper, our assumption is that the domain divergence is caused by both the marginal and conditional distributions. Therefore, d M + C c=1 d c can represent the whole divergence. Eventually, µ can be estimated aŝ</p><formula xml:id="formula_11">µ = 1 − d M d M + C c=1 d c .<label>(11)</label></formula><p>Note that the number of labeled samples in the source domain is often much larger than that in the target domain. Therefore, in order to solve this imbalanced classification problem, we perform upsampling <ref type="bibr" target="#b33">[34]</ref> on the target domain to make the samples with almost the same size. We also notice that this upsampling process is random, thus we repeat this step several times to get the averaged µ value.</p><p>This estimation has to be computed during every iteration of the dynamic distribution adaptation, since the feature distribution may vary after evaluating the conditional distribution each time. To the best of our knowledge, this is the first solution to quantitatively estimate the relative importance of each distribution. In fact, this estimation can be significant for future research in transfer learning and domain adaptation.</p><p>Remark: Currently, the quantitative evaluation of µ only supports the situation where the label space of the source and the target domains are the same. However, it is important to note that this evaluation is also open for open set domain adaptation <ref type="bibr" target="#b46">[47]</ref> or partial transfer learning <ref type="bibr" target="#b71">[72]</ref>, where the label spaces are not the same. In such cases, we should consider the different similarity between each class in two domains. For instance, we can regard the classes that do not belong to the target domain as outliers and perform outlier detection before calculating µ. Then, we can select the most similar samples and classes in both domains and perform DDA. We leave this part for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MDDA: Manifold Dynamic Distribution Adaptation</head><p>In this section, we introduce the learning of DDA through manifold learning. We propose Manifold Dynamic Distribution Adaptation (MDDA) as shown in <ref type="figure" target="#fig_2">Fig. 2(a)</ref>. Manifold feature learning can serve as the feature learning step to mitigate the influence of feature distortion <ref type="bibr" target="#b1">[2]</ref> in transfer learning. The features in manifold space can reflect a more detailed structure and property of the domains, thus avoiding feature distortion. MDDA learns д(·) in the Grassmann manifold G(d) <ref type="bibr" target="#b29">[30]</ref> since features in the manifold have some geometrical structures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref> that can avoid distortion in the original space. In addition, G(d) can facilitate classifier learning by treating the original d-dimensional subspace (i.e. feature vector) as its basic elements. Feature transformation and distribution alignment often have efficient numerical forms (i.e., they can be represented as matrix operations easily) and facilitate domain adaptation on G(d) <ref type="bibr" target="#b29">[30]</ref>. There are several approaches to transform the features into G <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref>. We embed Geodesic Flow Kernel (GFK) <ref type="bibr" target="#b24">[25]</ref> to learn д(·) for its computational efficiency.</p><p>When learning manifold features, MDDA tries to model the domains with d-dimensional subspaces and then embed them into G(d). Let P s and P t denote the PCA subspaces for the source and the target domain, respectively. G can thus be regarded as a collection of all d-dimensional subspaces. Each original subspace can be seen as a point in G. Therefore, the geodesic flow {Φ(t) : 0 ≤ t ≤ 1} between two points can be used to establish a path between the two subspaces, where t denotes the calculus variant between two domains. If we let P s = Φ(0) and P t = Φ(1), then finding a geodesic flow from Φ(0) to Φ(1) equals to transforming the original features into an infinite-dimensional feature space, which eventually eliminates the domain shift. This approach can be seen as an incremental way of 'walking' from Φ(0) to Φ(1). Specifically, the new features can be represented as z = д(x) = Φ(t) T x. From <ref type="bibr" target="#b24">[25]</ref>, the inner product of transformed features z i and z j gives rise to a positive semidefinite geodesic flow kernel</p><formula xml:id="formula_12">⟨z i , z j ⟩ = ∫ 1 0 (Φ(t) T x i ) T (Φ(t) T x j ) dt = x T i Gx j .<label>(12)</label></formula><p>The geodesic flow can be parameterized as</p><formula xml:id="formula_13">Φ(t) = P s U 1 Γ(t) − R s U 2 Σ(t) = P s R s U 1 0 0 U 2 Γ(t) −Σ(t) ,<label>(13)</label></formula><p>where R s ∈ R D×d presents the orthogonal complements to P s . U 1 ∈ R D×d and U 2 ∈ R D×d are two orthonormal matrices that can be computed by singular value decomposition (SVD)</p><formula xml:id="formula_14">P T S P T = U 1 ΓV T , R T S P T = −U 2 ΣV T<label>(14)</label></formula><p>According to GFK <ref type="bibr" target="#b24">[25]</ref>, the geodesic flow kernel G can be calculated by</p><formula xml:id="formula_15">G = P s U 1 R s U 2 Λ 1 Λ 2 Λ 2 Λ 3 U T 1 P T s U ⊤ 2 R T s ,<label>(15)</label></formula><p>where Λ 1 , Λ 2 , Λ 3 are three diagonal matrices with elements</p><formula xml:id="formula_16">λ 1i = 1 + sin (2θ i ) 2θ i , λ 2i = cos (2θ i ) − 1 2θ i , λ 3i = 1 − sin (2θ i ) 2θ i .<label>(16)</label></formula><p>Thus, the features in the original space can be transformed into Grassmann manifold with</p><formula xml:id="formula_17">z = д(x) = √</formula><p>Gx. After manifold feature learning and dynamic distribution alignment, f can be learned by summarizing SRM over Ω s and distribution alignment. By adopting the square loss l 2 , f can be expressed as</p><formula xml:id="formula_18">f = arg min f ∈H K n i=1 (y i − f (z i )) 2 + η|| f || 2 K + λD f (Ω s , Ω t ) + ρR f (Ω s , Ω t ).<label>(17)</label></formula><p>In order to perform efficient learning, we now further reformulate each term. SRM on the Source Domain: Using the representer theorem <ref type="bibr" target="#b3">[4]</ref>, f can be expanded as where β = (β 1 , β 2 , · · · ) T ∈ R (n+m)×1 is the coefficients vector and K is a kernel. Then, SRM on Ω s becomes</p><formula xml:id="formula_19">f (z) = n+m i=1 β i K(z i , z),<label>(18)</label></formula><formula xml:id="formula_20">n i=1 (y i − f (z i )) 2 + η|| f || 2 K = n+m i=1 A ii (y i − f (z i )) 2 + η|| f || 2 K = ||(Y − β T K)A|| 2 F + ηtr(β T Kβ),<label>(19)</label></formula><p>where || · || F is the Frobenious norm.</p><formula xml:id="formula_21">K ∈ R (n+m)×(n+m) is the kernel matrix with K i j = K(z i , z j ), and A ∈ R (n+m)×(n+m) is a diagonal domain indicator matrix with A ii = 1 if i ∈ Ω s , otherwise A ii = 0. Y = [y 1 , · · · ,</formula><p>y n+m ] is the label matrix from the source and the target domains. tr(·) denotes the trace operation. η is the shrinkage parameter. Although the labels for Ω t are unavailable, they can be filtered out by the indicator matrix A. Dynamic distribution adaptation: Using the representer theorem and kernel tricks, dynamic distribution alignment in equation <ref type="formula" target="#formula_8">(9)</ref> becomes</p><formula xml:id="formula_22">D f (Ω s , Ω t ) = tr β T KMKβ<label>(20)</label></formula><p>where</p><formula xml:id="formula_23">M = (1 − µ)M 0 + µ C c=1 M c is the MMD matrix with its element calculated by (M 0 ) i j =          1 n 2 , z i , z j ∈ Ω s 1 m 2 , z i , z j ∈ Ω t − 1 mn , otherwise<label>(21)</label></formula><formula xml:id="formula_24">(M c ) i j =                    1 n 2 c , z i , z j ∈ D (c) s 1 m 2 c , z i , z j ∈ D (c) t − 1 m c n c , z i ∈ D (c) s , z j ∈ D (c) t z i ∈ D (c) t , z j ∈ D (c) s 0, otherwise<label>(22)</label></formula><p>where n c = |D (c) s | and m c = |D (c) t |. Laplacian Regularization: Additionally, we add a Laplacian regularization term to further exploit the similar geometrical property of nearest points in manifold G <ref type="bibr" target="#b3">[4]</ref>. We denote the pair-wise affinity matrix as</p><formula xml:id="formula_25">W i j = sim(z i , z j ), z i ∈ N p (z j ) or z j ∈ N p (z i ) 0, otherwise ,<label>(23)</label></formula><p>where sim(·, ·) is a similarity function (such as cosine distance) for measuring the distance between two points. N p (z i ) denotes the set of p-nearest neighbors to point z i . p is a free parameter and must be set in the method. By introducing Laplacian matrix L = D − W with diagonal matrix D ii = n+m j=1 W i j , the final regularization can be expressed as</p><formula xml:id="formula_26">R f (Ω s , Ω t ) = n+m i, j=1 W i j (f (z i ) − f (z j )) 2 = n+m i, j=1 f (z i )L i j f (z j ) = tr β T KLKβ .<label>(24)</label></formula><p>Overall Reformulation: By combining equations <ref type="bibr" target="#b18">(19)</ref>, <ref type="bibr" target="#b19">(20)</ref> and <ref type="bibr" target="#b23">(24)</ref>, f in equation <ref type="formula" target="#formula_1">(17)</ref> can be reformulated as</p><formula xml:id="formula_27">f = arg min f ∈H K ||(Y − β T K)A|| 2 F + η tr(β T Kβ) + tr β T K(λM + ρL)Kβ .<label>(25)</label></formula><p>Setting derivative ∂ f /∂β = 0, we obtain the solution</p><formula xml:id="formula_28">β ⋆ = ((A + λM + ρL)K + ηI) −1 AY T .<label>(26)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 MDDA: Manifold Dynamic Distribution Adaptation</head><p>Input: Data matrix X = [X s , X t ], source domain labels y s , manifold subspace dimension d, regularization parameters λ, η, ρ, and #neighbor p. Output: Classifier f . <ref type="bibr">1:</ref> Learn manifold feature transformation kernel G via equation <ref type="bibr" target="#b11">(12)</ref>, and get manifold feature Z = √ GX. 2: Train a base classifier using Ω s , then apply prediction on Ω t to get its soft labelsŷ t . 3: Construct kernel K using transformed features Z s = Z 1:n,: and Z t = Z n+1:n+m,: . 4: repeat <ref type="bibr">5:</ref> Calculate the adaptive factorμ using equation <ref type="bibr" target="#b10">(11)</ref>. and compute M 0 and M c by Eq. <ref type="formula" target="#formula_1">(21)</ref> and <ref type="bibr" target="#b21">(22)</ref>. <ref type="bibr">6:</ref> Compute β ⋆ by solving equation <ref type="bibr" target="#b25">(26)</ref> and obtain f via the representer theorem in Eq. (18).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Update the soft labels of Ω t :ŷ t = f (Z t ). 8: until Convergence 9: return Classifier f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">DDAN: Dynamic Distribution Adaptation Network</head><p>In this section, we propose Dynamic Distribution Adaptation Network (DDAN) to perform endto-end learning of not only the feature learning function д(·), but the classifier f . DDAN is able to leverage the ability of the recent advance of deep neural networks in learning representative features through end-to-end training <ref type="bibr" target="#b5">[6]</ref>. Specifically, we exploit a backbone network to learn useful feature representations, while simultaneously performing domain adaptation using DDA.</p><p>The network architecture of DDAN is shown in <ref type="figure" target="#fig_2">Fig 2(b)</ref>. Firstly, the samples from the source and target domains serve as the inputs into the deep neural networks. Secondly, the CNN network (the purple part) such as AlexNet <ref type="bibr" target="#b34">[35]</ref> and ResNet <ref type="bibr" target="#b30">[31]</ref> can extract high-level features from the inputs. Thirdly, the features are going through a fully-connected layer (the blue part) to perform soft-max classification to obtain the labels y. The novel contribution here is to align the source and target domain features using the dynamic distribution alignment (DDA, the yellow part).</p><p>Adopting the DDA, the learning objective of DDAN can be expressed as</p><formula xml:id="formula_29">f = min Θ n i=1 J (f (x s i ), y s i ) + λD f (Ω s , Ω t ) + ρR f (Ω s , Ω t ),<label>(27)</label></formula><p>where J (·, ·) is the cross-entropy loss function and Θ = {w, b} containing the weight and bias parameters of the neural network. Note that DDAN is based on the deep neural network, so instead of using the whole domain data, we use the batch data by following the mini-batch stochastic gradient descent (SGD) training procedure. Therefore, the dynamic distribution adaptation is only calculated between batches rather than whole domains. This is more practical and efficient in real applications where the data are coming in a streaming manner. Most MMD based deep transfer learning methods <ref type="bibr" target="#b57">[58]</ref> are based on Eq. (4) and only adopted the linear kernel for simplicity. Since the formulation in Eq. (4) is based on pairwise similarity and is computed in quadratic time complexity, it is prohibitively time-consuming for using mini-batch stochastic gradient decent (SGD) in CNN-based transfer learning methods. Gretton et al. <ref type="bibr" target="#b28">[29]</ref> further suggest an unbiased approximation of MMD with linear complexity. Without loss of generality, by Sample a mini-batch data from both the source and target domain <ref type="bibr">3:</ref> Feed the mini-batch data into the network and get the pseudo labels for Ω t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Update the parameters {Θ, b} by computing the mini-batch gradient according to Eq. (30).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>After an epoch, calculate µ using Eq. (11) and calculate the loss 6: until Convergence 7: return Classifier f . assuming M = N , MMD can then be computed as</p><formula xml:id="formula_30">MMD 2 l (s, t) = 2 M M /2 i=1 h l (z i ),<label>(28)</label></formula><p>where h l is an operator defined on a quad-tuple</p><formula xml:id="formula_31">z i = x s 2i−1 , x s 2i , x t 2j−1 , x t 2j , h l (z i ) = k x s 2i−1 , x s 2i + k x t 2j−1 , x t 2j − k x s 2i−1 , x t 2j − k x s 2i , x t 2j−1<label>(29)</label></formula><p>The approximation in Eq. 28 takes a summation form and is suitable for gradient computation in a mini-batch manner.</p><p>The gradient of the parameters can be computed as:</p><formula xml:id="formula_32">∆ Θ = ∂J (·, ·) ∂Θ + λ ∂D f (·, ·) ∂Θ + ρ ∂R f (·, ·) ∂Θ .<label>(30)</label></formula><p>Updating µ: Another important aspect is to dynamically update µ in DDAN. Similar to the above mini-batch learning of MMD distance, it seems natural to calculate µ after each mini-batch learning. However, the labels on the source domain and the pseudo labels on the target domain are likely to be inconsistent after mini-batch learning. For instance, assume that the batch size for both domains is b size = 32 and the total class number |C| = 31. Then, after a forward operation for one batch, we can obtain 32 pseudo labels for the target domain. Since b size is rather close to |C|, it is highly likely that the mini-batch labels for both domains do not match, which will easily lead to the mode collapse or gradient exploding problem.</p><p>In order to avoid this problem, we propose to update µ after each epoch of iteration, rather than each mini-batch data. In fact, this step is very similar to that of MDDA, which uses all the data to perform learning. The learning process of DDAN is summarized in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussions</head><p>Both MDDA and DDAN are generic learning methods that are suitable for all transfer classification problems. In this section, we briefly discuss their differences.</p><p>MDDA is for traditional learning, while DDAN is for deep learning. Compared to DDAN which is designed for deep neural networks, MDDA can be easily applied on the small resource constraint devices. One possible limitation of MDDA maybe that it relies on certain feature extraction methods. For instance, on image dataset, we probably need to extract SIFT, SURF, or HOG features. Luckily, MDDA can also use the deep features extracted by deep neural networks such as AlexNet <ref type="bibr" target="#b34">[35]</ref> and ResNet <ref type="bibr" target="#b30">[31]</ref>. MDDA has a very useful property: it can learn the cross-domain function directly without the need for explicit classifier training. This makes it significantly more advantageous compared to most existing work such as JGSA <ref type="bibr" target="#b73">[74]</ref> and SCA <ref type="bibr" target="#b23">[24]</ref> which needs to train an extra classifier after learning transferrable features.</p><p>On the other hand, DDAN is suitable for cloud computing. The model of DDAN can be trained in an end-to-end manner and then be used for inference on the device. DDAN does not need extra feature extraction and classifier training procedure. All steps can be unified in one single deep neural network. This advantage makes it useful for large-scale datasets, which will probably result in prohibitive computations for MDDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND EVALUATIONS</head><p>In this section, we evaluate the performance of MDDA through extensive experiments on large-scale public datasets. The source code for MDDA is available at http://transferlearning.xyz. We will focus on evaluating the performance of MDDA since most of our contributions can be covered by MDDA. In the last part of this section, we will evaluate the performance of the deep version of MDDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>5.1.1 Datasets. We adopted five public image datasets: USPS+MNIST, Amazon review <ref type="bibr" target="#b6">[7]</ref>, Office-31 <ref type="bibr" target="#b49">[50]</ref>, ImageCLEF-DA <ref type="bibr" target="#b40">[41]</ref>, and Office-Home <ref type="bibr" target="#b59">[60]</ref>. These datasets are popular for benchmarking domain adaptation algorithms and have been widely adopted in most existing work such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref>. <ref type="figure">Fig. 3</ref> shows some samples of the datasets, and <ref type="table" target="#tab_3">Table 1</ref> lists their statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Office-31</head><p>Top: USPS+MNIST and Amazon Review; Down: ImageCLEF-DA Office-Home <ref type="figure">Fig. 3</ref>. Samples from the datasets in this paper USPS (U) and MNIST (M) are standard digit recognition datasets containing handwritten digits from 0-9. Since the same digits across two datasets follow different distributions, it is necessary to perform domain adaptation. USPS consists of 7,291 training images and 2,007 test images of size 16 × 16. MNIST consists of 60,000 training images and 10,000 test images of size 28 × 28. We construct two tasks: U → M and M → U. In the rest of the paper, we use A → B to denote the knowledge transfer from source domain A to the target domain B.</p><p>Amazon review <ref type="bibr" target="#b6">[7]</ref> is the benchmark dataset for cross-domain sentiment analysis. This dataset includes reviews about the Kitchen appliances (K), DVDs (D), Books (B) and Electronics (E). The reviews of each product can be regarded as data from the same domain. There are 1000 positive and 1000 negative instances on each domain. Transfer learning can be conducted between any two domains, leading to 12 tasks.</p><p>Office-31 <ref type="bibr" target="#b49">[50]</ref> consists of three real-world object domains: Amazon (A), Webcam (W) and DSLR (D). It has 4,652 images with 31 categories. Caltech-256 (C) contains 30,607 images and 256 categories. We constructed 6 tasks:</p><formula xml:id="formula_33">A → D, A → W, D → A, D → W, W → A, W → D.</formula><p>ImageCLEF-DA <ref type="bibr" target="#b40">[41]</ref> is a dataset presented in the ImageCLEF 2014 domain adaptation challenge. It is composed by selecting the 12 common classes shared by three public datasets (domains): Caltech-256 (C), ImageNet ILSVRC 2012 (I), and Pascal VOC 2012 (P). There are 50 images in each category and 600 images in each domain, while Office-31 has different domain sizes. We permute domains and build 6 transfer tasks: C → I, C → P, I → C, I → P, P → C, P → I.</p><p>Office-Home [60] is a new dataset which consists of 15,588 images from 4 different domains: Artistic images (Ar), Clip Art (Cl), Product images (Pr), and Real-World images (Rw). For each domain, the dataset contains images of 65 object categories collected from office and home settings. We use all domains and construct 12 transfer learning tasks: Ar → Cl, · · · , Rw → Pr.</p><p>In total, we constructed 2 + 12 + 6 + 6 + 12 = 38 tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">State-of-the-art Comparison Methods.</head><p>We compared the performance of MDDA with several state-of-the-art traditional and deep transfer learning approaches. Traditional transfer learning methods:</p><p>• NN, SVM, and PCA • TCA: Transfer Component Analysis <ref type="bibr" target="#b44">[45]</ref>, which performs marginal distribution alignment • GFK: Geodesic Flow Kernel <ref type="bibr" target="#b24">[25]</ref>, which performs manifold feature learning • JDA: Joint Distribution Adaptation <ref type="bibr" target="#b39">[40]</ref>, which adapts marginal &amp; conditional distribution • CORAL: CORrelation Alignment <ref type="bibr" target="#b51">[52]</ref>, which performs second-order subspace alignment • SCA: Scatter Component Analysis <ref type="bibr" target="#b23">[24]</ref>, which adapts scatters in subspace • JGSA: Joint Geometrical and Statistical Alignment <ref type="bibr" target="#b73">[74]</ref>, which aligns marginal &amp; conditional distributions with label propagation Deep transfer learning methods:</p><p>• AlexNet <ref type="bibr" target="#b34">[35]</ref> and ResNet <ref type="bibr" target="#b30">[31]</ref>, as baseline networks • DDC: Deep Domain Confusion <ref type="bibr" target="#b57">[58]</ref>, which is a single-layer deep adaptation method • DAN: Deep Adaptation Network <ref type="bibr" target="#b36">[37]</ref>, which is a multi-layer adaptation method • DANN: Domain Adversarial Neural Network <ref type="bibr" target="#b21">[22]</ref>, which is a adversarial deep neural network • ADDA: Adversarial Discriminative Domain Adaptation <ref type="bibr" target="#b56">[57]</ref>, which is a general framework for adversarial transfer learning • JAN: Joint Adaptation Networks <ref type="bibr" target="#b40">[41]</ref>, which is a deep network with joint MMD distance • CAN: Collaborative and Adversarial Network <ref type="bibr" target="#b74">[75]</ref>, which is based on joint training • CDAN: Conditional Domain Adversarial Networks <ref type="bibr" target="#b37">[38]</ref>, which is a conditional network 5.1.3 Implementation Details. MDDA requires to extract features from the raw inputs. For USPS+MNIST datasets, we adopted the 256 SURF features by following existing work <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b73">74]</ref>. For Amazon review dataset, we follow the feature generation method to exploit marginalized denoising autoencoders <ref type="bibr" target="#b11">[12]</ref> to improve the feature representations. For Office-31, ImageCLEF DA, and Office-Home datasets, we adopted the 2048 fine-tuned ResNet-50 features for a fair comparison. As for DDAN, we only report its results on three image datasets since USPS+MNIST and Amazon review datasets are rather simple to transfer. DDAN is able to take the original image data as inputs. We also adopted ResNet-50 as the baseline network for fair comparison <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b74">75]</ref>.</p><p>For the comparison methods, we either cite the results reported in their original papers or run experiments using their publicly available codes. As for MDDA, we set the manifold feature dimension d = 30, 30, 50, 60, 200 for the five datasets, respectively. The number of iteration is set to T = 10. We use the RBF kernel with the bandwidth set to be the variance of inputs. The regularization parameters are set as p = 10, λ = 4.5, η = 0.1, and ρ = 1. Additionally, the experiments on parameter sensitivity and convergence analysis in Section 5.5 indicate that the performance of MDDA and DDAN stays robust with a wide range of parameter choices. For DDAN, we set the learning rate to be 0.01 with the batch size to be 32 and a weight decay of 5e − 4. Other parameters are tuned by following transfer cross validation <ref type="bibr" target="#b76">[77]</ref>.</p><p>Although MDDA is easy to use, and its parameters do not have to be fine-tuned. For research purpose, we also investigate how to further tune those parameters. We choose parameters according to the following rules. Firstly, SRM on source domain is very important. Thus, we prefer a small η to make sure MDDA does not degenerate. Secondly, distribution alignment is required by SRM. Thus, we choose a slightly larger λ to make it effective. Thirdly, we choose ρ by following the existing work <ref type="bibr" target="#b3">[4]</ref>. Fourthly, p is set following <ref type="bibr" target="#b8">[9]</ref>.</p><p>We adopt classification accuracy on Ω t as the evaluation metric, which is widely used in existing literature <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b61">62]</ref>:  <ref type="table" target="#tab_5">Table 2</ref>. On the digit recognition tasks, MDDA outperforms the best method JGSA by a large margin of 8.9%. These results clearly indicate that MDDA significantly outperforms existing methods. Moreover, the performances of distribution alignment methods (TCA, JDA, and JGSA) and subspace learning methods (GFK, CORAL, and SCA) were generally inferior to MDDA. Each method has its limitations and cannot handle domain adaptation in specific tasks, especially with degenerated feature transformation and unevaluated distribution alignment. After manifold or subsapce learning, there still exists large domain shift <ref type="bibr" target="#b1">[2]</ref>; while feature distortion will undermine the distribution alignment methods.  <ref type="table" target="#tab_6">Table 3</ref>. From the results, we can observe that our proposed MDDA outperforms the best baseline method CORAL by a large margin of 6.0%. This clearly indicated that MDDA is able to dramatically reduce the divergence between different text domains.</p><formula xml:id="formula_34">Accuracy = |x : x ∈ Ω t ∧ŷ(x) = y(x)| |x : x ∈ Ω t | .<label>(31)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.3</head><p>Results on Image Datasets. The classification accuracy results on the Office-31, ImageCLEF DA, and Office-Home datasets are shown in <ref type="table" target="#tab_7">Tables 4, 5</ref>, and 6, respectively. From those results, we can make the following observations. Firstly, MDDA outperforms all other traditional and deep comparison methods in most tasks (20/24 tasks). The average classification accuracy achieved by MDDA on all the image tasks is  <ref type="bibr" target="#b74">[75]</ref> and CDAN <ref type="bibr" target="#b37">[38]</ref> all require to train an adversarial neural network, which clearly needs more time to converge. In this way, DDAN is much more efficient than these networks.</p><p>Thirdly, we also note that traditional methods such as TCA, JDA, and CORAL can also achieve good performance compared to ResNet, 1NN, and SVM. This clearly indicates the necessity of transfer learning when building models from two domains. Again, our proposed MDDA and DDAN can achieve the best performances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation of Dynamic Distribution Adaptation</head><p>We verify the effectiveness of dynamic distribution adaptation in this section. We answer two important questions: 1) Does the different effect of marginal and conditional distributions exist in transfer learning? And 2) Is our evaluation algorithm for DDA effective? It is worth noting that there is no ground-truth for µ. Therefore, in order to verify our evaluation, we record the performance of DDA by searching different µ values. The hint is that better µ value contributes better performance. Specifically, we run DDA by searching µ ∈ {0, 0.1, · · · , 0.9, 1.0}. To answer the first question, we draw the results of DDA under different values of µ in <ref type="figure" target="#fig_3">Fig. 4(a)</ref>. To answer   the second question, we compare the error made by random search (Random), average search (Average), and our evaluation in <ref type="figure" target="#fig_3">Fig. 4(b)</ref>. Firstly, it is clear that the classification accuracy varies with different choices of µ. This indicates the necessity to consider the different effects between marginal and conditional distributions. We can also observe that the optimal µ value varies on different tasks (µ = 0.2, 0, 1 for the three tasks, respectively). Thus, it is necessary to dynamically adjust the distribution alignment between <ref type="table">Table 7</ref>. Comparison of the performance between our evaluation of µ and average search (AVSE). Suppose the results of grid search are 0. domains according to different tasks. Moreover, the optimal value of µ is not unique for a given task. The classification results may be the same even for different values of µ. Secondly, we also report the results of our evaluation and average search in <ref type="table">Table 7</ref>. Combining the results in <ref type="figure" target="#fig_3">Fig. 4(b)</ref>, we can conclude that our evaluation of µ is significantly better than random search and average search. Additionally, both random search and average search require to run the whole MDDA or DDAN algorithm several times to get steady results, our evaluation is only required once in each iteration of the algorithm. This means that our evaluation is more efficient. It is worth noting that our evaluation is extremely close to the results from grid search. Note that on task M → U of <ref type="table">Table 7</ref>, our evaluation exceeds the results of grid search. Considering that there is often few or none labels in the target domain, grid search is not actually possible. Therefore, our evaluation of µ can be used to approximate the ground truth in real applications.</p><formula xml:id="formula_35">Task M → U B → E E → D A → W W → A C → P P → C Ar → Rw Cl → Pr Rw → Cl</formula><p>Thirdly, we noticed that on image classification datasets, the performance of MDDA is slightly better than DDAN. MDDA is a shallow learning method, which is much easier to tune hyperparameters than DDAN, which is based on deep learning. We think that after a more extensive hyperparameter tuning process, the performance of DDAN will be the same or better as MDDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>In this section, we conduct ablation study of MDDA and DDAN. MDDA mainly consists of four components: SRM, manifold learning, DDA, and Laplacian regularization. DDAN is composed of a deep network, DDA, and Laplacian regularization. We extensively analyze the performance of MDDA and DDAN on some tasks from each dataset and present the results in <ref type="figure">Fig. 5</ref>.</p><p>The results clearly indicate that each component is important to DDA. Of all the components, it is shown that our proposed DDA component is the most important part, which dramatically increases the results of transfer learning. For MDDA, manifold feature learning shows marginal improvement, while it could help to eliminate the feature distortion of the original space <ref type="bibr" target="#b1">[2]</ref>. For DDAN, we can clearly see that our DDA component is better than DAN, which is only adapting the marginal distributions. This again clarifies the importance of the proposed DDA framework. Finally, It seems that Laplacian regularization also generates marginal improvements except on the digit datasets (USPS+MNIST). We add Laplacian regularization since it helps the algorithm to converge quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Parameter Sensitivity and Convergence Analysis</head><p>As with other state-of-the-art domain adaptation algorithms <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b73">74]</ref>, MDDA and DDAN also involve several parameters. In this section, we evaluate the parameter sensitivity of them.  Experimental results demonstrated the robustness of MDDA and DDAN under a wide range of parameter choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">MDDA.</head><p>We investigated the sensitivity against manifold subspace dimension d and #neighbor p through experiments with a wide range of d ∈ {10, 20, · · · , 100} and p ∈ {2, 4, · · · , 64} on randomly selected tasks. From the results in <ref type="figure" target="#fig_4">Fig. 6</ref>(a) and 6(b), it can be observed that MDDA is robust with regard to different values of d and p. Therefore, they can be selected without in-depth knowledge of specific applications.</p><p>We ran MDDA with a wide range of values for regularization parameters λ, η, and ρ on several random tasks and compare its performance with the best baseline method. We only report the results of λ in <ref type="figure" target="#fig_4">Fig. 6(c)</ref>, and the results of ρ and η are following the same tendency. We observed that MDDA can achieve a robust performance with regard to a wide range of parameter values. Specifically, the best choices of these parameters are: λ ∈ [0.5, 1, 000], η ∈ [0.01, 1], and ρ ∈ [0.01, 5].</p><p>We evaluate the convergence of MDDA through experimental analysis. From the results in <ref type="figure" target="#fig_4">Fig. 6(f)</ref>, it can be observed that MDDA can reach a steady performance in only a few (T &lt; 10) iterations. It indicates the training advantage of MDDA in cross-domain tasks. 5.5.2 DDAN. DDAN involves three key parameters: λ, p, and ρ. Similar to MDDA, we report the parameter sensitivity and convergence in <ref type="figure" target="#fig_5">Fig. 7</ref>. It is clear that DDAN is robust to these parameters. Therefore, in real applications, the hyperparameters of DDAN do not have to be cherry-picked. This is extremely important in deep learning since it is rather time-consuming to tune the hyperparameters.</p><p>We also extensively evaluate the convergence of DDAN in <ref type="figure" target="#fig_5">Fig. 7(d)</ref>. It is shown that DDAN is able to converge quickly with steady performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Time complexity.</head><p>We empirically check the running time of MDDA and DDAN, and present the results in <ref type="table" target="#tab_11">Table 8</ref>. Note that for image classification tasks, the running time of ARTL, JGSA, and MDDA are the summation of deep feature extraction and algorithm running time, since these algorithms require to extract features before transfer learning. It is shown that both MDDA and DDAN can achieve efficient computing time while achieving better performance compared to these comparison methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, to solve the transfer learning problem, we propose the novel dynamic distribution adaptation (DDA) concept. DDA is able to dynamically evaluate the relative importance between the source and target domains. Based on DDA, we propose two novel methods: the manifold DDA (MDDA) for traditional transfer learning, and deep DDA networks (DDAN) for deep transfer learning. Extensive experiments on digit recognition, sentiment analysis, and image classification have demonstrated that both MDDA and DDAN could achieve the best performance compared to other state-of-the-art traditional and deep transfer learning methods.</p><p>In the future, we plan to extend the DDA framework into the heterogeneous transfer learning areas as well as apply it to more complex transfer learning situations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Performance of our DDA and the latest methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>(a) The different effect of marginal and conditional distributions. (b) Performance comparison between our dynamic distribution adaptation and the latest transfer learning methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The main idea of MDDA (Manifold Dynamic Distribution Adaptation) and DDAN (Dynamic Distribution Adaptation Network)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>(a) Performance of several tasks when searching µ in [0, 1]. (b) Performance comparison of Random guessing (Random), average search (AVSE), and our DDA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Parameter sensitivity analysis and convergence of MDDA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Parameter sensitivity and convergence analysis of DDAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>DDAN: Dynamic Distribution Adaptation NetworkInput: Source domain (x s , y s ), target domain data x t , regularization parameters λ, ρ, and #neighbor p.</figDesc><table><row><cell>Transfer Learning with Dynamic Distribution Adaptation</cell><cell>1:13</cell></row><row><cell>Algorithm 2 Output: Classifier f .</cell><cell></cell></row><row><cell>1: repeat</cell><cell></cell></row><row><cell>2:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the five benchmark datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Sample #Feature for MDDA #Class</cell><cell>Domain</cell><cell>Type</cell></row><row><cell>USPS+MNIST</cell><cell>3,800</cell><cell>256</cell><cell>10</cell><cell>U, M</cell><cell>Digit</cell></row><row><cell>Amazon review</cell><cell>1,123</cell><cell>400</cell><cell>2</cell><cell>B, D, E, K</cell><cell>Text</cell></row><row><cell>Office-31</cell><cell>4,110</cell><cell>2,048</cell><cell>31</cell><cell>A, W, D</cell><cell>Image</cell></row><row><cell>ImageCLEF DA</cell><cell>1,800</cell><cell>2,048</cell><cell>12</cell><cell>C, I, P</cell><cell>Image</cell></row><row><cell>Office-Home</cell><cell>15,500</cell><cell>2,048</cell><cell>65</cell><cell cols="2">Ar, Cl, Pr, Rw Image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy (%) on USPS-MNIST datasets with SURF features Results on Sentiment Analysis Dataset. The results on Amazon review datasets are shown in</figDesc><table><row><cell>Task</cell><cell cols="4">1NN SVM TCA GFK JDA CORAL SCA JGSA MDDA</cell></row><row><cell cols="2">U → M 44.7 62.2 51.2 46.5 59.7</cell><cell>30.5</cell><cell>48.0 68.2</cell><cell>76.8</cell></row><row><cell cols="2">M → U 65.9 68.2 56.3 61.2 67.3</cell><cell>49.2</cell><cell>65.1 80.4</cell><cell>89.6</cell></row><row><cell>AVG</cell><cell>55.3 65.2 53.8 53.9 63.5</cell><cell>39.9</cell><cell>56.6 74.3</cell><cell>83.2</cell></row><row><cell>5.2.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Classification accuracy (%) on Amazon review dataset Method 1NN TCA GFK SA JDA CORAL JGSA MDDA B → D 49.6 63.6 66.4 67.0 64.2 Specifically, on the hardest Office-Home dataset, MDDA significantly outperforms the latest deep transfer learning method CDAN [38] by 4.5%, which clearly demonstrates the effectiveness of MDDA. The results indicates that MDDA is capable of significantly reducing the distribution divergence in domain adaptation problems. Secondly, DDAN also substantially outperforms all the traditional and deep methods on most tasks. Note that DDAN is only based on deep neural network without adversarial training, while other deep methods such as CAN</figDesc><table><row><cell>71.6</cell><cell>66.6</cell><cell>77.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Classification accuracy (%) on Office-31 dataset with ResNet-50 as the baseline</figDesc><table><row><cell>Method</cell><cell></cell><cell>Baseline</cell><cell cols="2">Traditional transfer learning</cell><cell cols="4">Deep transfer learning</cell><cell>DDA</cell></row><row><cell>Task</cell><cell cols="10">ResNet 1NN SVM TCA GFK JDA CORAL DDC DAN DANN ADDA JAN CAN MDDA DDAN</cell></row><row><cell>A → D</cell><cell>68.9</cell><cell cols="2">79.1 76.9 74.1 77.9 80.7</cell><cell>81.5</cell><cell>76.5 78.6</cell><cell>79.7</cell><cell>77.8</cell><cell>84.7 85.5</cell><cell>86.3</cell><cell>84.9</cell></row><row><cell>A → W</cell><cell>68.4</cell><cell cols="2">75.8 73.3 72.7 72.6 73.6</cell><cell>77.0</cell><cell>75.6 80.5</cell><cell>82.0</cell><cell>86.2</cell><cell>85.4 81.5</cell><cell>86.0</cell><cell>88.8</cell></row><row><cell>D → A</cell><cell>62.5</cell><cell cols="2">60.2 64.1 61.7 62.3 64.7</cell><cell>65.9</cell><cell>62.2 63.6</cell><cell>68.2</cell><cell>69.5</cell><cell>68.6 65.9</cell><cell>72.1</cell><cell>65.3</cell></row><row><cell>D → W</cell><cell>96.7</cell><cell cols="2">96.0 96.5 96.7 95.6 96.5</cell><cell>97.1</cell><cell>96.0 97.1</cell><cell>96.9</cell><cell>96.2</cell><cell>97.4 98.2</cell><cell>97.1</cell><cell>96.7</cell></row><row><cell>W → A</cell><cell>60.7</cell><cell cols="2">59.9 64.9 60.9 62.8 63.1</cell><cell>64.3</cell><cell>61.5 62.8</cell><cell>67.4</cell><cell>68.9</cell><cell>70.0 63.4</cell><cell>73.2</cell><cell>65.0</cell></row><row><cell>W → D</cell><cell>99.3</cell><cell cols="2">99.4 99.0 99.6 99.0 98.6</cell><cell>99.6</cell><cell>98.2 99.6</cell><cell>99.1</cell><cell>98.4</cell><cell>99.8 99.7</cell><cell>99.2</cell><cell>100</cell></row><row><cell>AVG</cell><cell>76.1</cell><cell cols="2">78.4 79.1 77.6 78.4 79.5</cell><cell>80.9</cell><cell>78.3 80.4</cell><cell>82.2</cell><cell>82.9</cell><cell>84.3 82.4</cell><cell>85.7</cell><cell>83.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Classification accuracy (%) on ImageCLEF DA with ResNet-50 as baseline</figDesc><table><row><cell>Method</cell><cell></cell><cell>Baseline</cell><cell cols="2">Traditional transfer learning</cell><cell></cell><cell cols="2">Deep transfer learning</cell><cell></cell><cell>DDA</cell></row><row><cell>Task</cell><cell cols="10">ResNet 1NN SVM TCA GFK JDA CORAL DAN DANN JAN CAN CDAN MDDA DDAN</cell></row><row><cell>C → I</cell><cell>78.0</cell><cell cols="2">83.5 86.0 89.3 86.3 90.8</cell><cell>83.0</cell><cell>86.3</cell><cell>87.0</cell><cell>89.5 89.5</cell><cell>91.2</cell><cell>92.0</cell><cell>91.0</cell></row><row><cell>C → P</cell><cell>65.5</cell><cell cols="2">71.3 73.2 74.5 73.3 73.6</cell><cell>71.5</cell><cell>69.2</cell><cell>74.3</cell><cell>74.2 75.8</cell><cell>77.2</cell><cell>78.8</cell><cell>76.0</cell></row><row><cell>I → C</cell><cell>91.5</cell><cell cols="2">89.0 91.2 93.2 93.0 94.0</cell><cell>88.7</cell><cell>92.8</cell><cell>96.2</cell><cell>94.7 94.2</cell><cell>96.7</cell><cell>95.7</cell><cell>94.0</cell></row><row><cell>I → P</cell><cell>74.8</cell><cell cols="2">74.8 76.8 77.5 75.5 75.3</cell><cell>73.7</cell><cell>74.5</cell><cell>75.0</cell><cell>76.8 78.2</cell><cell>78.3</cell><cell>79.8</cell><cell>78.0</cell></row><row><cell>P → C</cell><cell>91.2</cell><cell cols="2">76.2 85.8 83.7 82.3 83.5</cell><cell>72.0</cell><cell>89.8</cell><cell>91.5</cell><cell>91.7 89.2</cell><cell>93.7</cell><cell>95.5</cell><cell>92.7</cell></row><row><cell>P → I</cell><cell>83.9</cell><cell cols="2">74.0 80.2 80.8 78.0 77.8</cell><cell>71.3</cell><cell>82.2</cell><cell>86.0</cell><cell>88.0 87.5</cell><cell>91.2</cell><cell>91.5</cell><cell>91.0</cell></row><row><cell>AVG</cell><cell>80.7</cell><cell cols="2">78.1 82.2 83.2 81.4 82.5</cell><cell>76.7</cell><cell>82.5</cell><cell>85.0</cell><cell>85.8 85.7</cell><cell>88.1</cell><cell>88.9</cell><cell>87.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Classification accuracy (%) on Office-Home dataset with ResNet-50 as baseline</figDesc><table><row><cell cols="3">Method</cell><cell></cell><cell>Baseline</cell><cell></cell><cell cols="4">Traditional transfer learning</cell><cell cols="4">Deep transfer learning</cell><cell>DDA</cell></row><row><cell cols="3">Task</cell><cell cols="12">ResNet 1NN SVM TCA GFK JDA CORAL DAN DANN JAN CDAN MDDA DDAN</cell></row><row><cell cols="3">Ar → Cl</cell><cell>34.9</cell><cell cols="4">45.3 45.3 38.3 38.9 38.9</cell><cell>42.2</cell><cell></cell><cell>43.6</cell><cell>45.6</cell><cell>45.9</cell><cell>46.6</cell><cell>54.9</cell><cell>51.0</cell></row><row><cell cols="3">Ar → Pr</cell><cell>50.0</cell><cell cols="4">60.1 65.4 58.7 57.1 54.8</cell><cell>59.1</cell><cell></cell><cell>57.0</cell><cell>59.3</cell><cell>61.2</cell><cell>65.9</cell><cell>75.9</cell><cell>66.0</cell></row><row><cell cols="3">Ar → Rw</cell><cell>58.0</cell><cell cols="4">65.8 73.1 61.7 60.1 58.2</cell><cell>64.9</cell><cell></cell><cell>67.9</cell><cell>70.1</cell><cell>68.9</cell><cell>73.4</cell><cell>77.2</cell><cell>73.9</cell></row><row><cell cols="3">Cl → Ar</cell><cell>37.4</cell><cell cols="4">45.7 43.6 39.3 38.7 36.2</cell><cell>46.4</cell><cell></cell><cell>45.8</cell><cell>47.0</cell><cell>50.4</cell><cell>55.7</cell><cell>58.1</cell><cell>57.0</cell></row><row><cell cols="3">Cl → Pr</cell><cell>41.9</cell><cell cols="4">57.0 57.3 52.4 53.1 53.1</cell><cell>56.3</cell><cell></cell><cell>56.5</cell><cell>58.5</cell><cell>59.7</cell><cell>62.7</cell><cell>73.3</cell><cell>63.1</cell></row><row><cell cols="3">Cl → Rw</cell><cell>46.2</cell><cell cols="4">58.7 60.2 56.0 55.5 50.2</cell><cell>58.3</cell><cell></cell><cell>60.4</cell><cell>60.9</cell><cell>61.0</cell><cell>64.2</cell><cell>71.5</cell><cell>65.1</cell></row><row><cell cols="3">Pr → Ar</cell><cell>38.5</cell><cell cols="4">48.1 46.8 42.6 42.2 42.1</cell><cell>45.4</cell><cell></cell><cell>44.0</cell><cell>46.1</cell><cell>45.8</cell><cell>51.8</cell><cell>59.0</cell><cell>52.0</cell></row><row><cell cols="3">Pr → Cl</cell><cell>31.2</cell><cell cols="4">42.9 39.1 37.5 37.6 38.2</cell><cell>41.2</cell><cell></cell><cell>43.6</cell><cell>43.7</cell><cell>43.4</cell><cell>49.1</cell><cell>52.6</cell><cell>48.4</cell></row><row><cell cols="3">Pr → Rw</cell><cell>60.4</cell><cell cols="4">68.9 69.2 64.1 64.6 63.1</cell><cell>68.5</cell><cell></cell><cell>67.7</cell><cell>68.5</cell><cell>70.3</cell><cell>74.5</cell><cell>77.8</cell><cell>72.7</cell></row><row><cell cols="3">Rw → Ar</cell><cell>53.9</cell><cell cols="4">60.8 61.1 52.6 53.8 50.2</cell><cell>60.1</cell><cell></cell><cell>63.1</cell><cell>63.2</cell><cell>63.9</cell><cell>68.2</cell><cell>67.9</cell><cell>65.1</cell></row><row><cell cols="3">Rw → Cl</cell><cell>41.2</cell><cell cols="4">48.3 45.6 41.7 42.3 44.0</cell><cell>48.2</cell><cell></cell><cell>51.5</cell><cell>51.8</cell><cell>52.4</cell><cell>56.9</cell><cell>57.6</cell><cell>56.6</cell></row><row><cell cols="3">Rw → Pr</cell><cell>59.9</cell><cell cols="4">74.7 75.9 70.5 70.6 68.2</cell><cell>73.1</cell><cell></cell><cell>74.3</cell><cell>76.8</cell><cell>76.8</cell><cell>80.7</cell><cell>81.8</cell><cell>78.9</cell></row><row><cell cols="3">Avg</cell><cell>46.1</cell><cell cols="4">56.4 56.9 51.3 51.2 49.8</cell><cell>55.3</cell><cell></cell><cell>56.3</cell><cell>57.6</cell><cell>58.3</cell><cell>62.8</cell><cell>67.3</cell><cell>62.5</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Random</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average</cell></row><row><cell>Accuracy (%)</cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>U B W</cell><cell>M E A</cell><cell>Error</cell><cell>5 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DDA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>C</cell><cell>P</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cl</cell><cell>Pr</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell></cell><cell>0</cell><cell>U</cell><cell>M B</cell><cell>E W</cell><cell>A C</cell><cell>P Cl</cell><cell>Pr</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Task</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">(a) Transfer results of different µ</cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) Comparison of estimating µ</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>Running time of MDDA and DDAN P 124.2 + 1321.4 198.3 + 1321.4 2342.1 2451.2 156.7 + 1321.4 2109.8 Cl → Pr 187.4 + 1768.7 244.3 + 1768.7 2877.7 2956.5 207.4 + 1768.7 2698.1</figDesc><table><row><cell>Task</cell><cell>ARTL</cell><cell>JGSA</cell><cell cols="2">DANN CDAN</cell><cell>MDDA</cell><cell>DDAN</cell></row><row><cell>U → M</cell><cell>29.1</cell><cell>14.6</cell><cell>-</cell><cell>-</cell><cell>31.4</cell><cell>-</cell></row><row><cell>B → E</cell><cell>22.8</cell><cell>18.7</cell><cell>-</cell><cell>-</cell><cell>23.5</cell><cell>-</cell></row><row><cell>W → A</cell><cell>45.6+763.6</cell><cell>66.5 + 763.6</cell><cell cols="4">1567.3 1873.2 48.8 + 7663.6 1324.1</cell></row><row><cell>C →</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t t i=1 r t .Max-min averaging is also simple to implement. We can search the value of µ in [0, 1] with the step of 0.1, which will generate a candidate set of µ: [0, 0.1, · · · , 0.9, 1.0]. Then, similar to random guessing, we can also obtain the averaged results as 111 11 i=1 r i . ACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article 1. Publication date: January 2019.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article 1. Publication date: January 2019.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported in part by National Key R &amp; D Program of China (2016YFB1001200), NSFC (61572471, 61972383), the support of grants from Hong Kong CERG projects 16209715, 16244616, Nanyang Technological University, Nanyang Assistant Professorship (NAP), and Beijing Municipal Science &amp; Technology Commission (Z171100000117017).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distribution-matching embedding for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="3760" to="3789" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation on the statistical manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2481" to="2488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 conference on empirical methods in natural language processing</title>
		<meeting>the 2006 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation with Distribution Matching Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 AAAI International Conference on Artificial Intelligence</title>
		<meeting>the 2018 AAAI International Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-position activity recognition with stratified transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pervasive and Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FedHealth: A Federated Transfer Learning Framework for Wearable Healthcare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI workshop on federated machine learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Co-clustering based classification for out-of-domain documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 13th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="210" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Boosting for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning (ICML)</title>
		<meeting>the 24th international conference on Machine learning (ICML)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey on heterogeneous transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transfer learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Asymmetric Heterogeneous Transfer Learning: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magda</forename><surname>Friedjungová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Jirina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1414" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimal kernel choice for large-scale two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivaraman</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sriperumbudur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1205" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grassmann discriminant analysis: a unifying view on subspace-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Hamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="376" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation With Label and Structural Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng-An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ren</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="5552" to="5562" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transfer learning from speaker verification to multispeaker text-to-speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4480" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data: open challenges and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartosz</forename><surname>Krawczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="221" to="232" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1645" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptation regularization: A general framework for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1076" to="1089" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep Transfer Learning with Joint Adaptation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Decomposition-based transfer distance metric learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="3789" to="3801" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Online Heterogeneous Transfer Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2525" to="2531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03944</idno>
		<title level="m">Transfer metric learning: Algorithms, applications and outlooks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Knowledge and Data Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>A survey on transfer learning</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panareda</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Transfer learning of structured representation for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dao-Qing</forename><surname>Chuan-Xian Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke-Kun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Rong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="5440" to="5454" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Return of Frustratingly Easy Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Subspace Distribution Alignment for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="24" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visual domain adaptation via transfer feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jafar</forename><surname>Tahmoresnezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sattar</forename><surname>Hashemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Transitive Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1155" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Vladimir Naumovich Vapnik and Vlamimir Vapnik</title>
	</analytic>
	<monogr>
		<title level="m">Statistical learning theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Everything about Transfer Learning and Domain Adapation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://transferlearning.xyz" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Balanced distribution adaptation for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuji</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1129" to="1134" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Stratified Transfer Learning for Crossdomain Activity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisha</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Pervasive Computing and Communications (PerCom)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Easy Transfer Learning By Exploiting Intra-domain Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Visual domain adaptation with manifold embedded distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep transfer learning for cross-domain activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Crowd Science and Engineering</title>
		<meeting>the 3rd International Conference on Crowd Science and Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Discriminative transfer subspace learning via low-rank and sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="850" to="863" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A Unified Framework for Metric Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Huaqing Min, and Hengjie Song</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Language-adversarial transfer learning for low-resource speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangyan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
	<note>TASLP)</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Saminger-Platz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Importance weighted adversarial nets for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8156" to="8164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04396</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Cross-Dataset Recognition: A Survey. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Joint Geometrical and Statistical Alignment for Visual Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Collaborative and Adversarial Network for Unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3801" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08114</idno>
		<title level="m">A survey on multi-task learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Cross domain distribution adaptation via kernel mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Verscheure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1027" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Multirepresentation adaptation network for cross-domain image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
