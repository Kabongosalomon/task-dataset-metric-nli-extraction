<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AssembleNet++: Assembling Modality Representations via Attention Connections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
							<email>mryoo@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics at Google</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics at Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhana</forename><surname>Kangaspunta</surname></persName>
							<email>juhana@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics at Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
							<email>anelia@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics at Google</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AssembleNet++: Assembling Modality Representations via Attention Connections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>video understanding</term>
					<term>activity recognition</term>
					<term>attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We create a family of powerful video models which are able to: (i) learn interactions between semantic object information and raw appearance and motion features, and (ii) deploy attention in order to better learn the importance of features at each convolutional block of the network. A new network component named peer-attention is introduced, which dynamically learns the attention weights using another block or input modality. Even without pre-training, our models outperform the previous work on standard public activity recognition datasets with continuous videos, establishing new state-of-the-art. We also confirm that our findings of having neural connections from the object modality and the use of peer-attention is generally applicable for different existing architectures, improving their performances. We name our model explicitly as AssembleNet++. The code will be available at: https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video understanding is a fundamental problem in vision with many novel approaches proposed recently. While many advanced neural architectures have been used for video understanding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43]</ref>, including two-stream and multi-stream ones <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>, learning of interactions between raw input modalities (e.g., RGB and motion) and semantic input modalities such as objects in the scene (e.g., persons and objects) have been limited.</p><p>Inspired by previous work, e.g. AssembleNet architectures for videos <ref type="bibr" target="#b33">[34]</ref> and RandWire architectures for images <ref type="bibr" target="#b52">[53]</ref> which proposed random or targeted connectivity between layers in a neural network, we create a family of powerful video models that explicitly learn interactions between spatial object-specific information and raw appearance and motion features. In particular, inter-block attention connectivity is searched for to best capture the interplay between different modality representations.</p><p>The main technical contributions of this paper include:</p><p>arXiv:2008.08072v1 [cs.CV] 18 Aug 2020</p><p>1. Optimizing neural architecture connectivity for object modality fusion. We discover that models with 'omnipresent' connectivity from object input allows the best multi-modal fusion. 2. Learning of video models with peer-attention on the connections. We newly introduce an one-shot model formulation to efficiently search for architectures with better peer-attention connectivity.</p><p>We test the approach extensively on challenging video understanding datasets, showing notable improvements: compared to the baseline backbone architecture we use, our new one-shot attention search model with object modality obtains +12.6% on Charades classification task and +6.22% on Toyota Smarthome dataset. Our approach also outperforms reported numbers of existing approaches on both datasets, establishing new state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous work</head><p>Video CNNs Convolutional neural network (CNNs) for videos <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> are a popular approach to video understanding, for example, solutions, such as 3D Video CNNs <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b11">12]</ref>, (2+1)D CNNs <ref type="bibr" target="#b42">[43]</ref> or even novel architecture searched models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref> are widely used. Action recognition has also been the topic of intense research <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Action recognition with objects Action recognition with objects has been traditionally studied years back <ref type="bibr" target="#b25">[26]</ref>. The presence of specific objects in video frames, has been shown to be important for video recognition, even in the context of advanced feature learned by deep neural models, e.g., Sigurdsson et al. <ref type="bibr" target="#b36">[37]</ref>; they are useful even if provided as a single label per frame. This is not surprising as many of the activities, e.g. 'speaking on the phone', or 'reading a book' are primarily determined by the objects themselves. Furthermore, clues about the location of persons, e.g., by 2D human pose has also been shown to be beneficial <ref type="bibr" target="#b4">[5]</ref>. Recent video CNNs have also tried to integrate object-related information, from segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref> or pre-training from image datasets <ref type="bibr" target="#b5">[6]</ref>. One-time late (or intermediate) fusion of object representation with RGB and flow representations has been widely used (e.g., <ref type="bibr" target="#b23">[24]</ref>). Ji et al. <ref type="bibr" target="#b15">[16]</ref> modeled scene relations on top of video CNNs using graph neural networks, for better usage of object information. However, we are not aware of any prior work that 'learns' the connectivity between among input modalities including object information, as we do in this paper.</p><p>Attention Use of attention within CNNs have been widely studied. Vaswani et al. <ref type="bibr" target="#b43">[44]</ref> investigated different forms and applications of attention while focusing on self-attention. Hu et al. <ref type="bibr" target="#b13">[14]</ref> introduced Squeeze-and-Excitation, which is a form of channel-wise self-attention. Researchers also developed other forms of channel-wise self-attention <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b46">47]</ref>, often together with spatial selfattention. Attention was also applied to video CNN models <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5]</ref>. However, we are not aware of prior work explicitly searching for inter-block attention connectivity (i.e., peer-attention) as we do in this paper.</p><p>Neural architecture search Neural Architecture Search (NAS) is the concept of automatically finding superior architectures based on training data <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref>. Multiple different strategies including learning of reinforcement learning controller (e.g., <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref> as well as evolutionary algorithms (e.g., <ref type="bibr" target="#b32">[33]</ref>) have been developed for NAS. In particular, one-shot differentiable architecture search <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref> has been successful as it does not require a massive amount of model training. RandWire network <ref type="bibr" target="#b52">[53]</ref> could also be interpreted as a form of differentiable architecture search, as it learns weights of (random) connections to minimize the classification loss.</p><p>However, architecture search for neural attention connectivity has been very limited. Ahmed and Torresani <ref type="bibr" target="#b0">[1]</ref> searched for layer connectivity and Ryoo et al. <ref type="bibr" target="#b33">[34]</ref> searched for multi-stream connectivity for video CNNs, but they were without any attention learning which becomes a crucial component when we have a mixture of input modalities. We believe this paper is the first paper to search for models with attention connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>This section describes the video CNN architecture framework, which will be used as a base for developing our approach.</p><p>We here adopt a multi-stream, multi-block architecture design from Assem-bleNet <ref type="bibr" target="#b33">[34]</ref>. AssembleNet design allows learning of connections between modalities and their intermediate features. This architecture is similar to other twostream models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>, but is more flexible in two ways: 1) it allows the use of more than two streams, and 2) it allows connections to be formed (and potentially learned) between individual blocks of the neural architecture.</p><p>More specifically, the architecture we use has multiple input blocks, each corresponding to an input modality. The network blocks have a structure inspired by ResNet architectures <ref type="bibr" target="#b12">[13]</ref>. Each input block is composed of a small number of pooling and convolutional layers attached directly on top of the input. The input blocks are then connected to network blocks at the next level. We follow the (2+1)D ResNet block structure from <ref type="bibr" target="#b42">[43]</ref>, where each module is composed of one 1D temporal conv., one 2D spatial conv., and one 1x1 conv. layer. A block is formed by repeating the (2+1)D residual module multiple times. This allows a fair and direct comparison between our approach and previous models using the same module and block <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Each network block (or block for short) can be connected to any block from any modality at the next level, including its own. Blocks are organized at levels so that connections do not form cycles. Connections can also be formed to skip levels. We note that since many connections between blocks are formed early, the neural blocks themselves will often contain information from many input modalities as early as the first level of the network. <ref type="figure" target="#fig_4">Figure 4</ref> (a) shows one example architecture, where the structure of the network and example connectivity can be seen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Input modalities and semantics</head><p>In addition to the standard raw RGB video input, motion information is added as a separate modality. More specifically, optical flow, either pre-computed for the dataset <ref type="bibr" target="#b54">[55]</ref>, or trained on the fly <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref>, has been shown to be a crucial input for achieving better accuracy across the board <ref type="bibr" target="#b3">[4]</ref>.</p><p>We here propose to use object segmentation information as a separate 'object' modality. Objects and their locations provide semantics information which conveys useful information about activities in a video. Crucially here, semantic information is incorporated in the full architecture so that it is able to interact with other modalities and the intermediate features from them (as described more in Sections 3.3 to 3.5), to maximize its utilization for the best representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input block details</head><p>We construct an input block for each input modality. Each input block is composed of one pooling and up to two convolutional layers for raw RGB and optical flow inputs, and just one pooling layer for semantic object inputs applied directly on top of the inputs. In the object input block, a segmentation mask having an integer class value per pixel is converted into a HxWxC O tensor using one-hot operation, where C O is the number of object classes. The segmentation masks are obtained from a model trained on a nonrelated image-based dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning weighted connections</head><p>Blocks in the network can potentially form connections with one or more blocks. While connectivity and the strength of the connectivity could also be handcoded, we formulate our networks so that they are learnable.</p><p>Let G be the connectivity graph of the network where (j, i) specifies that there is a connection from the jth block to ith block. We allow each block to receive its inputs from multiple different input blocks as well as intermediate convolutional blocks, and generates an output. Specifically, we formulate the input of the block as a weighted summation over multiple connections where we learn one weight for each connection.</p><formula xml:id="formula_0">x in i = (j,i)∈G σ(w ji ) · x out j<label>(1)</label></formula><p>where i and j are block indexes, x in i corresponds on the final input to the ith block, and x out i corresponds to the output of the block. σ is a sigmoid function.</p><p>Learning of the connection weights together with the other convolutional layer parameters with the standard back propagation allows the network to optimize itself on which connections to use and which to not based on the training data. In our approach, this is done by initially connecting every possible blocks in the graph while using the block levels to avoid cycles, and then learning them. We consider every connection (j, i) from the jth block to ith block as valid as long as L(j) &lt; L(i) where L(i) indicates the level of the block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Attention connectivity and peer-attention</head><p>In addition to having and learning static weights per connection, we use attention to dynamically control the behavior of each connection. The intuition is that objects and activities are correlated, and using attention allows the model to focus on important objects based on motion context and vice versa. For instance, motion features of 'drinking' could suggest another network stream to focus more on objects related to such motion (e.g., 'cups' and 'bottles').</p><p>We formulate our connectivity graph G to have one more component for each edge: ((j, i), k), where k is the convolutional block influencing the connection (j, i) via attention. A channel-wise attention is used to implement this behavior. Let C i be the size of the input channel of block i. For each connection (j, i), the attention vector of size C i is computed per frame as:</p><formula xml:id="formula_1">A i (x) = [a 1 , . . . , a Ci ] = σ(f (GAP(x)))<label>(2)</label></formula><p>where f is a function (one fully connected layer in our case) mapping a vector to a vector of size C. GAP is the global average pooling over spatial resolution in the input tensor, making GAP(x) to have a form of a vector per frame. Using A i (x), the input for each block i is computed by combining every connection (j, i) while considering its attention from block k:</p><formula xml:id="formula_2">x in i = ((j,i),k)∈G σ(w ji ) · (A i (x out k ) · x out j ).<label>(3)</label></formula><p>The simplest special case of our attention is self-attention, which is done by making x k and x j to be identical. In this form, the usage of attention becomes similar to Squeeze-and-Excitation <ref type="bibr" target="#b13">[14]</ref>. Importantly, in our approach, we learn to select different x k where x k = x j , which we discuss more in the following subsection. Attention with x k = x j implies that the channels to use for the connection is dynamically decided based on another input modality and peer blocks. We more explicitly name this approach as peer-attention. In principle, we define a 'peer' as any block p that could potentially be connected to i. In our formulation where the convolutional blocks are organized into multiple levels (to avoid cycles), the set of peers P for a connection (j, i) is computed as P (j,i) = {p | L(p) &lt; L(i)} where L(p) indicates the level of the block p. We consider the attention connection ((j, i), k) to be valid as long as k ∈ P (j,i) . <ref type="figure" target="#fig_1">Figure 1</ref> compares connectivity without attention and connectivity with selfand peer-attention.</p><p>...  </p><formula xml:id="formula_3">w 1 w 2 x in (2+1)D Residual Block + (2+1)D Residual Block (2+1)D Residual Block w 3 ... + + (2+1)D Residual Block w 4 · + · GAP FC · GAP FC GAP FC w 1 w 2 x in (2+1)D Residual Block + (2+1)D Residual Block (2+1)D Residual Block (2+1)D Residual Block w 3 ... + + (2+1)D Residual Block w 4 · + · GAP FC · GAP FC (2+1)D Residual Block GAP FC · GAP FC · (2+1)D Residual Block (2+1)D Residual Block GAP FC · GAP FC · ... (b) Connectivity with self-attention (c) Connectivity with peer-attention w 1 w 2 x in (2+1)D Residual Block + (2+1)D Residual Block (2+1)D Residual Block w 3 ... + + (2+1)D Residual Block w 4 + (2+1)D Residual Block (2+1)D Residual Block</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">One-shot attention search model</head><p>Given a set of convolutional blocks, instead of hand-designing peer-attention connections, we search for the attention connectivity. Our new one-shot attention search model is introduced, which optimizes the model's peer-attention configuration directly based on training data. Our one-shot attention search model is formulated by combining attention from all possible peer blocks for each connection with learnable weights. The idea is to enable the model to soft-select the best peer for each block by learning differentiable weights, maximizing the recognition performance. All possible attention connectivity is considered as a consequence, and the searching is done solely based on the standard backpropagation.</p><p>For each pair of blocks (j, i) where L(j) &lt; L(i), we place a weight for every k ∈ P (j,i) . Let h be a weight vector of size m = |P (j,i) |, and X out P = [x out 1 , . . . , x out m ] be the tensor concatenating x out k of every possible peer k in P . Then, we reformulate Equation 3 as:</p><formula xml:id="formula_4">x in i = (j,i)∈G σ(w ji ) · (A(x) · x out j ) where x = 1 T softmax (h) · X out P (j,i) . (4)</formula><p>1 is a vector of size m having 1 as all its element values, making x to be a weighted sum of peer block outputs x out k . Use of softmax function allows one-hot like behavior (i.e., selecting one peer to control the attention) based on learned weights h = [h 1 , . . . , h m ]. <ref type="figure" target="#fig_2">Figure 2</ref> visualizes the process.</p><p>The entire process is fully differentiable, allowing the one-shot training to learn the attention weights h together with the connection weights w ji . This is unlike AssembleNet which partially relies on exponential mutations to explore connections. Once the attention weights are found, we can either prune the connections by only leaving the argmax over h k or leave them with softmax. We confirmed that they do not make different in practice, allowing us to only maintain one peer-attention per block as shown in <ref type="figure" target="#fig_1">Figure 1 (c)</ref>. Peer-attention only causes 0.151% increase in computation, which we describe more in Appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Model implementation details</head><p>In order to provide fair comparison to previous work, we comply with the same block structure as AssembleNet <ref type="bibr" target="#b33">[34]</ref>, which by itself is comparable to (2+1)D ResNet-50. We build two RGB input blocks (whose temporal resolutions are searched), two optical flow input blocks, and one object input block. RGB blocks and optical flow blocks have the same number of channels and layers as AssembleNet, while the object input block only has one max spatial pooling layer which does not increase the number of parameters of the model.</p><p>The object input block obtains its input from a fixed object segmentation model trained independently with the ADE-20K object segmentation dataset <ref type="bibr" target="#b56">[57]</ref>. We treat this module as a blackbox and do not propagate gradients into it. Because this is an off-the-shelf segmentation module and was not trained on any video dataset, its outputs become noisy when directly applied to video datasets as shown in <ref type="figure">Figure 3</ref>.</p><p>Our model has convolutional blocks of four levels (five levels if we count input blocks). The sum of channel sizes are held as a constant at each level (regardless the number of blocks), in order to maintain the total number of parameters. The total channels are 128 at input level, and 128, 256, 512, and 512 at levels 1 to 4 following the ResNet module and block formulation. As a result, all models have equivalent number of parameters to standard two-stream CNNs with (2+1)D residual modules.</p><p>Each convolutional block was implemented by alternating 2-D residual modules and (2+1)D residual modules as was done in <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b33">34]</ref>. (2+1)D module is composed of 1D temporal convolution layer followed by 2D spatial convolution layer, followed by 1x1 convolution layer. The temporal resolution of each block is controlled using temporally dilated 1-D convolution, avoiding hard frame downsampling. More details of the blocks are in the supplementary material.</p><p>Although the number of blocks at each level could be hand-designed, we use AssembleNet architecture search (with an evolutionary algorithm) to find the optimal combination of convolutional blocks and their temporal resolutions. Once we have the blocks, we connect blocks with weighted connections (doing weighted summation) following Section 3.3. Finally, the one-shot attention search model obtained by implementing our peer-attention with softmax-weighted-sum, as described in Section 3.5.</p><p>Approach summary The overall process could be summarized as follows:</p><p>1. Prepare blocks. We use AssembleNet evolution to find convolutional blocks, roughly connected. 2. Initialize our one-shot search model by including all possible block connections as well as new attention connections, as described in Sections 3.3∼3.5. 3. Train the one-shot model, learning the attention connectivity weights. 4. Prune low weight connections to make the model more compact. We maintain only one peer-attention per block.</p><p>We name our final approach specifically as AssembleNet++. <ref type="figure">Fig. 3</ref>. Examples of the segmentation CNN applied directly on Charades video frames with in-home activities. These noisy masks serve as an input to the object input block, suggesting that our video model is required to learn to handle such noisy input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head><p>We conduct experiments on popular video recognition datasets: multi-class multilabel Charades <ref type="bibr" target="#b35">[36]</ref>, and also the recent Toyota Smarthome dataset <ref type="bibr" target="#b4">[5]</ref>, which records natural person activities in their homes. We note that we report results without any pre-training on a large-scale video dataset, which is unlike most of previous work. Regardless of that, Assem-bleNet++ outperforms prior work. We conduct multiple ablation experiments to confirm the benefit of our multi-modal model formulation with peer-attention and our one-shot attention search.</p><p>Charades dataset. The Charades dataset <ref type="bibr" target="#b35">[36]</ref> is composed of continuous videos of humans interacting with objects. This dataset is a multi-class multi-label video dataset with a total of 66,500 annotations. The videos in the dataset involve motion of small objects in real-world home environments, making it a very challenging dataset. Example video frames of the Charades dataset could be found in <ref type="figure">Figure 3</ref>. We follow the standard v1 classification setting of the dataset, reporting mAP %. We use Charades as our main dataset for ablations, as it is a realistic dataset explicitly requiring modeling of interactions between object information and other raw inputs such as RGB.</p><p>Toyota Smarthomes dataset The Toyota Smarthomes dataset <ref type="bibr" target="#b4">[5]</ref> consists of real-world activities of humans in their daily lives, such as reading, watching TV, making coffee or breakfast, etc. Humans often interact with objects in this dataset (e.g., 'drink from a can' and 'cook-cut'). The dataset contains 16,115 videos of 31 action classes, and the videos are taken from 7 different camera viewpoints. We only use RGB frames from this dataset, although depth and skeleton inputs are also present in the dataset.</p><p>Baselines As a baseline model, we use AssembleNet architecture backbone <ref type="bibr" target="#b33">[34]</ref> which consists of multiple configurable (2+1)D ResNet blocks. Ablations, including our models without the object input block and without peer-attention, are also implemented and compared.</p><p>In our ablation experiments which compare different aspects of the proposed approach (Sections 4.1, 4.2, 4.4, and 4.5), we train the models for 50K iterations with cosine decay for the Charades dataset. When using the Toyota dataset, we train our models for 15K iterations with cosine decay as this is a smaller dataset than Charades (66,500 annotations in Charades vs. 16,115 segmented videos in Toyota Smarthome). Further, when comparing against the state-of-the-art, we use the learning rate following a cosine decay function with 'warm-restart' <ref type="bibr" target="#b22">[23]</ref>, which we discuss more in Section 4.3.</p><p>Since the model is a one-shot architecture search to discover the attention connectivity, training is efficient and takes only 20∼30 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Using object modality</head><p>In this ablation experiment, we explore the importance of the object input. For this study, our model learns the block connectivity from the Charades training, while not using any attention (i.e., they look like <ref type="figure" target="#fig_1">Figure 1 (a)</ref>). <ref type="figure" target="#fig_4">Figure 4 (a)</ref> shows the best connectivity the one-shot model discovered. This is obtained by (i) evolving the blocks with 100 rounds of architecture evolution, (ii) connecting all blocks, (iii) training the weights in one-shot, and then (iv) pruning the low-weight connections. The connections weights w ji with values higher than 0.2 are visualized. Interestingly, the best model is obtained by connecting the object input block to every possible block. The model with this 'omnipresent' object connectivity obtains 50.43 mAP on Charades compared to 47.18 mAP of the model without any object connections, which attests the the usefulness of the object modality. The learned weights of each object connection is more than 0.7, suggesting the strong usage of it.</p><p>Motivated by the finding that the usage of object information at every block is beneficial (i.e., omnipresent object modality connectivity), we ran an experiment to investigate how performance changes with respect to the best models found with different number of object connections. <ref type="figure" target="#fig_4">Figure 4 (b)</ref> shows the Charades classification performances of our best found models with full vs. restricted object input usage. X-axis of the graph corresponds to how often the model uses the direct input from the object input block. 0 means that it does not use object information at all, and 1 means it fuses the object information at every block. We are able to clearly observe that the performance increases proportionally to the usage of the object information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attention search</head><p>Next, we confirm the effectiveness of our proposed AssembleNet++ with attention search. <ref type="table" target="#tab_0">Table 1</ref> illustrates how much performance gain we get by using attention connections as opposed to the standard weighted connections. In addition to attention connectivity with self-attention <ref type="figure" target="#fig_1">(Figure 1 (b)</ref>) and peer-attention <ref type="figure" target="#fig_1">(Figure 1 (c)</ref>), we implemented and tested 'static attention'. This is when learning fixed weights not influenced by any input. We are able to observe that our approach of one-shot attention search (with peer-attention) greatly improves the performance. The benefit was even higher (i.e., by ∼6% mAP) when using the object input. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to the state-of-the-art</head><p>In this section, we compare the performance of our AssembleNet++ model with the previous state-of-the-art approaches. We use the model with optimal peerattention found using our one-shot attention search, and compare it against the results reported by previous work. Unlike most of the existing methods benefiting from pre-training with a large-scale video dataset, we demonstrate that we are able to outperform state-of-the-art without such pre-training. Below, we show our results on Charades and Toyota Smarthome datasets. We also note that the proposed learned attention mechanisms are very powerful, as also seen in the ablation experiments in Section 4.2, and even without object information and without pre-training can outperform, or be competitive to, the state-of-the-art. <ref type="table" target="#tab_1">Table 2</ref> shows the results of our method on the Charades dataset. Notice that we are establishing a new state-of-the-art number on this dataset, outperforming previous approaches relying on pre-training. Further, we emphasize that our model is organized to have a maximum of 50 convolutional layers as its depth, explicitly denoting it as 'AssembleNet++ 50'. Our model performs, without pre-training, even superior to AssembleNet with the depth of 101 layers that uses a significantly larger number of parameters. We also note that the use of object modality and attention mechanism proposed here, improves the corresponding AssembleNet baseline by +12.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Charades dataset</head><p>For this experiment, we use the learning rate with 'warm-restart' <ref type="bibr" target="#b22">[23]</ref>. More specifically, we use a cosine decay function that restarts to a provided initial learning rate at every cycle. The motivation is to train our models with an identical amount of training iterations compared to the other state-of-the-art. We apply 100K training iterations with two 50K cycles while only using Charades videos, in contrast to previous work (e.g., AssembleNet <ref type="bibr" target="#b33">[34]</ref> and SlowFast <ref type="bibr" target="#b8">[9]</ref>) that used 50K pre-training with another dataset and did 50K fine-tuning with Charades on top of it. We note that the results of our model without 'warmrestart' and without pre-training (as seen in the ablation results in <ref type="table" target="#tab_0">Table 1</ref>) at 56.38 are also very competitive to the state-of-the-art.</p><p>Toyota Smarthome dataset We follow the dataset's Cross-Subject (CS) evaluation setting, and measure performance in terms of two standard metrics of the dataset <ref type="bibr" target="#b4">[5]</ref>: (1) activity classification accuracy (%) and (2) 'mean per-class accuracies' (%). <ref type="table" target="#tab_2">Table 3</ref> reports our results. Compared to <ref type="bibr" target="#b4">[5]</ref>, which benefits from Kinetics pre-training and additional 3D skeleton joint information, we obtain superior performance while training the model from scratch and without skeletons. We believe we are establishing new state-of-the-art numbers on this dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation</head><p>In this experiment, we explicitly compare AssembleNet++ using peer-attention with its modifications using the same number of parameters. Specifically, we compare our model against (i) the model using 1x1 convolutional layers instead of attention and (ii) the model using peer-attention but with random attention connectivity. For (i), we make the number of 1x1 convolutional layer parameters identical to the number of parameters in FC layers for attention. <ref type="table">Table 4</ref> compares the accuracies of these models on Charades and Toyota Smarthome datasets. While using the identical number of parameters, our one-shot peerattention search model obtains superior results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">General applicability of the findings</head><p>Based on the findings that (1) having 'omnipresent' neural connectivity from the object modality and (2) using attention connectivity are beneficial, we investigate further whether such findings are generally applicable for many different CNN models. We add object modality connections and attention to (i) standard R(2+1)D network, (ii) two-stream R(2+1)D network, (iii) original AssembleNet, and (iv) our Charades-searched network (without object connectivity and attention), and observe how their recognition accuracy changes compared to the original models. Our model without object and attention is obtained by manually removing connections from the object input block. <ref type="table">Table 5</ref> shows the results tested on Charades. We are able to confirm that our findings are applicable to other manually designed, as well as, architecture searched architectures. The increase in accuracy is significant for all architectures. Note that our architecture itself is not significantly superior to Assem-bleNet without object. However, since its connectivity was searched together the object input block (i.e., Section 3.5), we are able to observe that our model better takes advantage of the object input via peer attention. 50K training iterations with cosine decay was used for this comparison. <ref type="table">Table 5</ref>. Comparison between original CNN models (without object modality and without attention) and their modifications based on our attention connectivity and object modality. The value corresponding to 'AssembleNet++' for the column 'base' is obtained by manually removing connections from the object input block and removing attention from our final one-shot attention search model. Measured with Charades classification (mAP, higher is better), trained from scratch for 50k iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a family of novel video models which are designed to learn interactions between the object modality input and the other raw inputs: Assem-bleNet++. We propose connectivity search to fuse new object input into the model, and introduce the concept of peer-attention to best capture the interplay between different modality representations. The concept of peer-attention generalizes previous channel-wise self-attention by allowing the attention weights to be computed based on other intermediate representations. An efficient differentiable one-shot attention search model is proposed to optimize the attention connectivity. Experimental results confirm that (i) our approach is able to appropriately take advantage of the object modality input (by learning connectivity to the object modality consistently) and that (ii) our searched peer-attention greatly benefits the final recognition. The method outperforms all existing approaches on two very challenging video datasets with daily human activities. Furthermore, we confirm that our proposed approach and the strategy are not just specific to one particular model but is generally applicable for different video CNN models, improving their performance notably.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Examples of convolutional block connectivity (a) without attention, (b) with self-attention, and (c) with peer-attention. Red lines indicate weighted connections from Section 3.3. Blue curves specify the attention connectivity. GAP is global average pooling and, FC is a fully connected layer. Our attention is channel-wise attention, and it is applied per frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Visualization of our one-shot attention search model. Magenta connections illustrate weights for the attention connection h. The softmax-sum module in the illustration corresponds to Eq. 4, fusing attentions from different blocks. These weights are fully differentiable and are learned together with convolutional filters, enabling the one-shot connectivity search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Learned connectivity between blocks (b) Charades performance with respect to the number of object connections</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>(a) Learned connectivity graph of the model and (b) Charades classification performance per object connection ratio. The highlighted blue edges correspond to the direct connections from the object input block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between performance with and without attention connections on Charades (mAP). The models were trained for 50K iterations.</figDesc><table><row><cell cols="2">Attention without object</cell><cell>with object</cell></row><row><cell>None</cell><cell>47.18</cell><cell>50.43</cell></row><row><cell>Static</cell><cell>48.82</cell><cell>51.15</cell></row><row><cell>Self</cell><cell>51.91</cell><cell>55.40</cell></row><row><cell>Peer</cell><cell>52.39</cell><cell>56.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Classification performance on the Charades dataset (mAP).</figDesc><table><row><cell>Method</cell><cell cols="2">Pre-training mAP</cell></row><row><cell>Two-stream [35]</cell><cell>UCF101</cell><cell>18.6</cell></row><row><cell>CoViAR [52] (Compressed)</cell><cell>ImageNet</cell><cell>21.9</cell></row><row><cell>Asyn-TF [35]</cell><cell>UCF101</cell><cell>22.4</cell></row><row><cell>MultiScale TRN [56] (RGB)</cell><cell>ImageNet</cell><cell>25.2</cell></row><row><cell>I3D [4] (RGB-only)</cell><cell>Kinetics</cell><cell>32.9</cell></row><row><cell>I3D from [48] (RGB-only)</cell><cell>Kinetics</cell><cell>35.5</cell></row><row><cell>I3D + Non-local [48] (RGB-only)</cell><cell>Kinetics</cell><cell>37.5</cell></row><row><cell>EvaNet [28] (RGB-only)</cell><cell>Kinetics</cell><cell>38.1</cell></row><row><cell>STRG [49] (RGB-only)</cell><cell>Kinetics</cell><cell>39.7</cell></row><row><cell>LFB-101 [51] (RGB-only)</cell><cell>Kinetics</cell><cell>42.5</cell></row><row><cell>SGFB-101 [16] (RGB-only)</cell><cell>Kinetics</cell><cell>44.3</cell></row><row><cell>SlowFast-101 [9] (RGB+RGB)</cell><cell>Kinetics</cell><cell>45.2</cell></row><row><cell>Two-stream (2+1)D ResNet-101</cell><cell>Kinetics</cell><cell>50.6</cell></row><row><cell>AssembleNet-50 [34]</cell><cell>MiT</cell><cell>53.0</cell></row><row><cell>AssembleNet-50 [34]</cell><cell>Kinetics</cell><cell>56.6</cell></row><row><cell>AssembleNet-101 [34]</cell><cell>Kinetics</cell><cell>58.6</cell></row><row><cell>AssembleNet-50 [34]</cell><cell>None</cell><cell>47.2</cell></row><row><cell cols="2">AssembleNet++ 50 (ours) without object None</cell><cell>54.98</cell></row><row><cell>AssembleNet++ 50 (ours)</cell><cell>None</cell><cell>59.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance on the Toyota Smarthome dataset. Classification % and mean per-class accuracy % are reported. Note that our models are being trained from scratch without any pre-training, while the previous work (e.g.,<ref type="bibr" target="#b4">[5]</ref>) relies on Kinetics pretraining.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">Classification % mean per-class</cell></row><row><cell>LSTM [25]</cell><cell></cell><cell>-</cell><cell>42.5</cell></row><row><cell>I3D (with Kinetics pre-training)</cell><cell></cell><cell>72.0</cell><cell>53.4</cell></row><row><cell>I3D (pre-trained) + NL [48]</cell><cell></cell><cell>-</cell><cell>53.6</cell></row><row><cell cols="2">I3D (pre-trained) + separable STA [5]</cell><cell>75.3</cell><cell>54.2</cell></row><row><cell>Baseline AssembleNet-50</cell><cell></cell><cell>77.77</cell><cell>57.42</cell></row><row><cell>Baseline + self-attention</cell><cell></cell><cell>77.59</cell><cell>57.84</cell></row><row><cell>Ours (object + self-attention)</cell><cell></cell><cell>79.08</cell><cell>62.30</cell></row><row><cell>Ours (object + peer-attention)</cell><cell></cell><cell>80.64</cell><cell>63.64</cell></row><row><cell cols="4">Table 4. Comparing AssembleNet++ using peer-attention vs. a modification using 1x1</cell></row><row><cell cols="4">convolutional layer instead of attention. They use an identical number of parameters.</cell></row><row><cell cols="4">Charades classification accuracy (mAP) and Toyota mean per-class accuracy (%) are</cell></row><row><cell>reported.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Charades</cell><cell>Toyota</cell></row><row><cell>Base</cell><cell></cell><cell>50.43</cell><cell>59.16</cell></row><row><cell>Base + 1x1 conv.</cell><cell></cell><cell>50.24</cell><cell>59.44</cell></row><row><cell>Random peer-attention</cell><cell></cell><cell>53.40</cell><cell>60.23</cell></row><row><cell>Our peer-attention</cell><cell></cell><cell>56.38</cell><cell>63.64</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Connectivity learning in multi-branch networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Meta-Learning (MetaLearn)</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object level visual reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toyota smarthome: Real-world activities of daily living</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koperski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Francesca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Holistic large scale video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>arxiv.org/pdf/1904.11451</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV Workshop on Action, Gesture, and Emotion Recognition</title>
		<meeting>the ICCV Workshop on Action, Gesture, and Emotion Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Action genome: Actions as composition of spatio-temporal scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture seach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7834" to="7843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going deeper into first-person activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularizing long short term memory with 3d humanskeleton sequences for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploiting human actions and object context for recognition tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Hayesiii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Architecture search of dynamic cells for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno>CoRR:1904.02371</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evolving space-time neural architectures for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning latent sub-events in activity videos using temporal attention filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence (AAAI</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scenes-objects-actions: A multi-task, multilabel video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">AssembleNet: Searching for multi-stream neural connectivity in video architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Asynchronous temporal fields for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Charadesego: A large-scale dataset of paired third and first person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09626</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">What actions are needed for understanding human actions in videos?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatiotemporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">C3d: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1412.0767</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03151</idno>
		<title level="m">Eca-net: Efficient channel attention for deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05038</idno>
		<title level="m">Long-term feature banks for detailed video understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Compressed video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6026" to="6035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1284" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
