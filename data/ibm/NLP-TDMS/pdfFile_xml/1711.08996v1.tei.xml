<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense 3D Regression for Hand Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengde</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zürich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Probst</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zürich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zürich</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>Leuven</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dense 3D Regression for Hand Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a simple and effective method for 3D hand pose estimation from a single depth frame. As opposed to previous state-of-the-art methods based on holistic 3D regression, our method works on dense pixel-wise estimation. This is achieved by careful design choices in pose parameterization, which leverages both 2D and 3D properties of depth map. Specifically, we decompose the pose parameters into a set of per-pixel estimations, i.e., 2D heat maps, 3D heat maps and unit 3D directional vector fields. The 2D/3D joint heat maps and 3D joint offsets are estimated via multitask network cascades, which is trained end-to-end. The pixel-wise estimations can be directly translated into a vote casting scheme. A variant of mean shift is then used to aggregate local votes while enforcing consensus between the the estimated 3D pose and the pixel-wise 2D and 3D estimations by design. Our method is efficient and highly accurate. On MSRA and NYU hand dataset, our method outperforms all previous state-of-the-art approaches by a large margin. On the ICVL hand dataset, our method achieves similar accuracy compared to the nearly saturated result obtained by <ref type="bibr" target="#b4">[5]</ref> and outperforms various other proposed methods. Code is available online 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision-based hand pose estimation has made significant progress in recent years. The increased performance can be attributed to two dominating trends: depth imaging and deep learning. First of all, hand pose estimation techniques have shifted almost entirely to using only depth inputs <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b51">52]</ref> since commodity depth sensors such as the MS Kinect and Intel Realsense have become widely available. As a 2.5D source of information, depth significantly resolves much of the ambiguities present in monocular RGB input. Secondly, deep learning has fundamentally transformed the way that vision problems are being solved. <ref type="bibr" target="#b0">1</ref> https://github.com/melonwan/denseReg The use of deep neural networks has become the norm for hand pose estimation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>In standard hand pose estimation pipelines, depth maps are almost always treated as images. This is especially true for deep learning-based approaches, which heavily rely on the machinery of (2D) convolutional neural networks (CNNs). One line of work for 3D hand poses estimation is holistic regression, that is aiming to directly map the depth images to 3D pose parameters such as joint angles or 3D coordinates. It bypasses having to solve for intermediate representations such as 2D coordinates and is able to capture global constraints and correlations among different joints. However, regressing from highly disparate domains such as image and pose is a very challenging learning task. Furthermore, holistic regression cannot generalize to combinations of local evidence such as different individual finger poses and suffers from translational variance and sensitivity to hand bounding box locations.</p><p>CNNs have been successfully applied to 2D body pose estimation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47]</ref>; in particular, fully convolutional networks (FCNs) can perform pixel-wise joint detection very accurately <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b46">47]</ref>. This is formulated as a pixel-wise classification of each pixel being the location of a joint. As such, a second line of work in pose estimation tries to create analogous networks for detecting joints in 2D. Through pixel-wise classification, joint detection can exploit local patterns more explicitly than holistic regression, helping the network to learn better feature maps. The 2D detections and 3D regression can then be combined with a multi-task setup <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42]</ref>, either by feeding the 2D detection heat map as an input to a 3D regression network, or by sharing the feature maps between detection and holistic regression. However, there is no guarantee that the regressed 3D joints, if they were to be projected back to 2D, will be in consensus with the original 2D detection heat-map. Moreover, by design, the aforementioned drawbacks of holistic regression are still not eliminated with this line of work. Other works in 2D detection apply inverse kinematics and use a model-based optimization; however, the severe selfocclusion of the hand creates ambiguities which are diffi-cult to resolve and as such, suffers from accuracy problems which are otherwise not present in body pose estimation.</p><p>Despite all the drawbacks of working in 2D, we do not want to directly solve a discrete volumetric detection problem with a 3D CNN. This becomes very parameter-heavy and as a result, severely limits the working resolution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b20">21]</ref>. Moreover, as input depth maps are only 2.5D, 3D CNNs struggle to resolve the ambiguities caused by the selfocclusion common in hand poses.</p><p>At the core of the problem is the mismatch between 2.5D depth data and traditional CNNs, be it in 2D or 3D. By treating depth maps as a 2D image, we can leverage the advances of CNNs, but we still under-utilize the information present. Yet we also want to avoid converting depth information to a volumetric representation due to the computational overhead and the associated ambiguities. To that end, we propose a combined pixel-wise detection and dense regression method for hand pose estimation. Our proposed method enjoys the benefits of 2D FCN-based detection such as translational-invariance and generalization to different finger gesture combinations. At the same time, dense regression allows us to make 3D estimates and benefit from the merits of holistic 3D regression, such as accounting for correlations and skeleton constraints, without having to work in the discrete volumetric domain.</p><p>We make two careful design choices in parameterization to stabilize our training and improve regression robustness. First, we work with offsets instead of absolute joint positions, i.e. we regress each pixel to a 3D offset of each joint. Joint offsets have been used in previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b45">46]</ref> and offer invariance towards translation. It also allows us to keep the original spatial resolution in spite of pooling operations in the CNN. Secondly, we reparameterize the 3D offsets as a heat map and directional unit vector, leading naturally to a joint detection and regression problem to solve for the two respectively. This form of parameterization leverages both the 2D and the 3D geometric properties of a 2.5D depth map. For a given depth map, we use a 2D CNN to capture local surface patterns but also treat the depth map as a set of 3D points to arrive at a final pose estimate in 3D.</p><p>To do so, we first extend the 2D detection heat map into 3D, i.e., value of the heat map is inversely proportional to the 3D distance of corresponding point on the depth map to a specific joint. In addition, we predict unit vector fields, where each vector field corresponds to the direction from the point on the depth map to a certain finger joint. Finally, we also detect the joints in 2D, in the form of a projected heat map. We aggregate all of the estimates together with the mean shift algorithm into a global estimate with consensus between the 2D and 3D estimates.</p><p>The proposed method is highly accurate and out performs all previous state-of-arts on three publicly available datasets, i.e., NYU <ref type="bibr" target="#b42">[43]</ref>, ICVL <ref type="bibr" target="#b38">[39]</ref> and MSRA <ref type="bibr" target="#b36">[37]</ref>. We also compare our method against several baselines that combine holistic regression with 2D joint detection. In these experiments we observe that, unlike in the case of full body pose estimation, those combination strategies can hardly improve holistic regression and are less accurate than our proposed method by a large margin. We attribute this to the depth ambiguity caused by self occlusion, towards which our proposed method is much more robust.</p><p>Our contribution can be summarized as follows:</p><p>• we formulate 3D hand pose estimation as a dense regression through a pose re-parameterization that can leverage both 2D surface geometric and 3D coordinate properties;</p><p>• we provide a non-parametric post-processing method aggregating pixel-wise estimates to 3D joint coordinates; this post-processing explicitly handles the holistic estimation and ensures consensus between the 2D and 3D estimates;</p><p>• we implement several baselines to investigate fusion strategies for holistic regression and 2D joint detection in a multi-task setup; such an analysis has never carried out before for hand pose estimation and provides valuable insights to the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Coupling 2D joint detection with 3D estimation 3D pose estimation based on 2D observations has a long history in computer vision. Early works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b33">34]</ref> are mainly based on low level visual cues, e.g., silhouette or optical flow, and use generative models to resolve the depth ambiguity. More recent works have shifted towoards mid-and high-level features, e.g. 2D joint detection heat maps or representations from CNNs, due to the availability of highly accurate 2D joint detectors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b46">47]</ref>. One line of work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42]</ref> formulates 3D pose estimation as a regression problem and couples 2D joint detection and 3D regression in a multi-task setup. Others <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b9">10]</ref> treat 3D estimation as an model-based optimization on top of the 2D joint detections. Our approach is similar to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42]</ref> in that both 2D and 3D estimations are performed in a multi-task setup. However, rather than using a holistic 3D regression, we perform pixel-wise 3D estimation. This type of fusion scheme is translation invariant and can better generalize to different combinations of finger gestures. Like many others, we also use a post-processing, but ours is much simpler with negligible effort when compared to the computationally expensive energy minimization of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b9">10]</ref>, nearest neighbour search <ref type="bibr" target="#b3">[4]</ref>, to employing an additional neural network <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>Pose Parameterization Skeleton models don't necessarily need to be parameterized with 3D joint coordinates. Many works have modelled pose parameters in other spaces to better exploit the skeleton structure. For example, <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b44">45]</ref> learn a latent space to model the correlation among different joints, while <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b35">36]</ref> parameterize pose hierarchically, i.e., location of child joint is dependent on its parent joint along the skeleton tree, to leverage dependencies in the skeleton. <ref type="bibr" target="#b21">[22]</ref> models skeleton as distance matrix among different joints and <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3]</ref> formulate pose parameters as heat maps together with offset vector fields to handle multiple instances 2D detection. Ours is inspired by <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3]</ref> whereas we work on 3D estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hand Pose Estimation</head><p>We limit our discussion to deep learning-based methods and refer the reader to <ref type="bibr" target="#b37">[38]</ref> for a detailed review of other model-based and random forest-based methods. Deep learning-based methods fall into two camps: two-stage approaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b50">51]</ref> with 2D joint detection followed model-based optimization versus single-stage approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref> of holistic pose regression. The current best-performing methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref> are all single stage, most likely due to the effective exploitation of joint correlations. Our method takes the advantages from both camps and well exploits the 2D and 3D properties of depth maps.</p><p>Offset Regression and Hough Voting Several previous works have successfully employed offset regression for localization and pose estimation tasks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b15">16]</ref>. Due to their local nature, they offer invariance to translation and their compatibility for bottom-up estimation. However, these methods typically rely on hand-crafted features, with the exception of works on 2D localization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b47">48]</ref>. In this work, we extend this idea by learning dense 3D offset regression end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We leverage both the 2D and 3D properties of a depth map to formulate hand pose estimation as a pixel-wise regression problem. From a 2D perspective, we treat the depth map as a 2D surface embedded in 3D and use a convolutional neural network(CNN) composed of 2D convolutional layers to capture surface local geometric patterns. From a 3D perspective, the depth map can also be regarded as a set of 3D points. It is for this set of points that we want to estimate offsets to the hand joints. More specifically, we use a CNN to estimate a dense vector field of offsets for each joint of hand. We re-parameterize the joint offset as a 3D heat map and a directional unit vector and solve for the two via detection and regression respectively (Sec 3.1). The resulting network is fully convolutional and compatible with current joint detection network architectures (Sec 3.2). Several networks can be stacked together as intermediate forms of supervision, with all the estimated results being fed into the next stage to boost pose accuracy. We adopt mean shift (Sec 3.3) to aggregate the pixel-wise regression estimates, while enforcing the 2D projections of the final estimated 3D joints to be in consensus with the pixel-wise 2D joint detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pose Parameterization</head><p>Instead of directly regressing 3D joint coordinates from the depth map, like most other regression-based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, we want to estimate an offset vector between depth points and hand joints. This makes the estimate translation-invariant and also generalizes better to different combinations of finger poses. However, directly regressing the 3D offset vector field is non-ideal. First of all, the regression for points that are far from a given hand joint will result in offset vectors with large norms that dominate the training loss. Furthermore, far away hand joints are beyond the scope of the receptive field of the convolutional filters anyway. As such, we decompose the 3D offset vector into two components -a 3D heat map S, estimated via detection, and a directional unit vector, V , via regression, as follows:</p><formula xml:id="formula_0">S j (p) = θ − p − p j 2 p − p j 2 ≤ θ, 0 otherwise; (1) V j (p) = p−pj p−pj 2 p − p j 2 ≤ θ, 0 otherwise.<label>(2)</label></formula><p>where p ∈ R 3 and p j ∈ R 3 are the 3D coordinates of a point from the depth map and of joint j respectively. θ defines the radius of a 3D ball centered at the joint position that establishes a candidate region(see <ref type="figure">Fig. 1</ref>) from which we consider support. The 3D heat map S j (p) can be regarded a direct extension of the 2D heat map.</p><p>In addition, we estimate the joint's 2D projection as a heatmap R,</p><formula xml:id="formula_1">R j (p) = τ − Π(p) − Π(p j ) 2 Π(p) − Π(p j ) 2 ≤ τ 0 otherwise ,<label>(3)</label></formula><p>where Π(·) denotes the 2D perspective projection function and τ is the radius of the candidate disk. Even though Eq. 1 and 2 are sufficient to recover the 3D joint location, the over-complete estimation with the 2D projection adds robustness to the local estimate. The 2D projection can be combined with the 3D joint estimate with non-parametric methods, which we elaborate in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network architecture</head><p>The architecture of the detection and regression network is shown in <ref type="figure">Fig 1.</ref> We use the hourglass network <ref type="bibr" target="#b22">[23]</ref> as the backbone because it is highly efficient, though any other joint detection network architecture, e.g. <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b27">28]</ref> could potentially be used. The 2D and 3D joint heat maps and the unit vector fields are estimated by network cascades in a learning multi-task manner. Specifically, for J joints, the network first outputs 2D and 3D joint heat maps with two separate sliding pixel-wise fully-connected layers on top of the output feature map of the hourglass module. Since the unit vector field V j is correlated with the heat map estimates, we concatenate the heat maps together with the hourglass output feature map to determine the unit vector field. To handle the discontinuity of 3D heat map and unit vector field regressions at surface edges, the initial depth map is also provided via concatenation to the input of the 3D heat map network. Similar to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref>, we multiply the binarized depth map as a mask with feature map and concatenate it with the initial feature map. This serves as the input for our unit vector field regression component.</p><p>Following the paradigm of <ref type="bibr" target="#b22">[23]</ref>, we stack together several modules with identical architectures to increase the learning power. Estimates from previous modules are used as inputs to the subsequent ones, while intermediate supervision is applied at the end of each module. Specifically, we define a L 2 loss over the J joints from T stacks as follows:</p><formula xml:id="formula_2">L = T t=1 L (t) R + L (t) S + L (t) V (4) = T t=1 J j=1 R (t) j − R * j 2 + S (t) j − S * j 2 + V (t) j − V * j 2 ,</formula><p>where R * j , S * j , V * j represent the respective ground-truth 2D heat maps, 3D heat maps and vector offsets of joint j and R t j , S t j , V t j are corresponding estimates from tth stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference</head><p>During inference, we aggregate all of the pixel-wise estimated evidences into holistic 3D joint coordinates with the mean shift algorithm. By design, this process explicitly ensures consensus between the joint detections in 2D and 3D. Since each joint is estimated with the same mean shift process, we omit the joint index j in this section for simplicity. As shown in Alg. 1, the N nearest points to the joint are selected based on the estimated 3D distance. We only select K because points with larger estimated 3D distances tend to amplify the estimation error of offset direction and thus degrade the recovered 3D joint position estimation.</p><p>In addition, we provide a more efficient "unweighted" approximation to Algorithm 1 without the 2D projection (step 4) and replace the weights with (1 + R) S 2 . <ref type="table">Table  1</ref> shows that both strategies have nearly identical results. In practice, we choose 5 nearest points as input to mean shift, 2 denotes element-wise multiplication Algorithm 1 Mean-shift estimation of one joint predefined constants: θ 3D distance threshold between point from D to joint K number of points selected as input to mean shift σ kernel width of mean shift kernel function N number of mean shift iterations Input: D ∈ R h×w×3 ∈ R input point cloud coordinates outputs from neural network:</p><formula xml:id="formula_3">R ∈ R h×w×1 2D heat map, see Eq. 3 S ∈ R h×w×1 3D heat map, see Eq. 1 V ∈ R h×w×3</formula><p>3D offset unit vector field, see Eq.  </p><formula xml:id="formula_4">p ← p i ,w i ∈(P,W ) K(p i −p)w i p i p i ,w i ∈{P,W } K(p i −p)w i K(x) = e</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>The network is implemented with Tensorflow[1] and optimized using the Adam <ref type="bibr" target="#b14">[15]</ref> with the initial learning rate set to 0.001 and the exponential decay rate of the momentum β 1 = 0.5. Following <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>, we randomly rotate the input depth map and change the aspect ratio for data augmentation. The batch size is set as 40 and we use batch re-normalization to accelerate training, which works better on small training mini-batches compared to batch normalization <ref type="bibr" target="#b13">[14]</ref>. During testing, we use two network stacks and have an average run time of 36ṁs per image (27.8ḞPS) on a single NVIDIA Titan X GPU card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments on 3 publicly available datasets, i.e. NYU <ref type="bibr" target="#b42">[43]</ref>, MSRA <ref type="bibr" target="#b36">[37]</ref> and ICVL <ref type="bibr" target="#b38">[39]</ref>. We choose the NYU dataset to conduct ablation experiments and compare against the baseline methods since it has a wider coverage of hand poses as opposed to the other two.</p><p>We quantitatively evaluate our method with two metrics: mean joint error (in mm) averaged over all joints and all frames, and percentage of frames in which all joints are below a certain threshold <ref type="bibr" target="#b40">[41]</ref>. Qualitative results of the estimated hand poses are shown in <ref type="figure" target="#fig_4">Fig. 5 and Fig. 8</ref>.</p><formula xml:id="formula_5">1 × 1 C R 3 × 3 R 3 × 3 C 7 × 7 stride=2 R 3 × 3 R 3 × 3 2 × 2 P Hourglass Module Element-wise add Element-wise mul Concatenate L (1) Element wise L (2)</formula><p>2D heat map 3D heat map unit vector field <ref type="figure">Figure 1</ref>. Network architecture. The abbreviations C, P, R stands for convolution layer, pooling and residual module respectively. We choose 128*128 as size of input depth map and 32*32 as the input and output resolution of hourglass module <ref type="bibr" target="#b22">[23]</ref> with 128 feature channels in each layer. In this paper, we use 2 stacks due to real-time performance constraint. The network estimate 2D,3D heat maps and unit vector field for each joint, we only show the pinky tip point here. Figure is best viewed in colour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baseline methods</head><p>In this section we analyze whether regression of 2D joint detections helps 3D regression and how different strategies to fuse 2D joint detections and 3D regressions impact the final pose accuracy. In addition, we also show the influence of choosing different 3D offset parameterizations.</p><p>Does 2D joint detection help with 3D regression? First, we would like to find out if 2D joint detection actually is helpful 3D holistic regression. To that end, we design two baseline methods: directly regressing 3D joint coordinates versus coupling 2D joint detection and 3D regression in a multi-task setup. Specifically, for baseline 1 (coordinate regression), the regression network follows the architecture from <ref type="figure" target="#fig_0">Fig. 2(a)</ref> which takes a depth map as input and directly outputs 3D joint coordinates. For baseline 2 (detec-tion+coordinate regression), we adopt a similar regression network architecture (see <ref type="figure" target="#fig_0">Fig. 2(b)</ref>) but add an hourglass module <ref type="bibr" target="#b22">[23]</ref>. We feed the depth map, the feature map from the hourglass module, and the 2D joint detection heat map all concatenated together as input into the brown module in <ref type="figure" target="#fig_0">Fig. 2</ref>,and train for regression. Furthermore, to ensure a fair comparison to our proposed method, we also stack two of such networks together for baseline 2.</p><p>As is shown in <ref type="figure">Fig. 3</ref>, there is only a minor improvement of 0.16mm in terms of the average joint error from direct coordinate regression to detection+coordinate regression. Furthermore, both methods perform similarly when the error threshold is larger than 25mm. We conclude that while 2D detection may help in learning a better feature map, coupling 2D detection together with 3D regression does not solve the inherent problems of 3D regression, e.g., translation variance and inability to generalize through combining local evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of fusion strategies</head><p>To further explore better strategies for fusion of 2D detection and 3D regression, we design an alternative method using the identical network architecture as detection+coordinate regression(see <ref type="figure" target="#fig_0">Fig. 2 (b)</ref>) except for the output layer. Instead of regressing (x, y, z) as per baseline 2, we regress only the z coordinate, and refer to this as baseline 3 (detection+depth regression). This output in the z axis is then combined with the 2D detection results which are used directly as the coordinates for x, y plane.</p><p>Surprisingly, detection+depth regression outperforms detection+coordinate regression both in terms of the average joint error and the percentage of frames below the error threshold from 20 to 50 mm (see <ref type="figure">Fig. 3</ref>). This suggests that 2D detection provides a more accurate estimate than coordinate regression. We conclude that it should be beneficial to explicitly enforce some form of consensus between the 3D estimates and the 2D detections. While the accuracy of this baseline is still lower than our proposed approach by a large margin (see <ref type="figure">Fig. 3</ref>), it shows that treating depth maps as 2D images and using CNNs for holistic depth regression is not enough to resolve the depth ambiguity in 3D hand pose estimation.</p><p>Besides fusing the 2D detection heat maps as input for coordinate regression, a second line of work <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b50">51]</ref> conducts model-based tracking based on inverse kinematics to recover the 3D pose. We compare against previous stateof-the-art methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b50">51]</ref> based on such a strategy and out-perform all of them (see Section 4.3). This validates the effectiveness of our proposed method in handling depth ambiguities arising from the severe self occlusions in the hand.</p><p>Impact of offset re-parameterization We implement a network which directly regresses the 3D offset without reparameterization into the 3D heatmap and directional unitvector as baseline 4 (mask loss)). As is shown in <ref type="figure" target="#fig_0">Fig. 2 (c)</ref>, the offset regression architecture follows exactly the same structure as the offset unit direction regression in <ref type="figure">Fig. 1</ref>. We use the 2D detection scores to select candidate points as inputs to the mean shift. In this baseline, we apply a 3D distance threshold to the loss function of the offset, as was done in <ref type="bibr" target="#b27">[28]</ref>, and effectively masks the regression so that we only regress a joint's neighbour points. Ideally, this baseline should be conducted without masking, but the training failed completely, with the loss oscillating back and forth without decreasing. As is shown in <ref type="figure">Fig. 3</ref>, pixel-wise dense estimation out-performs holistic regression method and validates the benefits of regressing point-wise 3D offsets.  <ref type="figure">Figure 3</ref>. Comparison with baselines. We compare our approach to four baseline methods (Sec. 4.1) on the NYU dataset <ref type="bibr" target="#b42">[43]</ref>. Number in the parenthesis of the legend indicates the average 3D error of the corresponding method.</p><formula xml:id="formula_6">1 × 1 C R 3 × 3 R 3 × 3 C 7 × 7 stride=2 R 3 × 3 R 3 × 3 2 × 2 P Hourglass Module Element-wise add Element-wise mul Concatenate R 3 × 3 2 × 2 P R 3 × 3 R 3 × 3 R 3 × 3</formula><p>Given the insights drawn from these baseline experiments, we attribute the high accuracy achieved by our method to the reparameterization and imposing the loss to all points for vector field regression. Decomposing the 3D offsets into the joint 3D heat map and offset direction and regressing the two in a cascaded way is easier to learn than directly regressing the offsets. Secondly, setting the offset vector to zero for outlier points instead of excluding them from the loss makes the estimation more robust to errors in regression during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Exploration studies</head><p>We first experiment on the number of stacked networks and the hyperparameters of mean-shift, i.e., the number of selected candidate points as input to the mean shift and the kernel width. As indicated in <ref type="table">Table 1</ref>, we find that the proposed method is quite robust to the mean shift hyperparameters. On the other hand, the number of network stacks is critical to the estimation accuracy. We test only up to 2 stacks to maintain real-time performance; however, as already shown in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b46">47]</ref>, adding more stacks could improve the accuracy.</p><p>In addition, as shown in the last two rows of <ref type="table">Table 1</ref>, the un-weighted mean shift approximation has a similar accuracy as the weighted version, with only 0.09mm difference with respect to the mean joint error. As such, we choose 2 stacks and 5 candidate points as input to mean shift, kernel width σ = 40mm and weighted mean shift as described in Alg. 1 in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to state-of-the-art NYU Dataset</head><p>The NYU hand dataset <ref type="bibr" target="#b42">[43]</ref> contains over 72K training and 8K testing frames. Its wide coverage of hand poses and noisy input depths make this dataset quite challenging. Since the hand region is not cropped out, we use an hourglass joint detector <ref type="bibr" target="#b22">[23]</ref>   <ref type="table">Table 1</ref>. Impact of hyperparameters. We report the mean 3D error (in mm) averaged over all joints and all frames on NYU dataset <ref type="bibr" target="#b42">[43]</ref>. We choose 2 stacks and 5 nearest points as input to mean shift, kernel width σ = 40mm and weighted mean shift as described in Alg. 1 as the default parameters.  <ref type="figure">Figure 4</ref>. Comparison with state-of-the-art on NYU <ref type="bibr" target="#b42">[43]</ref>. We plot the percentage of frames in which all joints are below a threshold.</p><p>the joints and take the median of estimated x and y coordinates over all joints respectively as the center point for cropping out the hand region. We only use view 1 for both training and testing and evaluate on a subset of 14 joints as in <ref type="bibr" target="#b42">[43]</ref> to make a fair comparison. We compare our method to the most recently proposed methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref>. All are 3D regression-based methods with sophisticated network architectures and surpass earlier works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b53">54]</ref> by a large margin. As is shown in <ref type="figure">Fig. 4</ref> and Tab. 2, our method outperforms all these state-of-the-art methods with a large margin for both metrics. Specifically, according to <ref type="figure">Fig. 4</ref>, our method significantly increases the percentage of successfully estimated frames by 8% (from 50% to 58%) on the error threshold of 20mm and by 9.2% (from 70% to 79.2%) on 30mm when compared to most accurate methods published to date ( <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref> respectively). We also show qualitative results on <ref type="figure" target="#fig_4">Fig. 5</ref>. The main reasons for the failure cases are severe self occlusions and noise in the depth map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average 3D error Xu et al. <ref type="bibr">[</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSRA Dataset</head><p>The MSRA hand dataset <ref type="bibr" target="#b36">[37]</ref> contains 76.5K images from 9 subjects with 17 hand gestures. Following the protocol of <ref type="bibr" target="#b36">[37]</ref>, we use a leave-one-subjectout training / testing split and average the results over the 9 subjects. We compare our methods with state-of-art methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref>. Specifically, <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b36">37]</ref> are based on the hierarchical regression forest. Similar to our approach, <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b36">37]</ref> regress 3D offsets and aggregate local estimations with the mean-shift algorithm. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref> are CNN based 3D holistic regression methods and outperforms other existing methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Again, our method outperforms all state-of-the-art by a large margin both in terms of percentage of successful frames (see <ref type="figure">Fig. 6</ref>) and average joint error (see Tab. 3). As is shown in <ref type="figure">Fig. 6</ref>, over 81% and 91% of frames have joint errors below 20mm and 30mm. This is a huge improvement over the most accurate existing results from <ref type="bibr" target="#b4">[5]</ref>, which has only 60% and 81% respectively. The qualitative results is shown in <ref type="figure" target="#fig_7">Fig. 8(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICVL Dataset</head><p>The ICVL hand dataset <ref type="bibr" target="#b38">[39]</ref> has 22K frames for training and 1.5k for testing. An additional 160k augmented frames with in-plane rotations are provided by  <ref type="figure">Figure 6</ref>. Comparison with state-of-the-art on MSRA <ref type="bibr" target="#b36">[37]</ref>. We plot the percentage of frames in which all joints are below a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average 3D error Ge et al. <ref type="bibr" target="#b10">[11]</ref> (3D CNN) 9.5mm Wan et al. <ref type="bibr" target="#b44">[45]</ref>  <ref type="table">(Crossing Nets)</ref> 12.2mm Oberweger et al. <ref type="bibr" target="#b23">[24]</ref> (DeepPrior++) 9.5mm Guo et al. <ref type="bibr" target="#b11">[12]</ref> (REN) 9.8mm Chen et al. <ref type="bibr" target="#b4">[5]</ref>  <ref type="table">(Pose Guided)</ref> 8.6mm Ours 7.2mm <ref type="table">Table 3</ref>. Comparison with state-of-art on MSRA <ref type="bibr" target="#b36">[37]</ref>. We plot the percentage of frames in which all joints are below a threshold.  <ref type="figure">Figure 7</ref>. Comparison with state-of-the-art on ICVL <ref type="bibr" target="#b38">[39]</ref>. We plot the percentage of frames in which all joints are below a threshold.</p><p>the dataset but we do not use them as we perform data augmentation on the fly during training as described in Sec. 3.4. The variance in pose is much smaller in ICVL compared to the NYU and MSRA datasets. We compare our method against <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref>. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b39">40]</ref> are based on hierarchical regression forest and others <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12</ref>, 5] on 3D holistic regression. As is shown in <ref type="figure">Fig. 7</ref>, our method achieves similar accuracy as <ref type="bibr" target="#b4">[5]</ref> and outperforms the rest. Our method has an average 3D error on par with <ref type="bibr" target="#b4">[5]</ref> and better than the others. We consider the differences between our method and <ref type="bibr" target="#b4">[5]</ref> as being less significant given the result is nearly saturated. The qualitative results can be seen in <ref type="figure" target="#fig_7">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average 3D error Wan et al. <ref type="bibr" target="#b44">[45]</ref>  <ref type="table">(Crossing Nets)</ref> 10.2mm Wan et al. <ref type="bibr" target="#b45">[46]</ref>  <ref type="table">(Surface Normal)</ref> 8.2mm Sun et al. <ref type="bibr" target="#b36">[37]</ref> (Cascaded Regression) 9.9mm Oberweger et al. <ref type="bibr" target="#b23">[24]</ref> (DeepPrior++) 8.1mm Guo et al. <ref type="bibr" target="#b11">[12]</ref> (REN) 7.5mm Chen et al. <ref type="bibr" target="#b4">[5]</ref> (Pose Guided) 6.8mm Ours 7.3mm  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and discussion</head><p>We propose a highly accurate method for 3D hand pose estimation from single depth map inputs. Given a depth camera frame, we decompose 3D pose parameters into a set of 2D/3D joint heat maps and 3D unit vector fields of offset directions. This reparameterization allows us to consider both the 2D and 3D properties of the depth map and makes it easy to leverage fully convolutional networks. We aggregate local estimations by a non-parametric mean shift variant, which explicitly enforces the estimated 3D joint coordinates to be in accordance with the 2D and 3D local estimations. Our method provides a better fusion scheme between 2D detection and 3D regression than previous stateof-the-art and the various baselines. As future work, we plan to further extend our method for 3D pose estimation from RGB inputs as well as for hands grasping objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 1:</head><label>2</label><figDesc>P = D + θ(1 − S) V recover the joint coordinate 2: I = topK(S) ∈ N K×2 select top K values' indices 3: P = P (I) ⊂ R 3 fetch estimated 3D joint coordinates 4: P 2d = {Π(p)|∀p ∈ P} ⊂ R 2 2D projection 5: W = R(P 2d ) ⊂ R fetch corresponding 2D heat map values as weights 6: p = init(W, P) ∈ R 3 Initialization 7: for n in N do 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>10 :</head><label>10</label><figDesc>Output: p i.e. K = 5 and the kernel width σ as 40mm based on ablative analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Baseline network architectures. (a) Direct 3D coordinate regression from depth map (baseline 1); (b) Network regresses 3D joint coordinates (baseline 2) or z-axis coupled 2D joint detection (baseline 3) together with the 2D joint detection heatmaps; (c) Regressing 3D offset vector field by masking the loss with the 3D distance to joint (baseline 4); (d) Detailed architecture configurations. The abbreviations C, P, R, FC stands for convolutional layer, pooling, residual module, and fully connected layer respectively. For (b) and (c), we experiment with a stack of 2 in the same way as the proposed method for fair comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>with all joints error within D Ge et. al.(CVPR 2017) Xu et. al.(IJCV 2017) Wan et. al.(CVPR 2017) Oberweger et. al.(ICCVW 2017) Guo et. al.(ICIP 2017) Chen et. al.(arXiv 2017) ours</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results. Hand pose estimation results on NYU dataset[43]. (a) Successful samples with largest joint error below 20mm; (b) Failed samples (top row) and the corresponding ground-truth(bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>with all joints error within D Ge et. al.(CVPR 2017) Sun et. al.(CVPR 2015) Wan et. al.(ECCV 2016) Wan et. al.(CVPR 2017) Oberweger et. al.(ICCVW 2017) Guo et. al.(ICIP 2017) Chen et. al.(arXiv 2017) ours</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>with all joints error within D Tang et. al.(ICCV 2015) Wan et. al.(CVPR 2017) Oberweger et. al.(ICCVW 2017) Guo et. al.(ICIP 2017) Chen et. al.(arXiv 2017) ours</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results. Hand pose estimation results from (a) ICVL[39], (b) MSRA[37].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-art on NYU. We report average 3D error on the NYU<ref type="bibr" target="#b42">[43]</ref> dataset.</figDesc><table><row><cell>49] (Lie-X) Wan et al. [45] (Crossing Nets) Oberweger et al.[24] (DeepPrior++) Guo et al.[12] (REN) Chen et al.[5] (Pose Guided) Ours</cell><cell>14.5mm 15.5mm 12.3mm 12.7mm 11.8mm 10.2mm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison with state-of-art on ICVL<ref type="bibr" target="#b38">[39]</ref> dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation = 2D Pose Estimation + Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pose guided structured region ensemble network for cascaded hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03416</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning hand articulations by hallucinating heat distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A collaborative filtering approach to Real-Time hand pose estimation: Supplementary material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">How to Refine 3D Hand Pose Estimation from Unlabelled Depth Data? In 3DV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dibra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust 3D hand pose estimation in single depth images: from singleview cnn to multi-view cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for efficient and robust hand pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Region ensemble network: Improving convolutional network for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03275</idno>
		<title level="m">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent 3D pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Occlusion aware hand pose recovery from sequences of depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carruesco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Andujar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzàlez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hough-cnn: Deep learning for segmentation of deep brain regions in mri and ultrasound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rozanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maiostre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ertl-Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bötzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Holistic planimetric prediction to local volumetric prediction for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04758</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepprior++: Improving fast and accurate 3D hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW, 2017. 3, 4</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06807</idno>
		<title level="m">Hands deep in deep learning for hand pose estimation</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training a feedback loop for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient model-based 3D tracking of hand articulations using kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kyriazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep multitask architecture for integrated 2D and 3D human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Realtime and robust hand tracking from depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yichen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiaoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient human pose estimation from single depth images. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic tracking of 3D human figures using 2D image motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Depth-Based hand pose estimation: Data, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3D articulated hand posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Opening the black box: Hierarchical sampling optimization for estimating human hand pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The vitruvian manifold: Inferring dense correspondences for one-shot human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Crossing nets: Combining GANs and VAEs with a shared latent space for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hand pose estimation from local surface normals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep voting: A robust approach toward nucleus localization in microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Lie-X: Depth image based articulated object pose estimation, tracking, and action recognition on lie groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Govindarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning probabilistic non-linear latent variable models for tracking complex activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spatial attention deep net with partial PSO for hierarchical hybrid hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Big-hand2. 2m benchmark: Hand pose dataset and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards 3D human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Modelbased deep hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
