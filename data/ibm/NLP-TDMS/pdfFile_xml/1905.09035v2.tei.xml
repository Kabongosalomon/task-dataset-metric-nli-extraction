<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Would You Expect? Anticipating Egocentric Actions With Rolling-Unrolling LSTMs and Modality Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
							<email>furnari@dmi.unict.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Catania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
							<email>gfarinella@dmi.unict.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Catania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What Would You Expect? Anticipating Egocentric Actions With Rolling-Unrolling LSTMs and Modality Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Egocentric action anticipation consists in understanding which objects the camera wearer will interact with in the near future and which actions they will perform. We tackle the problem proposing an architecture able to anticipate actions at multiple temporal scales using two LSTMs to 1) summarize the past, and 2) formulate predictions about the future. The input video is processed considering three complimentary modalities: appearance (RGB), motion (optical flow) and objects (object-based features). Modalityspecific predictions are fused using a novel Modality AT-Tention (MATT) mechanism which learns to weigh modalities in an adaptive fashion. Extensive evaluations on two large-scale benchmark datasets show that our method outperforms prior art by up to +7% on the challenging EPIC-Kitchens dataset including more than 2500 actions, and generalizes to EGTEA Gaze+. Our approach is also shown to generalize to the tasks of early action recognition and action recognition. Our method is ranked first in the public leaderboard of the EPIC-Kitchens egocentric action anticipation challenge 2019. Please see the project web page for code and additional details: http://iplab.dmi.unict.it/rulstm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anticipating the near future is a natural task for humans and a fundamental one for intelligent systems when it is necessary to react before an action is completed (e.g., to anticipate a pedestrian crossing the street from an autonomous vehicle <ref type="bibr" target="#b9">[9]</ref>) or even before it starts (e.g., to notify a user who is performing the wrong action in a known workflow <ref type="bibr" target="#b53">[53]</ref>). Additionally, tasks such as action anticipation <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b58">58]</ref> and early action recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b39">39</ref>] pose a series of key challenges from a computational perspective. Indeed, methods addressing these tasks need to model the relationships between past, future events and incomplete observations. First Person (Egocentric) Vision <ref type="bibr" target="#b29">[29]</ref>, in particular, offers an interesting scenario to study anticipation problems. On one hand, whilst being a natural task for humans, anticipating  <ref type="figure">Figure 1</ref>. Egocentric Action Anticipation. See text for notation.</p><p>the future from egocentric video is computationally challenging due to the ability of wearable cameras to acquire long videos of complex activities involving many objects and actions performed by a user from their unique point of view. On the other hand, investigating these tasks is fundamental for the construction of intelligent wearable systems able to anticipate the user's goal and assist them <ref type="bibr" target="#b29">[29]</ref>. In this paper, we address the problem of egocentric action anticipation. As defined in <ref type="bibr" target="#b7">[8]</ref> and illustrated in <ref type="figure">Figure 1</ref>, the task consists in recognizing an action starting at time τ s by observing a video segment preceding the action starting at time τ s − (τ o + τ a ) and ending at time τ s − τ a , where the "observation time" τ o indicates the length of the observed segment, whereas the "anticipation time" τ a denotes how many seconds in advance actions are to be anticipated. While action anticipation has been investigated in classic third person vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b28">28]</ref>, less attention has been devoted to the egocentric scenario <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b47">47]</ref>.</p><p>We observe that egocentric action anticipation methods need to address two sub-tasks: 1) summarizing what has been observed in the past (e.g., "a container has been washed" in the observed segment in <ref type="figure">Figure 1</ref>), and 2) making hypotheses about what will happen in the future (e.g., "put-down container", "close tap", "take spoon" in <ref type="bibr">Figure 1)</ref>. While previous approaches attempted to address these two sub-tasks jointly <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b58">58]</ref>, our method disentangles them using two separate LSTMs. The "Rolling" LSTM (R-LSTM) is responsible for continuously encoding streaming observations to summarize the past. When the method is required to anticipate actions, the "Unrolling" LSTM (U-LSTM) takes over the current hidden and cell states of the R-LSTM and makes predictions about the future. Differently from previous works which considered a fixed anticipation time <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b58">58]</ref>, the proposed method can anticipate an action at multiple anticipation times, e.g., 2s, 1.5s, 1s and 0.5s before it occurs. The network is pretrained with a novel "Sequence Completion Pre-training" (SCP) technique, which encourages the disentanglement of the two sub-tasks. To take advantage of the complementary nature of different input modalities, the proposed Rolling-Unrolling LSTM (RU) processes spatial observations (RGB frames), motion (optical flow), as well as object-based features. Multimodal predictions are fused with a novel "Modality ATTention" mechanism (MATT), which adaptively estimates optimal fusion weights for each modality by considering the outputs of the modality-specific R-LSTM components. Experiments on two large-scale datasets of egocentric videos, EPIC-KTICHENS <ref type="bibr" target="#b7">[8]</ref> and EGTEA Gaze+ <ref type="bibr" target="#b36">[36]</ref>, show that the proposed method outperforms several state-of-the-art approaches and baselines in the task of egocentric action anticipation and generalizes to the tasks of early action recognition and action recognition.</p><p>The contributions of this work are as follows: 1) we are the first to systematically investigate the problem of egocentric action anticipation within the framework of the challenge proposed in <ref type="bibr" target="#b7">[8]</ref>; 2) our investigation benchmarks popular ideas and approaches to action anticipation and leads to the definition of RU, an architecture able to anticipate egocentric actions at multiple temporal scales; 3) the proposed model is shown to benefit from two techniques specific to the investigated problem, i.e., i) "Sequence Completion Pretraining" and ii) adaptive fusion of multi-modal predictions through Modality ATTention; 4) extensive evaluations highlight the limits of previous approaches and show significant improvements of the proposed method over the state of the art. To support future research, the code implementing the proposed method will be released upon publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action Recognition Our work is related to previous research on action recognition from third person vision <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b67">67]</ref> and first person vision <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b56">56]</ref>. Specifically, we build on previous ideas investigated in the context of action recognition such as the use of multiple modali-ties for video analysis <ref type="bibr" target="#b49">[49]</ref>, the use of Temporal Segment Networks <ref type="bibr" target="#b61">[61]</ref> as a principled way to train CNNs for action recognition, as well as the explicit encoding of object-based features <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b56">56]</ref> to analyze egocentric video. However, in contrast with the aforementioned works, we address the problem of egocentric action anticipation and show that approaches designed for action recognition, such as TSN <ref type="bibr" target="#b61">[61]</ref> and early/late fusion to merge multi-modal predictions <ref type="bibr" target="#b49">[49]</ref> are not directly applicable to the problem of egocentric action anticipation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Early Action Recognition in Third Person Vision</head><p>Early action recognition consists in recognizing an ongoing action as early as possible from partial observations <ref type="bibr" target="#b9">[9]</ref>. The problem has been widely investigated in the domain of third person vision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b46">46]</ref>. Differently from these works, we address the task of anticipating actions from egocentric video, i.e., the action should be recognized before it starts, hence it cannot even be partially observed at the time of prediction. Given the similarity between early recognition and anticipation, we consider and evaluate some ideas investigated in the context of early action recognition, such as the use of LSTMs to process streaming video <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b39">39]</ref>, and the use of dedicated loss functions <ref type="bibr" target="#b39">[39]</ref>. Moreover, we show that the proposed architecture also generalizes to the problem of early egocentric action recognition, achieving state-of-the-art performances.</p><p>Action Anticipation in Third Person Vision Action anticipation is the task of predicting an action before it occurs <ref type="bibr" target="#b18">[18]</ref>. Previous works investigated different forms of action and activity anticipation from third person video <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b64">64]</ref>. While we consider the problem of action anticipation from egocentric visual data, our work builds on some ideas explored in past works such as the use of LSTMs to anticipate actions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b28">28]</ref>, the use of the encoder-decoder framework to encode past observations and produce hypotheses of future actions <ref type="bibr" target="#b18">[18]</ref>, and the use of object specific features <ref type="bibr" target="#b40">[40]</ref> to determine which objects are present in the scene. Additionally, we show that other approaches, such as the direct regression of future representations <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b58">58]</ref>, do not achieve satisfactory performance in the considered scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anticipation in First Person Vision</head><p>Past works on anticipation from egocentric visual data have investigated different problems and considered different evaluation frameworks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b68">68]</ref>. Instead, we tackle the egocentric action anticipation challenge recently proposed in <ref type="bibr" target="#b7">[8]</ref>, which has been little investigated so far <ref type="bibr" target="#b16">[16]</ref>. While a direct comparison of the proposed approach with the aforementioned works is unfeasible due to the lack of a common framework, our method incorporates some ideas from past approaches, such as the analysis of past actions <ref type="bibr" target="#b47">[47]</ref> and the detection of the objects present in the scene to infer future actions <ref type="bibr" target="#b17">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>Processing Strategy <ref type="figure">Figure 2</ref> illustrates the processing strategy adopted by the proposed method. The video is processed in an on-line fashion, with a short video snippet V t consumed every α seconds, where t indexes the current time-step. Specifically, an action occurring at time τ s is anticipated by processing a video segment of length l starting at time τ s − l − α and ending at time τ s − α. The input video ends at time τ s − α as our method aims at anticipating actions at least α seconds before they occur. The processing is performed in two stages: an "encoding" stage, which is carried out for S enc time-steps, and an "anticipation" stage, which is carried out for S ant time-steps. In the encoding stage, the model summarizes the semantic content of the S enc input video snippets without producing any prediction, whereas in the anticipation stage the model continues to encode the semantics of the S ant input video snippets and outputs S ant action scores s t which can be used to perform action anticipation. This scheme effectively allows to formulate S ant predictions for a single action at multiple anticipation times. In our experiments, we set α = 0.25s, S enc = 6 and S ant = 8. In these settings, the model analyzes video segments of length l = α(S enc + S ant ) = 3.5s and outputs 8 predictions at the following anticipation times: τ a ∈ {2s, 1.75s, 1.5s, 1.25s, 1s, 0.75s, 0.5s, 0.25s}. It should be noted that, since the predictions are produced while processing the video, at time step t the effective observation time will be equal to α·t. Hence, the 8 predictions are performed at the following effective observation times: τ o ∈ {1.75s, 2s, 2.25s, 2.5s, 2.75s, 3s, 3.25s, 3.5s}. Our formulation generalizes the one proposed in <ref type="bibr" target="#b7">[8]</ref>, which is illustrated in <ref type="figure">Figure 1</ref>. For instance, at time-step t = 11, our model will anticipate actions with an effective observation time equal to τ o = α · t = 2.75s and an anticipation time equal to τ a = α(S ant + S enc + 1 − t) = 1s, as in <ref type="bibr" target="#b7">[8]</ref>.</p><p>Rolling-Unrolling LSTM The proposed method uses two separate LSTMs to encode past observations and formulate predictions about the future. Following previous literature <ref type="bibr" target="#b49">[49]</ref>, we include multiple identical branches which  analyze the video according to different modalities. Specifically, at each time-step t, the input video snippet V t is represented using different modality-specific representation functions ϕ 1 , . . . , ϕ M depending on learnable parameters θ ϕ1 , . . . , θ ϕ M . This process allows to obtain the modalityspecific feature vectors</p><formula xml:id="formula_0">f 1,t = ϕ 1 (V t ), . . . , f M,t = ϕ M (V t ),</formula><p>where M is the total number of modalities (i.e., the total number of branches in our architecture), and f m,t is the feature vector computed at time-step t for the modality m. The feature vector f m,t is fed to the m th branch of the architecture. While our model can easily incorporate different modalities, in this work we consider M = 3 modalities, i.e., RGB frames (spatial branch), optical flow (motion branch) and object-based features (object branch). <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the processing taking place in a single branch m of the proposed RU model. For illustration purposes only, the figure shows an example in which S enc = 1 and S ant = 3. At time step t, the feature vector f m,t is fed to the Rolling LSTM (R-LSTM), which encodes its semantic content recursively, as follows:</p><formula xml:id="formula_1">(h R m,t , c R m,t ) = LST M θ R m (fm,t, h R m,t−1 , c R m,t−1 )<label>(1)</label></formula><p>where LST M θ R m denotes the R-LSTM of branch m, depending on the learnable parameters θ R m , whereas h R m,t and c R m,t are the hidden and cell states computed at time t in the modality m. The initial hidden and cell states of the R-LSTM are initialized with zeros: h R m,0 = 0, c R m,0 = 0. In the anticipation stage, at time step t, the Unrolling LSTM (U-LSTM) is used to make predictions about the future. The U-LSTM takes over the hidden and cell vectors of the R-LSTM at the current time-step (i.e., h R m,t and c R m,t ) and iterates over the representation of the current video snippet f m,t for a number of times n t equal to the number of time-steps required to reach the beginning of the action, i.e., n t = S ant + S enc − t + 1. Hidden and cell states of the U-LSTM are computed as follows at the j th iteration:</p><formula xml:id="formula_2">(h U m,j , c U m,j ) = LST M θ U m (fm,t, h U m,j−1 , c U m,j−1 )<label>(2)</label></formula><p>where LST M θ U m is the U-LSTM of branch m, depending on the learnable parameters θ U m , and h U m,t , c U m,t are the hidden and cell states computed at iteration j for the modality  m. The initial hidden and cell states of the U-LSTM are initialized from the current hidden and cell states computed by the R-LSTM:</p><formula xml:id="formula_3">h U m,0 = h R m,t , c U m,0 = c R m,t .</formula><p>Note that the input f m,t of the U-LSTM does not depend on j (see eq. (2)) because it is fixed during the "unrolling" procedure. The main rationale of "unrolling" the U-LSTM for a different number of times at each time-step is to encourage it to differentiate predictions at different anticipation times.</p><p>Modality-specific action scores s m,t are computed at time-step t by processing the last hidden vector of the U-LSTM with a linear transformation with learnable parame-</p><formula xml:id="formula_4">ters θ W m and θ b m : s m,t = θ W m h U m,nt + θ b m .</formula><p>Sequence Completion Pre-Training (SCP) The two LSTMs composing the RU architecture are designed to address two specific sub-tasks: the R-LSTM is responsible for encoding past observations and summarizing what has happened up to a given time-step, whereas the U-LSTM focuses on anticipating future actions conditioned on the hidden and cell vectors of the R-LSTM. To encourage the two LSTMs to specialize on the two different sub-tasks, we propose to train the architecture using a novel Sequence Completion Pre-training (SCP) procedure. During SCP, the connections of the U-LSTM are modified to allow it to process future representations, rather than iterating on the current one. In practice, the U-LSTM hidden and cell states are computed as follows during SCP:</p><formula xml:id="formula_5">(h U m,j , c U m,j ) = LST M θ U m (fm,t+j−1, h U m,j−1 , c U m,j−1 ) (3)</formula><p>where the input representations f m,t+j−1 are sampled from future time-steps t+j −1. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates an example of the connection scheme used during SCP for time-step t = 2.</p><p>The main goal of pre-training the RU with SCP is to allow the R-LSTM to focus on summarizing past representations without trying to anticipate the future.</p><p>Modality ATTention (MATT) Coherently with past work on egocentric action anticipation <ref type="bibr" target="#b7">[8]</ref>, we found it suboptimal to fuse multi-modal predictions with classic approaches such as early and late fusion. This is probably due to the fact that, when anticipating egocentric actions, <ref type="figure">Figure 5</ref>. Example of the complete RU architecture with two modalities and the Modality ATTention mechanism (MATT).</p><formula xml:id="formula_6">R-LSTM U-LSTM U-LSTM U-LSTM 1,1 ( = 0.75 ) 1 R-LSTM U-LSTM U-LSTM U-LSTM 2,1 ( = 0.75 ) × × + 1 ( = 0.75 ) 1,1 2,1 S Modality Attention Network (MATT) 1, 2, S SoftMax S = 1 L L Message passing Linear transformation L Input video snippets R-LSTM U-LSTM U-LSTM 1,2 ( = 0.5 ) 2 R-LSTM U-LSTM U-LSTM 2,2 ( = 0.5 ) × × + 2 ( = 0.5 ) 1,2 2,2 S = 2 L L R-LSTM U-LSTM 1,3 ( = 0.25 ) 3 R-LSTM U-LSTM 2,3 ( = 0.25 ) × × + 3 ( = 0.25 ) 1,3 2,3 S = 3 L L</formula><p>one modality might be more useful than another (e.g., appearance over motion), depending on the processed sample. Inspired by previous work on attention <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b62">62]</ref> and multi-modal fusion <ref type="bibr" target="#b41">[41]</ref>, we introduce a Modality ATTention (MATT) module which computes a set of attention scores indicating the relative importance of each modality for the final prediction. At a given time-step t, such scores are obtained by processing the concatenation of the hidden and cell vectors of the R-LSTM networks belonging to all branches m = 1, . . . , M with a deep neural network D depending on the learnable parameters θ M AT T :</p><formula xml:id="formula_7">λt = D θ M AT T (⊕ M m=1 (h R m,t ⊕ c R m,t ))<label>(4)</label></formula><p>where ⊕ denotes the concatenation operator and</p><formula xml:id="formula_8">⊕ M m=1 (h R m,t ⊕ c U m,t )</formula><p>is the concatenation of the hidden and cell vectors produced by the R-LSTM at time-step t across all modalities. Late fusion weights can be obtained normalizing the score vector λ t with the softmax function in order to make sure that fusion weights sum to one: w m,t = exp(λt,m) k exp(λ t,k ) , where λ t,m is the m th component of the score vector λ t . The final set of fusion weights is obtained at time-step t by merging the modality-specific predictions produced by the different branches with a linear combination as follows: s t = m w m,t · s m,t . <ref type="figure">Figure 5</ref> illustrates an example of a complete RU with two modalities and the MATT fusion mechanism. For illustration purposes, the figure shows only three anticipation steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Branches and Representation Functions</head><p>We instantiate the proposed architecture with 3 branches: a spatial branch which processes RGB frames, a motion branch which processes optical flow, and an object branch which processes object-based features. Our architecture analyzes video snippets of 5 frames V t = {I t,1 , I t,2 , . . . , I t,5 }, where I t,i is the i th frame of the video snippet. The representation function ϕ 1 of the spatial branch computes the feature vector f 1,t by extracting features from the last frame I t,5 of the video snippet using a Batch Normalized Inception CNN <ref type="bibr" target="#b26">[26]</ref> trained for action recognition. The representation function ϕ 2 of the motion branch extracts optical flow from the 5 frames of the current video snippet as proposed in <ref type="bibr" target="#b61">[61]</ref>. The computed x and y optical flow form a tensor with 10 channels, which is fed to a Batch Normalized Inception CNN trained for action recognition to obtain the feature vector f 2,t . Note that ϕ 1 and ϕ 2 allow to obtain "action-centric" representations of the input frame which can be used by the R-LSTM to summarize what has happened in the past. The representation function ϕ 3 of the object branch extracts objects from the last frame I t,5 of the input snippet V t using an object detector. A fixed-length representation f 3,t is obtained by accumulating the confidence scores of all bounding boxes predicted for each object class. Specifically, let b t,i be the i th bounding box detected in image I t,5 , let b c t,i be its class and let b s t,i be its detection confidence score. The j th component of the output representation vector f 3,t is obtained by summing the confidence scores of all detected objects of class j, i.e.,</p><formula xml:id="formula_9">f 3,t,j = i [b c t,i = j]b s t,i , where [·]</formula><p>denotes the Iverson bracket. This representation only encodes the presence of an object in the scene, discarding its position in the frame, similarly to the representation proposed in <ref type="bibr" target="#b44">[44]</ref> for egocentric activity recognition. We found this holistic representation to be sufficient in the case of egocentric action anticipation. Differently from ϕ 1 and ϕ 2 , ϕ 3 produces object-centric features, which carry information on what objects are present in the scene and hence could be interacted next. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Settings</head><p>Datasets We perform experiments on two large-scale datasets of egocentric videos: EPIC-Kitchens <ref type="bibr" target="#b7">[8]</ref> and EGTEA Gaze+ <ref type="bibr" target="#b36">[36]</ref>. EPIC-Kitchens contains 39, 596 action annotations, 125 verbs, and 352 nouns. We split the public training set of EPIC-Kitchens (28, 472 action segments) into training (23, 493 segments) and validation (4, 979 segments) sets by randomly choosing 232 videos for training and 40 videos for validation. We considered all unique (verb, noun) pairs in the public training set, thus obtaining 2, 513 unique actions. EGTEA Gaze+ contains 10, 325 action annotations, 19 verbs, 51 nouns and 106 unique actions. Methods are evaluated on EGTEA Gaze+ reporting the average performance across the three splits provided by the authors of the dataset <ref type="bibr" target="#b36">[36]</ref>.</p><p>Evaluation Measures Methods are evaluated using Top-k evaluation measures, i.e., we deem a prediction correct if the ground truth action falls in the top-k predictions. As observed in previous works <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b32">32]</ref>, this evaluation scheme is appropriate given the uncertainty of future predictions (i.e., many plausible actions can follow an observation). Specifically, we use the Top-5 accuracy as a class-agnostic mea- To assess the timeliness of anticipations, we design a new evaluation measure inspired by the AMOC curve <ref type="bibr" target="#b23">[23]</ref>. Let s t be the score anticipated at time-step t for an action of ground truth class c, let τ t be the anticipation time at timestep t, and let tk(s t ) be the set of top-k actions as ranked by the scores s t . We define as "time to action" at rank k the largest anticipation time (i.e., the time of earliest anticipation) in which a correct prediction has been made according to the top-k criterion: T tA(k) = max{τ (s t )|c ∈ tk(s t ), ∀t}. If an action is not correctly anticipated in any of the time-steps, we set T tA(k) = 0. The mean time to action over the whole test set mT tA(k) indicates how early, in average, a method can anticipate actions.</p><p>Performances are evaluated for verb, noun and action predictions on the EPIC-Kitchens dataset. We obtain verb and noun scores by marginalization over action scores for all methods except the one proposed in <ref type="bibr" target="#b7">[8]</ref>, which predicts verb and noun scores directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Methods</head><p>We compare the proposed method with respect to 7 state-of-the approaches and baselines. Specifically, we consider the Deep Multimodal Regressor (DMR) proposed in <ref type="bibr" target="#b58">[58]</ref>, the Anticipation Temporal Segment Network (ATSN) of <ref type="bibr" target="#b7">[8]</ref>, the anticipation Temporal Segment Network trained with verb-noun Marginal Cross Entropy Loss (MCE) described in <ref type="bibr" target="#b16">[16]</ref>, and the Encoder-Decoder LSTM (ED) introduced in <ref type="bibr" target="#b18">[18]</ref>. We also consider baselines obtained adapting approaches proposed for early action recognition to the problem of egocentric action anticipation: the Feedback Network LSTM (FN) proposed in <ref type="bibr" target="#b19">[19]</ref>, a single LSTM architecture (we use the same parameters of our R-LSTM) trained using the Ranking Loss on Detection Score proposed in <ref type="bibr" target="#b39">[39]</ref> (RL), and an LSTM trained using the Exponential Anticipation Loss proposed in <ref type="bibr" target="#b28">[28]</ref> (EL). These baselines adopt the video processing scheme illustrated in <ref type="figure">Figure 2</ref> and are implemented as two stream networks with a spatial and a temporal branch whose predictions are fused by late fusion. <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Anticipation Results on EPIC-KITCHES <ref type="table" target="#tab_1">Table 1</ref> compares RU with respect to the state-of-the-art on our EPIC- The proposed approach outperforms all competitors by consistent margins according to all evaluation measures, reporting an average improvement over prior art of about 5% on Top-5 action accuracy with respect to all anticipation times. Note that this margin is significant given the large number of 2, 513 action classes present in the dataset. Methods based on TSN (ATSN and MCE) generally achieve low performance, which suggests that simply adapting action recognition methods to the problem of anticipation is insufficient. Interestingly, DMR and ED, which are explicitly trained to anticipate future representations, achieve sub-optimal Top-5 action accuracy as compared to methods trained to predict future actions directly from input images (e.g., compare DMR with MCE, and ED with FN/RL/EL). This might be due to the fact that anticipating future representations is particularly challenging in the case of egocentric video, where the visual content changes continuously because of the mobility of the camera. RL consistently achieves second best results with respect to all anticipation times, except τ a = 0.25, where it is outperformed by EL. RU is particularly strong on nouns, achieving a Top-5 noun accuracy of 51.79% and a mean Top-5 noun recall of 49.90%, which improves over prior art by +7.26% and +7.31%. The small drop in performance when passing from class-agnostic measures to class-aware measures (i.e., 51.79% to 49.90%) suggests that our method does not over-fit to the distribution of nouns of the training set. It is worth noting that mean Top-5 Recall values are averaged over fairly large sets of 26 many-shot verbs, 71 many-shot nouns, and 819 many-shot actions, as specified in <ref type="bibr" target="#b7">[8]</ref>. Differently, all compared methods exhibit large drops in verb and action performance when passing from class-agnostic to class-aware measures. Our insight into this different pattern is that anticipating the next object which will be used (i.e., anticipating nouns) is much less ambiguous than anticipating the way in which the object will be used (i.e., anticipating verbs and actions). It is worth noting that second best Top-5 verb and noun accuracy are achieved by different methods (i.e., ATSN in the case of verbs and RL in the case of nouns), while both are outperformed by the proposed RU. Despite its low performance with respect to class-agnostic measures, ED systematically achieves second best results with respect to mean Top-5 recall and mean T tA <ref type="bibr" target="#b4">(5)</ref>. This highlights that there is no clear second-best performing method. Finally, mean T tA(k) highlights that the proposed method can anticipate verbs, nouns and actions 1.62, 1.11 and 0.76 seconds in advance respectively. <ref type="table">Table 8</ref> assesses the performance of the proposed method on the official test sets of EPIC-Kitchens dataset. <ref type="bibr" target="#b2">3</ref> RU outperforms all competitors by consistent margins on both the "seen" test, which includes scenes appearing in the training set (S1) and the "unseen" test set, with scenes not appearing in the training set (S2). Also in this case, RU is strong on nouns, obtaining +6.56% and +8.8% in S1, as well as +4.78% and +4.88 in S2. Improvements in terms of actions are also significant: +3.63% and +5.52% in S1, as well as +2.59% and +5.39% on S2.</p><p>Anticipation Results on EGTEA Gaze+ <ref type="table">Table 3</ref> reports Top-5 action accuracy scores achieved by the compared methods on EGTEA Gaze+ with respect to the 8 considered anticipation times. The table also reports mean T tA <ref type="bibr" target="#b4">(5)</ref> action scores, denoted as T tA. The proposed method outperforms all competitors at all anticipation times. Note that the margins of improvement are smaller on EGTEA Gaze+  <ref type="table">Table 3</ref>. Anticipation results on EGTEA Gaze+.</p><p>due to its smaller-scale (106 actions in vs 2, 513 actions in EPIC-KITCHENS). Differently from <ref type="table" target="#tab_1">Table 1</ref>, EL systematically achieves second best performance on EGTEA Gaze+, which highlights again that there is no clear second best competitor to RU in our evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study on EPIC-Kitchens</head><p>To assess the role of rolling-unrolling, we consider a strong baseline composed of a single LSTM (same configuration as R-LSTM) and three branches (RGB, Flow, OBJ) with late fusion (BL). To tear apart the influence of rolling-unrolling and MATT, Table 4(a) compares this baseline with the proposed RU architecture, where MATT has been replaced with late fusion. The proposed RU approach brings systematic improvements over the baseline for all anticipation times, with larger improvements in the case of the object branch. <ref type="table">Table 4</ref>(b) reports the performances of the single branches of RU and compares MATT with respect to late fusion (i.e., averaging predictions) and early fusion (i.e., feeding the model with the concatenation of the modalityspecific representations). MATT always outperforms late fusion, which consistently achieves second best results. Early fusion always leads to sub-optimal results. All fusion schemes always improve over the single branches. <ref type="figure">Figure 6</ref> shows regression plots of modality attention weights computed on all samples of the validation set. RGB and OBJ weights are characterized by a strong and steep correlation. A similar pattern applies to Flow and OBJ weights, whereas Flow and RGB weights are characterized by a small positive correlation. This suggests that MATT gives more credit to OBJ when RGB and Flow are less informative, whereas the it relies on RGB and Flow when objects are not necessary. <ref type="table">Table 4</ref>(c) assesses the role of pre-training the proposed architecture with sequence completion. As can be noted, the proposed pre-training procedure brings small but consistent improvements for most anticipation times. <ref type="table">Table 4</ref>(d) compares RU with the strong baseline of Table 4(a). The comparison highlights the cumulative effect of all the proposed procedures/component with respect to a strong baseline using three modalities. It is worth noting that the proposed architecture brings improvements for all anticipation times, ranging from +1.53% to +4.08%. <ref type="figure" target="#fig_6">Figure 7</ref> reports two examples of anticipations made by the proposed method at four anticipa-  <ref type="table">Table 4</ref>. Ablation study on EPIC-KITCHENS. <ref type="figure">Figure 6</ref>. Correlations between modality attention weights tion times. Under each frame we report top-4 predictions, whereas modality weights computed by MATT are reported in percentage on the right. Green bounding boxes are shown around the detected objects and the optical flow is illustrated in orange. In the first example (top), the model can predict "close door" based on context and past actions (e.g., taking objects out of the cupboard), hence it assigns large weights to RGB and Flow and low weights to OBJ. In the second example (bottom), the model initially predicts "squeeze lime" at τ a = 2s. Later, the prediction is corrected to "squeeze lemon" as soon as the lemon can be reliably detected. Note that in this case the network assigns larger weights to OBJ as compared to the previous example. <ref type="bibr" target="#b3">4</ref> Early Action Recognition We also observe that the proposed method generalizes to the task of early action recognition. We adapt our processing scheme by sampling 8 video snippets from each action segment uniformly in time and set S enc = 0, S ant = 8. The snippets are fed to the model, which produces predictions at each time-step, corresponding to the following observation rates: 12.5%, 25%, 37.5%, 50%, 62.5%, 75%, 87.5%, 100%. Branches are fused by late fusion in this case. We compare the proposed method with respect to FN, EL and RL.    <ref type="table">Table 6</ref>. Early recognition results on EGTEA Gaze+. <ref type="table" target="#tab_6">Table 5</ref> reports Top-1 early action recognition accuracy results obtained by the compared methods on our validation set of EPIC-Kitchens. The proposed method consistently outperforms the competitors at all observation rates. Interestingly, RU achieves an early action recognition accuracy of 33.09% when observing only 75% of the action, which is already comparable to the accuracy of 34.07% achieved when the full action is observed. This indicates that RU can timely recognize actions before they are completed. RL achieves second best results up to observation rate 75%, whereas EL achieves second best results when more than 75% of the action is observed, which confirms the lack of a clear second-best performer. <ref type="table">Table 6</ref> reports Top-1 accuracy results obtained on EGTEA Gaze+. The proposed RU outperforms the competitors for all observation rates by small but consistent margins. Coherently with <ref type="table" target="#tab_6">Table 5</ref>, second best results are obtained by RL and EL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Egocentric Action Recognition Results</head><p>The proposed method can be used to perform egocentric action recogni- tion by considering the predictions obtained for the observation rate of 100%. <ref type="table">Table 9</ref> compares the performance of the proposed method with other egocentric action recognition methods on the two test sets of EPIC-Kitchens. <ref type="bibr" target="#b4">5</ref> Our RU outperforms all competitors in recognizing actions and nouns on both sets by significant margins, whereas it achieves second-best results in most cases for verb recognition. RU obtains an action recognition accuracy of 60 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented RU-LSTM, a learning architecture which processes RGB, optical flow and object-based features using two LSTMs and a modality attention mechanism to anticipate actions from egocentric video. Experiments on two datasets show the superiority of the approach with respect to prior art and highlight generalization over datasets and tasks: anticipation, early recognition, and recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research is supported by Piano della Ricerca 2016-2018, linea di Intervento 2 of DMI, University of Catania.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details and Training Procedure of the Proposed Method</head><p>This section reports the implementation and training details of both the proposed and compared methods. A diagram of our architecture is reported in <ref type="figure">Figure A</ref> for the convenience of the reader. The reader is referred to the paper for a description of the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Architectural Details of RU-LSTM and MATT</head><p>We use a Batch Normalized Inception CNN <ref type="bibr" target="#b26">[26]</ref> (BNInception) in the spatial and flow branches and consider the 1024-dimensional vectors produced by the last global average pooling layer of the network as output representations. Optical flows are extracted using the TVL1 algorithm <ref type="bibr" target="#b63">[63]</ref>. Specifically, we use the pre-computed optical flows provided by the authors in the case of EPIC-Kitchens (see http://EPIC-Kitchens.github.io/) and compute optical flows on EGTEA Gaze+ using the code provided in https://github.com/feichtenhofer/ gpu_flow with default parameters. At test time, the CNNs are fed with input images and optical flows resized to 456 × 256 pixels. Note that, due to global average pooling, the output of the BNInception CNN will be a 1024 feature vector regardless size of the input image. We found this setting leading to better performance as compared to extracting a 224 × 224 crop from the center of the image. For the object branch, we use a Faster R-CNN object detector <ref type="bibr" target="#b20">[20]</ref> with a ResNet-101 backbone <ref type="bibr" target="#b22">[22]</ref>, as implemented in <ref type="bibr" target="#b21">[21]</ref>. Both the Rolling LSTM (R-LSTM) and the Unrolling LSTM (U-LSTM) contain a single layer with 1024 hidden units. Dropout with p = 0.8 is applied to the input of each LSTM and to the input of the final fully connected layer used to obtain class scores. The Modality ATTention network (MATT) is a feed-forward network with three fully connected layers containing respectively h/4, h/8 and 3 hidden units, where h = 6144 is the dimension of the input to the attention network (i.e., the concatenation of the hidden and cell states of 1024 units each related to the three R-LSTMs). Dropout with p = 0.8 is applied to the input of the second and third layers of the attention network to avoid over-fitting. ReLU activation function are used within the attention network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Training Procedure of RU-LSTM and MATT</head><p>While the proposed architecture could be in principle trained in an end-to-end fashion, we found it extremely challenging to avoid over-fitting during end-to-end training. This is mainly due to the indirect relationship between input video and future actions. Indeed, differently from action recognition, where the objects and actions to be recognized are present or take place in the input video, in the case of action anticipation, the system should be able to anticipate objects and actions which do not always appear in the input video, which makes it hard to learn good representations end-to-end. To avoid over-fitting, the proposed architecture is trained as follows. First, we independently train the spatial and motion CNNs for the task of egocentric action recognition within the framework of TSN <ref type="bibr" target="#b61">[61]</ref>. Specifically, we set the number of segments to 3 and train the TSN models with Stochastic Gradient Descent (SGD) using standard cross entropy for 160 epochs with an initial learning rate equal to 0.001, which is decreased by a factor of 10 after 80 epochs. We use a mini-batch size of 64 samples and train the models on a single Titan X. For all other parameters, we use the values recommended in <ref type="bibr" target="#b61">[61]</ref>. We train the object detector to recognize the 352 object classes of the EPIC-Kitchens dataset. We use the same object detector trained on EPIC-Kitchens when performing experiments on EGTEA Gaze+, as the latter dataset does not contain object bounding box annotations. This training procedure allows to learn the parameters θ 1 , θ 2 and θ 3 of the representation functions related to the three modalities (i.e., RGB, Flow, OBJ). After this procedure, these parameters are fixed and they are no more optimized. For efficiency, we pre-compute representations over the whole dataset.</p><p>Each branch of the RU-LSTM is training with SGD using the cross entropy loss with a fixed learning rate equal to 0.01 and momentum equal to 0.9. Each branch is first pre-trained with Sequence Completion Pre-training (SCP). Specifically, appearance and motion branches are trained for 100 epochs, whereas the object branch is trained for 200 epochs. Branches are then fine-tuned for the action anticipation task. Once each branch has been trained, the complete architecture with three branches is assembled to form a three-branch network and the model is further fine-tuned for 100 epochs using cross entropy and the same learning parameters. In <ref type="figure">Figure A</ref> an example of a two-branches architecture is shown.</p><p>In the case of early action recognition, each branch is trained for 200 epochs (both SCP and main task) with a fixed learning rate equal to 0.01 and momentum equal to 0.9.</p><p>Note that, in order to improve performances, we apply early stopping at each training stage. This is done by choosing the iterations of the intermediate and final models which obtain the best Top-5 action anticipation accuracy for the anticipation time τ a = 1s on the validation set. In the case of early action recognition, we choose the epoch obtaining the best average Top-1 action accuracy across observation rates. The same early stopping strategy is applied to all the methods for fair comparison. The proposed RU-LSTM architecture has been implemented using the PyTorch library <ref type="bibr" target="#b43">[43]</ref>. The code will be provided upon publication, together with all details and data useful to replicate the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Note on End-To-End Training</head><p>We chose to fix the feature extractors in our work as we experienced over-fitting when training the model end-to-end. Specifically, we tried the following: (1) Training the RGB branch end-to-end from scratch (except the CNN, which is pre-trained on Imagenet), (2) Pre-training the CNN on the action recognition task with TSN, then training the RGB branch end-to-end, (3) Training the RGB branch using fixed representations as described in the paper, then fine-tuning the CNN + RU-LSTM model end-to-end. In our experiments, (1) and (2) led to poor performance already at the sequence-completion stage, while (3) did not improve performance. Our insight is that the indirect relationship between the observed scene and the action yet to take place can make learning representations end-to-end much more difficult than in the case of recognition. For instance, when anticipating the action "take cup", the object "cup" may or may not be present in the observed video segment, which makes unclear what visual features the CNN should extract.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Inference At a Fixed Anticipation Time</head><p>Our model makes multiple anticipations at time-steps 7 − 14. Also, since the predictions are updated as the video is processed and more evidence is acquired, such predictions may indeed be inconsistent (with anticipations performed closer to the beginning of the action being more likely to be correct). However, it should be noted that each prediction is deemed to be specific to a given anticipation time. For instance, at time-step 11, the model tries to an-ticipate actions happening in 1s. Therefore, the proposed approach can be used to anticipate actions at a fixed anticipation time by processing the buffered video up to the related time-step, discarding all other predictions. E.g., if the anticipation time is set to τ a = 1s, the model should process the last 11 time-steps.</p><p>A.4. Choice of Parameters α, S enc and S ant .</p><p>In this work, we set α = 0.25s and S ant = 8 to generalize the settings of the EPIC-Kitchens anticipation challenge. Indeed, in these settings, we can anticipate actions up to 2s in advance (8 × 0.25s), while still being able to produce anticipations at anticipation time τ a = 1s (4 × 0.25s) as required for the challenge. We investigated the effect of S enc when we fix α = 0.25s and S dec = 8. We noted that the choice of S enc affects performance lightly and hence chose S enc = 6 to maximize action anticipation performance for anticipation time τ a = 1s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details of the Compared Methods</head><p>Since no official public implementation is available for the compared methods, we performed experiments using our own implementations. In this section, we report the implementation details of each of the compared method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Deep Multimodal Regressor (DMR)</head><p>We implement the Deep Multimodal Regressor proposed in <ref type="bibr" target="#b58">[58]</ref> setting the number of multi-modal branches with interleaved units to k = 3. For fair comparisons, we substituted the AlexNet backbone originally considered in <ref type="bibr" target="#b58">[58]</ref> with a BNInception CNN pre-trained on ImageNet. The CNN is trained to anticipate future representations extracted using BNInception pre-trained on ImageNet using the procedure proposed by the authors. Specifically, we perform mode update every epoch. Since training an SVM with large number of classes is challenging (in our settings, we have 2, 513 different action classes), we substituted the SVM with a Multi Layer Perceptron (MLP) with 1024 hidden units and dropout with p = 0.8 applied to the input of the first and second layer. To comply with the pipeline proposed in <ref type="bibr" target="#b58">[58]</ref>, we pre-train the model on our training split of EPIC-Kitchens in an unsupervised fashion and train the MLP separately on representations pre-extracted from the training set using the optimal modes found at training time. As a result, during the training of the MLP, the weights of the CNN are not optimized. The DMR architecture is trained with Stochastic Gradient Descent using a fixed learning rate equal to 0.1 and a momentum equal to 0.9. The network is trained for several epochs until the validation loss saturates. Note that training the CNN on the EPIC-Kitchens dataset takes several days on a single Titan X GPU using our implementation. After training, we apply early stopping by selecting the iteration with the lowest validation loss. The MLP is then trained with Stochastic Gradient Descent with fixed learning rate equal to 0.01 and momentum equal to 0.9. Early stopping is applied also in this case considering the iteration of the MLP achieving the highest Top-5 action accuracy on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Anticipation TSN (ATSN)</head><p>We implement this model considering the TSN architecture used to pre-train the CNNs employed in the RGB and Flow branches of our architecture. We modify the network to output verb and noun scores and train it summing the cross entropy losses applied independently to verbs and nouns as specified in <ref type="bibr" target="#b7">[8]</ref>. At test time, we obtain action probabilities by assuming independence of verbs and nouns as follows: p(a = (v, n)|x) = p(v|x) · p(n|x), where a = (v, n) is an action involving verb v and noun n, x is the input sample, whereas p(v|x) and p(n|x) are the probabilities computed directly by the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. ATSN + VNMCE Loss (MCE)</head><p>This method is implemented training the TSN architecture used for ATSN with the Verb-Noun Marginal Cross Entropy Loss proposed in <ref type="bibr" target="#b16">[16]</ref>. We used the official code provided by the authors (https://github.com/ fpv-iplab/action-anticipation-losses/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Encoder-Decoder LSTM (ED)</head><p>We implement this model following the details specified in <ref type="bibr" target="#b18">[18]</ref>. For fair comparison with respect to the proposed method, the model takes RGB and Flow features obtained using the representation functions as input for the RGB and Flow modalities used in our RU architecture. Differently from <ref type="bibr" target="#b18">[18]</ref>, we do not include a reinforcement learning term in the loss as our aim is not to distinguish the action from the background as early as possible as proposed in <ref type="bibr" target="#b18">[18]</ref>. The hidden state of the LSTMs is set to 2048 units. The model encodes representations for 20 steps, while decoding is carried out for 10 steps at a step-size of 0.25s. The architecture is trained on top of pre-extracted representations for 100 epochs with the Adam optimizer and a learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Feedback-Network LSTM (FN)</head><p>The method proposed in <ref type="bibr" target="#b19">[19]</ref> has been implemented considering the most performing architecture investigated by the authors, which comprises the "optional" LSTM layer and performs fusion by concatenation. The network uses our proposed video processing strategy. For fair comparison, we implement the network as a two-stream architecture with two branches processing independently RGB and Flow features. Final predictions are obtained with late fusion (equal weights for the two modalities). We use the representation functions of our architecture to obtain RGB and Flow features. The model has hidden layers of 1024 units, which in our experiments leaded to improved results with respect to the 128 features proposed by the authors <ref type="bibr" target="#b19">[19]</ref>. The model is trained using the same parameters used in the proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6. RL &amp; EL</head><p>These two methods are implemented considering a single LSTM with the same parameters of our Rolling LSTM. Similarly to FN, the models are trained as two-stream models with late fusion used to obtain final predictions. The input RGB and Flow features are computed using the representation functions considered in our architecture. The models are trained with the same parameters used in the proposed architecture. RL is trained using the ranking loss on the detection score proposed in <ref type="bibr" target="#b39">[39]</ref>, whereas EL is trained using the exponential anticipation loss proposed in <ref type="bibr" target="#b28">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Results</head><p>This section reports the full set of anticipation and recognition results on EPIC-Kitchens, including precision and recall, as well as the full table of comparisons of the proposed method on EGTEA Gaze+ for action recognition. <ref type="table">Table 8</ref> compares the proposed method with respect to the competitors according to the full set of measures proposed along with the egocentric action anticipation challenge <ref type="bibr" target="#b7">[8]</ref>, including precision and recall (which could not be included in the paper due to space limits). The proposed approach outperforms all competitors also according to precision and recall on S1 and S2, except for average verb precision, where it is outperform by the two-stream CNN. Note that, coherently with Top-1 and Top-5 accuracy, the proposed method achieves large gains for noun precision and recall. Also note the small drop in performance between Top-1 noun accuracy and average noun recall on S1 (from 22.78% to 19.81%), which highlights balanced noun predictions. <ref type="table">Table 9</ref> compares the proposed method with respect to the competitors according to the full set of measures proposed with the egocentric action recognition challenge <ref type="bibr" target="#b7">[8]</ref>. Similarly to <ref type="table">Table 8</ref>, this includes precision and recall, which could not be included in the paper due to space limits. Similarly to what observed in the case of top-1 and top-5 accuracy, the proposed method outperforms the competitors according to most of the considered measures, despite not being explicitly designed to tackle the recognition task (i.e., our architecture was designed for the egocentric action anticipation task.) <ref type="table" target="#tab_1">Table 10</ref> compares the proposed RU method against the state-of-the-art when tackling the task of egocentric action recognition on EGTEA Gaze+. It is worth noting that the proposed method outperforms many recent approaches by significant margins. It is also comparable with other stateof-the-art approaches such as the ones proposed in <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56]</ref>. Again, note that our architecture generalizes despite not being explicitly designed for the recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. EPIC-Kitchens Egocentric Action Anticipation Challenge Leaderboards</head><p>The proposed RULSTM approach has been used to participate in the EPIC-Kitchens egocentric action anticipation competition. Specifically, we considered an ensemble model including features extracted using a BNInception and a ResNet-50 CNN trained for action recognition. <ref type="figure">Figure 9</ref> reports a screenshot of the EPIC-Kitchens egocentric action anticipation challenge at the end of the competition. The screenshot has been acquired from https: //epic-kitchens.github.io/ on the 1 st of August, 2019. Note that our submission (team name "DMI-UNICT") is ranked first on both S1 and S2. <ref type="figure">Figure 10</ref> reports qualitative results of three additional success action anticipation examples. For improved clarity, we report frames with and without optical flows for each example. In the top example, MATT assigns a small weight to the object branch as the contextual appearance features (i.e., RGB) are already enough to reliably anticipate the next actions. In the middle example object detection is fundamental to correctly anticipate "put down spoon", as soon as the object is detected. The bottom example shows a complex scene with many objects. The ability to correctly recognize objects is fundamental to anticipate certain actions (i.e., "wash spoon"). The algorithm can anticipate "wash" well in advance. As soon as the spoon is detected (τ a = 2s), "wash spoon" is correctly anticipated. Note that, even if the spoon is not correctly detected at time τ a = 0.5s, "wash spoon" is still correctly anticipated. <ref type="figure">Figure 11</ref> reports three failure examples. In the top example, the model fails to predict "adjust chair", probably due to the inability of the object detector to identify the chair. Note that, when the object "pan" on the table is detected, "take curry" is wrongly anticipated. In the middle example, the algorithm successfully detects the fridge and tries to anticipate "close fridge" and some actions involving the "take" action, with wrong objects. This is probably due to the inability of the detector to detect "mozzarella", which is not yet appearing in the scene. In the bottom example, the method tries to anticipate actions involving "jar", as soon as "jar" is detected. This misleads the algorithm as the correct </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Qualitative Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc.% Imp. Lit et al. <ref type="bibr" target="#b37">[37]</ref> 46.50 +13.7 Li et al. <ref type="bibr" target="#b36">[36]</ref> 53.30 +6.90 Two stream <ref type="bibr" target="#b49">[49]</ref>  action is "pour coffee". The reader is referred to the videos in the supplementary material for additional success and failure qualitative examples. The supplementary material also reports qualitative examples of the proposed method when applied to the problem of early action recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Example of RU modality-specific branch with Senc = 1 and Sant = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Example of connection scheme used during SCP for time-step t = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1</head><label></label><figDesc>See supp. for details on implementation and training of our method. sure and Mean Top-5 Recall as a class aware metric. Top-5 recall [16] for a given class c is defined as the fraction of samples of ground truth class c for which the class c is in the list of the top-5 anticipated actions. Mean Top-5 Recall averages Top-5 recall values over classes. When evaluating on EPIC-Kitchens, Top-5 Recall is averaged over the provided list of many-shot verbs, nouns and actions. Results on the EPIC-Kitchens official test set are reported using the suggested evaluation measures, i.e., Top-1 accuracy, Top-5 accuracy, Precision and Recall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative examples (best seen on screen). Legend for attention weights: blue -RGB, orange -Flow, green -objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Example of the proposed architecture with M = 2 modalities. In our experiments, we use three modalities: RGB, Flow and OBJ. Modules belonging to different branches are illustrated using different color shades.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Top-5 ACTION Accuracy% @ different τ a (s) VERB NOUN ACT. VERB NOUN ACT. VERB NOUN ACT. ] 21.53 22.22 23.20 24.78 25.75 26.69 27.66 29.74 75.46 42.96 25.75 41.77 42.59 10.97 01.60 01.02 00.63 FN [19] 23.47 24.07 24.68 25.66 26.27 26.87 27.88 28.96 74.84 40.87 26.27 35.30 37.77 06.64 01.52 00.86 00.56 RL [39] 25.95 26.49 27.15 28.48 29.61 30.81 31.86 32.84 Improv. +3.49 +4.24 +5.09 +4.93 +5.71 +5.53 +5.51 +5.43 +2.25 +7.26 +5.71 +1.95 +7.31 +4.13 +0.02 +0.09 +0.13 Egocentric action anticipation results on the EPIC-KITCHENS dataset Kitchens validation set. The left part of the table reports Top-5 action accuracy for the 8 anticipation times. Note that some methods [8, 16, 58] can anticipate actions only at a fixed anticipation time. The right part of the table breaks down Top-5 accuracy and Mean Top-5 Recall for verbs, nouns and actions, for anticipation time τ</figDesc><table><row><cell>Top-5 Acc.% @1s</cell><cell>M. Top-5 Rec.% @1s</cell><cell>Mean T tA(5)</cell></row></table><note>a = 1s, and re- ports mean T tA(5) scores. Best results are in bold, whereas second-best results are underlined. The last row reports the improvement of RU with respect to second best results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">Top-1 Accuracy%</cell><cell cols="3">Top-5 Accuracy%</cell></row><row><cell></cell><cell></cell><cell cols="6">VERB NOUN ACT. VERB NOUN ACT.</cell></row><row><cell></cell><cell>2SCNN [8]</cell><cell>29.76</cell><cell>15.15</cell><cell cols="2">04.32 76.03</cell><cell>38.56</cell><cell>15.21</cell></row><row><cell>S1</cell><cell>ATSN [8] MCE [16]</cell><cell>31.81 27.92</cell><cell>16.22 16.09</cell><cell cols="2">06.00 76.56 10.76 73.59</cell><cell>42.15 39.32</cell><cell>28.21 25.28</cell></row><row><cell></cell><cell>RU</cell><cell>33.04</cell><cell>22.78</cell><cell cols="2">14.39 79.55</cell><cell>50.95</cell><cell>33.73</cell></row><row><cell></cell><cell cols="2">Improvement +1.23</cell><cell cols="3">+6.56 +3.63 +2.99</cell><cell cols="2">+8.80 +5.52</cell></row><row><cell></cell><cell>2SCNN [8]</cell><cell>25.23</cell><cell>09.97</cell><cell cols="2">02.29 68.66</cell><cell>27.38</cell><cell>09.35</cell></row><row><cell>S2</cell><cell>ATSN [8] MCE [16]</cell><cell>25.30 21.27</cell><cell>10.41 09.90</cell><cell cols="2">02.39 68.32 05.57 63.33</cell><cell>29.50 25.50</cell><cell>06.63 15.71</cell></row><row><cell></cell><cell>RU</cell><cell>27.01</cell><cell>15.19</cell><cell cols="2">08.16 69.55</cell><cell>34.38</cell><cell>21.10</cell></row><row><cell></cell><cell cols="2">Improvement +1.71</cell><cell cols="3">+4.78 +2.59 +0.89</cell><cell cols="2">+4.88 +5.39</cell></row></table><note>. Anticipation results on the EPIC-KITCHENS test sets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>] 45.03 46.22 46.86 48.36 50.22 51.86 49.99 49.17 1.24 FN [19] 54.06 54.94 56.75 58.34 60.12 62.03 63.96 66.45 1.26 RL [39] 55.18 56.31 58.22 60.35 62.56 64.65 67.35 70.42 1.29 EL [28] 55.62 57.56 59.77 61.58 64.62 66.89 69.60 72.38 1.32 RU 56.82 59.13 61.42 63.53 66.40 68.41 71.84 74.28 1.41 Imp +1.20 +1.57 +1.65 +1.95 +1.78 +1.52 +2.24 +1.89 +0.09</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Top-5 ACTION Accuracy% @ different τa(s)</cell><cell></cell><cell>T tA</cell></row><row><cell></cell><cell>2</cell><cell>1.75</cell><cell>1.5</cell><cell>1.25</cell><cell>1.0</cell><cell>0.75</cell><cell>0.5</cell><cell>0.25</cell><cell></cell></row><row><cell cols="2">DMR [58] /</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>55.70</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>ATSN [8]</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>40.53</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>MCE [16]</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>56.29</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>ED [18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>28.76 29.99 31.09 32.02 33.09 34.13 34.92 0.66 RU (Late) 29.10 29.77 31.72 33.09 34.23 35.28 36.10 37.61 0.73 Imp. +1.14 +1.01 +1.73 +2.00 +2.21 +2.19 +1.97 +2.69 +0.07 (a) Rolling-Unrolling Mechanism. RU (RGB) 25.44 26.89 28.32 29.42 30.83 32.00 33.31 34.47 0.69 RU (Flow) 17.38 18.04 18.91 19.97 21.42 22.37 23.49 24.18 0.51 RU (OBJ) 24.56 25.60 26.61 28.32 29.89 30.85 31.82 33.39 0.67 Early Fusion 25.58 27.25 28.58 29.59 31.88 32.78 33.99 35.62 0.72 Late Fusion 29.10 29.77 31.72 33.09 34.23 35.28 36.10 37.61 0.73 MATT 29.44 30.73 32.24 33.41 35.32 36.34 37.37 38.98 0.76 Imp. +0.34 +0.96 +0.52 +0.32 +1.09 +1.06 +1.27 +1.37 +0.03 .73 32.24 33.41 35.32 36.34 37.37 38.98 0.76 Imp. (Fusion) +1.48 +1.97 +2.25 +2.32 +3.30 +3.25 +3.24 +4.06 +0.1</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Top-5 ACTION Accuracy% @ different τa(s)</cell><cell>T tA</cell></row><row><cell></cell><cell>2</cell><cell>1.75</cell><cell>1.5</cell><cell>1.25</cell><cell>1.0</cell><cell>0.75</cell><cell>0.5</cell><cell>0.25</cell></row><row><cell>BL (Late)</cell><cell cols="7">27.96 (b) Modality Attention Fusion Mechanism.</cell></row><row><cell>w/o SCP</cell><cell cols="8">29.22 30.43 32.34 33.37 34.75 35.84 36.79 37.93 0.75</cell></row><row><cell>with SCP</cell><cell cols="8">29.44 30.73 32.24 33.41 35.32 36.34 37.37 38.98 0.76</cell></row><row><cell>Imp. of SCP</cell><cell cols="8">+0.22 +0.30 -0.10 +0.04 +0.57 +0.50 +0.58 +1.05 +0.01</cell></row><row><cell></cell><cell></cell><cell cols="6">(c) Sequence-Completion Pre-training.</cell></row><row><cell>BL (Fusion)</cell><cell cols="8">27.96 28.76 29.99 31.09 32.02 33.09 34.13 34.92 0.66</cell></row><row><cell>RU (Fusion)</cell><cell cols="2">29.44 30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(d) Overall comparison wrt strong baseline.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Top-1 ACTION Accuracy% @ different observation rates 12.5% 25.0% 37.5% 50.0% 62.5% 75.0% 87.5% 100% +2.55 +2.25 +2.29 +2.59 +2.96 +2.72 +2.61</figDesc><table><row><cell cols="2">FN [19] 19.61</cell><cell>23.85</cell><cell>25.66</cell><cell>26.85</cell><cell>27.47</cell><cell>28.34</cell><cell>28.26 28.38</cell></row><row><cell cols="2">EL [28] 19.69</cell><cell>23.27</cell><cell>26.03</cell><cell>27.49</cell><cell>29.06</cell><cell>29.97</cell><cell>30.91 31.46</cell></row><row><cell cols="2">RL [39] 22.53</cell><cell>25.08</cell><cell>27.19</cell><cell>28.64</cell><cell>29.57</cell><cell>30.13</cell><cell>30.45 30.47</cell></row><row><cell>RU</cell><cell>24.48</cell><cell>27.63</cell><cell>29.44</cell><cell>30.93</cell><cell>32.16</cell><cell>33.09</cell><cell>33.63 34.07</cell></row><row><cell>Imp.</cell><cell>+1.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Early recognition results on EPIC-KITCHENS.</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Top-1 ACTION Accuracy% @ different observation rates</cell></row><row><cell></cell><cell cols="7">12.5% 25.0% 37.5% 50.0% 62.5% 75.0% 87.5% 100%</cell></row><row><cell cols="2">FN [19] 44.02</cell><cell>50.32</cell><cell>53.34</cell><cell>55.10</cell><cell>56.58</cell><cell>57.31</cell><cell>57.95 57.72</cell></row><row><cell cols="2">EL [28] 40.31</cell><cell>48.08</cell><cell>51.84</cell><cell>54.71</cell><cell>56.93</cell><cell>58.45</cell><cell>59.55 60.18</cell></row><row><cell cols="2">RL [39] 45.42</cell><cell>51.00</cell><cell>54.20</cell><cell>56.54</cell><cell>58.09</cell><cell>58.93</cell><cell>59.29 59.50</cell></row><row><cell>RU</cell><cell>45.94</cell><cell>51.84</cell><cell>54.39</cell><cell>57.05</cell><cell>58.15</cell><cell>59.31</cell><cell>60.10 60.20</cell></row><row><cell>Imp.</cell><cell cols="7">+0.51 +0.84 +0.20 +0.51 +0.06 +0.38 +0.55 +0.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Recognition results on the EPIC-KITCHENS test sets.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Top-1 Accuracy%</cell><cell cols="3">Top-5 Accuracy%</cell></row><row><cell></cell><cell></cell><cell cols="6">VERB NOUN ACTION VERB NOUN ACTION</cell></row><row><cell></cell><cell cols="2">2SCNN [8] 42.16</cell><cell>29.14</cell><cell>13.23</cell><cell>80.58</cell><cell>53.70</cell><cell>30.36</cell></row><row><cell></cell><cell>TSN [8]</cell><cell>48.23</cell><cell>36.71</cell><cell>20.54</cell><cell>84.09</cell><cell>62.32</cell><cell>39.79</cell></row><row><cell>S1</cell><cell>LSTA [55]</cell><cell>59.55</cell><cell>38.35</cell><cell>30.33</cell><cell>85.77</cell><cell>61.49</cell><cell>49.97</cell></row><row><cell></cell><cell>MCE [16]</cell><cell>54.22</cell><cell>38.85</cell><cell>29.00</cell><cell>85.22</cell><cell>61.80</cell><cell>49.62</cell></row><row><cell></cell><cell>RU</cell><cell>56.93</cell><cell>43.05</cell><cell>33.06</cell><cell>85.68</cell><cell>67.12</cell><cell>55.32</cell></row><row><cell></cell><cell>Imp.</cell><cell>-2.62</cell><cell>+4.20</cell><cell>+2.73</cell><cell>-0.09</cell><cell>+4.80</cell><cell>+5.35</cell></row><row><cell></cell><cell cols="2">2SCNN [8] 36.16</cell><cell>18.03</cell><cell>07.31</cell><cell>71.97</cell><cell>38.41</cell><cell>19.49</cell></row><row><cell></cell><cell>TSN [8]</cell><cell>39.40</cell><cell>22.70</cell><cell>10.89</cell><cell>74.29</cell><cell>45.72</cell><cell>25.26</cell></row><row><cell>S2</cell><cell>LSTA [55]</cell><cell>47.32</cell><cell>22.16</cell><cell>16.63</cell><cell>77.02</cell><cell>43.15</cell><cell>30.93</cell></row><row><cell></cell><cell>MCE [16]</cell><cell>40.90</cell><cell>23.46</cell><cell>16.39</cell><cell>72.11</cell><cell>43.05</cell><cell>31.34</cell></row><row><cell></cell><cell>RU</cell><cell>43.67</cell><cell>26.77</cell><cell>19.49</cell><cell>73.30</cell><cell>48.28</cell><cell>37.15</cell></row><row><cell></cell><cell>Imp.</cell><cell>-3.65</cell><cell>+3.31</cell><cell>+2.86</cell><cell>-3.72</cell><cell>+2.56</cell><cell>+5.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>.2% on EGTEA Gaze+. Despite being designed for action anticipation, RU outperforms recent approaches, such as Li et al. [36] (+6.9% wrt 53.3%) and Zhang et al. [66] (+3.19% wrt 57.01% -reported from [55]), and obtaining performances comparable to state-of-the-art approaches such as Sudhakaran and Lanz [56] (−0.56% wrt 60.76) and Sud-</figDesc><table /><note>hakaran et al. [55] (−1.66% wrt 61.86%). 6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Top-1 Accuracy% Top-5 Accuracy% Avg Class Precision% Avg Class Recall% VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION S1 2SCNN (Fusion) [8] 29.76 15.15 04.32 76.03 38.56 15.21 13.76 17.19 02.48 07.32 10.72 01.81 TSN (Fusion) [8] 31.81 16.22 06.00 76.56 42.15 28.21 23.91 19.13 03.13 09.33 11.93 02.39 VNMCE [16] 27.92 16.09 10.76 73.59 39.32 25.28 23.43 17.53 06.05 14.79 11.65 05.11 RU-LSTM 33.04 22.78 14.39 79.55 50.95 33.73 25.50 24.12 07.37 15.73 19.81 07.66 Imp. wrt best +1.23 +6.56 +3.63 +2.99 +8.80 +5.52 +1.59 +4.99 +1.32 +0.94 +7.88 +2.55 Egocentric action anticipation results on the EPIC-Kitchens test set. Egocentric action recognition results on the EPIC-Kitchens test set.</figDesc><table><row><cell></cell><cell cols="2">2SCNN (Fusion) [8] 25.23 09.97</cell><cell>02.29</cell><cell>68.66 27.38</cell><cell>09.35</cell><cell>16.37 06.98</cell><cell>00.85</cell><cell>05.80 06.37</cell><cell>01.14</cell></row><row><cell>S2</cell><cell>TSN (Fusion) [8] VNMCE [16]</cell><cell>25.30 10.41 21.27 09.90</cell><cell>02.39 05.57</cell><cell>68.32 29.50 63.33 25.50</cell><cell>06.63 15.71</cell><cell>07.63 08.79 10.02 06.88</cell><cell>00.80 01.99</cell><cell>06.06 06.74 07.68 06.61</cell><cell>01.07 02.39</cell></row><row><cell></cell><cell>RU-LSTM</cell><cell>27.01 15.19</cell><cell>08.16</cell><cell>69.55 34.38</cell><cell>21.10</cell><cell>13.69 09.87</cell><cell>03.64</cell><cell>09.21 11.97</cell><cell>04.83</cell></row><row><cell></cell><cell>Imp. wrt best</cell><cell>+1.71 +4.78</cell><cell cols="2">+2.59 +0.89 +4.88</cell><cell>+5.39</cell><cell>-2.68 +1.08</cell><cell cols="2">+1.65 +1.53 +5.23</cell><cell>+2.44</cell></row><row><cell></cell><cell></cell><cell cols="2">Top-1 Accuracy%</cell><cell cols="2">Top-5 Accuracy%</cell><cell cols="2">Avg Class Precision%</cell><cell cols="2">Avg Class Recall%</cell></row><row><cell></cell><cell></cell><cell cols="8">VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION</cell></row><row><cell></cell><cell cols="2">2SCNN (Fusion) [8] 42.16 29.14</cell><cell>13.23</cell><cell>80.58 53.70</cell><cell>30.36</cell><cell>29.39 30.73</cell><cell>05.53</cell><cell>14.83 21.10</cell><cell>04.46</cell></row><row><cell></cell><cell>TSN (Fusion) [8]</cell><cell>48.23 36.71</cell><cell>20.54</cell><cell>84.09 62.32</cell><cell>39.79</cell><cell>47.26 35.42</cell><cell>10.46</cell><cell>22.33 30.53</cell><cell>08.83</cell></row><row><cell>S1</cell><cell>LSTA [55]</cell><cell>59.55 38.35</cell><cell>30.33</cell><cell>85.77 61.49</cell><cell>49.97</cell><cell>42.72 36.19</cell><cell>14.46</cell><cell>38.12 36.19</cell><cell>17.76</cell></row><row><cell></cell><cell>VNMCE [16]</cell><cell>54.22 38.85</cell><cell>29.00</cell><cell>85.22 61.80</cell><cell>49.62</cell><cell>53.87 38.18</cell><cell>18.22</cell><cell>35.88 32.27</cell><cell>16.56</cell></row><row><cell></cell><cell>RU-LSTM</cell><cell>56.93 43.05</cell><cell>33.06</cell><cell>85.68 67.12</cell><cell>55.32</cell><cell>50.42 39.84</cell><cell>18.91</cell><cell>37.82 38.11</cell><cell>19.12</cell></row><row><cell></cell><cell>Imp.</cell><cell>-2.62 +4.20</cell><cell>+2.73</cell><cell>-0.09 +4.80</cell><cell>+5.35</cell><cell>-3.45 +1.66</cell><cell>+0.69</cell><cell>-0.30 +1.92</cell><cell>+1.36</cell></row><row><cell></cell><cell cols="2">2SCNN (Fusion) [8] 36.16 18.03</cell><cell>07.31</cell><cell>71.97 38.41</cell><cell>19.49</cell><cell>18.11 15.31</cell><cell>02.86</cell><cell>10.52 12.55</cell><cell>02.69</cell></row><row><cell></cell><cell>TSN (Fusion) [8]</cell><cell>39.40 22.70</cell><cell>10.89</cell><cell>74.29 45.72</cell><cell>25.26</cell><cell>22.54 15.33</cell><cell>05.60</cell><cell>13.06 17.52</cell><cell>05.81</cell></row><row><cell>S2</cell><cell>LSTA [55]</cell><cell>47.32 22.16</cell><cell>16.63</cell><cell>77.02 43.15</cell><cell>30.93</cell><cell>31.57 17.91</cell><cell>08.97</cell><cell>26.17 17.80</cell><cell>11.92</cell></row><row><cell></cell><cell>VNMCE [16]</cell><cell>40.90 23.46</cell><cell>16.39</cell><cell>72.11 43.05</cell><cell>31.34</cell><cell>26.62 16.83</cell><cell>07.10</cell><cell>15.56 17.70</cell><cell>10.17</cell></row><row><cell></cell><cell>RU-LSTM</cell><cell>43.67 26.77</cell><cell>19.49</cell><cell>73.30 48.28</cell><cell>37.15</cell><cell>23.40 20.82</cell><cell>09.72</cell><cell>18.41 21.59</cell><cell>13.33</cell></row><row><cell></cell><cell>Imp.</cell><cell>-3.65 +3.31</cell><cell>+2.86</cell><cell>-3.72 +2.56</cell><cell>+5.81</cell><cell>-8.17 +2.91</cell><cell>+0.75</cell><cell>-7.76 +3.79</cell><cell>+1.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>Recognition results on EGTEA Gaze+.</figDesc><table><row><cell></cell><cell cols="2">41.84 +18.7</cell></row><row><cell>I3D [7]</cell><cell cols="2">51.68 +8.52</cell></row><row><cell>TSN [61]</cell><cell cols="2">55.93 +4.27</cell></row><row><cell>eleGAtt [66]</cell><cell cols="2">57.01 +3.19</cell></row><row><cell>ego-rnn [56]</cell><cell>60.76</cell><cell>-0.56</cell></row><row><cell>LSTA [55]</cell><cell>61.86</cell><cell>-1.66</cell></row><row><cell>RU</cell><cell>60.20</cell><cell>/</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See supp. for implementation details of the considered methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">See supp. for precision and recall results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">See supp. and https://iplab.dmi.unict.it/rulstm/ for additional examples and videos.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">See supp. for precision and recall results.<ref type="bibr" target="#b5">6</ref> See supp. for the full table.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">When will you do what?-anticipating temporal occurrences of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5343" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Encouraging lstms to anticipate actions very early</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="280" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Am I done? predicting action progress in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Becattini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term activity forecasting using first-person vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Bokhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="346" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognize human activities from partially observed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Mark</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2658" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epickitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Screenshots of the EPIC-Kitchens Egocentric Action Anticipation Challenge Leaderboards at the end of the competition</title>
		<ptr target="https://epic-kitchens.github.io" />
	</analytic>
	<monogr>
		<title level="m">The team name of the proposed method is &quot;DMI-UNICT</title>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
	<note>Figure 9</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Forecasting hand and object locations in future frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<idno>abs/1705.07328</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<title level="m">Understanding ego-International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="314" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What will happen next? forecasting player moves in sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3342" to="3351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Leveraging uncertainty to rethink loss functions and evaluation measures for egocentric action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Next-active-object prediction from egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="401" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RED: Reinforced encoderdecoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling temporal structure with lstm for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications in Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Max-margin early event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequential max-margin event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="410" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action-reaction: Forecasting the dynamics of human interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="489" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Car that knows before you do: Anticipating maneuvers via learning temporal driving models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3182" to="3190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for driver activity anticipation via sensory-fusion architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3118" to="3125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">First-person vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2442" to="2453" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="201" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Anticipating human activities using object affordances for reactive robotic response. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="14" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A hierarchical representation for future action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">In the eye of beholder: Joint learning of gaze and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="287" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper into firstperson activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1894" to="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint prediction of activity labels and starting times in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5773" to="5782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Choosing smartly: Adaptive multimodal fusion for object detection in changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting><address><addrLine>Deajeon, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Egocentric future localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4697" to="4705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">First-person activity forecasting with online inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3696" to="3705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human activity prediction: Early recognition of ongoing activities from streaming videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1036" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robot-centric activity prediction from firstperson videos: What will they do to me</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Human-Robot Interaction</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pooled motion features for first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="896" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Krishnacam: Using a longitudinal, single-person, egocentric dataset for scene understanding tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">First person action recognition using deep learned descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2620" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Trajectory aligned features for first person action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generating notifications for missing actions: Don&apos;t forget to turn the lights off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Soran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4669" to="4677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Temporal segmentation and activity classification from first-person sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Spriggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Lsta: Long short-term attention for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10698</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention is all we need: Nailing down object-centric attention for egocentric activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Carlos Niebles. Visual forecasting by imitating dynamics in natural sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep future gaze: Gaze anticipation on egocentric videos using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3539" to="3548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Adding attentiveness to the neurons in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Temporal perception and prediction in ego-centric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4498" to="4506" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
