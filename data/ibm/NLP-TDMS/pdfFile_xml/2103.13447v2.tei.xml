<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DRANet: Disentangling Representation and Adaptation Networks for Unsupervised Cross-Domain Adaptation Sunghoon Im * DGIST</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghun</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DGIST</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Sunghyun Cho POSTECH CSE &amp; GSAI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DRANet: Disentangling Representation and Adaptation Networks for Unsupervised Cross-Domain Adaptation Sunghoon Im * DGIST</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present DRANet, a network architecture that disentangles image representations and transfers the visual attributes in a latent space for unsupervised crossdomain adaptation. Unlike the existing domain adaptation methods that learn associated features sharing a domain, DRANet preserves the distinctiveness of each domain's characteristics. Our model encodes individual representations of content (scene structure) and style (artistic appearance) from both source and target images. Then, it adapts the domain by incorporating the transferred style factor into the content factor along with learnable weights specified for each domain. This learning framework allows bi-/multi-directional domain adaptation with a single encoderdecoder network and aligns their domain shift. Additionally, we propose a content-adaptive domain transfer module that helps retain scene structure while transferring style. Extensive experiments show our model successfully separates content-style factors and synthesizes visually pleasing domain-transferred images. The proposed method demonstrates state-of-the-art performance on standard digit classification tasks as well as semantic segmentation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The use of deep neural networks (DNN) has led to significant performance improvements in a variety of areas, including computer vision <ref type="bibr" target="#b5">[6]</ref>, machine learning <ref type="bibr" target="#b12">[13]</ref>, and natural language processing <ref type="bibr" target="#b6">[7]</ref>. However, problems remain, particularly domain gaps between data, which can significantly degrade model performance. Extensive efforts have been made to generalize the models across domains using unsupervised domain adaptation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39]</ref>. Unsupervised domain adaptation attempts to align the distribution shift in labeled source data with unlabeled target data. Various strategies have been explored to bridge the gap across domains, for example, by feature learning and generative pixel-level adaptation. * Corresponding author.</p><p>(a) Traditional domain adaptation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref> (b) Linear feature separation <ref type="bibr" target="#b41">[42]</ref> and domain adaptation (c) Our feature separation and domain adaptation (DRANet) <ref type="figure">Figure 1</ref>. Illustration of DRANet and the competitive methods (domain adaptation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>, representation disentanglement <ref type="bibr" target="#b41">[42]</ref>. Note that E, S, and G are an encoder, a separator, and a generator.</p><p>Feature-level methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37</ref>] learn features that combine task-discrimination and domaininvariance, where both domains are mapped into a common feature space. Domain invariance typically involves minimizing some feature distance metric <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref> or adversarial discriminator accuracy <ref type="bibr" target="#b8">[9]</ref>. Pixel-level approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2]</ref> perform a similar distribution alignment, not in a feature space but in the raw pixel space by leveraging the power of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref>. They adapt source domain images so that they appear as if drawn from the target domain. Some studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref> incorporate both pixel-level and feature-level approaches to achieve complementary benefits.</p><p>Recently, the field of study has been further advanced by learning disentangled representations into the exclusive and shared components in a latent feature space <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b44">45]</ref>. They demonstrate that representation disentanglement improves a model's ability to extract domain invariant features, as well as the domain adaptation performance. However, these methods still focus on the associated features between two domains such as shared and exclusive components, so they require multiple encoders and generators specialized in individual domains. Moreover, the network training relies heavily on a task classifier with ground-truth class labels, in addition to domain classifiers.</p><p>To tackle these issues, we propose DRANet, a single feed-forward network, that does not require any groundtruth task labels for cross-domain adaptation. In contrast to previous approaches in <ref type="figure">Fig. 1</ref>-(a) that map all domain images into a shared feature space, we focus on extracting the domain-specific features that preserve individual domain characteristics in <ref type="figure">Fig. 1</ref>-(c). Then, we disentangle the discriminative features of individual domains into the content and style components using a separator, which are later used to generate the domain-adaptive features. Unlike the previous feature separation work <ref type="bibr" target="#b41">[42]</ref>, which linearly divides latent vectors into two components in <ref type="figure">Fig. 1-(b)</ref>, our separator is tailored to disentangle latent variables in a nonlinear manifold. Our intuition behind the network design is that different domains may have different distributions for their contents and styles, which cannot be effectively handled by the linear separation of latent vectors. Thus, to handle such difference, our network adopts the non-linear separation and domain-specific scale parameters that are dedicated to handle such inter-domain difference.</p><p>To the best of our knowledge, DRANet is the first approach based solely on the individual domain characteristics for unsupervised cross-domain adaptation. It enables us to apply a single encoder-decoder network for a multidirectional domain transfer from fully unlabeled data. The distinctive points of our approach are summarized as follows:</p><p>• We present DRANet, which disentangles image representation and adapts the visual attributes in a latent space to align the domain shift. • We propose a content-adaptive domain transfer module that helps to synthesize realistic images of complex segmentation datasets, such as CityScapes <ref type="bibr" target="#b4">[5]</ref> and GTA5 <ref type="bibr" target="#b28">[29]</ref>. • We demonstrate that images synthesized by our approach boost the task performances and achieve stateof-the-art performance on standard digit classification tasks as well as semantic segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Unsupervised Domain Adaptation</head><p>Feature-level domain adaptation methods typically align learning distribution by modifying the discriminative representation space. The strategy is to guide feature learning by minimizing the difference between the feature space statistics of the source and target. Early deep adaptive approaches minimize some measurements of domain shift such as maximum mean discrepancy <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23]</ref> or correlation distances <ref type="bibr" target="#b31">[32]</ref>. Recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> learn the representation that is discriminative of source labels while not being able to distinguish between domain using an adversarial loss inspired by the work <ref type="bibr" target="#b0">[1]</ref>. The domain-invariant features are discovered using standard backpropagation training with minimax loss <ref type="bibr" target="#b8">[9]</ref>, domain confusion loss <ref type="bibr" target="#b35">[36]</ref>, or GAN loss <ref type="bibr" target="#b36">[37]</ref>.</p><p>Another approach to unsupervised domain adaptation is the generative pixel-level domain adaptation, which synthesizes images with the content of source images and the style of target images using the adversarial training <ref type="bibr" target="#b13">[14]</ref>. Liu and Tuzel <ref type="bibr" target="#b20">[21]</ref> accomplish to learn the joint distribution of source and target representations by weight sharing, using a specific layer responsible for decoding abstract semantics. Bousmalis et al. <ref type="bibr" target="#b1">[2]</ref> use GANs to learn transformations in the pixel space from one domain to another. Hoffman et al. <ref type="bibr" target="#b14">[15]</ref> adapt representations both at the pixel and feature levels while enforcing both structural and semantic consistency using a cycle-consistency loss. Ye et al. <ref type="bibr" target="#b38">[39]</ref> also incorporate both pixel and feature-level domain classifiers to calibrate target domain images whose representations are close to those of the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Disentangling Internal Representation</head><p>The separation of style and content components in a latent space has been widely studied for artistic style transfer <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. Tenebaum and Freeman <ref type="bibr" target="#b32">[33]</ref> show how perceptual systems can separate the content and style factors, and propose bilinear models to solve these two factors. Elgammal and Lee <ref type="bibr" target="#b7">[8]</ref> introduce a method to separate style and content on manifolds representing dynamic objects. Gatys et al. <ref type="bibr" target="#b10">[11]</ref> show how generic feature representations learned by a CNN manipulate the content and style of natural images. Zhang et al. <ref type="bibr" target="#b42">[43]</ref> propose a neural network representing each style and content with a small set of images, while separating the representations. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> bimodally divide feature representations into the content and style components.</p><p>Among the studies on domain adaptation, the search for approaches to disentangling internal representations has recently grown in interest. Bousmalis et al. <ref type="bibr" target="#b2">[3]</ref> learn to extract image representations that are partitioned into two subspaces: private and shared components and show that the modeling of unique features helps to extract domaininvariant features. Gonzalez-Garcia et al. <ref type="bibr" target="#b11">[12]</ref> attempt to disentangle factors that are exclusive in both domains, and factors that are shared across domains. Liu et al. <ref type="bibr" target="#b21">[22]</ref> propose a cross-domain representation disentangler that bridges the information across data domains and transfers the attributes. Zou et al. <ref type="bibr" target="#b44">[45]</ref> introduce a joint learning framework that separates id-related/unrelated features for person re-identification tasks. We discuss the major differences between our work and the listed works in Sec. 1.</p><formula xml:id="formula_0">!→# × Source image ! Target image " Source feature ℱ ! Target feature ℱ " Content &amp; Style (source) ! , ! Content &amp; Style (target) " , " + ! " #→! × + " ! X→Y image !→" Reconstructed target image ′ " Reconstructed source image ′ ! Y→X image "→! Domain transferred feature ℱ !→" Domain transferred feature ℱ "→! ! Adversarial Loss ℒ $%&amp; ! , ℒ $%&amp; " " Consistency Loss ℒ '() ! , ℒ '() " Perceptual Loss ℒ *+, ! , ℒ *+, " ′ " "→! !→" !→" "→! ! , - ! " , - " Reconstruct ion Loss ℒ .+/ ! , ℒ .+/ " ′ ! | − ′ | 0 Source domain Target domain ′ " !→" "→! ′ ! ! "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DRANet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The overall pipeline of our method is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. Our framework can be extended to domain transfer across three domains, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, although the example only shows two domain case for simple illustration. The networks consist of an encoder E, a feature separator S, a generator G, two discriminators of the source and target domains D X , D Y , and a perceptual network P . In the training phase, we learn all of the parameters of these networks, as well as the feature scaling factors w X→Y , w Y →X which compensate for the distribution of two domains. Given the source and target images I X , I Y , the encoder E extracts the individual features F X , F Y that later pass through the generator G to reconstruct the original input images I X , I Y . The separator S disentangles each feature F X , F Y into the components of scene structure and artistic appearance, which in this paper we call the content C X , C Y and the style S X , S Y , respectively. Then, the transferred domain features F X→Y , F Y →X are synthesized with the learnable scale parameters w X→Y , w Y →X . The generator G maps the original features F X , F Y and the transferred features F X→Y , F Y →X into their image space</p><formula xml:id="formula_1">I X , I Y , I X→Y , I Y →X , respectively.</formula><p>The pretrained perceptual network P , extracts perceptual features to impose the constraints on both content similarity and style similarity. We use two discriminators, D X , D Y , to impose the adversarial loss on both domains. In the test phase, just the encoder E, the separator S, the generator G, and domain weights w are used to produce domain transferred images I X→Y , I Y →X given source and target images I X , I Y . With the single feed-forward network E-S-G, our method enables the bi-directional domain transfer of input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Disentangling Representation and Adaptation</head><p>In this subsection, we describe the motivation for the design of our separator S. We first extract the individual image features F X , F Y using the weight-shared encoder:</p><formula xml:id="formula_2">F X = E(I X ), F Y = E(I Y ).<label>(1)</label></formula><p>The separator disentangles these features into scene structure and artistic appearance factors. We hypothesize that the nonlinear manifold learning is still necessary to map each domain-specific representation into the content or style spaces as demonstrated in <ref type="bibr" target="#b7">[8]</ref>. Thus, we learn a non-linear projection function S that separates the features F X into content C X and style S X factors, as follows:</p><formula xml:id="formula_3">C X = w X S(F X ), S X = F X − w X S(F X ),<label>(2)</label></formula><p>where w X is the weight parameter that normalizes the distribution of content space, which helps to compensate for the distribution shift. The content component is obtained using the non-linear function and the learnable feature scaling parameters, while the style component is defined by subtracting content components from the whole feature. The target representation F Y is also passed through the same separator S, and outputs the target content and style C Y , S Y , but for simplicity here we only denote the source domain case.</p><p>The disentangled representation is used to transfer the domain of features across domains as follows:</p><formula xml:id="formula_4">F X→Y = w X→Y C X + S Y , F Y →X = w Y →X C Y + S X , where w X→Y = w Y w X , w Y →X = w X w Y .<label>(3)</label></formula><p>In our implementation, we directly learn the relative scale parameters w X→Y , w Y →X along with all model parameters. Finally, we pass all representations involving the domain adaptive features F X→Y , F Y →X and the original source and target features F X , F Y through the generator G to project them into image space as follows:</p><formula xml:id="formula_5">I X→Y = G(F X→Y ), I Y →X = G(F Y →X ), I X = G(F X ), I Y = G(F Y ),<label>(4)</label></formula><p>where I X→Y , I Y →X are the domain adapted images and I X , I Y are the reconstructed images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Content-Adaptive Domain Transfer (CADT)</head><p>Style transfer tends to struggle with the complex scenes containing various objects, such as a driving scene. This is because those images are composed of different scene structure, as well as various object composition. To tackle this problem, we present a Content-Adaptive Domain Transfer (CADT). The key idea of this module is to search the target features whose content component is most similar to the source features. Then, the domain transfer is conducted by reflecting more style information from more suitable target features. To achieve this, we design a content similarity ma-trix for the database in a mini-batch, as follows:</p><formula xml:id="formula_6">H row = σ row C X · C Y =    C 11 · · · C 1b . . . . . . . . . C b1 · · · C bb    , C X , C Y ∈ R B×N ,<label>(5)</label></formula><p>where σ row is the softmax operation in the row dimension. The size of the content factors C X is defined by the batch size B and the feature dimension N . The matrix H row contains information about the level of similarity between components in the mini-batch. Based on the similarity matrix, we build a content-adaptive style feature as follows:</p><formula xml:id="formula_7">S Y = H row S Y , where S Y ∈ R B×N .<label>(6)</label></formula><p>More visually pleasing results can be expected than when using the normal transferring method because the content features are more likely to be stylized by the scenes containing similar structure and object composition. We empirically demonstrate this in <ref type="figure">Fig. 8</ref>. To apply the contentadaptive domain transfer in the opposite direction, the content similarity matrix is simply obtained:</p><formula xml:id="formula_8">H col = σ col C X · C Y ,<label>(7)</label></formula><p>where σ col is the softmax in the column direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Loss</head><p>We train our network with an encoder E, a separator S, and a generator G by minimizing the loss function L d while the discriminator D d tries to maximize it:</p><formula xml:id="formula_9">min E,S,G d∈{X,Y } max D d L d ,<label>(8)</label></formula><p>where the domain d is either a source or target domain X, Y . The overall loss of our framework consists of the reconstruction L Rec , consistency L Con , perceptual L P er , and adversarial L GAN loss with the balancing term α i :</p><formula xml:id="formula_10">L d = α 1 L d Rec + α 2 L d GAN + α 3 L d Con + α 4 L d P er . (9)</formula><p>The followings are the details of each loss.</p><p>Reconstruction Loss. We impose an L1 loss to learn E and G that minimizes the difference between input image I d and the reconstructed image I d :</p><formula xml:id="formula_11">L d Rec = L 1 (I d , I d ), where I d = G(E(I d )).<label>(10)</label></formula><p>Adversarial Loss. We apply two discriminators D d∈{X,Y } to evaluate the adversarial loss on the source and target domain, respectively. The following is the adversarial loss for the domain adaptation of X to Y : We impose the same adversarial loss L X GAN for the adaptation of Y to X as well. We apply spectral normalization <ref type="bibr" target="#b24">[25]</ref> to all layers in G and D, and use PatchGAN Discriminator <ref type="bibr" target="#b16">[17]</ref> with the hinge version of adversarial loss <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41]</ref> for driving scene adaptation.</p><formula xml:id="formula_12">L Y GAN = E y∼p data(Y ) [log D Y (y)] + E (x,y)∼p data(X,Y ) [log(1 − D Y (I X→Y (x, y))] .<label>(11)</label></formula><p>Consistency Loss. The consistency loss attempts to retain the content and style components after re-projecting the domain transferred images into the representation space denoted as:</p><formula xml:id="formula_13">L X Con = L 1 C X , C X→Y ) + L 1 (S X , S Y →X ), L Y Con = L 1 C Y , C Y →X ) + L 1 (S Y , S X→Y ),<label>(12)</label></formula><p>where the content C X→Y , C Y →X and style S X→Y , S Y →X factors are extracted by passing the domain transferred images I X→Y , I Y →X , respectively, through the same encoder E and separator S. This loss explicitly encourages the scene structure consistency and artistic appearance consistency before and after domain adaptation. Perceptual Loss. Conventionally, the GT class labels in (semi-)supervised training are provided as the semantic cues guiding the representation disentanglement. However, our framework trains disentangling representations without any labeled data. To learn the disentangler in an unsupervised manner, we impose a perceptual loss <ref type="bibr" target="#b17">[18]</ref> which is widely known as a typical framework for style transfer, defined as:</p><formula xml:id="formula_14">L X P er = L X Content + λL X Style , L Y P er = L Y Content + λL Y Style ,<label>(13)</label></formula><p>where L X Content , L Y Content are the content losses, and L X Style , L Y Style are the style losses defined as:</p><formula xml:id="formula_15">L Y Content = l∈L C P l (I X ) − P l (I X→Y ) 2 2 , L Y Style = l∈L S G(P l (I Y )) − G(P l (I X→Y )) 2 F ,<label>(14)</label></formula><p>where the set of layers L C , L S are the subset of the perceptual network P . The weight parameter λ balances the two losses, and G is the function that builds a Gram Matrix, given the features of each layer l <ref type="bibr" target="#b9">[10]</ref>. We also apply batchinstance normalization <ref type="bibr" target="#b26">[27]</ref> for better stylization. Details of architecture are described in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate DRANet for unsupervised domain adaptation on digit classification in Sec. 4.1 and driving scene segmentation in Sec. 4.2. We compare our bi-/tri-directional domain transfer results against multiple state-of-the-art un-/semi-supervised domain adaptation methods. We also conduct an extensive ablation study to demonstrate the effectiveness of each proposed module in Sec. 4.3. For the evaluations, we use the standard split of training and test sets the same as the existing unsupervised domain adaptations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref>. We train a task-classifier using stylized source training sets produced by DRANet and evaluate its performance on the target domain test sets. We describe the training details in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Adaptation for Digit Classification</head><p>Unlike existing domain adaptation methods, where a single model is responsible for domain transfer in one direction, our single model is able to deal with multidirectional domain adaption. We demonstrate the versatility of DRANet by transferring images across the multiple domains using three digit datasets: MNIST <ref type="bibr" target="#b18">[19]</ref>, MNIST-M <ref type="bibr" target="#b8">[9]</ref>, and USPS <ref type="bibr" target="#b15">[16]</ref>. We train our model for bi-directional domain adaptation (MNIST to MNIST-M or USPS, and its opposite direction) as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. We also train the adaptation model tri-directionally (MNIST to MNIST-M and USPS, and their opposite directions) and show the results in <ref type="figure" target="#fig_1">Fig. 3</ref>. Note that we have not explicitly transferred the domain between MNIST-M and USPS during training, but the results show that DRANet is also applicable for the adaptation between them.</p><p>As shown in Tab. 1, our model, either trained for two or three domains, outperforms all the competitive methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>. The results also show that our model even achieves higher performance than the model  <ref type="table">Table 1</ref>. Result comparison of DRANet to state-of-the-art methods on domain adaptation for digit classification. We report the performance from both bi-directional and tri-directional domain adaptation. Note that ours(bi-directional) and ours(tri-directional) use two models (MNIST-USPS, MNIST-MNISTM) and a model (MNIST-USPS-MNISTM), respectively to evaluate all four domain adaptation tasks. trained only on target except for the experiment of USPS to MNIST. This is because DRANet augments as many images as the number of target images using one source image as shown in <ref type="figure">Fig. 4</ref>. DRANet-based data augmentation makes the classifier even more robust than the target-only model. Moreover, we show the content similarity matrix in <ref type="figure" target="#fig_3">Fig. 5</ref> that reveals how well our model disentangles the representation into content and style components. We use 10 images with similar content from MNIST and MNIST-M each, and observe the confusion matrix has the highest diagonal values. We also observe that both higher values around 50% for both samples of digit one. The results show that our model disentangles the representation of content and style while maintaining each domain's characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Adaptation for Semantic Segmentation</head><p>To show the applicability of DRANet on the complex real-world scenario, we use GTA5 <ref type="bibr" target="#b28">[29]</ref> and Cityscapes <ref type="bibr" target="#b4">[5]</ref>, which contain driving scene images with dense annotations. We train our model using 24966 images in GTA5 and 2975 images in Cityscapes train set, and we train DRN-26 <ref type="bibr" target="#b39">[40]</ref> with 19 common classes for synthetic to real adaptation. The results in <ref type="figure">Fig. 6</ref> show that our model generates stylized images following the artistic appearance of target images while keeping the scene structure of source images. We also evaluate the domain adaptation performance on semantic segmentation.  <ref type="table">Table 2</ref>. Result comparison of DRANet to state-of-the-art methods on domain adaptation for semantic segmentation. We also report the performance of DRANet with and without Content-Adaptive Domain Transfer (CADT).</p><p>(a) Test images (CityScapes) (b) Source prediction.</p><p>(c) Our prediction (d) Ground Truth <ref type="figure">Figure 7</ref>. Semantic segmentation results for GTA5 to CityScapes. Note that we do not use any GT segmentation labels for training DRANet. in all three main metrics for semantic segmentation: mIoU, fwIoU, and pixel accuracy. Among the 19 segmentation labels, our method outperforms the competitive methods in 14 categories. Especially, the accuracy of sky labels is improved by a large margin. We believe that our model designed for maintaining the scene structure allows to stably generate domain transferred images as shown in <ref type="figure">Fig. 6</ref>, and leads the performance improvement as shown in <ref type="figure">Fig. 7.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Representation Disentangler We design our separator incorporating two key ideas: one is the non-linearity of feature mapping and the other is domain normalization factor. To show the effectiveness of these key contributions, we set four experiment settings with/without non-linearity and normalization factors in our framework. We evaluate DRANet in each set for two bidirectional domain transfer tasks (one between MNIST and USPS, the other between MNIST and MNIST-M). We compare the classifi-cation results of each case in unsupervised domain adaptation. As shown in Tab. 3, our model involving both nonlinearity and normalization factors shows the best performance among four different settings. In the adaptation task between MNIST and MNIST-M, all model, even without non-linearity and normalization factor, produces the reasonable performance because both datasets contain the same content representation. Note that MNIST-M is one variation on MNIST proposed for unsupervised domain adaptation, which replaces the background of images while maintaining each MNIST digit <ref type="bibr" target="#b8">[9]</ref>. However, there is a large gap in each case for adaptation between MNIST and USPS, which have obviously different content representation.   <ref type="table">Table 3</ref>. Ablation study on the separator design to verify the effectiveness of the non-linearity in representation disentanglement (Non-linearity) and distribution scale parameters (Normalization).</p><p>the best among the other settings. We empirically demonstrate that non-linear mapping affords better representation disentanglement and the drastic performance improvement.</p><p>As the advantages of non-linear mapping function of features proven in <ref type="bibr" target="#b30">[31]</ref>, we believe that the non-linearity is considerably responsible for clear separation of representations. We also show the normalization factor further boosts the adaptation performance. We can conclude that both factors play an important role in representation disentanglement as well as in an unsupervised domain adaptation. Content-Adaptive Domain Transfer This subsection shows two advantages of our CADT for domain adaptation. One is that it prevents the model to be trained with bad training samples and the other is that it encourages the model to generate better-stylized images. During the early phase of training, the separator is not able to clearly disentangle the content and style components, which means each separation does not solely involve its identical information. Consequently, the model generates content-mixed images at the early training stage, and it might disturb the training by fooling discriminator, especially in the case the two images have quite different content. These strengths can be observed in <ref type="figure">Fig. 8</ref> that shows the comparison of the results from the model with/without CADT trained with less than 1000 iterations. <ref type="figure" target="#fig_2">Fig. 8-(a)</ref> is a source image (GTA5), and <ref type="figure">Fig. 8-(d)</ref> contains multiple target images (Cityscapes) in one minibatch. The bottom-right digit indicates the content similarity with source image. We show the domain transferred images with/without CADT in <ref type="figure">Fig. 8-(b)</ref>,(c), respec-tively. The result in <ref type="figure">Fig. 8-(c)</ref> is generated by adapting the domain of rightmost target image in <ref type="figure">Fig. 8-(d)</ref>, which has the lowest similarity. The results show the normal domain transfer causes the significant artifact in the early stage of training while the proposed CADT reasonably synthesizes the image. It means that CADT helps to disentangle the representation even at just a few iterations. We also show that the general performance improvement by CADT in Tab. 2 by comparing the domain adaptation results with/without CADT. The table demonstrates the effectiveness of our content-adaptive domain transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a new network architecture called DRANet which disentangles individual feature representations into two factors, content and style, and transfers domains by applying the style features of another domain. In contrast to conventional methods which focus on the associations of features among domains, we learn the distinctive features of each domain, then separate the features into two components. This design enables us to transfer the domains multi-directionally with our single model. In addition, our method does not require any class labels for adapting domains. Another contribution of this work is to propose the a content-adaptive domain transfer method to synthesize more realistic images from the complex scene structures. Extensive experiments show that our model synthesizes visually pleasing images transferred across domains, and the synthesized images boost the performance of the classification and semantic segmentation tasks. We also demonstrate that the proposed method outperforms the state-of-the-art domain adaptation methods despite the absence of any labeled data for training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our model. (Left) Image translation blocks involving an encoder E, a separator S, and a generator G. The source and target images IX , IY are the input, and the reconstructed images I X , I Y and domain transferred images IX→Y , IY →X are the output. (Right) The training losses involving reconstruction LRec, consistency LCon, perceptual LP er , and adversarial LGAN loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Tri-directional domain adaptation results from our single network. DRANet keeps the content of source images and transfers the domain of target images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )</head><label>a</label><figDesc>Source and target images (b) Domain transferred images Figure 4. Various domain transferred examples from MNIST to MNIST-M. (a) Top-left image is source image of digit 2 and the others are target images. (b) Domain transferred images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Content similarity between MNIST and MNIST-M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) GTA5 original images (b) Transferred images using (a) GTA5 content and (c) CityScapes style. (c) CityScapes original images (d) Transferred images using (c) CityScapes content and (a) GTA5 style.Figure 6. Domain adaptation results from our single DRANet in driving scenes. (a), (c) Original images. (b), (d) Transferred images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The model without both components results in poor classification performance of one side. This means the model can only adapt either directional domain adaptation (MNIST to USPS or USPS to MNIST), like what the existing methods do. The model with either non-linearity or normalization improves the performance while our model with both factors achieves (a) Source image (GTA5) (b) Ours: CADT using all image in (d) (c) DT using the rightmost image in (d) (d) Target images (CityScapes) in a minibatch and their content similarity with the source image in (a) Figure 8. Comparison on image synthesis using Content-Adaptive Domain Transfer (CADT) and normal Domain Transfer (DT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MethodMNIST to USPS USPS to MNIST MNIST to MNIST-M MNIST-M to MNIST</figDesc><table><row><cell>Source Only</cell><cell>80.2</cell><cell>44.9</cell><cell>62.5</cell><cell>97.8</cell></row><row><cell>DANN [9]</cell><cell>85.1</cell><cell>73.0</cell><cell>77.4</cell><cell>-</cell></row><row><cell>DSN [3]</cell><cell>91.3</cell><cell>-</cell><cell>83.2</cell><cell>-</cell></row><row><cell>ADDA [37]</cell><cell>90.1</cell><cell>95.2</cell><cell>-</cell><cell>-</cell></row><row><cell>CoGAN [21]</cell><cell>91.2</cell><cell>89.1</cell><cell>62.0</cell><cell>-</cell></row><row><cell>pixelDA [2]</cell><cell>95.9</cell><cell>-</cell><cell>98.2</cell><cell>-</cell></row><row><cell>CyCADA [15]</cell><cell>95.6</cell><cell>96.5</cell><cell>-</cell><cell>-</cell></row><row><cell>LC + CycleGAN [39, 44]</cell><cell>97.1</cell><cell>98.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (Bi-directional)</cell><cell>98.2</cell><cell>97.8</cell><cell>98.7</cell><cell>99.3</cell></row><row><cell>Ours (Tri-directional)</cell><cell>97.6</cell><cell>96.9</cell><cell>98.3</cell><cell>99.0</cell></row><row><cell>Target Only</cell><cell>97.8</cell><cell>99.1</cell><cell>96.2</cell><cell>99.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The quantitative results in Tab. 2 show that our model achieves state-of-the-art performance 33.1 77.9 23.4 17.3 32.1 33.3 31.8 81.5 26.7 69.0 62.8 14.7 74.5 20.9 25.6 6.9 18.8 20.4 39.5 72.4 82.3 LC [39] 83.5 35.2 79.9 24.6 16.2 32.8 33.1 31.8 81.7 29.2 66.3 63.0 14.3 81.8 21.0 26.5 8.5 16.7 24.0 40.5 75.1 84.0 Ours (without CADT) 83.5 33.7 80.7 22.7 19.5 25.2 28.6 25.8 84.1 32.8 84.4 53.3 13.6 75.7 21.7 30.6 15.8 20.3 19.5 40.6 75.6 84.9 Ours (with CADT) 85.0 35.8 82.0 26.4 21.6 27.0 29.2 28.1 84.2 34.0 81.9 53.6 15.9 73.6 21.1 31.0 16.7 17.2 22.8 41.4 76.4 85.7</figDesc><table><row><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorbike</cell><cell>bicycle</cell><cell>mIoU</cell><cell>fwIoU</cell><cell>Pixel Acc.</cell></row><row><cell>Source only</cell><cell cols="4">42.7 26.3 51.7 5.5</cell><cell cols="11">6.8 13.8 23.6 6.9 75.5 11.5 36.8 49.3 0.9 46.7 3.4</cell><cell>5.0</cell><cell>0.0</cell><cell>5.0</cell><cell cols="4">1.4 21.7 47.4 62.5</cell></row><row><cell cols="23">CyCADA [15] 79.1 Target only 97.3 79.8 88.6 32.5 48.2 56.3 63.6 73.3 89.0 58.9 93.0 78.2 55.2 92.2 45.0 67.3 39.6 49.9 73.6 67.4 89.6 94.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised pixellevel domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3722" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Separating style and content on a nonlinear manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan-Su</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-to-image translation for cross-domain disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>2016. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Deep learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Geometric gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detach and adapt: Learning cross-domain disentangled deep representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ying</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Chien</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<title level="m">Spectral normalization for generative adversarial networks. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<title level="m">cgans with projection discriminator. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch-instance normalization for adaptively style-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyo-Eun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2558" to="2567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kernel principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="583" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Joshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08896</idno>
		<title level="m">Deep and hierarchical implicit models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gotta adapt&apos;em all: Joint pixel and feature-level domain adaptation for recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2672" to="2681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Light-weight calibrator: a separable component for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailu</forename><surname>Shaokai Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Sia Huat Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>PMLR, 2019. 5</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Style separation and synthesis via generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="183" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Separating style and content for generalized style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yexun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8447" to="8455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint disentangling and adaptation for crossdomain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
