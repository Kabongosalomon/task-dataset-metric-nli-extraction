<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VPN: Learning Video-Pose Embedding for Activities of Daily Living</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srijan</forename><surname>Das</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA Université Nice Côte d&apos;Azur</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Sharma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA Université Nice Côte d&apos;Azur</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA Université Nice Côte d&apos;Azur</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Brémond</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA Université Nice Côte d&apos;Azur</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monique</forename><surname>Thonnat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA Université Nice Côte d&apos;Azur</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VPN: Learning Video-Pose Embedding for Activities of Daily Living</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>action recognition</term>
					<term>video</term>
					<term>pose</term>
					<term>embedding</term>
					<term>attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we focus on the spatio-temporal aspect of recognizing Activities of Daily Living (ADL). ADL have two specific properties (i) subtle spatio-temporal patterns and (ii) similar visual patterns varying with time. Therefore, ADL may look very similar and often necessitate to look at their fine-grained details to distinguish them. Because the recent spatio-temporal 3D ConvNets are too rigid to capture the subtle visual patterns across an action, we propose a novel Video-Pose Network: VPN. The 2 key components of this VPN are a spatial embedding and an attention network. The spatial embedding projects the 3D poses and RGB cues in a common semantic space. This enables the action recognition framework to learn better spatio-temporal features exploiting both modalities. In order to discriminate similar actions, the attention network provides two functionalities -(i) an end-to-end learnable pose backbone exploiting the topology of human body, and (ii) a coupler to provide joint spatio-temporal attention weights across a video. Experiments 1 show that VPN outperforms the state-of-the-art results for action classification on a large scale human activity dataset: NTU-RGB+D 120, its subset NTU-RGB+D 60, a real-world challenging human activity dataset: Toyota Smarthome and a small scale human-object interaction dataset Northwestern UCLA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Monitoring human behavior requires fine-grained understanding of actions. Activities of Daily Living (ADL) may look simple but their recognition is often more challenging than activities present in sport, movie or Youtube videos. ADL often have very low inter-class variance making the task of discriminating them from one another very challenging. The challenges characterizing ADL are illustrated in <ref type="figure" target="#fig_0">fig 1:</ref> (i) short and subtle actions like pouring water and pouring grain while making coffee ; (ii) actions exhibiting similar visual patterns while differing in motion patterns like rubbing hands and clapping; and finally, (iii) actions observed from different camera views. In the recent literature, the main focus is the recognition of actions from internet videos <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b56">54,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b14">13]</ref> and very few studies have attempted to recognize ADL in indoor scenarios <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b9">8]</ref>. <ref type="bibr">Cut</ref>    For instance, state-of-the-art 3D convolutional networks like I3D <ref type="bibr" target="#b5">[5]</ref> pretrained on huge video datasets <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b22">21]</ref> have successfully boosted the recognition of actions from internet videos. But, these networks with similar spatiotemporal kernels applied across the whole space-time volume cannot address the complex challenges exhibited by ADL. Attention mechanisms have thus been proposed on top of these 3D convolutional networks to guide them along the regions of interest of the targeted actions <ref type="bibr" target="#b56">[54,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b14">13]</ref>. Following a different direction, action recognition for ADL has been dominated by the use of human 3D poses <ref type="bibr" target="#b59">[57,</ref><ref type="bibr" target="#b58">56]</ref>. They provide a strong clue for understanding the visual patterns of an action over time. 3D poses are robust to illumination changes, view adaptive and provide critical geometric information about human actions. However, they lack incorporating the appearance information which is an essential property in ADL (especially for human-object interaction).</p><p>Consequently, attempts have been made to utilize 3D poses to weight the discriminative parts of a RGB feature map <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b9">8]</ref>. These methods have improved the action recognition performance but they do not take into account the alignment of the RGB cues and the corresponding 3D poses. Therefore, we propose a spatial embedding to project the visual features and the 3D poses in the same referential. Before describing our contribution, we answer two intuitive questions below. First, why is spatial embedding important? -Previous pose driven attention networks can be perceived as guiding networks to help the RGB cues focus on the salient information for action classification. For these guiding networks, it is important to have an accurate correspondences between the poses and RGB data. So, the objective of the spatial embedding is to find correspondences between the 3D human joints and the image regions representing these joints as illustrated in fig 2. This task of finding correlation between both modalities can (i) provide informative pose aware feedback to the RGB cues, and (ii) improve the functionalities of the guiding network. Second, why not performing temporal embedding? -We argue that the need of embedding is to provide proper alignment between the modalities. Across time, the 3D poses are already aligned assuming that there is a 3D pose for every images. However, even if the number of 3D poses does not correspond to the number of image frames (as in <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b9">8]</ref>), the fact that variance in poses for few consecutive frames is negligible, especially for ADL, implies temporal embedding is not needed.</p><p>We propose a recognition model based on a Video-Pose Network, VPN to recognize a large variety of human actions. VPN consists of a spatial embedding and an attention network. VPN exhibits the following novelties: (i) a spatial embedding learns an accurate video-pose embedding to enforce the relationships between the visual content and 3D poses, (ii) an attention network learns the attention weights with a tight spatio-temporal coupling for better modulating the RGB feature map, (iii) the attention network takes the spatial layout of the human body into account by processing the 3D poses through Graph Convolutional Networks (GCNs). The proposed recognition model is end-to-end trainable and our proposed VPN can be used as a layer on top of any 3D ConvNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Below, we discuss the relevant action recognition algorithms w.r.t. their input modalities. RGB -Traditionally, image level features <ref type="bibr" target="#b51">[49,</ref><ref type="bibr" target="#b52">50]</ref> have been aggregated over time using encoding techniques like Fisher Vector <ref type="bibr" target="#b36">[34]</ref> and NetVLAD <ref type="bibr" target="#b0">[1]</ref>. But these video descriptors do not encode long-range temporal information. Then, temporal patterns of actions have been modelled in videos using sequential networks. These sequential networks like LSTMs are fed with convolutional features from images <ref type="bibr" target="#b11">[10]</ref> and thus, they model the temporal information based on the evolution of appearance of the human actions. However, these methods first process the image level features and then capture their temporal evolution preventing the computation of joint spatio-temporal patterns over time.</p><p>Due to this reason, Du et al. <ref type="bibr" target="#b50">[48]</ref> have proposed 3D convolution to model the spatio-temporal patterns within an action. The 3D kernels provide tight coupling of space and time towards better action classification. Later on, holistic methods like I3D <ref type="bibr" target="#b5">[5]</ref>, slow-fast network <ref type="bibr" target="#b13">[12]</ref>, MARS <ref type="bibr" target="#b6">[6]</ref> and two-in-one stream network <ref type="bibr" target="#b61">[59]</ref> have been fabricated for generic datasets like Kinetics <ref type="bibr" target="#b18">[17]</ref> and UCF-101 <ref type="bibr" target="#b47">[45]</ref>. But these networks are trained globally over the whole 3D volume of a video and thus, are too rigid to capture salient features for subtle spatio-temporal patterns for ADL.</p><p>Recently several attention mechanisms have been proposed on top of the aforementioned 3D ConvNets to extract salient spatio-temporal patterns. For instance, Wang et al. <ref type="bibr" target="#b56">[54]</ref> have proposed a non-local module on top of I3D which computes the attention of each pixel as a weighted sum of the features of all pixels in the space-time volume. But this module relies too much on the appearance of the actions, i.e., pixel position within the space-time volume. As a consequence, this module though effective for the classification of actions in internet videos, fails to disambiguate ADL with similar motion and fails to address view invariant challenges. 3D Poses -To focus on the view-invariant challenge, temporal evolution of 3D poses have been leveraged through sequential networks like LSTM and GRU for skeleton based action recognition <ref type="bibr" target="#b59">[57,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b58">56]</ref>. Taking a step ahead, LSTMs have also been used for spatial and temporal attention mechanisms to focus on the salient human joints and key temporal frames <ref type="bibr" target="#b46">[44]</ref>. Another framework represents 3D poses as pseudo image to leverage the successful image classification CNNs for action classification <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b28">27]</ref>. Recently, graph-based methods model the data as a graph with joints as vertexes and bones as edges <ref type="bibr" target="#b57">[55,</ref><ref type="bibr" target="#b49">47,</ref><ref type="bibr" target="#b42">40]</ref>. Compared to sequential networks and pseudo image based methods, graph-based methods make use of the spatial configuration of the human body joints and thus, are more effective. However, the skeleton based action recognition lacks in encoding the appearance information which is critical for ADL recognition. RGB + 3D Poses -In order to make use of the pros of both modalities, i.e. RGB and 3D Poses, it is desirable to fuse these multi-modal information into an integrated set of discriminative features. As these modalities are heterogeneous, they must be processed by different kinds of network to show their effectiveness. This limits their performance in simple multi-modal fusion strategy <ref type="bibr" target="#b40">[38,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b32">30]</ref>. As a consequence, many pose driven attention mechanisms have been proposed to guide the RGB cues for action recognition. In <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4]</ref>, the pose driven attention networks implemented through LSTMs, focus on the salient image features and the key frames. Then, with the success of 3D CNNs, 3D poses have been exploited to compute the attention weights of a spatio-temporal feature map. Das et al. <ref type="bibr" target="#b7">[7]</ref> have proposed a spatial attention mechanism on top of 3D ConvNets to weight the pertinent human body parts relevant for an action. Then, authors in <ref type="bibr" target="#b9">[8]</ref> have proposed a more general spatial and temporal attention mechanism in a dissociated manner. But these methods have the following drawbacks: (i) there is no accurate correspondence between the 3D poses and the RGB cues in the process of computing the attention weights <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b9">8]</ref>; (ii) the attention subnetworks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b9">8]</ref> neglect the topology of the human body while computing the attention weights; (iii) the attention weights in <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">8]</ref> provide identical spatial attention along the video. As a result, action pairs with similar appearance like jumping and hopping are mis-classified.</p><p>In contrast, we propose a new spatial embedding to enforce the correspondences between RGB and 3D pose which has been missing in the state-of-the-art methods. The embedding is built upon an end-to-end learnable attention network. The attention network considers the human topology to better activate the relevant body joints for computing the attention weights. To the best of our knowledge, none of the previous action recognition methods have combined human topology with RGB cues. In addition, the proposed attention network couples the spatial and temporal attention weights in order to provide spatial attention weights varying along time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Action Recognition Model</head><p>Our objective is to design an accurate spatial embedding of poses and visual content to better extract the discriminative spatio-temporal patterns. As shown in <ref type="figure" target="#fig_3">fig. 3</ref>, the input of our proposed recognition model are the RGB images and their 3D poses. The 3D poses are either extracted from depth sensor or from RGB using LCRNet <ref type="bibr" target="#b39">[37]</ref>. The proposed Video-Pose Network VPN takes as input the visual feature map and the 3D poses. Below, we discuss the action recognition model in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video Representation</head><p>Taking as input a stack of human cropped images from a video clip, the spatiotemporal representation f is computed by a 3D convolutional network (the visual backbone in <ref type="figure" target="#fig_3">fig. 3</ref>). f is a feature map of dimension t c × m × n × c, where t c denotes the temporal dimension, m × n the spatial scale and c the channels. Then, the feature map f and the corresponding poses P are processed by the proposed network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VPN</head><p>VPN can be thought as a layer which can be placed on top of any 3D convolutional backbone. VPN takes as input a 3D feature map (f ) and its corresponding 3D poses (P ) to perform two functionalities. First, to provide an accurate alignment of the human joints with the feature map f . Second, to compute a modulated feature map (f ) which is further classified for action recognition. The modulated feature map (f ) is weighted along space and time as per its relevance. VPN exploits the highly informative 3D pose information to transform the visual feature map f and finally, compute the attention weights. This network has two major components as shown in <ref type="figure" target="#fig_4">fig 4:</ref> (I) an attention network and (II) a spatial embedding. Though the intrinsic parameters of the attention network and the spatial embedding learns in parallel, we present these two components in the following order for better understanding. the spatio-temporal attention weights. They carry meaningful information in a compact way, so the proposed attention network can efficiently focus on salient action parts. For the Pose Backbone, we use GCNs to learn the spatial relationships between the 3D human joints to provide attention weights to the visual feature map (f ). We aim at exploiting the graphical structure of the 3D poses. In <ref type="figure" target="#fig_4">fig. 4</ref>(I), we illustrate our GCN pose backbone (marked (A)). For each pose input P t ∈ R 3×J with J joints, we first construct a graph G t (P t , E) where E is the J × J weighted adjacency matrix:</p><formula xml:id="formula_0">e ij =        0, if i = j α,</formula><p>if joint i and joint j are connected β, if joint i and joint j are disconnected Each graph G t at time t is processed by a GCN to compute feature f + t :</p><formula xml:id="formula_1">f + t = D − 1 2 (E + I)D − 1 2 G t W t ,<label>(1)</label></formula><p>where W t is the weight matrix and D is the diagonal degree matrix with</p><formula xml:id="formula_2">D ii = Σ j (E ij + I ij ) its diagonal elements. For all t = 1, 2, ..., t p , the GCN output features f + t are aggregated along time, resulting in a 3D tensor [f + 1 , f + 2 , ..., f + tp ]</formula><p>. Finally, the 3D pose tensor is combined with the original pose input by a residual connection followed by a set of convolutional operations. Now, the GCN pose backbone provides salient features h * because of its use of the graphical structure of the 3D joints.</p><p>Spatio-temporal Coupler -The attention network in VPN learns the spatiotemporal attention weights from the output of Pose Backbone in two steps as shown in <ref type="figure" target="#fig_4">fig. 4</ref>(I)(B). In the first step, the spatial and temporal attention weights (A S and A T ) are classically trained as in <ref type="bibr" target="#b46">[44]</ref> to get the most important body part and key frames for an action. The output feature h * of Pose Backbone follows two separate non-linear mapping functions to compute the spatial and temporal attention weights. These spatial A S and temporal A T weights are defined as</p><formula xml:id="formula_3">A S = σ(z 1 ); A T = sof tmax(z 2 )<label>(2)</label></formula><p>where z r = W zr tanh(W hr h * + b hr ) + b zr (for r = 1, 2) with subscripted W and b, the corresponding weights and biases are the latent spatial and temporal attention vectors. The dissociated attention weights A S and A T having dimension m × n and t c respectively, can undergo a linear mapping to obtain spatially and temporally modulated feature maps. The resultant model is equivalent to the separable STA model <ref type="bibr" target="#b9">[8]</ref>. In contrast, we propose to further perform a coupling of the spatial and temporal attention weights. Thus in the second step, joint spatiotemporal attention weights are computed by performing a Hadamard product on the spatial and temporal attention weights. In order to perform this matrix multiplication, the spatial and temporal attention weights are inflated by duplicating the same attention weights in temporal and spatial dimension respectively.</p><formula xml:id="formula_4">Hence, the m × n × t c dimensional spatio-temporal attention weights A ST are obtained by A ST = inf late(A S )•inf late(A T )</formula><p>. This two-step attention learning process enables the attention network to compute spatio-temporal attention weights in which the spatial saliency varies with time. The obtained attention weights are crucial to disambiguate actions with similar appearance as they may have dissimilar motion over time.</p><p>Finally, the spatio-temporal attention weights A ST are linearly multiplied with the input video feature map f , followed by a residual connection with the original feature map f to output the modulated feature map f . The residual connection enables the network to retain the properties of the original visual features.</p><p>(II) Spatial Embedding of RGB and Pose -The objective of the embedding model is to provide tight correspondences between both pose and RGB modalities used in VPN. The state-of-the-art methods <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">8]</ref> attempt to provide the attention weights on the RGB feature map using 3D pose information without projecting them into the same 3D referential. The mapping with the pose is only done by cropping the person within the input RGB images. The spatial attention computed through the 3D joint coordinates does not correspond to the part of the image (no pixel to pixel correspondence), although it is crucial for recognizing fine-grained actions. To correlate both modalities, an embedding technique inspired from image captioning task <ref type="bibr" target="#b34">[32,</ref><ref type="bibr" target="#b35">33]</ref> is used to build an accurate RGB-Pose embedding in order to enable the poses to represent the visual content of the actions (see <ref type="figure" target="#fig_4">fig. 4</ref>(II)).</p><p>We assume that a low dimensional embedding exists for the global spatial representation of video feature map f s = Σ tc i=1 f (i, :, :, :) (a D v dimensional vector) and its corresponding pose based latent spatial attention vector z 1 (a D p dimensional vector). The mapping function can be derived from this embedding by f e = T v f s and P e = T p z 1 ,</p><p>where T v ∈ R De×Dv and T p ∈ R De×Dp are the transformation matrices that project the video content and the 3D poses into the common D e dimensional embedding space. This mapping function is applied on the global spatial representation of the visual feature map and the pose based features in order to attain the aforementioned objective of the spatial embedding.</p><p>To measure the correspondence between the video content and the 3D poses, we compute the distance between their mappings in the embedding space. Thus, we define an embedding loss as a hypersphere feature metric space</p><formula xml:id="formula_6">L e = || T v f s − T p z 1 || 2 2 s.t. ||T v || 2 = ||T p || 2 = 1<label>(4)</label></formula><p>T v f s = Tvfs ||Tvfs||2 and T p z 1 = Tpz1 ||Tpz1||2 are the feature representations projected to the unit hypersphere. The norm constraint ||T v || 2 = 1 &amp; ||T p || 2 = 1 simply prevents the trivial solutionT v =T p = 0. This embedding loss along with the global classification loss provides a linear transformation on the RGB feature map that preserves the low-rank structure for the action representation and introduces a maximally separated features for different actions. Now, the kernels at the visual backbone are updated with a gradient proportional to (f e − P e ), which in turn transforms the visual feature map to learn pose aware characteristics. Consequently, we strengthen the correspondences between video and poses by minimizing the embedding loss. This embedding ensures that the pose information to be used for computing the spatial attention weights aligns with the content of the video.</p><p>Note that the embedding loss also provides feedback to the pose based latent spatial attention vectors (z 1 ), which in turn transfers knowledge from the 2D image space to pose 3D referential. This allows the attention network to provide better and meaningful spatial attention weights (A s ) compared to the attention network without the embedding. We will quantify this observation in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training jointly the 3D ConvNet and VPN</head><p>VPN can be trained as a layer on top of any 3D ConvNet. The 3D ConvNet can be pre-trained for the action classification task for faster convergence. Finally, VPN is plugged into the 3D ConvNet for an end-to-end training with a regularized loss L formulated as</p><formula xml:id="formula_7">L = λ 1 L C + (1 − λ 1 )L e + λ 2 L a<label>(5)</label></formula><p>Here, L C is the cross-entropy loss, L e is the embedding loss; the trade-off between these two losses is captured by linear fusion with a positive parameter λ 1 ; L a is the attention regularizer with λ 2 weighting factor. The attention regularizer consists of the spatial and temporal attention weight regularizer and is formulated as</p><formula xml:id="formula_8">L a = m×n j=1 A s (j) 2 + tc j=1 (1 − A tc (j)) 2<label>(6)</label></formula><p>This additional regularization term L a ensures that the attention weights are not biased to provide extremely high values to the parts of the spatio-temporal feature map with more relevance and completely neglecting the other parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the effectiveness of our model for action classification. We consider four public datasets which are the popular datasets for ADL: NTU-60 <ref type="bibr" target="#b41">[39]</ref>, NTU-120 <ref type="bibr" target="#b26">[25]</ref>, Toyota-Smarthome <ref type="bibr" target="#b9">[8]</ref> and Northwestern-UCLA <ref type="bibr" target="#b53">[51]</ref>.</p><p>NTU RGB+D (NTU-60 &amp; NTU-120): NTU-60 is acquired with a Kinect v2 camera and consists of 56880 video samples with 60 activity classes. The activities were performed by 40 subjects and recorded from 80 viewpoints. For each frame, the dataset provides RGB, depth and a 25-joint skeleton of each subject in the frame. For evaluation, we follow the two protocols proposed in <ref type="bibr" target="#b41">[39]</ref>: crosssubject (CS) and cross-view (CV). NTU-120 is a super-set of NTU-60 adding a lot of new similar actions. NTU-120 dataset contains 114k video clips of 106 distinct subjects performing 120 actions in a laboratory environment with 155 camera views. For evaluation, we follow a cross-subject (CS 1 ) protocol and a cross-setting (CS 2 ) protocol proposed in <ref type="bibr" target="#b26">[25]</ref>. Toyota-Smarthome (Smarthome) is a recent ADL dataset recorded in an apartment where 18 older subjects carry out tasks of daily living during a day. The dataset contains 16.1k video clips, 7 different camera views and 31 complex activities performed in a natural way without strong prior instructions. This dataset provides RGB data and 3D skeletons which are extracted from LCRNet <ref type="bibr" target="#b39">[37]</ref>. For evaluation on this dataset, we follow cross-subject (CS) and cross-view (CV 1 and CV 2 ) protocols proposed in <ref type="bibr" target="#b9">[8]</ref>.</p><p>Northwestern-UCLA Multiview activity 3D Dataset (N-UCLA) is acquired simultaneously by three Kinect v1 cameras. The dataset consists of 1194 video samples with 10 activity classes. The activities were performed by 10 subjects, and recorded from three viewpoints. We performed experiments on N-UCLA using the cross-view (CV) protocol proposed in <ref type="bibr" target="#b53">[51]</ref>: we trained our model on samples from two camera views and tested on the samples from the remaining view. For instance, the notation V 3 1,2 indicates that we trained on samples from view 1 and 2, and tested on samples from view 3. The presence of ADL challenges like fine-grained and similar appearance activities is in higher magnitude in NTU-120 and Smarthome datasets. So, we perform all our ablation studies on these two datasets. We abbreviate Smarthome as SH in table 1, 2, 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Training. In our experiments, the selected visual backbone is I3D <ref type="bibr" target="#b5">[5]</ref> network pre-trained on ImageNet <ref type="bibr" target="#b10">[9]</ref> and Kinetics-400 <ref type="bibr" target="#b18">[17]</ref>. The visual backbone takes 64 video frames as input. The input of the VPN consists of the feature map extracted from Mixed 5c layer of I3D and the corresponding 3D poses. The pose backbone takes as input a sequence of t p 3D poses uniformly sampled from each clip. Hyper-parameter t p = 20, 20, 30 and 5 for NTU-60, NTU-120, Smarthome and N-UCLA respectively. For the pose backbone, we use t p number of GCNs, each processing a pose from the sequence. The weighting parameters α and β for computing the adjacency matrix of the pose based graph are set to 5 and 2 respectively. GCN projects the input joint coordinates to a 64 − dimensional space. The output of the GCN is passed to a set of convolutional operations (see <ref type="figure" target="#fig_4">fig. 4</ref>(I)(A)) which consists of three 2D convolutional layers each are followed by a Batch Normalization layer and a ReLU layer. The output channels of the convolutional layers are 64, 64 and 128.</p><p>For classification, a global-average pooling layer followed by a dropout <ref type="bibr" target="#b48">[46]</ref> of 0.3 and a softmax layer are added at the end of the recognition model for class prediction. Our recognition model is trained with a 4-GPU machine where each GPU has 4 video clips in a mini-batch. Our model is trained for 30 epochs in total, with SGD optimizer having initial learning rate of 0.01 and decay rate of 0.1 after every 10 epochs. The trade off (λ 1 ) and regularizer (λ 2 ) parameters are set to 0.8 and 0.00001 respectively for all the experiments. Inference. For the recognition model, we perform fully convolutional inference in space as in <ref type="bibr" target="#b56">[54]</ref>. The final classification is obtained by max-pooling the softmax scores.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Our model includes two novel components, the spatial embedding and the attention network. Both of them are critical for good performance on ADL recognition. We show the importance of the attention network and the spatial embedding of VPN in table 1. We also show the effectiveness of the spatial embedding with different instantiation of the attention network in table 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How effective is VPN?</head><p>In order to answer this point, we show the action classification accuracy with baseline I3D (l 1 ) which is the visual backbone and then incorporate the VPN components: the attention network (l 2 ) and the spatial embedding (l 3 ) one-by-one in table 1. The attention network (l 2 ) improves significantly the classification of the actions (upto 8.4% on NTU-120 and 5.4% on Smarthome) by providing spatio-temporal saliency to the I3D feature maps. With the spatial embedding (l 3 ), the action classification further improves (upto 0.9% on NTU-120 and 4.4% on Smarthome). <ref type="table" target="#tab_2">In table 2</ref>, we further illustrate the importance of each component in the attention network, i.e. the Pose Backbone and the spatio-temporal coupler. We have designed a baseline attention network with LSTM as pose backbone following <ref type="bibr" target="#b9">[8]</ref>. We compare the LSTM pose backbone in l 4 and l 6 with our proposed GCN instantiation in l 5 and l 7 . The attention network without a spatio-temporal coupler provides dissociated spatial and temporal attention weights in l 4 and l 5 in contrast to our proposed coupler in l 6 and l 7 . Firstly, we observe that the GCN pose backbone makes use of the human joint topology, thus improves the classification accuracy in all scenarios with or without the coupler. Consequently, actions like Snapping Finger (+24.5%) and Apply cream on face (+23.9%) improves significantly with GCN instantiation (l 6 ) compared to LSTM (l 7 ). Secondly, we observe that the spatiotemporal coupler provides fine spatial attention weights for the most important frames in a video, which enables the model to disambiguate actions with similar appearance but dissimilar motion. Consequently, the coupler (l 7 ) improves the classification accuracy up to 1% on NTU-120 and 0.7% on Smarthome w.r.t. dissociating the attention weights (l 5 ). For instance, with dissociation of the attention weights, rubbing two hands was confused with clapping and flicking hair was confused with putting on headphone. With VPN, the coupler improves the classification accuracy of actions rubbing two hands and flicking hair by 25% and 19.6% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagnosis of the attention network -</head><p>Which loss is better for learning the spatial embedding? In this ablation study <ref type="table" target="#tab_3">(Table 3)</ref>, we compare different losses for projecting the 3D poses and RGB cues in a common semantic space. First, we compare the KL-divergence losses <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b16">15]</ref> (D KL (f e ||P e ) and D KL (P e ||f e )) from P e to f e and vice-versa. Then, we compare a bi-directional KL-divergence loss <ref type="bibr" target="#b60">[58,</ref><ref type="bibr" target="#b54">52,</ref><ref type="bibr" target="#b31">29]</ref> (D KL (f e ||P e ) + D KL (P e ||f e )) to our normalized euclidean loss. We observe that (i) the loss using D KL (f e ||P e ) and D KL (P e ||f e ) deteriorates the action classification accuracy as the feedback is in one direction either towards RGB or poses, implying two-way feedback for the visual features and the attention network is necessary, (ii) our normalized euclidean loss outperforms the bi-directional KL divergence loss, exhibit its superiority. <ref type="table" target="#tab_4">In table 4</ref>, we show the impact of spatial embedding on the attention network providing spatial attention only. We perform the experiments with different choice of Pose Backbone, i.e. LSTM as discussed above and our proposed GCN. The spatial embedding provides a tight correspondence between the RGB data and poses. As a result, it boosts the classification accuracy in all the experiments. It is worth noting that the improvement is significant for Smarthome as it contains many fine-grained actions with videos captured by fixed cameras in an unconstrained Field of View. Thus, enforcing the embedding loss enhances the spatial precision during inference. As a result, the classification accuracy of fine-grained actions like pouring water (+77.7%), pouring grains (+76.1%) for making coffee, cutting bread (+50%), pouring from kettle (+42.8%) and inserting teabag (+35%) improves VPN with GCN pose backbone compared to its counterpart without embedding.  They are presented in a sequence of the human body topological order (follow first row of <ref type="figure" target="#fig_5">fig. 5(a)</ref>) for convenient visualization. VPN is able to disambiguate actions with similar appearance like hopping and jumping due to high order activation at relevant joints of the human legs. The discriminative leg joints with high activation have been marked with a red bounding box in <ref type="figure" target="#fig_5">fig. 5</ref>(a) (third row). Similarly, for actions like put on headphone with two hands and flicking hair with one hand, the blue bounding boxes demonstrate high activation of both the hand joints for the former action as compared to high activation of a single hand joints for the latter. For a very fine-grained action like thumbs up, the thumb joint is highly activated as compared to the other joints. This shows that the GCN pose backbone in VPN is a crucial ingredient for better action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Embedding on Spatial attention -</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Analysis</head><p>In <ref type="figure" target="#fig_5">fig. 5(b)</ref>, we compare the heatmap of the VPN and I3D feature maps for different time stamps. We observe the sharpness in the VPN feature maps compared to that of I3D for thumbs down action which is localized over a small space. For similar actions like put on headphone and flicking hair, along with salience precision of the VPN feature map, the activations of their corresponding receptive fields show the discriminative power of VPN. In <ref type="figure" target="#fig_8">fig. 6(a)</ref>, we illustrate the performance of VPN w.r.t. I3D baseline for the dynamicity of an action along the videos. This dynamicity is computed by averaging the Procrustes distance <ref type="bibr" target="#b21">[20]</ref> between subsequent 3D poses along the videos. If the average distance is large, it means the poses change a lot in an action. VPN significantly improves for actions with subtle motion like hush (+52.7%), staple book (+40.7%) and reading (+36.2%) which indicates the efficacy of VPN for fine-grained actions. The degradation of the VPN performance for high action dynamicity is negligible(-0.8%). In <ref type="figure" target="#fig_8">fig. 6(b)</ref>, we show the t-SNE plots of the feature spaces produced by I3D and VPN for some selected actions with similar appearance. It clearly shows the discriminative power of VPN for actions with similar appearance which is a frequent challenge in ADL.   <ref type="table">Table 5</ref>: Results (accuracies in %) on NTU-60 with cross-subject (CS) and cross-view (CV) settings (at left) and NTU-120 with cross-subject (CS1) and cross-setup (CS2) settings (at right); Att indicates attention mechanism, • indicates that the modality has only been used for training, the methods indicated with * are reproduced on this dataset. 3D ResNeXt-101 is abbreviated as RNX3D101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Pose RGB Att CS CV </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the state-of-the-art</head><p>We compare VPN to the state-of-the-art (SoA) on NTU-60, NTU-120, Smarthome and N-UCLA in table 5, 6 and 7. VPN outperforms on each of them. In table 5 (at left), for input modality RGB+Poses, VPN improves the SoA <ref type="bibr" target="#b7">[7]</ref> by up to 0.8% on NTU-60 even by using one-third parameters compared to <ref type="bibr" target="#b7">[7]</ref>. The SoA using Poses only <ref type="bibr" target="#b42">[40]</ref> yields classification accuracy near to VPN for cross-view protocol (with 0.1% difference) due to their robustness to view changes. However,  the lack of appearance information restricts these methods <ref type="bibr" target="#b42">[40,</ref><ref type="bibr" target="#b44">42]</ref> to disambiguate actions with similar visual appearance, thus resulting in lower accuracy for cross-subject protocol. We have also tested VPN with 3D ResNeXt-101 <ref type="bibr" target="#b15">[14]</ref> on NTU-60 dataset. The results in <ref type="table">table 5</ref> show that VPN can be adapted with other existing video backbones.</p><p>Compared to the SoA results, the improvement by 3.9% and 4.9% (averaging over the protocols) on NTU-120 and Smarthome respectively are significant. It is worth noting that VPN improves further the classification of actions with similar appearance as compared to Separable STA <ref type="bibr" target="#b9">[8]</ref>. For example, actions like clapping (+44.3%) and flicking hair (+19.1%) are now discriminated with better accuracy. In addition, the superior performance of VPN in cross-view protocol for both NTU-120 and Smarthome implies that it provides better view-adaptive characterization compared to all the prior methods. For N-UCLA which is a small-scale dataset, we pre-train the visual backbone with NTU-60 for a fair comparison with <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b9">8,</ref><ref type="bibr" target="#b7">7]</ref>. We also outperform the SoA <ref type="bibr" target="#b7">[7]</ref> by 0.4% on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper addresses the challenges of ADL classification. We have proposed a novel Video-Pose Network VPN which provides an accurate video-pose embedding. We show that the embedding along with attention network yields a more discriminative feature map for action classification. The attention network leverages the topology of the human joints and with the coupler provides precise spatio-temporal attention weights along the video.</p><p>Our recognition model outperforms the state of-the-art results for action classification on 4 public datasets. This is a first step towards combining RGB and Pose through an explicit embedding. A future perspective of this work is to exploit this embedding even in case of noisy 3D poses in order to also boost action recognition for internet videos. This embedding could even help to refine these noisy 3D poses in a weakly supervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix overview</head><p>We provide in section 1 computational details regarding the normalization of Euclidean loss provided in Spatial Embedding of RGB and Pose (section 3.2 (II)). Section 2 provides the details of the baseline with LSTM pose backbone with or without coupler in <ref type="table" target="#tab_2">Table 2 &amp; 4</ref> from the ablation studies. Section 3 provides the details of the divergence losses used for comparing with Normalized Euclidean loss in <ref type="table" target="#tab_3">Table 3</ref> from ablation studies. Finally, we provide some more insights about VPN in section 4 to illustrate its effectiveness. For convenience, we use the same notation as in the main paper for this supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Details on normalization of Euclidean loss</head><p>In equation <ref type="formula" target="#formula_6">(4)</ref>,</p><formula xml:id="formula_9">T v f s = Tvfs ||Tvfs|| 2 = fe ||fe|| 2 and T p z 1 = Tpz 1 ||Tpz 1 || 2 = Pe ||Pe|| 2</formula><p>are the feature representations projected to the unit hypersphere.</p><p>Here, we compute the norm ||f e || 2 and ||P e || 2 using</p><formula xml:id="formula_10">||f e || 2 = Σ i f 2 e i + &amp; ||P e || 2 = Σ i P 2 e i +<label>(7)</label></formula><p>where is a small positive value to prevent dividing zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LSTM Pose backbone with or without coupler baselines</head><p>For the LSTM Pose Backbone in <ref type="table" target="#tab_2">Table 2</ref> &amp; 4, we use a 3-layer stacked LSTM, pre-trained for action classification, as a Pose Backbone by freezing the weights of their cell gates following <ref type="bibr" target="#b9">[8]</ref>. The output feature vector h * is computed by concatenating all the LSTM output features over time. To have a fair comparison with our GCN Pose Backbone, we also introduced residual connections between the original pose input and the LSTM output tensor. However, these residual connections do not improve the action classification accuracy.</p><p>For the experiments in <ref type="table" target="#tab_2">Table 2</ref> to implement the attention network without the coupler, we do not compute A ST . Instead, we multiply the attention weights inf late(A S ) and inf late(A T ) separately with the RGB feature map f in two streams following <ref type="bibr" target="#b9">[8]</ref>. Finally, the modulated feature maps from both the streams are concatenated to classify the actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baselines with KL divergence loss</head><p>In <ref type="table" target="#tab_3">Table 3</ref>, we compare different forms of KL divergence loss with normalized euclidean loss for spatial embedding of RGB and 3D poses. The KL-divergence losses D KL (f e ||P e ) and D KL (P e ||f e ) for n samples are computed by</p><formula xml:id="formula_11">D KL (f e ||P e ) = n i=1 f i e log( f i e P i e )<label>(8)</label></formula><formula xml:id="formula_12">D KL (P e ||f e ) = n i=1 P i e log( P i e f i e )<label>(9)</label></formula><p>where f i e and P i e are visual and pose embedding of the i th input sample. Finally, the bi-directional KL-divergence loss is given by D KL (f e ||P e )+ D KL (P e ||f e ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Detailed qualitative analysis of VPN</head><p>In this section, we provide illustrations to show the impact of each VPN components in section 4.1, superiority of VPN compared to other representative baselines in section 4.2, and some result visualization to highlight the solved and remaining challenges in ADL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Illustration to show the impact of VPN components</head><p>In <ref type="figure">fig. 7</ref>, we illustrate a set of graphs showing the top-5 improvement of action classification accuracy using different components of VPN compared to I3D baseline. As discussed in the ablation studies of the primary paper, each component in VPN is critical for good performance on ADL recognition.</p><p>-The spatial embedding provides an accurate alignment of the RGB images and the 3D poses. As a result, the recognition performance of the fine-grained actions improves compared to its counterpart without embedding (see <ref type="figure">fig. 7 (a)</ref>). -The GCN pose backbone of the attention network, not only provides a strategy to globally optimize the recognition model but also takes the human joint configuration into account for computing the attention weights. This further boosts the action classification performance (see <ref type="figure">fig. 7 (b)</ref>).</p><p>-The spatio-temporal coupler of the attention network provides discriminative spatio-temporal attention weights which enables the recognition model to better disambiguate the actions with similar appearance (see <ref type="figure">fig. 7 (c)</ref>). <ref type="bibr" target="#b21">20</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Illustration to show the superiority of VPN</head><p>We illustrate in <ref type="figure" target="#fig_9">fig. 8</ref>, the top-5 per-class classification improvement compared to baseline I3D <ref type="bibr" target="#b5">[5]</ref> and to an attention mechanism (Separable STA <ref type="bibr" target="#b9">[8]</ref>) from the state-of-the-art, utilizing 3D poses. The significant accuracy improvements for actions with subtle motion like hush (+52.7%), staple book (+40.7%) and reading (+36.2%) as depicted in <ref type="figure" target="#fig_9">fig. 8</ref> (a) illustrate the efficacy of VPN for fine-grained actions. It is worth noting that VPN improves further the classification of actions possessing similar appearance as compared to separable STA in <ref type="figure" target="#fig_9">fig. 8 (b)</ref>. For example, actions like clapping (+44.3%) and flicking hair (+19.1%) are now discriminated with better accuracy. Further, in <ref type="figure" target="#fig_9">fig. 8 (c)</ref> we present a radar for the average mis-classification score of few action-pairs. The smaller area under the curve for VPN compared to I3D baseline and Separable STA shows that it is able to better disambiguate the action-pairs even with low inter-class variation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Result visualization</head><p>In this section, we provide the confusion matrix for action classification on NTU RGB+D 120 and Toyota Smarthome using VPN. In <ref type="figure" target="#fig_10">fig 9,</ref> we present the confusion matrix of VPN on NTU RGB+D (on right) and a zoom of it around the red bounding box (on left). We also present the corresponding zoom of the confusion matrix of I3D. We are particularly interested in the mis-classifications performed by VPN and thus, we zoom into the region with relatively low classification accuracy. We observe that actions like staple book and taking something out of bag were confused with cutting papers and put something into a bag respectively when classified with I3D. However, with VPN these actions with similar motion are now better discriminated, improving their classification accuracy by approximately 42% and 27% respectively. Similarly, in <ref type="figure" target="#fig_0">fig. 10</ref> (a), we present the confusion matrix of VPN on Toyota Smarthome dataset. In <ref type="figure" target="#fig_0">fig. 10 (b)</ref>, we show the poses for some images belonging to action videos mis-classified by I3D. Thanks to the high quality 3D poses for these videos, now VPN can correctly classify these actions taking the human topology of the 3D poses into account. We provide some visual results in <ref type="figure" target="#fig_0">fig. 11</ref> where VPN outperforms I3D baseline. We notice that actions like Drink from glass are not recognized due to extremely low number of training samples. We further notice that actions like using tablet are recognized with low accuracy of 13% and largely confused with using laptop. However, I3D completely mis-classifies the action using tablet. We also observe that still few action classes are recognized with extremely low classification accuracy. We infer that these poor classification </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of the challenges in Activities of Daily Living: fine-grained actions (top), actions with similar visual pattern (middle) and actions viewed from different cameras (below).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of spatial embedding. Input is a RGB image and its corresponding 3D poses. For convenience, we only show 6 relevant human joints. The embedding enforces the human joints to represent the relevant regions in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Proposed Action Recognition Model: Our model takes as input RGB images with their corresponding 3D poses. The RGB images are processed by a visual backbone which generates a spatio-temporal feature map (f ). The proposed VPN takes as input the feature map (f ) and the 3D poses (P ). VPN consists of two components: an attention network and a spatial embedding. The attention network further consists of a Pose Backbone and a spatio-temporal Coupler. VPN computes a modulated feature map f . This modulated feature map f is then used for classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>The components in VPN: (I) Attention Network (left) and (II) Spatial Embedding (right). We present a zoom of the attention Network with: (A) a GCN Pose Backbone, and (B) a spatio-temporal Coupler to generate spatio-temporal attention weights AST (I) Attention Network -The attention network consists of a Pose Backbone and a spatio-temporal Coupler. Such a framework for pose driven attention network is unique compared to the other state-of-the-art methods using poses and RGB. The proposed attention network unlike [3,4,7,8] takes into account the human spatial configuration and it also learns coupled spatio-temporal attention weights for the visual feature map f . Pose Backbone -The input poses along the video are processed in a Pose Backbone. The pose based input of VPN are the 3D human joint coordinates P ∈ R 3×J×tp stacked along t p temporal dimension, where J is the number of skeleton joints. The Pose Backbone processes these 3D poses to compute pose features h * which are used further in the attention network for computing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>(a) The heatmaps of the activations of the 3D joint coordinates (output of GCN) in the attention network of VPN. The area in the colored bounding boxes shows that different joints are activated for similar actions. (b) Heatmaps of visual feature maps &amp; corresponding activated kernels for different time stamps. These heatmaps show that VPN has better discriminative power than I3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 (</head><label>5</label><figDesc>a) visualizes the activation of the human joints at the output of pose backbone (with GCNs) in VPN. The figure depicts the activations of the 3D joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>[0,0.25] [0.25,0.5] [0.5,0.75] [0.75,1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 :</head><label>6</label><figDesc>(a) We compare our model against baseline I3D across action dynamicity. Our model significantly improves for most actions. (b) t-SNE plots of feature spaces produced by I3D and VPN for similar appearance actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Graphs illustrating the superiority of VPN compared to the state-of-the-art methods. We present the Top-5 per class improvement for VPN over (a) I3D baseline and (b) Separable STA. In (c), we present a radar for the average mis-classification score of few action-pairs: lower scores indicate lesser ambiguities between the action-pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 :</head><label>9</label><figDesc>results on certain videos are due to occlusion, low resolution of the actions and low quality poses as illustrated in fig 12. VPN VPN Confusion Matrix of VPN on NTU RGB+D (CS protocol) Confusion matrix of VPN on NTU RGB+D (CS Protocol) on the right. Zoom of the red bounding box on the left along with the corresponding confusion matrix of I3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study to show the effectiveness of each VPN component.</figDesc><table><row><cell>VPN components</cell><cell cols="3">NTU-120 NTU-120 SH</cell><cell>SH</cell></row><row><cell></cell><cell>CS1</cell><cell>CS2</cell><cell cols="2">CS CV2</cell></row><row><cell>l1: visual backbone</cell><cell>77.0</cell><cell>80.1</cell><cell cols="2">53.4 45.1</cell></row><row><cell>l2: l1 + attention network</cell><cell>85.4</cell><cell>86.9</cell><cell cols="2">56.4 50.5</cell></row><row><cell>l3: l2 + spatial embedding</cell><cell>86.3</cell><cell>87.8</cell><cell cols="2">60.8 53.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of VPN with different choices of Attention Network.</figDesc><table><row><cell>Model</cell><cell>Pose</cell><cell cols="4">Coupler NTU-120 NTU-120 SH</cell><cell>SH</cell></row><row><cell></cell><cell>Backbone</cell><cell></cell><cell>CS1</cell><cell>CS2</cell><cell cols="2">CS CV2</cell></row><row><cell>l4: VPN</cell><cell>LSTM</cell><cell>×</cell><cell>84.7</cell><cell>83.6</cell><cell cols="2">57.1 50.6</cell></row><row><cell>l5: VPN</cell><cell>GCN</cell><cell>×</cell><cell>85.6</cell><cell>86.8</cell><cell cols="2">60.1 53.1</cell></row><row><cell>l6: VPN</cell><cell>LSTM</cell><cell></cell><cell>85.3</cell><cell>84.1</cell><cell cols="2">57.6 51.5</cell></row><row><cell>l7: VPN</cell><cell>GCN</cell><cell></cell><cell>86.3</cell><cell>87.8</cell><cell cols="2">60.8 53.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of VPN with different embedding losses le.</figDesc><table><row><cell>Loss</cell><cell cols="3">NTU-120 NTU-120 SH</cell><cell>SH</cell></row><row><cell></cell><cell>CS1</cell><cell>CS2</cell><cell cols="2">CS CV2</cell></row><row><cell>KL-divergence DKL(fe||Pe)</cell><cell>85.5</cell><cell>87.1</cell><cell cols="2">57.2 50.9</cell></row><row><cell>KL-divergence DKL(Pe||fe)</cell><cell>85.6</cell><cell>86.9</cell><cell cols="2">57.0 51.1</cell></row><row><cell>Bi-directional KL-divergence</cell><cell>86.1</cell><cell>87.2</cell><cell cols="2">57.2 51.7</cell></row><row><cell>Normalized Euclidean loss</cell><cell>86.3</cell><cell>87.8</cell><cell cols="2">60.8 53.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Impact of Spatial Embedding on Spatial Attention.</figDesc><table><row><cell>Model</cell><cell>Pose</cell><cell>Spatial</cell><cell cols="3">NTU-120 NTU-120 SH</cell><cell>SH</cell></row><row><cell></cell><cell cols="2">Backbone Embedding</cell><cell>CS1</cell><cell>CS2</cell><cell cols="2">CS CV2</cell></row><row><cell>VPN</cell><cell>LSTM</cell><cell>×</cell><cell>81.7</cell><cell>81.2</cell><cell cols="2">45.5 50.0</cell></row><row><cell>VPN</cell><cell>LSTM</cell><cell></cell><cell>82.7</cell><cell>82.0</cell><cell cols="2">56.5 52.6</cell></row><row><cell>VPN</cell><cell>GCN</cell><cell>×</cell><cell>82.6</cell><cell>84.3</cell><cell cols="2">49.1 51.7</cell></row><row><cell>VPN</cell><cell>GCN</cell><cell></cell><cell>83.1</cell><cell>85.3</cell><cell cols="2">58.4 53.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results on Smarthome dataset with cross-subject (CS) and cross-view (CV1 and CV2) settings (accuracies in %). Att indicates attention mechanism.</figDesc><table><row><cell>Methods</cell><cell cols="2">Pose RGB Att CS CV1 CV2</cell></row><row><cell>DT [49]</cell><cell>×</cell><cell>× 41.9 20.9 23.7</cell></row><row><cell>LSTM [31]</cell><cell>×</cell><cell>× 42.5 13.4 17.2</cell></row><row><cell>I3D [5]</cell><cell>×</cell><cell>× 53.4 34.9 45.1</cell></row><row><cell>I3D+NL [54]</cell><cell>×</cell><cell>53.6 34.3 43.9</cell></row><row><cell>P-I3D [7]</cell><cell></cell><cell>54.2 35.1 50.3</cell></row><row><cell>Separable STA [8]</cell><cell></cell><cell>54.2 35.2 50.3</cell></row><row><cell>VPN</cell><cell></cell><cell>60.8 43.8 53.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Results on N-UCLA dataset with cross-view V<ref type="bibr" target="#b3">3</ref> 1,2 settings (accuracies in %); P ose indicate its usage only in the training phase.</figDesc><table><row><cell>Methods</cell><cell>Data</cell><cell>Att V 3 1,2</cell></row><row><cell>HPM+TM [36]</cell><cell>Depth</cell><cell>× 91.9</cell></row><row><cell>Ensemble TS-LSTM [22]</cell><cell>Pose</cell><cell>× 89.2</cell></row><row><cell>NKTM [35]</cell><cell>RGB</cell><cell>× 85.6</cell></row><row><cell>Glimpse Cloud [4]</cell><cell>RGB+ P ose</cell><cell>90.1</cell></row><row><cell>Separable STA [8]</cell><cell>RGB+Pose</cell><cell>92.4</cell></row><row><cell>P-I3D [7]</cell><cell>RGB+Pose</cell><cell>93.1</cell></row><row><cell>VPN</cell><cell>RGB+Pose</cell><cell>93.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Makecoffe.Pourwater Maketea.Insertteabag</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We are grateful to INRIA Sophia Antipolis -Mediterranean "NEF" computation cluster for providing resources and support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cutbread</head><p>Pour.Fromkettle  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human action recognition: Pose-based attention draws focus to hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCVW.2017.77</idno>
		<ptr target="https://doi.org/10.1109/ICCVW.2017.77" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human activity recognition with pose-driven attention to rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Glimpse clouds: Human activity recognition from unstructured feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">MARS: Motion-Augmented RGB Stream for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crasto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Where to focus on for human action recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019-01" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/WACV.2019.00015</idno>
		<ptr target="https://doi.org/10.1109/WACV.2019.00015" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Toyota smarthome: Real-world activities of daily living</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koperski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Francesca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACPR.2015.7486569</idno>
		<ptr target="https://doi.org/10.1109/ACPR.2015.7486569" />
	</analytic>
	<monogr>
		<title level="m">3rd IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<date type="published" when="2015-11" />
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1812.02707</idno>
		<ptr target="http://arxiv.org/abs/1812.02707" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<idno>abs/1812.01289</idno>
		<ptr target="http://arxiv.org/abs/1812.01289" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2018.2812099</idno>
		<ptr target="https://doi.org/10.1109/TIP.2018.2812099" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2842" to="2855" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved training for online end-toend speech recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-2517</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2018" to="2517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Principles of Multivariate Analysis: A Users Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Krzanowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>Inc., USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Action recognition based on 3d skeleton and rgb frame fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS40897.2019.8967570</idno>
		<ptr target="https://doi.org/10.1109/IROS40897.2019.8967570" />
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="258" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.391</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.391" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3671" to="3680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2916873</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.2916873" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.patcog.2017.02.030</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0031320317300936" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a recurrent residual fusion network for multimodal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.442</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.442" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="4127" to="4136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph distillation for action detection with privileged modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regularizing long short term memory with 3d humanskeleton sequences for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3054" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno>abs/1804.02516</idno>
		<ptr target="http://arxiv.org/abs/1804.02516" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298860</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298860" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="2458" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d action recognition from novel viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.167</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.167" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LCR-Net++: Multi-person 2D and 3D Pose Detection in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-modal feature fusion for action recognition in rgb-d sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCCSP.2014.6877819</idno>
		<ptr target="https://doi.org/10.1109/ISCCSP.2014.6877819" />
	</analytic>
	<monogr>
		<title level="m">6th International Symposium on Communications, Control and Signal Processing (ISCCSP)</title>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00132</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00132" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2627435.2670313" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00558</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00558" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.510</idno>
		<ptr target="http://dx.doi.org/10.1109/ICCV.2015.510" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV). pp. 4489-4497. ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV). pp. 4489-4497. ICCV &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Action Recognition by Dense Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://hal.inria.fr/inria-00583818/en" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="http://hal.inria.fr/hal-00873267" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.339</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.339" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="2649" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.541</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.541" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Val Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On geometric features for skeleton-based action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV.2017.24</idno>
		<ptr target="https://doi.org/10.1109/WACV.2017.24" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2017-03" />
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Deep cross-modal projection learning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dance with flow: Two-in-one stream action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
