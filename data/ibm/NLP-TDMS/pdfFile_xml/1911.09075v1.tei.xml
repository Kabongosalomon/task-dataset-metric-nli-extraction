<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time Emotion Recognition via Attention Gated Hierarchical Memory Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiang</forename><surname>Jiao</surname></persName>
							<email>wxjiao@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<region>N.T</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
							<email>lyu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<region>N.T</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
							<email>king@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<region>N.T</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-Time Emotion Recognition via Attention Gated Hierarchical Memory Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-time emotion recognition (RTER) in conversations is significant for developing emotionally intelligent chatting machines. Without the future context in RTER, it becomes critical to build the memory bank carefully for capturing historical context and summarize the memories appropriately to retrieve relevant information. We propose an Attention Gated Hierarchical Memory Network (AGHMN) to address the problems of prior work: (1) Commonly used convolutional neural networks (CNNs) for utterance feature extraction are less compatible in the memory modules; (2) Unidirectional gated recurrent units (GRUs) only allow each historical utterance to have context before it, preventing information propagation in the opposite direction; (3) The Soft Attention for summarizing loses the positional and ordering information of memories, regardless of how the memory bank is built. Particularly, we propose a Hierarchical Memory Network (HMN) with a bidirectional GRU (BiGRU) as the utterance reader and a BiGRU fusion layer for the interaction between historical utterances. For memory summarizing, we propose an Attention GRU (AGRU) where we utilize the attention weights to update the internal state of GRU. We further promote the AGRU to a bidirectional variant (BiAGRU) to balance the contextual information from recent memories and that from distant memories. We conduct experiments on two emotion conversation datasets with extensive analysis, demonstrating the efficacy of our AGHMN models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Emotion recognition is a significant research topic because of the potential application in developing empathetic machines in present artificial intelligence (AI) era. We focus on the real-time scenario to detect the emotion state of speakers in an ongoing conversation at utterance-level. According to <ref type="bibr" target="#b8">(Olson 1977)</ref>, an utterance is a unit of speech bounded by breathes and pauses. We term this task as Real-Time Emotion Recognition (RTER). Inherently, emotion recognition is a multi-modal learning task that could involve text, video and audio features, but the text feature plays the most significant role <ref type="bibr" target="#b1">(Chen et al. 1998;</ref><ref type="bibr" target="#b12">Poria, Cambria, and Gelbukh</ref> Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>Utterance <ref type="bibr">[oth]</ref> [sur]</p><p>[neu]</p><p>[joy]</p><p>[sad] t t-1 t-2 t-3 t-4 Emotion <ref type="figure">Figure 1</ref>: The specification of the RTER task. 2015; <ref type="bibr" target="#b9">Poria et al. 2017;</ref><ref type="bibr" target="#b3">Hazarika et al. 2018b)</ref>. Thus, in this paper, we tackle the RTER task in text conversations.</p><p>Without the future context, in RTER, it becomes critical to exploit the contextual information from the historical utterances. For this purpose, one needs to take good care of two factors, i.e., the memory bank for capturing historical context, and the summarizing technique for the query to extract relevant information from the memory bank. The memory bank is usually built in a two-level fashion to simulate the hierarchical structure of conversations, i.e., words-to-utterance and utterances-to-conversation. Specifically, existing models <ref type="bibr" target="#b2">(Hazarika et al. 2018a;</ref><ref type="bibr" target="#b3">Hazarika et al. 2018b;</ref>) obtain their memory banks by utilizing convolutional neural networks (CNNs) to learn utterance features and unidirectional gated recurrent units (GRUs) <ref type="bibr" target="#b1">(Cho et al. 2014)</ref> to capture relationship of utterances. However, through our exploration, we find that a bidirectional GRU (BiGRU) learns better utterance features than commonly used CNNs. Moreover, the unidirectional GRU only allows each historical utterance to have context before but not after it, which may prevent information propagation in the opposite direction. As for the summarizing techniques, the commonly used Soft Attention produces a weighted sum of the memories, which can be regarded as a bag-of-memories. Just as the bag-of-words in word representation area <ref type="bibr" target="#b7">(Mikolov et al. 2013</ref>) that lacks sensitivity to word order <ref type="bibr" target="#b6">(Ling et al. 2015)</ref>, the bag-of-memories loses the positional and ordering information of the memories, regardless of how the memory bank is built.</p><p>Incorporating these factors, in this paper, we propose an Attention Gated Hierarchical Memory Network (AGHMN) to better extract the utterance features and the contextual information for the RTER task. Specifically, we summarize our contributions as below: (1) We propose a Hierarchical Memory Network (HMN) to improve the utterance features and the memory bank for extracting contextual information. The HMN is essentially a two-level GRU encoder, including an utterance reader and a fusion layer. The utterance reader applies a BiGRU to model the word sequence of each utterance, which we demonstrate that it is more compatible with the hierarchical structure. The fusion layer adopts a BiGRU to read the historical utterances, which enables sufficient interaction between them. (2) We propose an Attention GRU (AGRU) to retain the positional and ordering information while summarizing the memories, and promote it to its bidirectional variant, i.e., BiAGRU, to capture more comprehensive context. The AGRU is formed by utilizing the attention weights of the query to the memories to update the internal state of a normal GRU. The final hidden state of AGRU serves as the contextual vector to help refine the representation of the query. The BiAGRU is dedicated to balancing the information from recent memories and that from distant memories. <ref type="formula">(3)</ref> We conduct experiments on two emotion conversation datasets with extensive analysis, demonstrating the efficacy of our proposed AGHMN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Text Classification. Text-based emotion recognition is usually treated as a text classification problem. Previously proposed methods can be mainly divided into three categories: keyword-based methods <ref type="bibr" target="#b15">(Wilson, Wiebe, and Hwa 2004)</ref>, learning-based methods <ref type="bibr" target="#b17">(Yang, Lin, and Chen 2007)</ref>, and hybrid methods <ref type="bibr" target="#b16">(Wu, Chuang, and Lin 2006)</ref>. Nowadays, deep learning is dominating the text classification area due to its powerful capability in learning latent features. Representative methods include convolutional neural network (CNN) <ref type="bibr" target="#b6">(Kim 2014)</ref>, recurrent neural network (RNN) (Abdul-Mageed and Ungar 2017), and hierarchical attention network (HAN) <ref type="bibr" target="#b14">(Tang, Qin, and Liu 2015)</ref>. These work are customized for data unit without context, e.g. independent reviews or documents.</p><p>Context-Dependent Models. Recognizing emotion state of speakers in conversations requires that the query should take into account the context to carry accurate information. Existing work can be divided into two streams: the static models, and the dynamic models. The static models include sequence-based and graph-based <ref type="bibr" target="#b18">(Zhang et al. 2019;</ref><ref type="bibr" target="#b2">Ghosal et al. 2019)</ref>, the former of which let each utterance to have the utterances both in the history and the future as context. Among sequence-based static models, cLSTM <ref type="bibr" target="#b9">(Poria et al. 2017)</ref> only adopts long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber 1997) to cap-ture the sequential relationship between the utterances. Hi-GRU <ref type="bibr" target="#b5">(Jiao et al. 2019</ref>) employs a self-attention mechanism for context weighting and summarizing, as well as a residual connection for feature fusion. <ref type="bibr">BiDialogueRNN (Majumder et al. 2019</ref>) is built on RNNs that keeps track of the individual party states throughout the conversation and uses this information for emotion recognition. These static models may adapt to the RTER task if we take their unidirectional variants. The dynamic models read the utterances in the order as they are generated so that each incoming utterance, i.e., the query, only depends on the historical utterances. These models include CMN <ref type="bibr" target="#b3">(Hazarika et al. 2018b)</ref>, DialogueRNN, and ICON <ref type="bibr" target="#b2">(Hazarika et al. 2018a</ref>). Among them, CMN and ICON are customized for dyadic conversations, which incorporate memory networks <ref type="bibr" target="#b13">(Sukhbaatar et al. 2015)</ref> to refine the contextual information and also consider the self-and inter-speaker emotional influence.</p><p>Our AGHMN models differ from these approaches in that we aim to produce better utterance features and memory representation by our proposed HMN and summarize the memories in a better way by our proposed AGRU and Bi-AGRU. We do not distinguish speakers explicitly as in Di-alogueRNN but find that the model itself can recognize the difference between speakers (see Case Study).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Specification</head><p>We first specify the task of Real-Time Emotion Recognition (RTER) as below:</p><p>Real-Time Emotion Recognition. Suppose that a conversation has proceeded for t turns so far with the utterance sequence C t = {u 1 , · · · , u t }, the t-th utterance is the query utterance q, and the others are historical ones. As illustrated in <ref type="figure">Fig 1,</ref> each utterance expresses a major emotion e among a set of emotions E, such as joy, sadness, and neutral. Our goal is to design and train a model M to predict the emotion expressed by q conditioned on the historical utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>In this section, we will introduce the AGHMN model as illustrated in <ref type="figure" target="#fig_0">Fig 2,</ref> which consists of a Word Embedding Layer, a Hierarchical Memory Network, an Attention GRU, and a Classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Embedding Layer</head><p>For an utterance in C t composed by a sequence of words <ref type="bibr">t]</ref>, N is the length of the utterance, and w n ∈ u t is the index of the word in the vocabulary. The utterance is fed into the word embedding layer to get a dense vector x ∈ R dw for each word, where d w is the size of the word vector. The weights of the word embedding layer are initialized by the publicly available 300-dimensional word2vec <ref type="bibr" target="#b7">(Mikolov et al. 2013</ref>) vectors 1 trained on 100 billion words from Google News. The words not included in the word2vec vocabulary will be initialized by randomly generated vectors. </p><formula xml:id="formula_0">u t = {w 1 , w 2 , · · · , w N }, where t ∈ [1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Memory Network</head><p>The Hierarchical Memory Network (HMN) is a two-level encoder, the lower one as an utterance reader and the upper one as the fusion layer.</p><p>Utterance Reader. Though current work <ref type="bibr" target="#b2">(Hazarika et al. 2018a;</ref><ref type="bibr" target="#b3">Hazarika et al. 2018b;</ref>) always utilize CNN to extract utterance features, we decide to adopt a BiGRU. The BiGRU is able to model the word sequence while gather the contextual information for each word in two directions, making it better for understanding a sentence sufficiently. Actually, we find that the BiGRU performs much better than a commonly used 1-D CNN as the utterance reader (see <ref type="table" target="#tab_4">Table 4</ref>). Specifically, the BiGRU takes an utterance represented by word vectors X = {x n } N n=1 as input:</p><formula xml:id="formula_1">− → h n = −−−→ GRU(x n , − → h n−1 ), (1) ← − h n = ←−−− GRU(x n , ← − h n+1 ),<label>(2)</label></formula><p>where − → h n , ← − h n ∈ R d1 are the hidden states of the forward GRU and the backward one, respectively, and d 1 is the hidden size. The hidden states of both directions are concatenated and fed to the max-over-time pooling layer. The resulted vector ← → h is then transformed to be the utterance embedding u ∈ R d1 through a tanh layer:</p><formula xml:id="formula_2">← → h = maxpool({[ − → h n , ← − h n ]} N n=1 ) (3) u = tanh(W u ← → h + b u ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_3">W u ∈ R d1×2d1 , and b u ∈ R d1 .</formula><p>Fusion Layer. At t-step, the representation of the query comes from the utterance encoder: q t = u t . For each query q t , we build a memory bank M t based on the most recent K historical utterances. Since the K utterances maintain a sequence, we hope to make them interact with each other so as to refine the memory representation. As shown in <ref type="figure" target="#fig_0">Fig 2,</ref> we consider two types of memory banks here:</p><p>• Unidirectional Fusion (UniF). Firstly, we utilize a unidirectional GRU to read these K utterances to model the sequential relationship between them. The independent utterance embeddings {u t−K−1+k } K k=1 are fed to the GRU, and are then connected to the output of the GRU to form the memory bank:</p><formula xml:id="formula_4">M t = { −−−→ GRU(u t−K−1+k ) + u t−K−1+k } K k=1 . • Bidirectional Fusion (BiF).</formula><p>The UniF memory bank only allows each memory to have the context before it but not after it, which may prevent information propagation from the opposite direction. To address such a problem, we propose to read the K utterances through a BiGRU, and combine the output and the input to form the memory bank:</p><formula xml:id="formula_5">M t = { ← −− → GRU(u t−K−1+k ) + u t−K−1+k } K k=1 .</formula><p>For simplicity, we use ← −− → GRU to denote the sum of the hidden states in the two directions of the BiGRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention GRU</head><p>Generally, the query in a conversation depends on the context it follows. Thus, it is crucial to weight and summarize the context to refine the representation of the query. This process is usually realized by an attention layer <ref type="bibr" target="#b2">(Hazarika et al. 2018a;</ref><ref type="bibr" target="#b3">Hazarika et al. 2018b)</ref>, which allows the query to interact with the memory bank and produce a contextual vector c t .</p><p>Context Weighting. The attention layer should be able to retrieve relevant context from the memory bank to help predict the emotion expressed by the query. To weight the importance of each memory to the query, we adopt the dotproduct attention with a softmax normalization. As a result, at t-step, the weight of the kth memory M t,k will be:</p><formula xml:id="formula_6">a k = exp(q t M t,k ) K k =1 exp(q t M t,k ) .<label>(5)</label></formula><p>Context Summarizing. Conventionally, the contextual vector c t can be produced by Soft Attention as a weighted sum of the memories, i.e., c t = K k=1 a k M t,k . This method is efficient for computing, but just as the bag-ofwords in word representation area <ref type="bibr" target="#b7">(Mikolov et al. 2013;</ref><ref type="bibr" target="#b6">Ling et al. 2015)</ref> it loses both the positional and ordering information of the memories. Thus, we propose an Attention GRU (AGRU) which uses the attention weight of the query to the memories to update the internal stateh t of a normal GRU. As a result, the output of the AGRU is:</p><formula xml:id="formula_7">h k = a k •h k + (1 − a k ) • h k−1 .<label>(6)</label></formula><p>The GRU is advantageous for retaining the positional and ordering information of the memories and the attention weight controls the amount of information to be passed to the next step. We take the final hidden state of the AGRU as the contextual vector, i.e., c t = h K , then the refined query representation, i.e. the output, will be:</p><formula xml:id="formula_8">o t = q t + c t .<label>(7)</label></formula><p>Furthermore, considering the tendency of RNNs to better represent recent inputs (Bahdanau, Cho, and Bengio 2015), the contextual vector from AGRU also tends to carry more information of the most recent memories. Accordingly, a backward AGRU can better represent memories distant from the query. Therefore, we promote AGRU to its bidirectional variant, i.e., BiAGRU, so as to make a balance between the information from recent memories and that from distant memories. We believe the BiAGRU is capable of capturing more comprehensive context from the memory bank, especially for long conversations. As a result, the contextual vectors produced by a BiAGRU are expressed as:</p><formula xml:id="formula_9">c f t = − −−−− → AGRU(M t,K , a K , − → h K−1 ),<label>(8)</label></formula><formula xml:id="formula_10">c b t = ← −−−− − AGRU(M t,1 , a 1 , ← − h 2 ),<label>(9)</label></formula><p>which are used to refine the query representation similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier</head><p>The refined representation of the query from the AGRU is used for prediction by a softmax layer: where</p><formula xml:id="formula_11">y t = softmax(W o o t + b o ),<label>(10)</label></formula><formula xml:id="formula_12">W o ∈ R d1×|E| , b o ∈ R |E| ,</formula><p>and |E| is the number of emotion classes.</p><p>We train the AGHMN model by a cross-entropy loss function, and the average loss of the whole training set can be computed as:</p><formula xml:id="formula_13">L = 1 L l=1 T l T l t=1 |E| e=1 y e t log(ŷ e t ),<label>(11)</label></formula><p>where T l is the number of utterances in the lth conversation, and L is the total number of conversations in the training set. y t denotes the one-hot vector of the target emotion labels, and y e t andŷ e t are the elements of y t andŷ t for the emotion class e, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>In this section, we will present the details of our experimental setup, including datasets, compared methods, implementation, and training.</p><p>Datasets. We train and test our model on two conversation emotion datasets, namely, IEMOCAP (Busso et al. 2008), and MELD <ref type="bibr" target="#b10">(Poria et al. 2019a</ref>).</p><p>• IEMOCAP 2 : The IEMOCAP dataset contains the acts of 10 speakers in a dyadic conversation fashion, providing text, audio, and video features. We follow the previous work <ref type="bibr" target="#b2">(Hazarika et al. 2018a)</ref> to use the first four sessions of transcripts as the training set, and the last one as the testing set. The validation set is extracted from the randomlyshuffled training set with the ratio of 80:20. Also, we focus on recognizing six emotion classes, namely, happy, sad, neutral, angry, excited, and frustrated.</p><p>• MELD 3 : The MELD dataset <ref type="bibr" target="#b10">(Poria et al. 2019a</ref>) is an extended version of the EmotionLines dataset <ref type="bibr" target="#b4">(Hsu et al. 2018)</ref>. The data comes from the Friends TV series with multiple speakers involved in the conversations. It is split into training, validation, and testing sets with 1039, 114, and 280 conversations, respectively. Each utterance has been labelled by one of the seven emotion types, namely, anger, disgust, sadness, joy, neutral, surprise and fear. Compared Methods. With the different combination of memory banks and AGRUs, we consider four variants of AGHMN 4 for experiments: UniF-AGRU, UniF-BiAGRU, BiF-AGRU, and BiF-BiAGRU. These variants are compared to the following baselines:   <ref type="table">Table 3</ref>: Performance of AGHMN models on MELD.</p><p>• scLSTM <ref type="bibr" target="#b9">(Poria et al. 2017</ref>) is the unidirectional variant that classifies utterances using historical utterance as context realized by a LSTM.</p><p>• CMN <ref type="bibr" target="#b3">(Hazarika et al. 2018b</ref>) models separate contexts for both speaker and listener to an utterance. These contexts are stored as memories to aid the prediction of an incoming utterance.</p><p>• <ref type="figure" target="#fig_0">DialogueRNN (Majumder et al. 2019)</ref> is the unidirectional variant with an attention layer that keeps track of the individual party states throughout the conversation and uses this information for emotion classification.</p><p>• ICON <ref type="bibr" target="#b2">(Hazarika et al. 2018a</ref>) incorporates the self-and inter-speaker emotional influences into global memories to help predict the emotional orientation of utterances. It is a unidirectional model with only historical context.</p><p>For IEMOCAP, we refer to the results of memnet <ref type="bibr" target="#b13">(Sukhbaatar et al. 2015)</ref>, <ref type="bibr">CMN, DialogueRNN from (Majumder et al. 2019)</ref>, and that of ICON from ♦ (Hazarika et al. 2018a). For MELD, we refer to the results of CNN from † <ref type="bibr" target="#b11">(Poria et al. 2019b)</ref>. We re-run scLSTM and DialogueRNN for both datasets. CMN and ICON cannot be adapted for MELD because they are customized for dyadic conversations and may encounter scalability issue for multiparty conversation datasets <ref type="bibr" target="#b11">(Poria et al. 2019b)</ref>.</p><p>Implementation. We implement scLSTM and our proposed AGHMN models from scratch on the Pytorch 5 framework. For the extraction of textual feature, we follow <ref type="bibr" target="#b2">(Hazarika et al. 2018a)</ref> to adopt a 1-D CNN with filters of 3, 4, and 5 each with 64 feature maps. The convolution result of each filter is 5 https://pytorch.org/ fed to a max-over-time pooling layer. The pooling results are concatenated and transformed to the utterance embeddings via a relu layer. The hidden size of the LSTM is 100. As for our AGHMN models, the hidden sizes of GRUs and AGRUs are also 100. By default, the context-window size K for the memory bank is 40 for IEMOCAP and 10 for MELD, which are around the average conversation lengths of each dataset, respectively. Besides, the implementation of DialogueRNN comes from the open-source codes 6 provided by the authors of DialogueRNN.</p><p>Training. We choose Adam (Kingma and Ba 2015) optimizer with an initial learning rate lr = 5 × 10 −4 . To regulate the models, we clip the gradients of model parameters with a max norm of 5 and apply dropout with a drop rate of 0.3. We monitor the macro-averaged F1-score (mF1) of the validation sets during training and decay the learning rate by 0.95 once the mF1 stops increasing. The training process is terminated by early stopping with a patience of 10. <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table">Table 3</ref> present the results on IEMOCAP and MELD testing sets, respectively. For both datasets, we report the accuracy and F1-score (Tong et al. 2017) for each emotion class and evaluate the overall classification performance using their weighted averages of all emotion classes. We also report the macro-average of F1-score (mF1) to reflect the model performance on minority emotion classes,  since the weighted-average is compromised by the majority classes. Each result provided by us in the tables is the average value of 10 times repeated experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, all of our AGHMN models perform better than the compared models. BiF-AGRU attains the best overall performance with significant improvement over the strongest baseline DialogueRNN (+4.2% Acc, +3.7% F1, +5.1% mF1). For each emotion, our ADHMN models achieve at least competitive performance as DialogueRNN . In particular, our models attain very large improvement on happy (at least +11.5% Acc, +17.3% F1), which is the emotion with the least utterances. This demonstrates the capability of our models in recognizing minority emotion classes.</p><p>In <ref type="table">Table 3</ref>, all the AGHMN models also outperform the compared methods significantly. But this time, UniF-BiAGRU becomes the best one (+2.8% Acc, +2.2% F1, +3.3% mF1). Our models perform the best on most emotion classes, especially the two minority classes fear and disgust, though this is accompanied by the performance degradation on anger. But referring to the mF1 value, it is safe to say that our models produce much more balanced results.</p><p>Baseline Methods. The bcLSTM model implemented by us is very strong, performing better than all the other baselines on both datasets except DialogueRNN on IEMOCAP. However, the results of DialogueRNN run by us are slightly worse than DialogueRNN . This may be because we follow the default settings of the provided codes, which are customized for BiDialogueRNN. No matter what, our AGHMN models outperform them on the two datasets, suggesting the efficacy of our context-modeling scheme.</p><p>AGHMN variants. UniF-AGRU performs the worst among all the four variants on both datasets. Consistently, UniF-BiAGRU and BiF-AGRU outperform UniF-AGRU, which demonstrates the superiority of BiAGRU over AGRU and BiF over UniF, respectively. However, BiF-BiAGRU does not attain the best performance, which we speculate that the model becomes too deep to learn from the two datasets. In fact, the performance difference between the variants on MELD is limited. This is mainly because the conversations in MELD contain much fewer turns than that in IEMOCAP, making it less sensitive to different modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Analysis</head><p>Utterance Readers. We argue that the BiGRU is a better reader for utterances. Here, we test UniF-BiAGRU and BiF-AGRU on IEMOCAP with the utterance reader replaced by    <ref type="table" target="#tab_4">Table 4</ref>, where we can easily conclude that the RNN variants (BiLSTM, and BiGRU) surpass 1-D CNN with significant margins. For BiLSTM and BiGRU, the latter one performs better because GRUs are usually more powerful than LSTMs on small datasets. These results indicate that RNNs are more compatible in this Hierarchical Memory Network. Moreover, as BiGRU, BiL-STM attains better performance in BiF-AGRU than in UniF-BiAGRU, suggesting the advantage of BiF. In addition, we also conduct experiments for the CNN reader with Soft Attention, named as CNN sof t , which encounters the degradation of performance. It demonstrates that the improvement of our models is not achieved by only the BiGRU reader.</p><p>Attention Choices &amp; Memory Banks. We investigate the advantage of AGRU over Soft Attention here. As presented in <ref type="table" target="#tab_6">Table 5</ref>, with the UniF memory bank, AGRU attains better results than Soft Attention on both datasets. The bidirectional variant of AGRU extends the advantage even further. It is noteworthy that we also include the results of Soft Attention with the BiF memory, which are considerably better than that with UniF. This further verifies that BiF can produce better memory representation.</p><p>Context-Window Size. We plot the performance trend of UniF-BiAGRU and BiF-AGRU on both datasets when varying the context-window size K for building the memory bank. On both datasets, the two models follow similar trends such that the performance increases at first and falls then with the increase of K. On IEMOCAP, the best results are obtained at K = 40 for BiF-AGRU and K = 50 for UniF-BiAGRU, which align with the average length of conversa-  [ang]</p><p>[fru]</p><p>[fru] tions in the dataset (see <ref type="table">Table 1</ref>). On MELD, the best values of K are 5 and 10, respectively, which are much lower than that on IEMOCAP. We speculate that it is because the data of MELD comes from the Friends TV sitcom with more rapid emotion fluctuation. Therefore, a longer context may result in model confusion. In contrast, the emotion state in IEMO-CAP evolves much more gently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>Attention Evolution. We find that the selection of memory differs between speakers as a conversation develops, though we do not distinguish the speakers explicitly in our model. In <ref type="figure" target="#fig_2">Fig 4,</ref> we visualize the attention weights of BiF-AGRU tested on a conversation fragment of IEMOCAP and MELD, respectively. For IEMOCAP, the Male is excited from the beginning so that the four utterances presented here pay the most attention to the first utterance of the conversation. In contrast, the attention of the Female is distributed over several historical utterances, including the first one and some intermediate ones (see <ref type="figure" target="#fig_3">Fig 5)</ref> that make her frustrated. As for MELD, Joey is joyful all the time and his attention is paid to his last joyful utterance. Monica pays her most attention to the conversation between her and Joey, providing clues for Chandler. Chandler focuses on Joey and where Joey is.</p><p>Attention Comparison. The selection of memory also varies between different attention mechanisms. In <ref type="figure" target="#fig_3">Fig 5,</ref> given a query utterance expressed by the Female with frustrated emotion, Soft Attention focuses on the utterance that also expresses frustrated. AGRU pays most attention to one utterance that could be the reason for the Female's frustrated emotion, but it classifies the query emotion as angry in this example. BiAGRU can sense both kinds of clues, providing more comprehensive memory. BiF improves the memory representation and helps AGRU to extract the memory as comprehensively as UniF-BiAGRU.</p><p>Error Analysis. In <ref type="figure" target="#fig_1">Fig 4, the 33rd</ref> utterance of IEMOCAP is recognized as frustrated not neutral. We argue that the original annotation might not be accurate. Given several turns of frustrated and the latest response (34th) from the Male, the Female could express frustrated again. Still, we cannot deal with minority classes very well on MELD, such as disgust.</p><p>With more data or multimodal features to disambiguate with other emotions, this issue might be better resolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We propose an Attention Gated Hierarchical Memory Network (AGHMN) for Real-Time Emotion Recognition. Firstly, the proposed Hierarchical Memory Network improves the quality of utterance features and memories. Then, the proposed Attention GRU summarizes better contextual information than the commonly used Soft Attention. We conduct extensive experiments on two emotion conversation datasets, and our models outperform the state-of-the-art approaches with significant margins. Lastly, ablation studies and attention visualization demonstrate the efficacy of each component of our AGHMN models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of our AGHMN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Trends in the performance of UniF-BiAGRU and BiF-AGRU with varying context-window size K. a 1-D CNN and a bidirectional LSTM (BiLSTM), respectively. The results are reported in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Evolution of memory selection as a conversation develops. The attention weights come from BiF-AGRU for both IEMOCAP and MELD. Each utterance is tagged with two labels, the first is ground truth and the second is the prediction by BiF-AGRU. IEMOCAP: M: Male, F: Female; MELD: J: Joey, M: Monica, C: Chandler.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of memory selection between UniF-Soft and UniF-AGRU, UniF-BiAGRU, and BiF-AGRU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of AGHMN models on IEMOCAP.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">MELD: Emotion Classes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">neutral</cell><cell cols="2">surprise</cell><cell cols="2">fear</cell><cell cols="2">sadness</cell><cell></cell><cell>joy</cell><cell cols="2">disgust</cell><cell cols="2">anger</cell><cell></cell><cell>Avg.</cell></row><row><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>mF1</cell></row><row><cell>CNN  †</cell><cell>-</cell><cell>74.9</cell><cell>-</cell><cell>45.5</cell><cell>-</cell><cell>3.7</cell><cell>-</cell><cell>21.1</cell><cell>-</cell><cell>49.4</cell><cell>-</cell><cell>8.2</cell><cell>-</cell><cell>34.5</cell><cell>-</cell><cell cols="2">55.0 33.9</cell></row><row><cell>scLSTM</cell><cell cols="5">78.4 73.8 46.8 47.7 3.8</cell><cell cols="6">5.4 22.4 25.1 51.6 51.3 4.3</cell><cell cols="6">5.2 36.7 38.4 57.5 55.9 35.3</cell></row><row><cell>DialogueRNN</cell><cell cols="5">72.1 73.5 54.4 49.4 1.6</cell><cell cols="6">1.2 23.9 23.8 52.0 50.7 1.5</cell><cell cols="6">1.7 41.9 41.5 56.1 55.9 34.5</cell></row><row><cell>UniF-AGRU</cell><cell cols="17">80.3 75.1 53.7 49.1 9.8 10.6 19.7 25.5 50.5 51.1 14.0 16.4 33.9 38.2 58.8 57.0 38.0</cell></row><row><cell cols="18">UniF-BiAGRU 83.4 76.4 49.1 49.7 9.2 11.5 21.6 27.0 52.4 52.4 12.2 14.0 34.9 39.4 60.3 58.1 38.6</cell></row><row><cell>BiF-AGRU</cell><cell cols="17">81.6 75.5 50.5 49.0 8.0 10.4 22.3 26.9 51.7 51.7 13.7 17.2 34.5 38.6 59.5 57.5 38.5</cell></row><row><cell>BiF-BiAGRU</cell><cell cols="17">80.7 75.4 50.7 47.9 7.2 10.0 20.9 26.3 53.9 52.1 10.4 12.8 33.9 38.1 59.2 57.1 37.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Testing results on IEMOCAP, with CNN, BiLSTM, and BiGRU as the utterance reader, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Testing results on IEMOCAP and MELD, with different choices of attention and memory bank.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://code.google.com/archive/p/word2vec/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://sail.usc.edu/iemocap/ 3 https://github.com/SenticNet/MELD 4 https://github.com/wxjiao/AGHMN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/SenticNet/conv-emotion/tree/master/ DialogueRNN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14208815 and No. CUHK 14210717 of the General Research Fund). We thank the three anonymous reviewers for their insightful suggestions on various aspects of this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emonet: Fine-grained emotion detection with gated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ungar ; Abdul-Mageed</forename><surname>Abdul-Mageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<meeting><address><addrLine>Busso, C</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
	<note>IEMOCAP: interactive emotional dyadic motion capture database</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>FG</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation</title>
		<idno>abs/1908.11540</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
	<note>ICON: interactive conversational memory network for multimodal emotion detection</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hazarika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotionlines: An emotion corpus of multiparty conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Higru: Hierarchical gated recurrent units for utterance-level emotion recognition</title>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Not all contexts are created equal: Better word representations with variable attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6818" to="6825" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From utterance to text: The bias of language in speech and writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard educational review</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="281" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextdependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MELD: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emotion recognition in conversation: Research challenges, datasets, and recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="100943" to="100953" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cambria</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2539" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sukhbaatar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Liu ; Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<idno>Tong et al. 2017</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1547" to="1556" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Just how mad are you? finding strong and weak opinion clauses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wiebe</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwa</forename><forename type="middle">;</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emotion recognition from text using semantic labels and separable mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin ;</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TALIP</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="183" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Emotion classification using web blog corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen ;</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="275" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling both context-and speakersensitive dependence for emotion detection in multi-speaker conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5415" to="5421" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
