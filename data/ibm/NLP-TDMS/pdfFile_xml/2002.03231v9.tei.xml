<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Soft Threshold Weight Reparameterization for Learnable Sparsity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Ramanujan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Somani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
						</author>
						<title level="a" type="main">Soft Threshold Weight Reparameterization for Learnable Sparsity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus of maximizing prediction accuracy given an overall parameter budget. Existing methods rely on uniform or heuristic non-uniform sparsity budgets which have sub-optimal layer-wise parameter allocation resulting in a) lower prediction accuracy or b) higher inference cost (FLOPs). This work proposes Soft Threshold Reparameterization (STR), a novel use of the soft-threshold operator on DNN weights. STR smoothly induces sparsity while learning pruning thresholds thereby obtaining a non-uniform sparsity budget. Our method achieves state-of-the-art accuracy for unstructured sparsity in CNNs (ResNet50 and Mo-bileNetV1 on ImageNet-1K), and, additionally, learns non-uniform budgets that empirically reduce the FLOPs by up to 50%. Notably, STR boosts the accuracy over existing results by up to 10% in the ultra sparse (99%) regime and can also be used to induce low-rank (structured sparsity) in RNNs. In short, STR is a simple mechanism which learns effective sparsity budgets that contrast with popular heuristics. Code, pretrained models and sparsity budgets are at https://github.com/RAIVNLab/STR. 1 One Multiply-Add is counted as one FLOP arXiv:2002.03231v9 [cs.LG] 22 Jun 2020 Soft Threshold Weight Reparameterization for Learnable Sparsity</p><p>Motivated by the above-mentioned challenges, this works addresses the following question: "Can we design a method to learn non-uniform sparsity budget across layers that is optimized per-layer, is stable, and is accurate?".</p><p>Most existing methods for learning sparse DNNs have their roots in the long celebrated literature of high-dimension statistics and, in particular, sparse regression. These methods are mostly based on well-known Hard and Soft Thresholding techniques, which are essentially projected gradient methods with explicit projection onto the set of sparse parameters. However, these methods require a priori knowledge of sparsity, and as mentioned above, mostly heuristic methods are used to set the sparsity levels per layer.</p><p>We propose Soft Threshold Reparameterization (STR) to address the aforementioned issues. We use the fact that the projection onto the sparse sets is available in closed form and propose a novel reparameterization of the problem. That is, for forward pass of DNN, we use soft-thresholded version (Donoho, 1995) of a weight tensor W l of the l-th layer in the DNN: S(W l , α l ) := sign (W l )·ReLU(|W l |− α l ) where α l is the pruning threshold for the l-th layer. As the DNN loss can be written as a continuous function of α l 's, we can use backpropagation to learn layer-specific α l to smoothly induce sparsity. Typically, each layer in a neural network is distinct unlike the interchangeable weights and neurons making it interesting to learn layer-wise sparsity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Neural Networks (DNNs) are the state-of-the-art models for many important tasks in the domains of Computer Vision, Natural Language Processing, etc. To enable highly accurate solutions, DNNs require large model sizes resulting in huge inference costs, which many times become the main * Equal contribution 1 University of Washington, USA 2 Allen Institute for Artificial Intelligence, USA 3 Microsoft Research, India. Correspondence to: Aditya Kusupati &lt;kusu-pati@cs.washington.edu&gt;.</p><p>Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). bottleneck in the real-world deployment of the solutions. During inference, a typical DNN model stresses the following aspects of the compute environment: 1) RAM -working memory, 2) Processor compute -Floating Point Operations (FLOPs 1 ), and 3) Flash -model size. Various techniques are proposed to make DNNs efficient including model pruning (sparsity) <ref type="bibr" target="#b19">(Han et al., 2015)</ref>, knowledge distillation <ref type="bibr" target="#b7">(Bucilu et al., 2006)</ref>, model architectures <ref type="bibr">(Howard et al., 2017)</ref> and quantization <ref type="bibr" target="#b25">(Rastegari et al., 2016)</ref>. Sparsity of the model, in particular, has potential for impact across a variety of inference settings as it reduces the model size and inference cost (FLOPs) without significant change in training pipelines. Naturally, several interesting projects address inference speed-ups via sparsity on existing frameworks <ref type="bibr">(Liu et al., 2015;</ref><ref type="bibr" target="#b13">Elsen et al., 2019)</ref> and commodity hardware <ref type="bibr">(Ashby et al.)</ref>. On-premise or Edge computing is another domain where sparse DNNs have potential for deep impact as it is governed by billions of battery limited devices with single-core CPUs. These devices, including mobile phones <ref type="bibr" target="#b2">(Anguita et al., 2012)</ref> and IoT sensors <ref type="bibr">(Patil et al., 2019;</ref><ref type="bibr" target="#b27">Roy et al., 2019)</ref>, can benefit significantly from sparsity as it can enable real-time on-device solutions.</p><p>Sparsity in DNNs, surveyed extensively in Section 2, has been the subject of several papers where new algorithms are designed to obtain models with a given parameter budget. But state-of-the-art DNN models tend to have a large number of layers with highly non-uniform distribution both in terms of the number of parameters as well as FLOPs required per layer. Most existing methods rely either on uniform sparsity across all parameter tensors (layers) or on heuristic non-uniform sparsity budgets leading to a suboptimal weight allocation across layers and can lead to a significant loss in accuracy. Furthermore, if the budget is set at a global level, some of the layers with a small number of parameters would be fully dense as their contribution to the budget is insignificant. However, those layers can have significant FLOPs, e.g., in an initial convolution layer, a simple tiny 3×3 kernel would be applied to the entire image. Hence, while such models might decrease the number of non-zeroes significantly, their FLOPs could still be large.</p><p>Due to layer-specific thresholds and sparsity, STR is able to achieve state-of-the-art accuracy for unstructured sparsity in CNNs across various sparsity regimes. STR makes even small-parameter layers sparse resulting in models with significantly lower inference FLOPs than the baselines. For example, STR for 90% sparse MobileNetV1 on ImageNet-1K results in a 0.3% boost in accuracy with 50% fewer FLOPs. Empirically, STR's learnt non-uniform budget makes it a very effective choice for ultra (99%) sparse ResNet50 as well where it is ∼10% more accurate than baselines on ImageNet-1K. STR can also be trivially modified to induce structured sparsity, demonstrating its generalizability to a variety of DNN architectures across domains. Finally, STR's learnt non-uniform sparsity budget transfers across tasks thus discovering an efficient sparse backbone of the model.</p><p>The 3 major contributions of this paper are: • Soft Threshold Reparameterization (STR), for the weights in DNNs, to induce sparsity via learning the per-layer pruning thresholds thereby obtaining a better non-uniform sparsity budget across layers. • Extensive experimentation showing that STR achieves the state-of-the-art accuracy for sparse CNNs (ResNet50 and MobileNetV1 on ImageNet-1K) along with a significant reduction in inference FLOPs. • Extension of STR to structured sparsity, that is useful for the direct implementation of fast inference in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section covers the spectrum of work on sparsity in DNNs. The sparsity in the discussion can be characterized as (a) unstructured and (b) structured while sparsification techniques can be (i) dense-to-sparse, and (ii) sparse-tosparse. Finally, the sparsity budget in DNNs can either be (a) uniform, or (b) non-uniform across layers. This will be a key focus of this paper, as different budgets result in different inference compute costs as measured by FLOPs. This section also discusses the recent work on learnable sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Unstructured and Structured Sparsity</head><p>Unstructured sparsity does not take the structure of the model (e.g. channels, rank, etc.,) into account. Typically, unstructured sparsity is induced in DNNs by making the parameter tensors sparse directly based on heuristics (e.g. weight magnitude) thereby creating sparse tensors that might not be capable of leveraging the speed-ups provided by commodity hardware during training and inference. Unstructured sparsity has been extensively studied and includes methods which use gradient, momentum, and Hessian based heuristics <ref type="bibr" target="#b14">(Evci et al., 2020;</ref><ref type="bibr">Lee et al., 2019;</ref><ref type="bibr">LeCun et al., 1990;</ref><ref type="bibr" target="#b20">Hassibi &amp; Stork, 1993;</ref><ref type="bibr" target="#b11">Dettmers &amp; Zettlemoyer, 2019)</ref>, and magnitude-based pruning <ref type="bibr" target="#b19">(Han et al., 2015;</ref><ref type="bibr" target="#b18">Guo et al., 2016;</ref><ref type="bibr" target="#b36">Zhu &amp; Gupta, 2017;</ref><ref type="bibr" target="#b15">Frankle &amp; Carbin, 2019;</ref><ref type="bibr" target="#b16">Gale et al., 2019;</ref><ref type="bibr">Mostafa &amp; Wang, 2019;</ref><ref type="bibr" target="#b6">Bellec et al., 2018;</ref><ref type="bibr">Mocanu et al., 2018;</ref><ref type="bibr">Narang et al., 2019;</ref><ref type="bibr">Kusupati et al., 2018;</ref><ref type="bibr" target="#b32">Wortsman et al., 2019)</ref>. Unstructured sparsity can also be induced by L 0 , L 1 regularization <ref type="bibr">(Louizos et al., 2018)</ref>, and Variational Dropout (VD) <ref type="bibr">(Molchanov et al., 2017)</ref>.</p><p>Gradual Magnitude Pruning (GMP), proposed in <ref type="bibr" target="#b36">(Zhu &amp; Gupta, 2017)</ref>, and studied further in , is a simple magnitude-based weight pruning applied gradually over the course of the training. Discovering Neural Wirings (DNW) <ref type="bibr" target="#b32">(Wortsman et al., 2019)</ref> also relies on magnitudebased pruning while utilizing a straight-through estimator for the backward pass. GMP and DNW are the state-of-theart for unstructured pruning in DNNs (especially in CNNs) demonstrating the effectiveness of magnitude pruning. VD gets accuracy comparable to GMP  for CNNs but at a cost of 2× memory and 4× compute during training making it hard to be used ubiquitously.</p><p>Structured sparsity takes structure into account making the models scalable on commodity hardware with the standard computation techniques/architectures. Structured sparsity includes methods which make parameter tensors lowrank <ref type="bibr">(Jaderberg et al., 2014;</ref><ref type="bibr" target="#b1">Alizadeh et al., 2020;</ref><ref type="bibr">Lu et al., 2016)</ref>, prune out channels, filters and induce block/group sparsity <ref type="bibr" target="#b35">(Liu et al., 2019;</ref><ref type="bibr" target="#b31">Wen et al., 2016;</ref><ref type="bibr">Li et al., 2017;</ref><ref type="bibr">Luo et al., 2017;</ref><ref type="bibr" target="#b17">Gordon et al., 2018;</ref><ref type="bibr" target="#b34">Yu &amp; Huang, 2019)</ref>. Even though structured sparsity can leverage speed-ups provided by parallelization, the highest levels of model pruning are only possible with unstructured sparsity techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dense-to-sparse and Sparse-to-sparse Training</head><p>Until recently, most sparsification methods were dense-tosparse i.e., the DNN starts fully dense and is made sparse by the end of the training. Dense-to-sparse training in DNNs encompasses the techniques presented in <ref type="bibr" target="#b19">(Han et al., 2015;</ref><ref type="bibr" target="#b36">Zhu &amp; Gupta, 2017;</ref><ref type="bibr">Molchanov et al., 2017;</ref><ref type="bibr" target="#b15">Frankle &amp; Carbin, 2019;</ref><ref type="bibr" target="#b26">Renda et al., 2020)</ref>.</p><p>The lottery ticket hypothesis <ref type="bibr" target="#b15">(Frankle &amp; Carbin, 2019)</ref> sparked an interest in training sparse neural networks end-toend. This is referred to as sparse-to-sparse training and a lot of recent work <ref type="bibr">(Mostafa &amp; Wang, 2019;</ref><ref type="bibr" target="#b6">Bellec et al., 2018;</ref><ref type="bibr" target="#b14">Evci et al., 2020;</ref><ref type="bibr">Lee et al., 2019;</ref><ref type="bibr" target="#b11">Dettmers &amp; Zettlemoyer, 2019)</ref> aims to do sparse-to-sparse training using techniques which include re-allocation of weights to improve accuracy.</p><p>Dynamic Sparse Reparameterization (DSR) (Mostafa &amp; Wang, 2019) heuristically obtains a global magnitude threshold along with the re-allocation of the weights based on the non-zero weights present at every step. Sparse Networks From Scratch (SNFS) <ref type="bibr" target="#b11">(Dettmers &amp; Zettlemoyer, 2019)</ref> utilizes momentum of the weights to re-allocate weights across layers and the Rigged Lottery (RigL) <ref type="bibr" target="#b14">(Evci et al., 2020)</ref> uses the magnitude to drop and the periodic dense gradients to regrow weights. SNFS and RigL are state-of-the-art in sparse-to-sparse training but fall short of GMP for the same experimental settings. It should be noted that, even though sparse-to-sparse can reduce the training cost, the existing frameworks <ref type="bibr">(Paszke et al., 2019;</ref><ref type="bibr" target="#b0">Abadi et al., 2016)</ref> consider the models as dense resulting in minimal gains.</p><p>DNW <ref type="bibr" target="#b32">(Wortsman et al., 2019)</ref> and Dynamic Pruning with Feedback (DPF) <ref type="bibr">(Lin et al., 2020)</ref> fall between both as DNW uses a fully dense gradient in the backward pass and DPF maintains a copy of the dense model in parallel to optimize the sparse model through feedback. Note that DPF is complementary to most of the techniques discussed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Uniform and Non-uniform Sparsity</head><p>Uniform sparsity implies that all the layers in the DNN have the same amount of sparsity in proportion. Quite a few works have used uniform sparsity , given its ease and lack of hyperparameters. However, some works keep parts of the model dense, including the first or the last layers <ref type="bibr">(Lin et al., 2020;</ref><ref type="bibr">Mostafa &amp; Wang, 2019;</ref><ref type="bibr" target="#b36">Zhu &amp; Gupta, 2017)</ref>. In general, making the first or the last layers dense benefits all the methods. GMP typically uses uniform sparsity and achieves state-of-the-art results.</p><p>Non-uniform sparsity permits different layers to have different sparsity budgets. Weight re-allocation heuristics have been used for non-uniform sparsity in DSR and SNFS. It can be a fixed budget like the ERK (Erdos-Renyi-Kernel) heuris-tic described in RigL <ref type="bibr" target="#b14">(Evci et al., 2020)</ref>. A global pruning threshold <ref type="bibr" target="#b19">(Han et al., 2015)</ref> can also induce non-uniform sparsity and has been leveraged in Iterative Magnitude Pruning (IMP) <ref type="bibr" target="#b15">(Frankle &amp; Carbin, 2019;</ref><ref type="bibr" target="#b26">Renda et al., 2020)</ref>. A good non-uniform sparsity budget can help in maintaining accuracy while also reducing the FLOPs due to a better parameter distribution. The aforementioned methods with non-uniform sparsity do not reduce the FLOPs compared to uniform sparsity in practice. Very few techniques like AMC <ref type="bibr" target="#b24">(He et al., 2018)</ref>, using expensive reinforcement learning, minimize FLOPs with non-uniform sparsity.</p><p>Most of the discussed techniques rely on intelligent heuristics to obtain non-uniform sparsity. Learning the pruning thresholds and in-turn learning the non-uniform sparsity budget is the main contribution of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Learnable Sparsity</head><p>Concurrent to our work, <ref type="bibr" target="#b28">(Savarese et al., 2019;</ref><ref type="bibr">Liu et al., 2020;</ref><ref type="bibr">Lee, 2019;</ref><ref type="bibr" target="#b33">Xiao et al., 2019;</ref><ref type="bibr" target="#b4">Azarian et al., 2020)</ref> have proposed learnable sparsity methods through training of the sparse masks and weights simultaneously with minimal heuristics. The reader is urged to review these works for a more complete picture of the field. Note that, while STR is proposed to induce layer-wise unstructured sparsity, it can be easily adapted for global, filter-wise, or per-weight sparsity as discussed in Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method -STR</head><p>Optimization under sparsity constraint on the parameter set is a well studied area spanning more than three decades <ref type="bibr" target="#b12">(Donoho, 1995;</ref><ref type="bibr" target="#b8">Candes et al., 2007;</ref><ref type="bibr">Jain et al., 2014)</ref>, and is modeled as:</p><formula xml:id="formula_0">min W L(W; D), s.t. W 0 ≤ k, where D := x i ∈ R d , y i ∈ R i∈[n]</formula><p>is the observed data, L is the loss function, W are the parameters to be learned and · 0 denotes the L 0 -norm or the number of non-zeros, and k is the parameter budget. Due to non-convexity and combinatorial structure of the L 0 norm constraint, it's convex relaxation L 1 norm has been studied for long time and has been at the center of a large literature on high-dimensional learning. In particular, several methods have been proposed to solve the two problems including projected gradient descent, forward/backward pruning etc.</p><p>Projected Gradient Descent (PGD) in particular has been popular for both the problems as the projection onto both L 0 as well as the L 1 ball is computable in almost closed form <ref type="bibr" target="#b5">(Beck &amp; Teboulle, 2009;</ref><ref type="bibr">Jain et al., 2014)</ref>; L 0 ball projection is called Hard Thresholding while L 1 ball projection is known as Soft Thresholding. Further, these methods have been the guiding principle for many modern DNN model pruning (sparsity) techniques <ref type="bibr" target="#b19">(Han et al., 2015;</ref><ref type="bibr" target="#b36">Zhu &amp; Gupta, 2017;</ref><ref type="bibr">Narang et al., 2019)</ref>.</p><p>However, projection-based methods suffer from the problem of dense gradient and intermediate parameter structure, as the gradient descent iterate can be arbitrarily out of the set and is then projected back onto L 0 or L 1 ball. At a scale of billions of parameters, computing such dense gradients and updates can be daunting. More critically, the budget parameter k is set at the global level, so it is not clear how to partition the budget for each layer, as the importance of each layer can be significantly different.</p><p>In this work, we propose a reparameterization, Soft Threshold Reparameterization (STR) based on the soft threshold operator <ref type="bibr" target="#b12">(Donoho, 1995)</ref>, to alleviate both the above mentioned concerns. That is, instead of first updating W via gradient descent and then computing its projection, we directly optimize over projected W. Let S g (W; s) be the projection of W parameterized by s and function g. S is applied to each element of W and is defined as:</p><formula xml:id="formula_1">S g (w, s) := sign (w) · ReLU(|w| − g(s)),<label>(1)</label></formula><p>where s is a learnable parameter, g : R → R, and α = g(s) is the pruning threshold. ReLU(a) = max(a, 0). That is, if |w| ≤ g(s), then S g (w, s) sets it to 0.</p><p>Reparameterizing the optimization problem with S modifies (note that it is not equivalent) it to:</p><formula xml:id="formula_2">min W L(S g (W, s), D).<label>(2)</label></formula><p>For L-layer DNN architectures, we divide W into:</p><formula xml:id="formula_3">W = [W l ] L l=1</formula><p>where W l is the parameter tensor for the l-th layer. As mentioned earlier, different layers of DNNs are unique can have significantly different number of parameters. Similarly, different layers might need different sparsity budget for the best accuracy. So, we set the trainable pruning parameter for each layer as s l . That is, s = [s 1 , . . . , s L ]. Now, using the above mentioned reparameterization for each W l and adding a standard L 2 regularization per layer, we get the following Gradient Descent (GD) update equation at the t-th step for W l , ∀ l ∈ [L]:</p><formula xml:id="formula_4">W (t+1) l ← (1 − η t · λ)W (t) l − η t ∇ Sg(W l ,s l ) L(S g (W (t) , s), D) ∇ W l S g (W l , s l ),<label>(3)</label></formula><p>where η t is the learning rate at the t-th step, and λ is the L 2 regularization (weight-decay) hyper-parameter.</p><formula xml:id="formula_5">∇ W l S g (W l , s l ) is the gradient of S g (W l , s l ) w.r.t. W l .</formula><p>Now, S is non-differentiable, so we use sub-gradient which leads to the following update equation:</p><formula xml:id="formula_6">W (t+1) l ← (1 − ηt · λ)W (t) l − ηt∇ Sg (W l ,s l ) L(Sg(W (t) , s), D) 1 Sg(W (t) l , s l ) = 0 ,<label>(4)</label></formula><p>where 1 {·} is the indicator function and A B denotes element-wise (Hadamard) product of tensors A and B.</p><p>Now, if g is a continuous function, then using the STR (2) and (1), it is clear that L(S g (W, s), D) is a continuous function of s. Further, sub-gradient of L w.r.t. s, can be computed and uses for gradient descent on s as well; see Appendix A.2. Algorithm 1 in the Appendix shows the implementation of STR on 2D convolution along with extensions to global, per-filter &amp; per-weight sparsity. STR can be modified and applied on the eigenvalues of a parameter tensor, instead of individual entries mentioned above, resulting in low-rank tensors; see Section 4.2.1 for further details. Note that s also has the same weight-decay parameter λ.</p><p>Naturally, g plays a critical role here, as a sharp g can lead to an arbitrary increase in threshold leading to poor accuracy while a flat g can lead to slow learning. Practical considerations for choice of g are discussed in Appendix A.1. For the experiments, g is set as the Sigmoid function for unstructured sparsity and the exponential function for structured sparsity. Typically, {s l } l∈[L] are initialized with s init to ensure that the thresholds {α l = g(s l )} l∈[L] start close to 0. <ref type="figure" target="#fig_0">Figure 1</ref> shows that the thresholds' dynamics are guided by a combination of gradients from L and the weight-decay on s. Further, the overall sparsity budget for STR is not set explicitly. Instead, it is controlled by the weight-decay parameter (λ), and can be further fine-tuned using s init . Interestingly, this curve is similar to the handcrafted heuristic for thresholds defined in <ref type="bibr">(Narang et al., 2019)</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> shows the overall learnt sparsity budget for ResNet50 during training. The curve looks similar to GMP (Zhu &amp; Gupta, 2017) sparsification heuristic, however, STR learns it via backpropagation and SGD. Finally, each parameter tensor learns a different threshold value, {α l } l∈ <ref type="bibr">[L]</ref> , resulting in unique final thresholds across  the layers, as shown in <ref type="figure" target="#fig_2">Figure 3</ref> for ResNet50. This, in turn, results in the non-uniform sparsity budget (see <ref type="figure" target="#fig_6">Figure 6</ref>) which is empirically shown to be effective in increasing prediction accuracy while reducing FLOPs. Moreover, (4) shows that the gradient update itself is sparse as gradient of L is multiplied with an indicator function of S g (W l ) = 0 which gets sparser over iterations ( <ref type="figure" target="#fig_1">Figure 2</ref>). So STR addresses both the issues with standard PGD methods (Hard/-Soft Thresholding) that we mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Analysis</head><p>The reparameterization trick using the projection operator's functional form can be used for standard constrained optimization problems as well (assuming the projection operator has a closed-form). However, it is easy to show that in general, such a method need not converge to the optimal solution even for convex functions over convex sets. This raises a natural question about the effectiveness of the technique for sparse weights learning problem. It turns out that for sparsity constrained problems, STR is very similar to backward pruning <ref type="bibr" target="#b21">(Hastie et al., 2009</ref>) which is a well-known technique for sparse regression. Note that, similar to Hard/-Soft Thresholding, standard backward pruning also does not support differentiable tuning thresholds which makes it challenging to apply it to DNNs.</p><p>To further establish this connection, let's consider a standard sparse regression problem where y = Xw * , X ij ∼ N (0, 1), and X ∈ R n×d . w * ∈ {0, 1} d has r d non-zeros, and d n r log d. Due to the initialization, g(s) ≈ 0 in initial few iterations. So, gradient descent converges to the least 2 -norm regression solution. That is, w = UU T w * where U ∈ R d×n is the right singular vector matrix of X and is a random n-dimensional subspace. As U is a random subspace. Since n r log d,</p><formula xml:id="formula_7">U S U T S ≈ r d · I where S = supp(w * ), and U S indexes rows of U corre- sponding to S. That is, min j∈S U j · U T w * ≥ 1 − o(1). On the other hand, U j · U T S w * √ nr d</formula><p>√ log d with high probability for j ∈ S. As n r log d, almost all the elements of supp(w * ) will be in top O (n) elements of w. Furthermore, XS g (w, s) = y, so |s| would decrease significantly via weight-decay and hence g(s) becomes large enough to prune all but say O (n) elements. Using a similar argument as above, leads to further pruning of w, while ensuring recovery of almost all elements in supp(w * ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section showcases the experimentation followed by the observations from applying STR for (a) unstructured sparsity in CNNs and (b) structured sparsity in RNNs.  <ref type="table" target="#tab_0">Table 1</ref>. The "+ ERK" suffix implies the usage of ERK budget <ref type="bibr" target="#b14">(Evci et al., 2020)</ref> instead of the original sparsity budget. Even though VD (Molchanov et al., 2017) achieves state-of-theart results, it is omitted due to the 2× memory and 4× compute footprint during training. Typically VD and IMP use a global threshold for global sparsity (GS) <ref type="bibr" target="#b19">(Han et al., 2015)</ref> which can also be learnt using STR. The unstructured sparsity experiments presented compare the techniques which induce layer-wise sparsity. Note that STR is generalizable to other scenarios as well. Open-source implementations, pre-trained models, and reported numbers of the available techniques were used as the baselines. Experiments were run on a machine with 4 NVIDIA Titan X (Pascal) GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Unstructured Sparsity in</head><p>All baselines use the hyperparameter settings defined in their implementations/papers. The experiments for STR use a batch size of 256, cosine learning rate routine and are trained for 100 epochs following the hyperparameter settings in <ref type="bibr" target="#b32">(Wortsman et al., 2019)</ref> using SGD + momentum. STR has weight-decay (λ) and s init hyperparameters to control the overall sparsity in CNNs and can be found in Appendix A.6. GMP 1.5×  and RigL 5× <ref type="bibr" target="#b14">(Evci et al., 2020)</ref> show that training the networks longer increases accuracy. However, due to the limited compute and environmental concerns <ref type="bibr" target="#b29">(Schwartz et al., 2019)</ref>, all the experiments were run only for around 100 epochs (∼3 days each). Unstructured sparsity in CNNs with STR is enforced by learning one threshold per-layer as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. PyTorch STRConv code can be found in Algorithm 1 of Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">RESNET50 ON IMAGENET-1K</head><p>A fully dense ResNet50 trained on ImageNet-1K has 77.01% top-1 validation accuracy. STR is compared extensively to other baselines on ResNet50 in the sparsity ranges of 80%, 90%, 95%, 96.5%, 98%, and 99%. <ref type="table" target="#tab_0">Table 1</ref> shows that DNW and GMP are state-of-the-art among the baselines across all the aforementioned sparsity regimes. As STR might not be able to get exactly to the sparsity budget, numbers are reported for the models which nearby. Note that the 90.23% sparse ResNet50 on ImageNet-1K with STR is referred to as the 90% sparse ResNet50 model learnt with STR.  STR comfortably beats all the baselines across all the sparsity regimes as seen in <ref type="table" target="#tab_0">Table 1</ref> and is the state-of-the-art for unstructured sparsity. <ref type="figure" target="#fig_5">Figure 4</ref> shows that STR forms a frontier curve encompassing all the baselines at all the levels of sparsity. Very few methods are stable in the ultra sparse regime of 98-99% sparsity and GMP can achieve  <ref type="figure">Figure 5</ref>. STR results in ResNet50 models on ImageNet-1K which have the lowest inference cost (FLOPs) for any given accuracy. 99% sparsity. STR is very stable even in the ultra sparse regime, as shown in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_5">Figure 4</ref>, while being up to 10% higher in accuracy than GMP at 99% sparsity.</p><p>STR induces non-uniform sparsity across layers, <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure">Figure 5</ref> show that STR produces models which have lower or similar inference FLOPs compared to the baselines while having better prediction accuracy in all the sparsity regimes. This hints at the fact that STR could be redistributing the parameters thereby reducing the FLOPs. In the 80% sparse models, STR is at least 0.19% better in accuracy than the baselines while having at least 60M (6.5%) lesser FLOPs. Similarly, STR has state-of-the-art accuracy in 90%, 95%, and 96.5% sparse regimes while having at least 68M (16.5%), 45M (22%) and 140M (54%) lesser FLOPs than the best baselines respectively. In the ultra sparse regime of 98% and 99% sparsity, STR has similar or slightly higher FLOPs compared to the baselines but is up to 4.6% and 10% better in accuracy respectively. Table 1 summarizes that the non-uniform sparsity baselines like SNFS, SNFS+ERK, and RigL+ERK can have up to 2-4× higher inference cost (FLOPs) due to non-optimal layer-wise distribution of the parameter weights. Observations: STR on ResNet50 shows some interesting observations related to sparsity and inference cost (FLOPs). These observations will be further discussed in Section 5:</p><p>1. STR is state-of-the-art for unstructured sparsity. 2. STR minimizes inference cost (FLOPs) while maintaining accuracy in the 80-95% sparse regime. 3. STR maximizes accuracy while maintaining inference cost (FLOPs) in 98-99% ultra sparse regime. 4. STR learns a non-uniform layer-wise sparsity, shown</p><p>in <ref type="figure" target="#fig_6">Figure 6</ref>, which shows that the initial layers of the CNN can be sparser than that of the existing non-uniform sparsity methods. All the learnt non-uniform budgets through STR can be found in Appendix A.3. 5. <ref type="figure" target="#fig_6">Figure 6</ref> also shows that the last layers through STR are denser than that of the other methods which is contrary to the understanding in the literature of non-uniform sparsity (Mostafa &amp; Wang, 2019; <ref type="bibr" target="#b11">Dettmers &amp; Zettlemoyer, 2019;</ref><ref type="bibr" target="#b14">Evci et al., 2020;</ref><ref type="bibr" target="#b16">Gale et al., 2019)</ref>. This leads to a sparser backbone for transfer learning. The backbone sparsities can be found in Appendix A.3. 6. <ref type="figure" target="#fig_7">Figure 7</ref> shows the layer-wise FLOPs distribution for the non-uniform sparsity methods. STR adjusts the FLOPs across layers such that it has lower FLOPs than the baselines. Note that the other non-uniform sparsity budgets lead to heavy compute overhead in the initial layers due to denser parameter tensors.</p><p>STR can also induce global sparsity (GS) <ref type="bibr" target="#b19">(Han et al., 2015)</ref> with similar accuracy at ∼ 2× FLOPs compared to layerwise for 90-98% sparsity (details in Appendix A.5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">MOBILENETV1 ON IMAGENET-1K</head><p>MobileNetV1 was trained on ImageNet-1K for unstructured sparsity with STR to ensure generalizability. Since GMP is the state-of-the-art baseline as shown earlier, STR was only compared to GMP for 75% and 90% sparsity regimes. A fully dense MobileNetV1 has a top-1 accuracy of 71.95% on ImageNet-1K. GMP <ref type="bibr" target="#b36">(Zhu &amp; Gupta, 2017)</ref> has the first layer and depthwise convolution layers dense for MobileNetV1 to ensure training stability and maximize accuracy. <ref type="table" target="#tab_1">Table 2</ref> shows the STR is at least 0.65% better than GMP for </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Structured Sparsity in RNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">EXPERIMENTAL SETUP</head><p>Google-12 is a speech recognition dataset that has 12 classes made from the Google Speech Commands dataset <ref type="bibr" target="#b30">(Warden, 2018)</ref>. HAR-2 is a binarized version of the 6-class Human Activity Recognition dataset <ref type="bibr" target="#b2">(Anguita et al., 2012)</ref>. These two datasets stand as compelling cases for on-device resource-efficient machine learning at the edge. Details about the datasets can be found in Appendix A.7.</p><p>FastGRNN (Kusupati et al., 2018) was proposed to enable powerful RNN models on resource-constrained devices. FastGRNN relies on making the RNN parameter matrices low-rank, sparse and quantized. As low-rank is a form of structured sparsity, experiments were done to show the effectiveness of STR for structured sparsity. The input vector to the RNN at each timestep and hidden state have D &amp;D dimensionality respectively. FastGRNN has two parameter matrices, W ∈ R D×D , U ∈ RD ×D which are reparameterized as product of low-rank matrices, W = W 1 W 2 , and vanilla training by up to 1.67% in four different model-size reducing rank settings on Google-12. Similarly, on HAR-2, STR is better than vanilla training in all the rank settings by up to 2.47%. Note that the accuracy of the low-rank models obtained by STR is either better or on-par with the full rank models while being around 50% and 70% smaller in size (low-rank) for Google-12 and HAR-2 respectively.</p><formula xml:id="formula_8">U = U 1 U 2 where W 1 ∈ R D×r W , W 2 ∈ R r W ×D , and (U 1 ) , U 2 ∈ R r U ×D . r W ,</formula><p>These experiments for structured sparsity in RNNs show that STR can be applied to obtain low-rank parameter tensors. Similarly, STR can be extended for filter/channel pruning and block sparsity <ref type="bibr" target="#b23">(He et al., 2017;</ref><ref type="bibr">Huang &amp; Wang, 2018;</ref><ref type="bibr" target="#b35">Liu et al., 2019)</ref> and details for this adaptation can be found in Appendix A.5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Drawbacks</head><p>STR's usage for unstructured sparsity leads to interesting observations as noted in Section 4.1.2. It is clear from Table 1 and Figures 4, 5 that STR achieves state-of-the-art accuracy for all the sparsity regimes and also reduces the FLOPs in doing so. STR helps in learning non-uniform sparsity budgets which are intriguing to study as an optimal non-uniform sparsity budget can ensure minimization of FLOPs while maintaining accuracy. Although it is not clear why STR's learning dynamics result in a non-uniform budget that minimizes FLOPs, the reduction in FLOPs is due to the better redistribution of parameters across layers.</p><p>Non-uniform sparsity budgets learnt by STR have the ini-tial and middle layers to be sparser than the other methods while making the last layers denser. Conventional wisdom suggests that the initial layers should be denser as the early loss of information would be hard to recover, this drives the existing non-uniform sparsity heuristics. As most of the parameters are present in the deeper layers, the existing methods tend to make them sparser while not affecting the FLOPs by much. STR, on the other hand, balances the FLOPs and sparsity across the layers as shown in <ref type="figure" target="#fig_6">Figures 6</ref>, 7 making it a lucrative and efficient choice. The denser final layers along with sparser initial and middle layers point to sparser CNN backbones obtained using STR. These sparse backbones can be viable options for efficient representation/transfer learning for downstream tasks.  <ref type="table" target="#tab_3">Table 4</ref> shows the effectiveness/transferability of the learnt non-uniform budget through STR for 90% sparse ResNet50 on ImageNet-1K using DNW <ref type="bibr" target="#b32">(Wortsman et al., 2019)</ref>. DNW typically takes in a uniform sparsity budget and has an accuracy of 74% for a 90% sparse ResNet50. Using ERK non-uniform budget for 90% sparsity results in a 0.1% increase in accuracy at the cost 2.35× inference FLOPs. Training DNW with the learnt budget from STR results in a reduction of FLOPs by 66M (16%) while maintaining accuracy. In the 95% sparsity regime, the learnt budget can improve the accuracy of DNW by up to 1.42% over uniform along with a reduction in FLOPs by at least 22M (11%). Similarly, these budgets can also be used for other methods like GMP <ref type="bibr" target="#b36">(Zhu &amp; Gupta, 2017)</ref>. <ref type="table" target="#tab_4">Table 5</ref> shows that the learnt sparsity budgets can lead to an increase in accuracy by 0.22% and 1.57% in 90% and 98% sparsity regimes respec-tively when used with GMP. Accuracy gains over uniform sparsity are also accompanied by a significant reduction in inference FLOPs. Note that the learnt non-uniform sparsity budgets can also be obtained using smaller representative datasets instead of expensive large-scale experiments.</p><p>The major drawback of STR is the tuning of the weightdecay parameter, λ and finer-tuning with s init to obtain the targeted overall sparsity. One way to circumvent this issue is to freeze the non-uniform sparsity distribution in the middle of training when the overall sparsity constraints are met and train for the remaining epochs. This might not potentially give the best results but can give a similar budget which can be then transferred to methods like GMP or DNW. Another drawback of STR is the function g for the threshold. The stability, expressivity, and sparsification capability of STR depends on g. However, it should be noted that sigmoid and exponential functions work just fine, as g, for STR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper proposed Soft Threshold Reparameterization (STR), a novel use of the soft-threshold operator, for the weights in DNN, to smoothly induce sparsity while learning layer-wise pruning thresholds thereby obtaining a nonuniform sparsity budget. Extensive experimentation showed that STR is state-of-the-art for unstructured sparsity in CNNs for ImageNet-1K while also being effective for structured sparsity in RNNs. Our method results in sparse models that have significantly lesser inference costs than the baselines. In particular, STR achieves the same accuracy as the baselines for 90% sparse MobileNetV1 with 50% lesser FLOPs. STR has ∼10% higher accuracy than the existing methods in ultra sparse (99%) regime for ResNet50 showing the effectiveness of the learnt non-uniform sparsity budgets. STR can also induce low-rank structure in RNNs while increasing the prediction accuracy showing the generalizability of the proposed reparameterization. Finally, STR is easy to adapt and the learnt budgets are transferable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Characterization of g</head><p>For the training dynamics of s, we propose some desired properties for choosing g : R → R ++ :</p><p>• 0 &lt; g(s), lim s→−∞ g(s) = 0, and lim s→∞ g(s) = ∞.</p><formula xml:id="formula_9">• ∃ G ∈ R ++ 0 &lt; g (s) ≤ G ∀ s ∈ R.</formula><p>• g (s init ) &lt; 1 providing us a handle on the dynamics of s.</p><p>For simplicity, the choice of g were the logistic sigmoid function, g(s) = k 1+e −s , and the exponential function, g(s) = ke s , for k ∈ R k , since in most of the experimental scenarios, we almost always have s &lt; 0 throughout the training making it satisfy all the desired properties in R − . One can choose k as an appropriate scaling factor based on the final weight distribution of a given DNN. All the CNN experiments in this paper we use the logistic sigmoid function with k = 1, as the weights' final learnt values are typically 1, and low-rank RNN use the exponential function with k = 1. It should be noted that better functional choices might exist for g and can affect the expressivity and dynamics of STR parameterization for inducing sparsity.</p><formula xml:id="formula_10">A.2. Gradient w.r.t. {s l } l∈[L]</formula><p>The gradient of s l ∀ l ∈ [L] takes an even interesting form</p><formula xml:id="formula_11">∇ s l L W l (s l ) = ∇ s l L (S g (W l , s l )) = −g (s l )P (W l , g(s l )) (5) Where P (W l , g(s l )) := ∇ W l (s l ) L W(s l ) , sign (W l ) 1 W l (s l ) = 0 . Thus the final update equation for s l ∀ l ∈ [L] becomes s (t+1) l ← s (t) l + η t g (s (t) l )P W (t) l , g s (t) l − η t λs (t) l<label>(6)</label></formula><p>where λ is its 2 regularization hyperparameter. <ref type="table" target="#tab_8">Table 6</ref> lists the non-uniform sparsity budgets learnt through STR across the sparsity regimes of 80%, 90%, 95%, 96.5%, 98% and 99% for ResNet50 on ImageNet-1K. The table also lists the backbone sparsities of every budget. It is clear that STR results in a higher than expected sparsity in the backbones of CNNs resulting in efficient backbones for transfer learning. <ref type="table" target="#tab_9">Table 7</ref> summarizes all the sparsity budgets for 90% sparse ResNet50 on ImageNet-1K obtained using various methods. This table also shows that the backbone sparsities learnt through STR are considerably higher than that of the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. ResNet50 Learnt Budgets and Backbone Sparsities</head><p>One can use these budgets directly for techniques like GMP and DNW for a variety of datasets and have significant accuracy gains as shown in the <ref type="table" target="#tab_3">Table 4</ref>. Note that GMP here makes the first and depthwise (dw) convolution layers dense, hence it is not the standard uniform sparsity. This table also shows that the backbone sparsities learnt through STR are considerably higher than that of GMP. <ref type="figure">Figure 8</ref> shows the sparsity distribution across layers when compared to GMP and <ref type="figure">Figure 9</ref> shows the FLOPs distribution across layers when compared to GMP for 90% sparse MobileNetV1 models on ImageNet-1K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. MobileNetV1 Sparsity and FLOPs Budget Distributions</head><p>It is interesting to notice that STR automatically keeps depthwise separable (the valleys in <ref type="figure">Figure 8</ref>) convolution layers less sparse than the rest to maximize accuracy which is the reason GMP keeps them fully dense.     <ref type="figure">Figure 8</ref>. Layer-wise sparsity budget for the 90% sparse Mo-bileNetV1 models on ImageNet-1K using various sparsification techniques. <ref type="figure">Figure 9</ref>. Layer-wise FLOPs distribution for the 90% sparse Mo-bileNetV1 models on ImageNet-1K using various sparsification techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. STR Adaptations</head><p>Algorithm 1 has comments suggesting the simple modifications required for global and per-weight sparsity.</p><p>A.5.1. STR FOR GLOBAL SPARSITY STR can be trivially modified to learn the global threshold to induce global sparsity like in <ref type="bibr" target="#b19">(Han et al., 2015;</ref><ref type="bibr" target="#b15">Frankle &amp; Carbin, 2019)</ref>. Instead of having an s l per layer l, share all the s l to create one single learnable global threshold s g . This can be implemented by a simple modification in Algorithm 1. STR's capability to induce global sparsity was evaluated on ResNet50 for ImageNet-1K for 90-98% sparsity regimes. <ref type="table" target="#tab_11">Table 9</ref> shows the performance of STR-GS that learns the global threshold to induce global sparsity. While the accuracies are comparable to the state-of-the-art if not better, they do come at cost of ∼ 2× inference cost compared to layer-wise sparsity due to poor non-uniform sparsity distribution which is a result of difference converged values of weights in each of the layers. STR-GS has numbers similar to IMP <ref type="bibr" target="#b15">(Frankle &amp; Carbin, 2019)</ref> while being able to learn the threshold stablely.  Let us assume there are n out filters of size k × k × n in in a given layer. Typically in channel/filter pruning techniques, each of these n out filters have an importance factor that represents the utility of the filter and is used to scale the corresponding filter. For a filter f i there exists a importance scalar m i learnt or obtained in some fashion and is used to get the effective filter in usef i = m i · f i where m i is broadcasted to scale f i . In practice, m i is heuristically made to go to 0 to induce structured sparsity through channel/filter pruning. Let us stack all the importance scalars of the filters in the layer, {m i } i∈ <ref type="bibr">[nout]</ref> , as vector m l where l is the layer index. Now, this reduces to the same problem of inducing sparsity in a vector as in the learning of low-rank in RNN presented in Section 4.2.1. STR will be applied to each of the {m l } l∈ <ref type="bibr">[L]</ref> where L is the total number of layers in a deep neural network. The inference will use the importance scalars through STR ensuring channel/filter pruning due to the induced sparsity. This is very similar to the work-flow we used to induce low-rank in RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.3. STR FOR PER-WEIGHT PRUNING OR MASK LEARNING</head><p>The adaptation of STR for per-weight pruning or mask learning is simple and is similar to layer-wise or global sparsity. Changing s l → S l ie., changing the layer-wise thresholds from a scalar to a tensor of the size of W l will hep STR adapt to do per-weight pruning or mask learning as discussed in the recent works <ref type="bibr" target="#b35">(Zhou et al., 2019;</ref><ref type="bibr" target="#b28">Savarese et al., 2019;</ref><ref type="bibr">Ramanujan et al., 2020)</ref>. We have explored this using a couple of experiments on CIFAR-10 (Krizhevsky et al., 2009) and ImageNet-1K. We observed that high amounts of sparsity were induced and the routine is very aggressive compared to other sparsification methods. For example, we were able to get 90% accuracy on CIFAR-10 using ResNet18 at a staggering 99.63% sparsity (270× lesser parameters than the dense model) which results in 41K parameters pushing it into very under parameterized regime. We suggest caution when running per-weight sparsity experiments with any method due to the high variance in the final accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Hyperparameters for Reproducibility</head><p>All the ResNet50 experiments use a batchsize of 256, cosine learning rate with warm-up as in <ref type="bibr" target="#b32">(Wortsman et al., 2019)</ref> and trained for 100 epochs. λ is the weight-decay hyperparameter. s init is the initial value of all s i where i is the layer number. The hyper parameter setting for each of the sparse model can be found in <ref type="table" target="#tab_0">Table 10</ref>.</p><p>All the MobileNetV1 experiments use a batchsize of 256, cosine learning rate with warm-up as in <ref type="bibr" target="#b32">(Wortsman et al., 2019)</ref> and trained for 100 epochs. λ is the weight-decay hyperparameter. s init is the initial value of all s i where i is the layer number. The hyper parameter setting for each of the sparse model can be found in <ref type="table" target="#tab_0">Table 11</ref>.</p><p>All the CNN experiments use g(s) = 1 1+e −s for the STR. All the FastGRNN experiments use a batchsize of 100, learning rate and optimizers as suggested in <ref type="bibr">(Kusupati et al., 2018)</ref> and trained for 300 epochs. Weight-decay parameter, λ is applied to both m W , m U resulting in the rank setting obtained. Each hyperparamter setting can lead to multi-    <ref type="bibr">11,</ref><ref type="bibr">35)</ref> 0.001 (9, 7) 0.001 (10, 31) 0.002 (8, 7) 0.001 (9, 24) 0.005 ple low-rank setting over the course of training. s init set such that g(s init ) ≈ 0 for the initialization of soft threshold pruning scalar for the low-rank vectors.</p><p>All the RNN experiments use g(s) = e s for the STR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Dataset and Model Details</head><p>ImageNet-1K: ImageNet-1K has RGB images with 224×224 dimensions. The dataset has 1.3M training images, 50K validation images and 1000 classes. Images were transformed and augmented with the standard procedures as in <ref type="bibr" target="#b32">(Wortsman et al., 2019)</ref>.</p><p>Google-12: Google Speech Commands dataset <ref type="bibr" target="#b30">(Warden, 2018)</ref> contains 1 second long utterances of 30 short words (30 classes) sampled at 16KHz. Standard log Mel-filter-bank featurization with 32 filters over a window size of 25ms and stride of 10ms gave 99 timesteps of 32 filter responses for a 1-second audio clip. For the 12 class version, 10 classes used in Kaggles Tensorflow Speech Recognition challenge were used and the remaining two classes were noise and background sounds (taken randomly from the remaining 20 short word utterances). The datasets were zero mean -unit variance normalized during training and prediction.</p><p>Google-12 has 22,246 training points, 3,081 testing points. Each datapoint has 99 timesteps with each input being 32 dimensional making the datapoint 3,168 dimensional.</p><p>HAR-2: Human Activity Recognition (HAR) dataset was collected from an accelerometer and gyroscope on a Samsung Galaxy S3 smartphone. The features available on the repository were directly used for experiments. The 6 activities were merged to get the binarized version. The classes {Sitting, Laying, Walking Upstairs} and {Standing, Walking, Walking Downstairs} were merged to obtain the two classes. The dataset was zero mean -unit variance normalized during training and prediction. HAR-2 has 7,352 training points and 2,947 test points. Each datapoint has 1,152 dimensions, which will be split into 128 timesteps leading to dimensional per timestep inputs.</p><p>ResNet50: ResNet50 is a very popular CNN architecture and is widely used to showcase the effectiveness of sparsification techniques. ResNet50 has 54 parameter layers (including fc) and a couple of pooling layers (which contribute minimally to FLOPs). All the batchnorm parameters are left dense and are learnt during the training. STR can be applied per-layer, per-channel and even per-weight to obtain unstructured sparsity and the aggressiveness of sparsification increases in the same order. This paper only uses per-layer STR which makes it have 54 additional learnable scalars. The layer-wise parameters and FLOPs can be seen in Tables 7 and 6. All the layers had no bias terms.</p><p>MobileNetV1: MobileNetV1 is a popular efficient CNN architecture. It is used to showcase the generalizability of sparsification techniques. MobileNetV1 has 28 parameter layers (including fc) and a couple of pooling layers (which contribute minimally to FLOPs). All the batchnorm parameters are left dense and are learnt during the training. STR can be applied per-layer, per-channel and even per-weight to obtain unstructured sparsity and the aggressiveness of sparsification increases in the same order. This paper only uses per-layer STR which makes it have 28 additional learnable scalars. The layer-wise parameters and FLOPs can be seen in Tables 8. All the layers had no bias terms.</p><p>FastGRNN: FastGRNN's update equations can be found in <ref type="bibr">(Kusupati et al., 2018)</ref>. FastGRNN, in general, benefits a lot from the low-rank reparameterization and this enables it to be deployed on tiny devices without losing any accuracy. FastGRNN's biases and final classifier are left untouched in all the experiments and only the input and hidden projection matrices are made low-rank. All the hyperparameters were set specific to the datasets as in <ref type="bibr">Kusupati et al. (2018)</ref>.</p><p>A.8. Hard Threshold vs Soft Threshold <ref type="figure" target="#fig_0">Figure 10</ref> shows the difference between hard thresholding and soft thresholding for the same threshold value of α = 2. It is clear from <ref type="figure" target="#fig_0">Figure 10</ref> that soft-threshold is a continuous function that is sub-differentiable. The abrupt change in hard-threshold leads to instability in training sometimes increasing dependence on fine tuning of the obtained sparse network. Soft-threshold is robust to such issues. <ref type="figure" target="#fig_0">Figure 10</ref>. A visualization of hard-threshold (left) and softthreshold (right) functions with the threshold α = 2. x-axis is the input and y-axis is the output.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The learnt threshold parameter, α = g(s), for layer 10 in 90% sparse ResNet50 on ImageNet-1K over the course of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The progression of the learnt overall budget for 90% sparse ResNet50 on ImageNet-1K over the course of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The final learnt threshold values, [α l ] 54 l=1 = [g(s l )] 54l=1 , for all the layers in 90% sparse ResNet50 on ImageNet-1K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>CNNs 4.1.1. EXPERIMENTAL SETUP ImageNet-1K (Deng et al., 2009) is a widely used largescale image classification dataset with 1K classes. All the CNN experiments presented are on ImageNet-1K. ResNet50 (He et al., 2016) and MobileNetV1 (Howard et al., 2017) are two popular CNN architectures. ResNet50 is extensively used in literature to show the effectiveness of sparsity in CNNs. Experiments on MobileNetV1 argue for the generalizability of the proposed technique (STR). Dataset and models' details can be found in Appendix A.7.STR was compared against strong state-of-the-art baselines in various sparsity regimes including GMP, DSR (Mostafa &amp; Wang, 2019), DNW (Wortsman et al., 2019), SNFS<ref type="bibr" target="#b11">(Dettmers &amp; Zettlemoyer, 2019)</ref>, RigL<ref type="bibr" target="#b14">(Evci et al., 2020)</ref> andDPF (Lin et al., 2020). GMP and DNW always use a uniform sparsity budget. RigL, SNFS, DSR, and DPF were compared in their original form. Exceptions for the uniform sparsity are marked in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>STR forms a frontier curve over all the baselines in all sparsity regimes showing that it is the state-of-the-art for unstructured sparsity in ResNet50 on ImageNet-1K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Layer-wise sparsity budget for the 90% sparse ResNet50 models on ImageNet-1K using various sparsification techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Layer-wise FLOPs budget for the 90% sparse ResNet50 models on ImageNet-1K using various sparsification techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>import torch import torch.nn as nn import torch.nn.functional as F from args import args as parser_args def softThreshold(x, s, g=torch.sigmoid):# STR on a weight x (can be a tensor) with "s" (typically a scalar, but can be a tensor) with function "g". return torch.sign(x) * torch.relu(torch.abs(x)-g(s)) class STRConv(nn.Conv2d): # Overloaded Conv2d which can replace nn.Conv2d def __init__(self, * args, ** kwargs): super().__init__( * args, ** kwargs) # "g" can be chosen appropriately, but torch.sigmoid works fine. self.g = torch.sigmoid # parser_args gets arguments from command line. sInitValue is the initialization of "s" for all layers. It can take in different values per-layer as well. self.s = nn.Parameter(parser_args.sInitValue * torch.ones([1, 1])) # "s" can be per-layer (a scalar), global (a shared scalar across layers), per-channel/filter (a vector) or per individual weight (a tensor of the size self.weight). All the experiments use per-layer "s" (a scalar) in the paper.def forward(self, x): sparseWeight = softThreshold(self.weight, self.s, self.g) # Parameters except "x" and "sparseWeight" can be chosen appropriately. All the experiments use default PyTorch arguments. x = F.conv2d(x, sparseWeight, self.bias, self.stride, self.padding, self.dilation, self.groups) return x # FC layer is implemented as a 1x1 Conv2d and STRConv is used for FC layer as well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>STR is the state-of-the-art for unstructured sparsity in ResNet50 on ImageNet-1K while having lesser inference cost (FLOPs) than the baselines across all the sparsity regimes. * and # imply that the first and last layer are dense respectively. Baseline numbers reported from their respective papers/open-source implementations and models. FLOPs do not include batch-norm.</figDesc><table><row><cell>Method</cell><cell>Top-1 Acc (%)</cell><cell>Params</cell><cell>Sparsity (%)</cell><cell>FLOPs</cell></row><row><cell>ResNet-50</cell><cell>77.01</cell><cell>25.6M</cell><cell>0.00</cell><cell>4.09G</cell></row><row><cell>GMP</cell><cell>75.60</cell><cell>5.12M</cell><cell>80.00</cell><cell>818M</cell></row><row><cell>DSR  * #</cell><cell>71.60</cell><cell>5.12M</cell><cell>80.00</cell><cell>1.23G</cell></row><row><cell>DNW</cell><cell>76.00</cell><cell>5.12M</cell><cell>80.00</cell><cell>818M</cell></row><row><cell>SNFS</cell><cell>74.90</cell><cell>5.12M</cell><cell>80.00</cell><cell>-</cell></row><row><cell>SNFS + ERK</cell><cell>75.20</cell><cell>5.12M</cell><cell>80.00</cell><cell>1.68G</cell></row><row><cell>RigL  *</cell><cell>74.60</cell><cell>5.12M</cell><cell>80.00</cell><cell>920M</cell></row><row><cell>RigL + ERK</cell><cell>75.10</cell><cell>5.12M</cell><cell>80.00</cell><cell>1.68G</cell></row><row><cell>DPF</cell><cell>75.13</cell><cell>5.12M</cell><cell>80.00</cell><cell>818M</cell></row><row><cell>STR</cell><cell>76.19</cell><cell>5.22M</cell><cell>79.55</cell><cell>766M</cell></row><row><cell>STR</cell><cell>76.12</cell><cell>4.47M</cell><cell>81.27</cell><cell>705M</cell></row><row><cell>GMP</cell><cell>73.91</cell><cell>2.56M</cell><cell>90.00</cell><cell>409M</cell></row><row><cell>DNW</cell><cell>74.00</cell><cell>2.56M</cell><cell>90.00</cell><cell>409M</cell></row><row><cell>SNFS</cell><cell>72.90</cell><cell>2.56M</cell><cell>90.00</cell><cell>1.63G</cell></row><row><cell>SNFS + ERK</cell><cell>72.90</cell><cell>2.56M</cell><cell>90.00</cell><cell>960M</cell></row><row><cell>RigL  *</cell><cell>72.00</cell><cell>2.56M</cell><cell>90.00</cell><cell>515M</cell></row><row><cell>RigL + ERK</cell><cell>73.00</cell><cell>2.56M</cell><cell>90.00</cell><cell>960M</cell></row><row><cell>DPF #</cell><cell>74.55</cell><cell>4.45M</cell><cell>82.60</cell><cell>411M</cell></row><row><cell>STR</cell><cell>74.73</cell><cell>3.14M</cell><cell>87.70</cell><cell>402M</cell></row><row><cell>STR</cell><cell>74.31</cell><cell>2.49M</cell><cell>90.23</cell><cell>343M</cell></row><row><cell>STR</cell><cell>74.01</cell><cell>2.41M</cell><cell>90.55</cell><cell>341M</cell></row><row><cell>GMP</cell><cell>70.59</cell><cell>1.28M</cell><cell>95.00</cell><cell>204M</cell></row><row><cell>DNW</cell><cell>68.30</cell><cell>1.28M</cell><cell>95.00</cell><cell>204M</cell></row><row><cell>RigL  *</cell><cell>67.50</cell><cell>1.28M</cell><cell>95.00</cell><cell>317M</cell></row><row><cell>RigL + ERK</cell><cell>70.00</cell><cell>1.28M</cell><cell>95.00</cell><cell>∼600M</cell></row><row><cell>STR</cell><cell>70.97</cell><cell>1.33M</cell><cell>94.80</cell><cell>182M</cell></row><row><cell>STR</cell><cell>70.40</cell><cell>1.27M</cell><cell>95.03</cell><cell>159M</cell></row><row><cell>STR</cell><cell>70.23</cell><cell>1.24M</cell><cell>95.15</cell><cell>162M</cell></row><row><cell>RigL  *</cell><cell>64.50</cell><cell>0.90M</cell><cell>96.50</cell><cell>257M</cell></row><row><cell>RigL + ERK</cell><cell>67.20</cell><cell>0.90M</cell><cell>96.50</cell><cell>∼500M</cell></row><row><cell>STR</cell><cell>67.78</cell><cell>0.99M</cell><cell>96.11</cell><cell>127M</cell></row><row><cell>STR</cell><cell>67.22</cell><cell>0.88M</cell><cell>96.53</cell><cell>117M</cell></row><row><cell>GMP</cell><cell>57.90</cell><cell>0.51M</cell><cell>98.00</cell><cell>82M</cell></row><row><cell>DNW</cell><cell>58.20</cell><cell>0.51M</cell><cell>98.00</cell><cell>82M</cell></row><row><cell>STR</cell><cell>62.84</cell><cell>0.57M</cell><cell>97.78</cell><cell>80M</cell></row><row><cell>STR</cell><cell>61.46</cell><cell>0.50M</cell><cell>98.05</cell><cell>73M</cell></row><row><cell>STR</cell><cell>59.76</cell><cell>0.45M</cell><cell>98.22</cell><cell>68M</cell></row><row><cell>GMP</cell><cell>44.78</cell><cell>0.26M</cell><cell>99.00</cell><cell>41M</cell></row><row><cell>STR</cell><cell>54.79</cell><cell>0.31M</cell><cell>98.79</cell><cell>54M</cell></row><row><cell>STR</cell><cell>51.82</cell><cell>0.26M</cell><cell>98.98</cell><cell>47M</cell></row><row><cell>STR</cell><cell>50.35</cell><cell>0.23M</cell><cell>99.10</cell><cell>44M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>STR is up to 3% higher in accuracy while having 33% lesser inference cost (FLOPs) for MobileNetV1 on ImageNet-1K.</figDesc><table><row><cell>Method</cell><cell>Top-1 Acc (%)</cell><cell>Params</cell><cell>Sparsity (%)</cell><cell>FLOPs</cell></row><row><cell>MobileNetV1</cell><cell>71.95</cell><cell>4.21M</cell><cell>0.00</cell><cell>569M</cell></row><row><cell>GMP</cell><cell>67.70</cell><cell>1.09M</cell><cell>74.11</cell><cell>163M</cell></row><row><cell>STR</cell><cell>68.35</cell><cell>1.04M</cell><cell>75.28</cell><cell>101M</cell></row><row><cell>STR</cell><cell>66.52</cell><cell>0.88M</cell><cell>79.07</cell><cell>81M</cell></row><row><cell>GMP</cell><cell>61.80</cell><cell>0.46M</cell><cell>89.03</cell><cell>82M</cell></row><row><cell>STR</cell><cell>64.83</cell><cell>0.60M</cell><cell>85.80</cell><cell>55M</cell></row><row><cell>STR</cell><cell>62.10</cell><cell>0.46M</cell><cell>89.01</cell><cell>42M</cell></row><row><cell>STR</cell><cell>61.51</cell><cell>0.44M</cell><cell>89.62</cell><cell>40M</cell></row></table><note>75% sparsity, while having at least 62M (38%) lesser FLOPs. More interestingly, STR has state-of-the-art accuracy while having up to 50% (40M) lesser FLOPs than GMP in the 90% sparsity regime. All the observations made for ResNet50 hold for MobileNetV1 as well. The sparsity and FLOPs distribution across layers can be found in Appendix A.4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>r U are the ranks of the respective matrices. In order to apply STR, the low-rank reparameterization can be changed to W = (W 1 1m W )W 2 , andU = (U 1 1m U )U 2 where m W = 1 D , and m U = 1D, WThe baseline is low-rank FastGRNN where the ranks of the matrices are preset(Kusupati et al., 2018). EdgeML(Dennis  et al.)  FastGRNN was used for the experiments with the hyperparameters suggested in the paper and is referred to as vanilla training. Hyperparameters for the models can be found in Appendix A.6.</figDesc><table><row><cell cols="5">4.2.2. FASTGRNN ON GOOGLE-12 AND HAR-2</cell><cell></cell></row><row><cell cols="6">Table 3 presents the results for low-rank FastGRNN with</cell></row><row><cell cols="6">vanilla training and STR. Full-rank non-reparameterized</cell></row><row><cell cols="6">FastGRNN has an accuracy of 92.60% and 96.10% on</cell></row><row><cell cols="6">Google-12 and HAR-2 respectively. STR outperforms</cell></row><row><cell cols="6">Table 3. STR can induce learnt low-rank in FastGRNN resulting</cell></row><row><cell cols="5">in up to 2.47% higher accuracy than the vanilla training.</cell><cell></cell></row><row><cell cols="2">Google-12</cell><cell></cell><cell cols="2">HAR-2</cell><cell></cell></row><row><cell>(r W , r U )</cell><cell cols="2">Accuracy (%)</cell><cell>(r W , r U )</cell><cell cols="2">Accuracy (%)</cell></row><row><cell></cell><cell>Vanilla Training</cell><cell>STR</cell><cell></cell><cell>Vanilla Training</cell><cell>STR</cell></row><row><cell>Full rank (32, 100)</cell><cell>92.30</cell><cell>-</cell><cell>Full rank (9, 80)</cell><cell>96.10</cell><cell>-</cell></row><row><cell>(12, 40)</cell><cell>92.79</cell><cell>94.45</cell><cell>(9, 8)</cell><cell>94.06</cell><cell>95.76</cell></row><row><cell>(11, 35)</cell><cell>92.86</cell><cell>94.42</cell><cell>(9, 7)</cell><cell>93.15</cell><cell>95.62</cell></row><row><cell>(10, 31)</cell><cell>92.86</cell><cell>94.25</cell><cell>(8, 7)</cell><cell>94.88</cell><cell>95.59</cell></row><row><cell>(9, 24)</cell><cell>93.18</cell><cell>94.45</cell><cell></cell><cell></cell><cell></cell></row></table><note>1 ∈ R D×D , W 2 ∈ R D×D , and U 1 , U 2 ∈ RD ×D . To learn the low-rank, STR is applied on the m W , and m U vectors. Learning low-rank with STR on m W , m U can be thought as inducing unstructured sparsity on the two trainable vectors aiming for the right r W , and r U .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Effect of various layer-wise sparsity budgets when used with DNW for ResNet50 on ImageNet-1K.</figDesc><table><row><cell>Method</cell><cell>Top-1 Acc (%)</cell><cell>Params</cell><cell>Sparsity (%)</cell><cell>FLOPs</cell></row><row><cell>Uniform</cell><cell>74.00</cell><cell>2.56M</cell><cell>90.00</cell><cell>409M</cell></row><row><cell>ERK</cell><cell>74.10</cell><cell>2.56M</cell><cell>90.00</cell><cell>960M</cell></row><row><cell>Budget from STR</cell><cell>74.01</cell><cell>2.49M</cell><cell>90.23</cell><cell>343M</cell></row><row><cell>Uniform</cell><cell>68.30</cell><cell>1.28M</cell><cell>95.00</cell><cell>204M</cell></row><row><cell>Budget from STR</cell><cell>69.72</cell><cell>1.33M</cell><cell>94.80</cell><cell>182M</cell></row><row><cell>Budget from STR</cell><cell>68.01</cell><cell>1.24M</cell><cell>95.15</cell><cell>162M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Effect of various layer-wise sparsity budgets when used with GMP for ResNet50 on ImageNet-1K.</figDesc><table><row><cell>Method</cell><cell>Top-1 Acc (%)</cell><cell>Params</cell><cell>Sparsity (%)</cell><cell>FLOPs</cell></row><row><cell>Uniform</cell><cell>73.91</cell><cell>2.56M</cell><cell>90.00</cell><cell>409M</cell></row><row><cell>Budget from STR</cell><cell>74.13</cell><cell>2.49M</cell><cell>90.23</cell><cell>343M</cell></row><row><cell>Uniform</cell><cell>57.90</cell><cell>0.51M</cell><cell>98.00</cell><cell>82M</cell></row><row><cell>Budget from STR</cell><cell>59.47</cell><cell>0.50M</cell><cell>98.05</cell><cell>73M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Howard, A. G.,Zhu, M., Chen, B., Kalenichenko, D., Wang,   W., Weyand, T., Andreetto, M., and Adam, H. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.Huang, Z. and Wang, N. Data-driven sparse structure selection for deep neural networks. In Proceedings of the Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Rethinking the value of network pruning. In International Conference on Learning Representations, 2019.Louizos, C., Welling, M., and Kingma, D. P. Learning sparse neural networks through l 0 regularization. In International Conference on Learning Representations, 2018.</figDesc><table><row><cell>European conference on computer vision (ECCV), pp.</cell><cell></cell></row><row><cell>304-320, 2018.</cell><cell>Lu, Z., Sindhwani, V., and Sainath, T. N. Learning compact</cell></row><row><cell></cell><cell>recurrent neural networks. In 2016 IEEE International</cell></row><row><cell>Jaderberg, M., Vedaldi, A., and Zisserman, A. Speeding up</cell><cell>Conference on Acoustics, Speech and Signal Processing</cell></row><row><cell>convolutional neural networks with low rank expansions.</cell><cell>(ICASSP), pp. 5960-5964. IEEE, 2016.</cell></row><row><cell>In Proceedings of the British Machine Vision Conference.</cell><cell></cell></row><row><cell>BMVA Press, 2014.</cell><cell>Luo, J.-H., Wu, J., and Lin, W. Thinet: A filter level pruning</cell></row><row><cell></cell><cell>method for deep neural network compression. In Proceed-</cell></row><row><cell>Jain, P., Tewari, A., and Kar, P. On iterative hard thresh-</cell><cell>ings of the IEEE international conference on computer</cell></row><row><cell>olding methods for high-dimensional m-estimation. In</cell><cell>vision, pp. 5058-5066, 2017.</cell></row><row><cell>Advances in Neural Information Processing Systems, pp.</cell><cell></cell></row><row><cell>685-693, 2014.</cell><cell>Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H.,</cell></row><row><cell></cell><cell>Gibescu, M., and Liotta, A. Scalable training of arti-</cell></row><row><cell>Krizhevsky, A., Hinton, G., et al. Learning multiple layers</cell><cell>ficial neural networks with adaptive sparse connectivity</cell></row><row><cell>of features from tiny images. 2009.</cell><cell>inspired by network science. Nature communications, 9</cell></row><row><cell>Kusupati, A., Singh, M., Bhatia, K., Kumar, A., Jain, P.,</cell><cell>(1):2383, 2018.</cell></row><row><cell>and Varma, M. Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network. In Advances in Neural Information Processing Systems, pp.</cell><cell>Molchanov, D., Ashukha, A., and Vetrov, D. Variational dropout sparsifies deep neural networks. In Proceed-ings of the 34th International Conference on Machine</cell></row><row><cell>9017-9028, 2018.</cell><cell>Learning-Volume 70, pp. 2498-2507. JMLR. org, 2017.</cell></row><row><cell>LeCun, Y., Denker, J. S., and Solla, S. A. Optimal brain damage. In Advances in neural information processing systems, pp. 598-605, 1990.</cell><cell>Mostafa, H. and Wang, X. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization. In International Conference on Ma-</cell></row><row><cell>Lee, N., Ajanthan, T., and Torr, P. SNIP: Single-shot net-</cell><cell>chine Learning, pp. 4646-4655, 2019.</cell></row><row><cell>work pruning based on connection sensitivity. In Interna-tional Conference on Learning Representations, 2019.</cell><cell>Narang, S., Elsen, E., Diamos, G., and Sengupta, S. Explor-ing sparsity in recurrent neural networks. In International</cell></row><row><cell>Lee, Y. Differentiable sparsification for deep neural net-</cell><cell>Conference on Learning Representations, 2019.</cell></row><row><cell>works. arXiv preprint arXiv:1910.03201, 2019.</cell><cell>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,</cell></row><row><cell>Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H. P. Pruning filters for efficient convnets. In International Conference on Learning Representations, 2017.</cell><cell>Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pp. 8024-8035, 2019.</cell></row><row><cell>Lin, T., Stich, S. U., Barba, L., Dmitriev, D., and Jaggi, M.</cell><cell></cell></row><row><cell>Dynamic model pruning with feedback. In International</cell><cell>Patil, S. G., Dennis, D. K., Pabbaraju, C., Shaheer, N.,</cell></row><row><cell>Conference on Learning Representations, 2020.</cell><cell>Simhadri, H. V., Seshadri, V., Varma, M., and Jain, P.</cell></row><row><cell></cell><cell>Gesturepod: Enabling on-device gesture-based interac-</cell></row><row><cell>Liu, B., Wang, M., Foroosh, H., Tappen, M., and Pensky, M.</cell><cell>tion for white cane users. In Proceedings of the 32nd</cell></row><row><cell>Sparse convolutional neural networks. In Proceedings</cell><cell>Annual ACM Symposium on User Interface Software and</cell></row><row><cell>of the IEEE Conference on Computer Vision and Pattern</cell><cell>Technology, pp. 403-415, 2019.</cell></row><row><cell>Recognition, pp. 806-814, 2015.</cell><cell></cell></row><row><cell></cell><cell>Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A.,</cell></row><row><cell></cell><cell>and Rastegari, M. What's hidden in a randomly weighted</cell></row><row><cell></cell><cell>neural network? In Proceedings of the IEEE/CVF Con-</cell></row><row><cell></cell><cell>ference on Computer Vision and Pattern Recognition, pp.</cell></row><row><cell></cell><cell>11893-11902, 2020.</cell></row></table><note>Liu, J., Xu, Z., Shi, R., Cheung, R. C. C., and So, H. K. Dy- namic sparse training: Find efficient sparse network from scratch with trainable masked layers. In International Conference on Learning Representations, 2020.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8</head><label>8</label><figDesc>summarizes all the sparsity budgets for 90% sparse MobileNetV1 on ImageNet-1K obtained using various methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>PyTorch code for STRConv with per-layer threshold.</figDesc><table><row><cell>Soft Threshold Weight Reparameterization for Learnable Sparsity</cell></row><row><cell>Algorithm 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>The non-uniform sparsity budgets for various sparsity ranges learnt through STR for ResNet50 on ImageNet-1K. FLOPs distribution per layer can be computed as 100−s i 100 * FLOPsi, where si and FLOPsi are the sparsity and FLOPs of the layer i.</figDesc><table><row><cell>Metric</cell><cell>Fully Dense Params</cell><cell>Fully Dense FLOPs</cell><cell>Sparsity (%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>The non-uniform sparsity budgets learnt multiple methods for 90% sparse ResNet50 on ImageNet-1K. FLOPs distribution per layer can be computed as 100−s i 100 * FLOPsi, where si and FLOPsi are the sparsity and FLOPs of the layer i.</figDesc><table><row><cell>Metric</cell><cell>Fully Dense Params</cell><cell>Fully Dense FLOPs</cell><cell cols="4">Sparsity (%) STR Uniform ERK SNFS</cell><cell>VD</cell><cell>GS</cell></row><row><cell>Overall</cell><cell cols="3">25502912 4089284608 90.23</cell><cell>90.00</cell><cell cols="3">90.07 90.06 90.27 89.54</cell></row><row><cell>Backbone</cell><cell cols="3">23454912 4087136256 92.47</cell><cell>90.00</cell><cell cols="3">89.82 89.44 91.41 90.95</cell></row><row><cell>Layer 1 -conv1</cell><cell>9408</cell><cell cols="2">118013952 59.80</cell><cell>90.00</cell><cell>58.00</cell><cell>2.50</cell><cell>31.39 35.11</cell></row><row><cell>Layer 2 -layer1.0.conv1</cell><cell>4096</cell><cell cols="2">12845056 83.28</cell><cell>90.00</cell><cell>0.00</cell><cell>2.50</cell><cell>39.50 56.05</cell></row><row><cell>Layer 3 -layer1.0.conv2</cell><cell>36864</cell><cell cols="2">115605504 89.48</cell><cell>90.00</cell><cell>82.00</cell><cell>2.50</cell><cell>67.87 75.04</cell></row><row><cell>Layer 4 -layer1.0.conv3</cell><cell>16384</cell><cell cols="2">51380224 85.80</cell><cell>90.00</cell><cell>4.00</cell><cell>2.50</cell><cell>64.87 70.31</cell></row><row><cell>Layer 5 -layer1.0.downsample.0</cell><cell>16384</cell><cell cols="2">51380224 83.34</cell><cell>90.00</cell><cell>4.00</cell><cell>2.50</cell><cell>60.38 66.88</cell></row><row><cell>Layer 6 -layer1.1.conv1</cell><cell>16384</cell><cell cols="2">51380224 89.89</cell><cell>90.00</cell><cell>4.00</cell><cell>2.50</cell><cell>61.35 75.09</cell></row><row><cell>Layer 7 -layer1.1.conv2</cell><cell>36864</cell><cell cols="2">115605504 90.60</cell><cell>90.00</cell><cell>82.00</cell><cell>2.50</cell><cell>64.38 80.42</cell></row><row><cell>Layer 8 -layer1.1.conv3</cell><cell>16384</cell><cell cols="2">51380224 91.70</cell><cell>90.00</cell><cell>4.00</cell><cell>2.50</cell><cell>65.83 80.00</cell></row><row><cell>Layer 9 -layer1.2.conv1</cell><cell>16384</cell><cell cols="2">51380224 88.07</cell><cell>90.00</cell><cell>4.00</cell><cell>2.50</cell><cell>68.75 75.21</cell></row><row><cell>Layer 10 -layer1.2.conv2</cell><cell>36864</cell><cell cols="2">115605504 87.03</cell><cell>90.00</cell><cell>82.00</cell><cell>2.50</cell><cell>70.86 74.95</cell></row><row><cell>Layer 11 -layer1.2.conv3</cell><cell>16384</cell><cell cols="2">51380224 90.99</cell><cell>90.00</cell><cell>4.00</cell><cell>2.50</cell><cell>54.05 79.28</cell></row><row><cell>Layer 12 -layer2.0.conv1</cell><cell>32768</cell><cell cols="2">102760448 85.95</cell><cell>90.00</cell><cell>43.00</cell><cell>2.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>The non-uniform sparsity budgets learnt multiple methods for 90% sparse MobileNetV1 on ImageNet-1K. FLOPs distribution per layer can be computed as 100−s i 100 * FLOPsi, where si and FLOPsi are the sparsity and FLOPs of the layer i.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>STR can stablely learn the global threshold to induce global sparsity resulting in models with comparable accuracies as layer-wise sparsity but with ∼ 2× the inference cost.</figDesc><table><row><cell>Method</cell><cell>Top-1 Acc (%)</cell><cell>Params</cell><cell>Sparsity (%)</cell><cell>FLOPs</cell></row><row><cell>STR-GS</cell><cell>74.13</cell><cell>2.42M</cell><cell>89.54</cell><cell>596M</cell></row><row><cell>STR-GS</cell><cell>71.61</cell><cell>1.58M</cell><cell>93.84</cell><cell>363M</cell></row><row><cell>STR-GS</cell><cell>67.95</cell><cell>1.01M</cell><cell>96.06</cell><cell>232M</cell></row><row><cell>STR-GS</cell><cell>62.17</cell><cell>0.54M</cell><cell>97.91</cell><cell>142M</cell></row><row><cell cols="4">A.5.2. STR FOR FILTER/CHANNEL PRUNING</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>The hyperparameters for various sparse ResNet50 models on ImageNet-1K using STR. λ is the weight-decay parameter and sinit is the initialization of all si for all the layers in ResNet50.</figDesc><table><row><cell>Sparse Model (%)</cell><cell>Weight-decay (λ)</cell><cell>s init</cell></row><row><cell>79.55</cell><cell>0.00001700000000</cell><cell>-3200</cell></row><row><cell>81.27</cell><cell>0.00001751757813</cell><cell>-3200</cell></row><row><cell>87.70</cell><cell>0.00002051757813</cell><cell>-3200</cell></row><row><cell>90.23</cell><cell>0.00002251757813</cell><cell>-3200</cell></row><row><cell>90.55</cell><cell>0.00002051757813</cell><cell>-800</cell></row><row><cell>94.80</cell><cell>0.00003051757813</cell><cell>-3200</cell></row><row><cell>95.03</cell><cell cols="2">0.00003351757813 -12800</cell></row><row><cell>95.15</cell><cell>0.00003051757813</cell><cell>-1600</cell></row><row><cell>96.11</cell><cell>0.00003051757813</cell><cell>-100</cell></row><row><cell>96.53</cell><cell cols="2">0.00004051757813 -12800</cell></row><row><cell>97.78</cell><cell cols="2">0.00005217578125 -12800</cell></row><row><cell>98.05</cell><cell cols="2">0.00005651757813 -12800</cell></row><row><cell>98.22</cell><cell cols="2">0.00006051757813 -12800</cell></row><row><cell>98.79</cell><cell cols="2">0.00007551757813 -12800</cell></row><row><cell>98.98</cell><cell cols="2">0.00008551757813 -12800</cell></row><row><cell>99.10</cell><cell cols="2">0.00009051757813 -12800</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 .</head><label>11</label><figDesc>The hyperparameters for various sparse MobileNetV1 models on ImageNet-1K using STR. λ is the weight-decay parameter and sinit is the initialization of all si for all the layers in MobileNetV1.</figDesc><table><row><cell>Sparse Model (%)</cell><cell>Weight-decay (λ)</cell><cell>s init</cell></row><row><cell>75.28</cell><cell>0.00001551757813</cell><cell>-100</cell></row><row><cell>79.07</cell><cell>0.00001551757813</cell><cell>-25</cell></row><row><cell>85.80</cell><cell>0.00003051757813</cell><cell>-3200</cell></row><row><cell>89.01</cell><cell cols="2">0.00003751757813 -12800</cell></row><row><cell>89.62</cell><cell>0.00003751757813</cell><cell>-3200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 .</head><label>12</label><figDesc>Hyperparameters for the low-rank FastGRNN with STR. The same weight-decay parameter λ is applied on both m W , m U . Multiple rank setting can be acheived during the training course of the FastGRNN model. g(sinit) ≈ 0 ie., sinit ≤ −10 for all the experiments.</figDesc><table><row><cell></cell><cell>Google-12</cell><cell>HAR-2</cell><cell></cell></row><row><cell cols="4">(r W , r U ) Weight-decay (λ) (r W , r U ) Weight-decay (λ)</cell></row><row><cell>(12, 40)</cell><cell>0.001</cell><cell>(9, 8)</cell><cell>0.001</cell></row><row><cell>(</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Keivan Alizadeh, Tapan Chugh, Tim  Dettmers, Erich Elsen, Utku Evci, Daniel Gordon, Gabriel   Ilharco, Sarah Pratt, James Park, Mohammad Rastegari and Matt Wallingford for helpful discussions and feedback. Mitchell Wortsman is in part supported by AI2 Fellowship in AI. Sham Kakade acknowledges funding from the Washington Research Foundation for Innovation in Dataintensive Discovery, and the NSF Awards CCF-1637360, CCF-1703574, and CCF-1740551. Ali Farhadi acknowledges funding from the NSF Awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, 67102239 and gifts from Allen Institute for Artificial Intelligence.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Fully Dense Params </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Butterfly transform: An efficient fft based neural architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Reyes-Ortiz</surname></persName>
		</author>
		<ptr target="https://archive.ics.uci.edu/ml/" />
	</analytic>
	<monogr>
		<title level="m">International Workshop on Ambient Assisted Living</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bastiaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bunting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cairncross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Corrigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Van Doorn</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>et al. Exploiting unstructured sparsity on next-generation datacenter hardware</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Azarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bhalgat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00075</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Learned threshold pruning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkagethresholding algorithm for linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on imaging sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep rewiring: Training very sparse deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kappel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Legenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The dantzig selector: Statistical estimation when p is much larger than n. The annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2313" to="2351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gaurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lovett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Simhadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edgeml</surname></persName>
		</author>
		<ptr target="https://github.com/Microsoft/EdgeML" />
		<title level="m">Machine Learning for resource</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sparse networks from scratch: Faster training without losing performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04840</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">De-noising by soft-thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="613" to="627" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09723</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Fast sparse convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rigging the lottery: Making all tickets winners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09574</idno>
		<title level="m">The state of sparsity in deep neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast &amp; simple resourceconstrained structure learning of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morphnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1586" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic network surgery for efficient dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1379" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference, and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparing finetuning and rewinding in neural network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">One size does not fit all: Multi-scale, cascaded rnns for radar classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation</title>
		<meeting>the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Winning the lottery with continuous sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maire</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04427</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Green</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10597</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<ptr target="http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discovering neural wirings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2680" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic network pruning by regularizing auxiliary parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoprune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13681" to="13691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Network slimming by slimmable networks: Towards one-shot architecture search for channel numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11728</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deconstructing lottery tickets: Zeros, signs, and the supermask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosinski</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3592" to="3602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: exploring the efficacy of pruning for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01878</idno>
		<idno>25502912 4089284608 79.55 81.27 87.70 90.23 90.55 94.80 95.03 95.15 96.11 96.53 97.78 98.05 98.22 98.79 98.98 99.10 Backbone 23454912 4087136256 82.07 83.79 90.08 92.47 92.77 96.51 96.71 96.84 97.64 97.92 98.82 98.99 99.11 99.46 99.58 99.64</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">43 Layer 15 -layer2.0.downsample.0 131072 102760448 85</title>
		<idno>54 -fc 2048000 2048000 50.65 52.46 60.48 64.50 65.12 75.20 75.73 75.80 78.57 80.69 85.96 87.26 88.03 91.11 92.15 92.87 57.10 70.89</idno>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">98</biblScope>
		</imprint>
	</monogr>
	<note>80 Layer 17 -layer2.1.conv2 147456 115605504 76. 66 99.33 Layer 18 -layer2.1.conv3 65536 51380224 84.76 84.71 93.10 93.66 93.23 97.00 98. 83 98.96 99.19 99.44 99.67 99.71 99.74 99.82 99.85 99.89 Layer 33 -layer3.2.conv2 589824 115605504 87.15 88.67 94.09 95.59 96.14 98.86 98.69 98.91 99.21 99.20 99.64 99.72 99.76 99.85 99.84 99.90 Layer 34 -layer3.2.conv3 262144 51380224 84.86 86.90 92.40 94.99 94.99 98.19 98.19 98.42 98.76 98.97 99.42 99.56 99.62 99.76 99.75 99.88 Layer 35 -layer3.3.conv1 262144 51380224 86.62 89.46 94.06 96.08 95.88 98.70 98.71 98.77 99.01 99.27 99.58 99.66 99.69 99.83 99.. Layer 36 -layer3.3.conv2 589824 115605504 86.52 87.97 93.56 96.10 96.11 98.70 98.82 98.89 99.19 99.31 99.68 99.73 99.77 99.88 99.87 99.93 Layer. 89 96.88 97.65 98.70 98.85 99.13 99.45 99.58 99.66 Layer 51 -layer4.2.conv1 1048576 51380224 76.34 77.93 84.98 87.57 88.47 93.90 93.87 94.16 95.55 95.91 97.66 97.97 98.15 98.88 99.08 99.22 Layer 52 -layer4.2.conv2 2359296 115605504 73.57 74.97 82.32 84.37 86.01 91.92 91.66 92.22 94.02 94.16 96.65 97.13 97.29 98.44 98.74 99.00 Layer 53 -layer4.2.conv3 1048576 51380224 68.78 70.38 78.11 80.29 81.73 89.64 89.43 89.65 91.40 92.65 96.02 96.72 96.93 98.47 98.83 99.15 Layer</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
