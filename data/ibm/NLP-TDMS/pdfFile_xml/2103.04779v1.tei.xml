<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CDLNet: Robust and Interpretable Denoising through Deep Convolutional Dictionary Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Janjušević</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>Brooklyn</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Khalilian-Gourtani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>Brooklyn</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wang</surname></persName>
							<email>yaowang@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>Brooklyn</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CDLNet: Robust and Interpretable Denoising through Deep Convolutional Dictionary Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-dictionary learning</term>
					<term>blind denoising</term>
					<term>inter- pretable deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning based methods hold state-of-the-art results in image denoising, but remain difficult to interpret due to their construction from poorly understood building blocks such as batch-normalization, residual learning, and feature domain processing. Unrolled optimization networks propose an interpretable alternative to constructing deep neural networks by deriving their architecture from classical iterative optimization methods, without use of tricks from the standard deep learning tool-box. So far, such methods have demonstrated performance close to that of state-of-the-art models while using their interpretable construction to achieve a comparably low learned parameter count. In this work, we propose an unrolled convolutional dictionary learning network (CDLNet) and demonstrate its competitive denoising performance in both low and high parameter count regimes. Specifically, we show that the proposed model outperforms the state-of-the-art denoising models when scaled to similar parameter count. In addition, we leverage the model's interpretable construction to propose an augmentation of the network's thresholds that enables state-of-the-art blind denoising performance and near-perfect generalization on noiselevels unseen during training.</p><p>Index Terms-dictionary learning, blind denoising, interpretable deep learning</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION AND BACKGROUND</head><p>In recent years, deep-learning methods have become stateof-the-art in imaging inverse problems such as denoising <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, compressed-sensing <ref type="bibr" target="#b9">[10]</ref>, super-resolution <ref type="bibr" target="#b17">[18]</ref>, and more. However, these deep neural networks (DNN) and their building blocks are currently not well understood, making it ineffective to draw on knowledge from decades of signal processing and optimization research for insight or improvement. As a result, poorly understood tools such as batch-normalization <ref type="bibr" target="#b5">[6]</ref>, residual-learning, and feature-domain processing, have become staples for deep-learning architectures. So called "unrolled optimization" DNNs <ref type="bibr" target="#b14">[15]</ref> propose a promising avenue to remedy this by deriving their network architecture from K iterations of an iterative optimization algorithm. Existing unrolled optimization DNNs are able to acheive state-of-the-art results on image restoration tasks without the use of the previously mentioned standard deep-learning toolbox <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, these formulations mostly leverage their interpretability to reduce network parameter count in comparison to popular DNNs. In this work, we present a further desirable quality to be gained from interpretable model construction: robustness.</p><p>Our proposed method tackles natural-image denoising via unrolled convolutional dictionary learning (CDL), which we call CDLNet. The convolutional dictionary learning model is rooted in the sparse representation prior, where we assume our signals x ∈ R N may be represented by a linear combination of only a few vectors (atoms) from a larger dictionary, D,</p><formula xml:id="formula_0">∃ z s.t. x = Dz, z 0 N,<label>(1)</label></formula><p>where z 0 := |{i : z[i] = 0}| is the 0 pseudo-norm (a.k.a counting norm). With some assumptions on the level of sparsity in z and diversity (incoherence) of the dictionary atoms, stable and unique recovery of z from x may be guaranteed <ref type="bibr" target="#b11">[12]</ref>. A primary focus of signal processing literature in the last decade and a half has been on the selection of a representation dictionary, in which learning based methods such as KSVD <ref type="bibr" target="#b0">[1]</ref> brought state-of-the-art performance in image restoration tasks. Given a data-set {x i } n i=1 , the dictionary learning problem may be formulated with an 1 -norm relaxation of the 0 pseudo-norm, which has been shown to promote sparsity,</p><formula xml:id="formula_1">minimize D∈C,{zi} 1 n n i=1 1 2 Dz i − x i 2 2 + λ z i 1 .<label>(2)</label></formula><p>The regularization parameter λ gives a data-fidelity vs. sparsity trade-off, and D is restricted to the constraint set of columns in the unit-ball,</p><formula xml:id="formula_2">C = {D : d j 2 ≤ 1 ∀j},<label>(3)</label></formula><p>to prevent arbitrary scaling of coefficients. This formulation is amenable to the denoising problem, y = x + n, by replacing the data fidelity term with in (2) with Dz i − y i 2 2 . In this work, we consider the denoising problem with additive white Gaussian noise (AWGN), where n ∼ N (0, σ 2 n I). The problem in (2) is non-convex and thus commonly solved via a Gauss-Seidel split, alternating between sparsecoding with a fixed dictionary, and updating the dictionary given sparse codes <ref type="bibr" target="#b0">[1]</ref>. The iterative soft thresholding algorithm (ISTA) is one such sparse-coding algorithm, whose iterates are defined as,</p><formula xml:id="formula_3">z (k+1) := ST z (k) − η (k) D (Dz (k) − y), η (k) λ , (4) where ST(x, τ )[i] := sign(x[i]) max(0, |x[i]| − τ )</formula><p>is the element-wise soft-thresholding operator with threshold τ ≥ 0, η (k) is a step-size parameter, and λ is the sparsity penalty from <ref type="bibr" target="#b1">(2)</ref>. In the case of a convolutional dictionary, we use M filters d j ∈ R p whose integer translates form the columns of D. The application of the dictionary (synthesis convolution) is given arXiv:2103.04779v1 [eess.IV] 5 Mar 2021 by Dz = M j=1 d j * z j , where z j is the j-th channel of z. The corresponding analysis convolution is defined channel-wise by (D x) j = d j * x where d j denotes the reversal of the filter. In the case of 2D input signals</p><formula xml:id="formula_4">x ∈ R √ N × √ N , we consider 2D square filters d j ∈ R √ p× √ p .</formula><p>Making a connection between ISTA and DNNs is tempting due to their iterative use of linear operators followed by pointwise non-linearities. Our work builds off the seminal work of <ref type="bibr" target="#b4">[5]</ref> by employing a learned ISTA encoder followed by a linear synthesis dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>The authors of <ref type="bibr" target="#b16">[17]</ref> proposed an unrolled CDL DNN for image denoising and image inpainting, demonstrating results competitive with that of KSVD <ref type="bibr" target="#b0">[1]</ref> in a fraction of the computational time. Our baseline proposed method builds off of their framework, introducing strided convolutions, preprocessing, and weight initialization as key steps to reaching denoising performance on par with state-of-the-art deep learning methods.</p><p>In <ref type="bibr" target="#b15">[16]</ref>, an argument is given suggesting the ill-conditioned nature of the convolutional sparse-coding (CSC) model in the representation of natural images. Their proposed model, CSCNet, uses large strides on the order of the filter size, along with averaging reconstructions from shifted input signals -effectively returning to a patch-based approach. CDLNet demonstrates an alternative approach to CSCNet that is not inconsistent with their analysis, achieving superior performance on image denoising benchmarks without the use of "shiftaveraging".</p><p>The authors of <ref type="bibr" target="#b13">[14]</ref> showed that several DNN denoising models, such as DnCNN <ref type="bibr" target="#b17">[18]</ref>, exhibit a catastrophic failure in denoising performance when presented with input noise-levels outside of the model's training range. They proposed "biasfree" versions of the networks (ex. BF-DnCNN), which were demonstrated to posses the desired noise-level generalization property. Our work provides an alternate route for bringing robustness to DNN denoising by adapting the network's thresholds with the input noise-level.</p><p>The authors of <ref type="bibr" target="#b6">[7]</ref> replace the ReLU non-linearities of DnCNN <ref type="bibr" target="#b17">[18]</ref> with soft-thresholding operators whose thresholds are proportional to the input signal standard-deviation. Our noise adaptive model instead employs soft-thresholding with thresholds proportional to the noise standard-deviation, and is shown to generalize on noise-levels outside the training range.</p><p>In <ref type="bibr" target="#b18">[19]</ref>, the proposed model (FFDNet) requires an input "noise-map" at inference, which allows it to handle spatially varying noise in addition to improved performance over a larger input noise-level training range. This noise statistic is only used as input to the network, whereas our method adapts the threshold of each layer of the model based on the estimated noise-level.</p><p>Summary of our contributions: In this work we construct a convolutional dictionary learning network (CDLNet) that outperforms other CDL based DNNs while remaining in the CSC framework (Section III-A). From our interpretable model construction, we derive a noise adaptive model (Section III-B). Experimentally, we demonstrate that state-of-theart performance in AWGN denoising is attainable when the model capacity is scaled to that of popular DNNs (Section IV-B). Additionally, we show that the proposed models with adaptive thresholds achieve state-of-the-art blind denoising performance and allow the model to generalize above and below training noise-levels (Section IV-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHODOLOGY A. The CDLNet architecture</head><p>In this section we introduce the convolutional dictionary learning network (CDLNet) for natural image denoising. The architecture involves a convolutional learned ISTA encoder with K unrollings and a convolutional synthesis dictionary,</p><formula xml:id="formula_5">D,x = Dz (K) , z (0) = 0, k = 0, 1, . . . , K − 1,<label>(5)</label></formula><formula xml:id="formula_6">z (k+1) = ST z (k) − A (k) (B (k) z (k) − y), τ (k) , where Θ = {[A (k) , B (k) , τ (k) ] K−1 k=0</formula><p>, D} are the set of learned parameters. B, D are all M -channel convolutional synthesis operators, and A are M -channel convolutional analysis operators. The non-negative thresholds τ (k) ∈ R M are subband dependent, corresponding to a learned weighted 1 norm in <ref type="bibr" target="#b1">(2)</ref>. The output denoised image is given byx = Dz (K) . The block diagram of the proposed CDLNet is given in <ref type="figure">Fig. 1</ref>.</p><p>The convolution analysis and synthesis operators as shown are highly redundant transformations, increasing the number of coefficients from N to M N in analysis. This level of redundancy can become computationally burdensome as the number of filters (M ) increases. To mitigate this, we can replace each of the convolution synthesis operators A, B, or D, represented by W, with a sub-sampled version W∆ s , where ∆ s is the zero-filling operator with stride s. Note that ∆ s is the subsampling operator with stride s. For 2D images we consider ∆ s with stride s in both horizontal and vertical directions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Noise-adaptive thresholds</head><p>We leverage CDLNet's signal-processing and optimization derivation to provide adaptation to varying input noise-levels in a single model. We augment the learned thresholds in <ref type="bibr" target="#b4">(5)</ref> to be proportional to the input noise standard deviation (σ n ),</p><formula xml:id="formula_7">τ (k) = λ (k) σ n , k = 0, 1, . . . , K − 1<label>(6)</label></formula><p>where λ (k) ∈ R M is a learned parameter. We propose this augmentation of the thresholds based on the "universal threshold theorem" <ref type="bibr" target="#b11">[12]</ref>, from the Wavelet denoising literature, τ = σ n 2 log e N . This can be shown to minimize the <ref type="figure">Fig. 1</ref>. Block diagram of CDLNet. Analysis and synthesis convolutions map from 1 to M and M to 1 channels, respectively. We say that CDLNet does not process signals in a "learned feature domain" to differentiate from the usage of multi-channel filtering (M to M channels) in DNNs such as DnCNN <ref type="bibr" target="#b17">[18]</ref>. Also note that CDLNet does not use batch-normalization or residual learning, in contrast to DnCNN <ref type="bibr" target="#b17">[18]</ref>.</p><formula xml:id="formula_8">− − z (0) y = x + nx = Dz (K) k = 0 B (k) Synthesis Conv Conv Dictionary D Analysis Conv A (k) ⊤ Soft-Thresholding ST(·, τ (k) ) z (1) − − k = K − 1 z (K) z (K−1)</formula><p>upper bound of the risk function of an element-wise denoising operator <ref type="bibr" target="#b11">[12]</ref>. With this augmentation, we interpret the proportionality constant λ (k) i as learning the gain factor between the image domain noise-level σ n and the i-th channel's noiselevel at layer k. This framework has the added benefit of decoupling noise-level estimation from denoising, allowing for trade-off between accurate estimation and speed at inference time. We explore this trade-off using two different noise-level estimation algorithms at inference time, in Section IV-D. We refer to CDLNet with the above noise-adaptive augmentation as CDLNet-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Training and Inference</head><p>Models: are trained by the Adam <ref type="bibr" target="#b8">[9]</ref> optimizer on the 2loss with parameter constraints,</p><formula xml:id="formula_9">minimize A (k) ,B (k) ,D∈C τ (k) ≥0</formula><p>x −x(y; Θ) 2 2 ,</p><p>where C is the unit-ball constraint (3) imposed on all convolution operators. Constraints are enforced by projection after each gradient descent step. Filters of size 7 × 7 are used. We denote the models with M =32, and K=20, when trained on a single noise level, as CDLNet-S, and when trained over a noise range with and without adaptive thresholds as CDLNet-A and CDLNet-B, respectively. Similarly, we denote the models with M =169 and K=30 as Big-CDLNet. Unless specified otherwise, CDLNet models use stride 1 convolutions and Big-CDLNet models use a stride of 2. Our framework is implemented in PyTorch and provided online <ref type="bibr" target="#b7">[8]</ref>.</p><p>Dataset: All CDLNet models and variants are trained on the BSD432 dataset <ref type="bibr" target="#b12">[13]</ref>. Input signals are preprocessed with division by 255, random crops of 128 × 128, random flips and rotations, and mean-subtraction. Models trained across noiselevels are done so by uniform sampling of σ n ∈ σ train n .</p><p>Training: A mini-batch size of 10 samples is used. An initial learning rate of 1e − 3 is used, and reduced by a factor of 0.95 every 50 epochs for a maximum of 6000 epochs or until convergence. Similar to the method in <ref type="bibr" target="#b9">[10]</ref>, we backtrack our model to the nearest checkpoint upon divergence, reducing the learning rate by a factor of 0.8.</p><p>Initialization: We initialize all convolution operators with the same weights drawn from a standard normal distribution.</p><p>This comes from the intuition that the majority of our learned channels will consist of band-pass signals modeling image texture, and so our filters should be zero-mean. Following <ref type="bibr" target="#b15">[16]</ref>, as an initialization step, we normalize the convolution operators by their spectral norm in correspondence with the maximum uniform step-size of ISTA.</p><p>Noise-level estimation: We employ two different noiselevel estimation algorithms for blind denoising: one based on the median absolute deviation (MAD) of the input's diagonal wavelet coefficients <ref type="bibr" target="#b1">[2]</ref>, and the other based on the principal component analysis of a subset of the input's patches (PCA) <ref type="bibr" target="#b10">[11]</ref>. These estimators offer two ends of the trade-off between speed (MAD) and accuracy (PCA).</p><p>Experiments were conducted with an Intel Xeon(R) Platinum 8268 CPU at 2.90GHz, an Nvidia RTX 8000 GPU, and 4GB of RAM, running Linux version 3.10.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Single noise-level performance</head><p>In these experiments, we train models for individual noise levels. <ref type="table" target="#tab_0">Table I</ref> shows the results of the CDLNet model in the small parameter count regime (CDLNet-S) against the other leading CDL based DNN, CSCNet <ref type="bibr" target="#b15">[16]</ref>, and in the large parameter count regime (Big-CDLNet-S) against stateof-the-art denoising models DnCNN <ref type="bibr" target="#b17">[18]</ref> and FFDNet <ref type="bibr" target="#b18">[19]</ref>. CDLNet outperforms CSCNet in much less computational time at inference, without the use of stride, by using fewer filters and untying its weights between unrollings. We are also able to outperform the listed state-of-the-art deep learning methods by scaling our model (Big-CDLNet) to comparable size. Inference timings given show that our larger model's use of stride yields a manageable computational complexity. The non-learned method BM3D <ref type="bibr" target="#b2">[3]</ref> is given as a classical baseline, and its timing is an order of magnitude greater than the learned methods. In <ref type="table" target="#tab_0">Table I</ref>, we use stride 1 convolutions in our small model and stride 2 convolutions for our big model. <ref type="table" target="#tab_0">Table II</ref> empirically verifies these choices as optimal by showing the effect of stride on output PSNR, averaged over a gray-scale version of the Kodak dataset <ref type="bibr" target="#b3">[4]</ref>. Note that unlike CSCNet we do not employ any "shift-averaging". We see that the redundancy of the large model allows for use of stride 2 without a denoising performance penalty. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison of learned dictionaries</head><p>In this section we compare the learned representations from CDLNet and CSCNet <ref type="bibr" target="#b15">[16]</ref>. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the convolutional dictionaries obtained from the CDLNet models and CSCNet. We see that Big-CDLNet offers a greater diversity in its learned filters compared to CSCNet. The baseline CDLNet model, despite having a relatively small number of filters, is shown to mostly learned directional "Gabor-like" filters, with some texture components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Blind denoising and generalization across noise-levels</head><p>In this section, we consider the blind denoising and generalization scenarios and compare the models equipped with the proposed adaptive threshold schemes to other models. In <ref type="figure" target="#fig_2">Fig. 3</ref>, we show the performance of the models trained on the noise range σ train n = <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">35]</ref> and tested on different noise levels σ test n ∈ <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">50]</ref>. Big-CDLNet-A and Big-CDLNet-B refer to the proposed model trained on the training noise range with and without adaptive thresholds, respectively. Additionally, we trained the blind denoising version of DnCNN <ref type="bibr" target="#b17">[18]</ref>, DnCNN-B, on σ train n = <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">35]</ref>, denoted as DnCNN-B * in <ref type="figure" target="#fig_2">Fig. 3</ref>. The single points on the plot in <ref type="figure" target="#fig_2">Fig. 3</ref> show the performance of Big-CDLNet-S model with single noise level training (i.e. σ train n = σ test n ).  <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">35]</ref> and tested on different σ test n . Average PSNR calculated over BSD68 <ref type="bibr" target="#b12">[13]</ref>.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, all networks perform closely over the training noise-range. On the other hand, when tested on noise-levels outside the training range, the network with adaptive thresholds (Big-CDLNet-A) greatly outperforms the other models. We observe that models without noise-adaptive thresholds have a very significant performance drop compared to the noise-adaptive model (Big-CDLNet-A) when generalizing above the training noise level, while Big-CDLNet-A nearly matches the performance of the models trained for a specific noise-level (Big-CDLNet-S) across the range. In spite of increasing input signal-to-noise ratio for noise-levels below the training range, we observe that models without noise-adaptive thresholds have diminishing performance returns (note the plateau of Big-CDLNet-B and DnCNN-B * in σ test n = <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>). On the other hand, denoising behavior of Big-CDLNet-A extends to the lower noise-range. We observe reduced generalization performance at the very low noise-level range of σ n = 5. This may be explained by the need for a different thresholding model when the signal variance is much greater than that of the noise <ref type="bibr" target="#b11">[12]</ref>. A visual comparison of the denoising generalization of Big-CDLNet-A and DnCNN-B * is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Our proposed adaptive model provides visually appealing results at the unseen noise-level (σ test n = 50), while DnCNN-B * fails to generalize and produces unwanted artifacts.</p><p>We further compare the blind denoising and generalization capabilities of the proposed method in <ref type="table" target="#tab_0">Table III</ref>. As observed in <ref type="bibr" target="#b13">[14]</ref>, the BF-DnCNN model has reduced performance in the training range compared to DnCNN-B while avoiding the failure outside the training range. Big-CDLNet-A outperforms the DnCNN-B inside the training range and also provides improved generalization outside the range compared to BF-DnCNN. Note that the Big-CDLNet-A still has minor performance drop compared to a single noise level mode (Big-CLDNet-S) but this reduction is less significant than that of BF-DnCNN <ref type="bibr" target="#b13">[14]</ref>. In the blind denoising and generalization experiments of <ref type="table" target="#tab_0">Table III</ref>, <ref type="figure" target="#fig_2">Figures 3 and 4</ref>, we employ the PCA based noise estimation algorithm <ref type="bibr" target="#b10">[11]</ref> at inference time. <ref type="table" target="#tab_0">Table IV</ref> shows the difference in denoising performance between using the ground-truth (GT) noise-level and the two previously mentioned noise-level estimation algorithms. The PCA based algorithm allows us to attain near ground-truth denoising performance at the cost of increased computation. The wavelet based estimation method (MAD) offers essentially no computational overhead but significantly decreases denoising performance at the lower noise-level ranges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have proposed CDLNet as an interpretable deep neural network construction for the image denoising task. CDLNet achieves superior performance to other CDL based networks at a small parameter count, while avoiding the computationally expensive shift-averaging scheme. Additionally, we propose small-strided convolutions to enable the use of an increased model parameter count while retaining a manageable computational complexity. We show that such models perform competitively with state-of-the-art DNNs. Our results demonstrate that many of the popular tools from the deep learning toolbox (batch-norm, residual learning, feature domain processing) may not be necessary for the denoising task. We further leverage the interpretability of our network to propose a noiselevel adaptive thresholding scheme that brings state-of-the-art blind denoising performance and robustness to noise-levels mismatch between training and inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The computational complexity of this network is roughly O(KM N p/s 2 ) for input signals in R √ N × √ N and filters of size √ p × √ p. Replacing the convolution synthesis operators A, B, D in (5) with their sub-sampled versions allows for the possibility of learning more diverse filters while keeping the computation under control.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Learned Filters for (a) Big-CDLNet, 169 filters of size 7 × 7, (b) CSCNet<ref type="bibr" target="#b15">[16]</ref>, 175 filters of size 11 × 11, and (c) CDLNet (in spatial domain (top) and frequency domain (bottom)), 32 filters of size 7 × 7. † This figure is obtained from the models provided online by<ref type="bibr" target="#b15">[16]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Performance of different networks trained on σ train n =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Visual comparison of Big-CDLNet-A and DnCNN-B * trained on σ train n = [15, 35] and tested on noise level σ test n = 50. PSNR value for each image is given in parentheses. Details are better visible by zooming.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DENOISING</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">PERFORMANCE (PSNR) ON BSD68 TESTSET</cell></row><row><cell cols="2">(σ = σ train n</cell><cell cols="6">= σ test n ). ALL LEARNED MODELS TRAINED ON BSD432.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">*  NUMBERS REPORTED IN [10].</cell></row><row><cell>σ</cell><cell cols="3">BM3D CSCNet  *</cell><cell>CDLNet-S</cell><cell cols="2">FFDNet DnCNN</cell><cell>Big-CDLNet-S</cell></row><row><cell>15</cell><cell cols="2">31.07</cell><cell>31.40</cell><cell>31.60</cell><cell>31.63</cell><cell>31.72</cell><cell>31.74</cell></row><row><cell>25</cell><cell cols="2">28.57</cell><cell>28.93</cell><cell>29.11</cell><cell>29.19</cell><cell>29.22</cell><cell>29.26</cell></row><row><cell>50</cell><cell cols="2">25.62</cell><cell>26.04</cell><cell>26.19</cell><cell>26.29</cell><cell>26.23</cell><cell>26.35</cell></row><row><cell>Params</cell><cell></cell><cell>-</cell><cell>64k</cell><cell>65k</cell><cell>486k</cell><cell>556k</cell><cell>510k</cell></row><row><cell>GPU time</cell><cell></cell><cell>-</cell><cell>143 ms</cell><cell>9 ms</cell><cell>7 ms</cell><cell>23 ms</cell><cell>15 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II EFFECT</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">OF STRIDE FOR σ train n</cell><cell>= σ test n</cell><cell>= 25.</cell></row><row><cell cols="5">PSNR VALUES AVERAGED OVER GRAY-SCALE KODAK [4] DATASET.</cell></row><row><cell>Stride</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>CDLNet-S</cell><cell>30.19</cell><cell>30.09</cell><cell>29.75</cell><cell>29.21</cell></row><row><cell cols="2">Big-CDLNet-S 30.37</cell><cell>30.39</cell><cell>30.28</cell><cell>29.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III BLIND</head><label>III</label><figDesc>DENOISING AND GENERALIZATION COMPARISON TO DNCNN-B [18] AND BF-CNN [14]. ALL MODELS ARE TRAINED ON σ train</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>n</cell><cell>= [0, 55].</cell></row><row><cell cols="5">AVERAGE TEST PSNR ON BSD68 [13] IS REPORTED.</cell><cell></cell></row><row><cell>Model</cell><cell>5</cell><cell>15</cell><cell>σ test n 25</cell><cell>50</cell><cell>75</cell></row><row><cell>DnCNN-B</cell><cell cols="4">37.65 31.60 29.15 26.22</cell><cell>18.74</cell></row><row><cell>BF-DnCNN</cell><cell cols="4">37.72 31.58 29.12 26.17</cell><cell>24.63</cell></row><row><cell cols="5">Big-CDLNet-A 37.73 31.62 29.20 26.30</cell><cell>24.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV EFFECT</head><label>IV</label><figDesc>OF NOISE ESTIMATION ALGORITHM ON PERFORMANCE OF THE NOISE ADAPTIVE MODEL BIG-CDLNET-A.</figDesc><table><row><cell>Est. Algo.</cell><cell>5</cell><cell>15</cell><cell>σ test n 25</cell><cell>50</cell><cell>75</cell><cell>GPU time</cell></row><row><cell>GT</cell><cell cols="2">37.75 31.63</cell><cell cols="2">29.20 26.31</cell><cell>24.80</cell><cell>13 ms</cell></row><row><cell>PCA [11]</cell><cell cols="2">37.73 31.62</cell><cell cols="2">29.20 26.30</cell><cell>24.76</cell><cell>23 ms</cell></row><row><cell>MAD [2]</cell><cell cols="2">37.18 31.55</cell><cell cols="2">29.18 26.30</cell><cell>24.75</cell><cell>13 ms</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive wavelet thresholding for image denoising and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1532" to="1546" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Kodak Color Image Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Franzen</surname></persName>
		</author>
		<ptr target="http://r0k.us/graphics/kodak/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning, ser. ICML&apos;10</title>
		<meeting>the 27th International Conference on Machine Learning, ser. ICML&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep shrinkage convolutional neural network for adaptive noise reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Isogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shiodera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takeguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="228" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Janjušević</surname></persName>
		</author>
		<ptr target="https://github.com/nikopj/CDLNet" />
		<title level="m">CDLNet repository</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A flexible framework for designing trainable priors with adaptive smoothing and game encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lecouat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single-image noise level estimation for blind denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5226" to="5237" />
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Wavelet Tour of Signal Processing: The Sparse Way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Elsevier Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighth IEEE International Conference on Computer Vision</title>
		<meeting>Eighth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust and interpretable blind image denoising via bias-free convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kadkhodaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="18" to="44" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking the CSC model for natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2274" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learned convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sreter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2191" to="2195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FFDNet: Toward a fast and flexible solution for CNN-based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4608" to="4622" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
