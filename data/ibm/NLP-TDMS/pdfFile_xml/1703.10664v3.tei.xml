<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision (CRCV)</orgName>
								<orgName type="institution">University of Central Florida (UCF)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision (CRCV)</orgName>
								<orgName type="institution">University of Central Florida (UCF)</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>shah@crcv.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision (CRCV)</orgName>
								<orgName type="institution">University of Central Florida (UCF)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has been demonstrated to achieve excellent results for image classification and object detection. However, the impact of deep learning on video analysis (e.g. action detection and recognition) has been limited due to complexity of video data and lack of annotations. Previous convolutional neural networks (CNN) based video action detection approaches usually consist of two major steps: frame-level action proposal generation and association of proposals across frames. Also, most of these methods employ two-stream CNN framework to handle spatial and temporal feature separately. In this paper, we propose an endto-end deep network called Tube Convolutional Neural Network (T-CNN) for action detection in videos. The proposed architecture is a unified deep network that is able to recognize and localize action based on 3D convolution features. A video is first divided into equal length clips and next for each clip a set of tube proposals are generated based on 3D Convolutional Network (ConvNet) features. Finally, the tube proposals of different clips are linked together employing network flow and spatio-temporal action detection is performed using these linked video proposals. Extensive experiments on several video datasets demonstrate the superior performance of T-CNN for classifying and localizing actions in both trimmed and untrimmed videos compared to state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of action detection is to detect every occurrence of a given action within a long video, and to localize each detection both in space and time. Deep learning learning based approaches have significantly improved video action recognition performance. Compared to action recognition, action detection is a more challenging task due to flexible volume shape and large spatio-temporal search space.</p><p>Previous deep learning based action detection approaches first detect frame-level action proposals by popular proposal algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref> or by training proposal networks <ref type="bibr" target="#b18">[19]</ref>. Then the frame-level action proposals are asso- ciated across frames to form final action detections through tracking based approaches. Moreover, in order to capture both spatial and temporal information of an action, twostream networks (a spatial CNN and a motion CNN) are used. In this manner, the spatial and motion information are processed separately.</p><p>Region Convolution Neural Network (R-CNN) for object detection in images was proposed by Girshick et al. <ref type="bibr" target="#b3">[4]</ref>. It was followed by a fast R-CNN proposed in <ref type="bibr" target="#b2">[3]</ref>, which includes the classifier as well. Later, faster R-CNN <ref type="bibr" target="#b20">[21]</ref> was developed by introducing a region proposal network. It has been extensively used to produce excellent results for object detection in images. A natural generalization of the R-CNN from 2D images to 3D spatio-temporal volumes is to study their effectiveness for the problem of action detection in videos. A straightforward spatio-temporal generalization of the R-CNN approach would be to treat action detection in videos as a set of 2D image detections using faster R-CNN. However, unfortunately, this approach does not take the temporal information into account and is not sufficiently expressive to distinguish between actions.</p><p>Inspired by the pioneering work of faster R-CNN, we propose Tube Convolutional Neural Network (T-CNN) for action detection. To better capture the spatio-temporal in-formation of video, we exploit 3D ConvNet for action detection, since it is able to capture motion characteristics in videos and shows promising result on video action recognition. We propose a novel framework by leveraging the descriptive power of 3D ConvNet. In our approach, an input video is divided into equal length clips first. Then, the clips are fed into Tube Proposal Network (TPN) and a set of tube proposals are obtained. Next, tube proposals from each video clip are linked according to their actionness scores and overlap between adjacent proposals to form a complete tube proposal for spatio-temporal action localization in the video. Finally, the Tube-of-Interest (ToI) pooling is applied to the linked action tube proposal to generate a fixed length feature vector for action label prediction.</p><p>Our work makes the following contributions:</p><p>• We propose an end-to-end deep learning based approach for action detection in videos. It directly operates on the original videos and captures spatio-temporal information using a single 3D network to perform action localization and recognition based on 3D convolution features. To the best of our knowledge, it is the first work to exploit 3D ConvNet for action detection.</p><p>• We introduce a Tube Proposal Network, which leverages skip pooling in temporal domain to preserve temporal information for action localization in 3D volumes.</p><p>• We propose a new pooling layer -Tube-of-Interest (ToI) pooling layer in T-CNN. The ToI pooling layer is a 3D generalization of Region-of-Interest (RoI) pooling layer of R-CNN. It effectively alleviates the problem with variable spatial and temporal sizes of tube proposals. We show that ToI pooling can greatly improve the recognition results.</p><p>• We extensively evaluate our T-CNN for action detection in both trimmed videos from UCF-Sports, J-HMDB and UCF-101 datasets and untrimmed videos from THU-MOS'14 dataset and achieve state-of-the-art performance. The source code of T-CNN will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Convolutional Neural Networks (CNN) have been demonstrated to achieve excellent results for action recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Karpathy et al. <ref type="bibr" target="#b13">[14]</ref> explore various framelevel fusion methods over time. Ng et al. <ref type="bibr" target="#b30">[31]</ref> use recurrent neural network employing the CNN feature. Since these approaches only use frame based CNN features, the temporal information is neglected. Simonyan et al. <ref type="bibr" target="#b22">[23]</ref> propose the two-stream CNN approach for action recognition. Besides a classic CNN which takes images as an input, it has a separate network for optical flow. Moreover, Wang et al. fuse the trajectories and CNN features. Although these methods, which take hand-crafted temporal feature as a separate stream, show promising performance on action recognition, however, they do not employ end to end deep network and require separate computation of optical flow and optimiza-tion of the parameters. 3D CNN is a logical solution to this issue. Ji et al. <ref type="bibr" target="#b8">[9]</ref> propose a 3D CNN based human detector and head tracker to segment human subjects in videos. Tran et al. <ref type="bibr" target="#b27">[28]</ref> leverage 3D CNN for large scale action recognition problem. Sun et al. <ref type="bibr" target="#b25">[26]</ref> propose a factorization of 3D CNN and exploit multiple ways to decompose convolutional kernels. However, to the best of our knowledge, we are the first ones to exploit 3D CNN for action detection.</p><p>Compared to action recognition, action detection is a more challenging problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>, which has been an active area of research. Ke et al. <ref type="bibr" target="#b14">[15]</ref> present an approach for event detection in crowded videos. Tian et al. <ref type="bibr" target="#b26">[27]</ref> develop Spatio-temporal Deformable Parts Model <ref type="bibr" target="#b0">[1]</ref> to detect actions in videos. Jain et al. <ref type="bibr" target="#b5">[6]</ref> and Soomro et al. <ref type="bibr" target="#b23">[24]</ref> use supervoxel and selective search to localize the action boundaries. Recently, researchers have leveraged the power of deep learning for action detection. Authors in <ref type="bibr" target="#b4">[5]</ref> extract frame-level action proposals using selective search and link them using Viterbi algorithm. While in <ref type="bibr" target="#b29">[30]</ref> frame-level action proposals are obtained by EdgeBox and linked by a tracking algorithm. Two-stream R-CNNs for action detection is proposed in <ref type="bibr" target="#b18">[19]</ref>, where a spatial Region Proposal Network (RPN) and a motion RPN are used to generate frame-level action proposals. However, these deep learning based approaches detect actions by linking frame-level action proposals and treat the spatial and temporal features of a video separately by training two-stream CNN. Therefore, the temporal consistency in videos is not well explored in the network. In contrast, we determine action tube proposals directly from input videos and extract compact and more effective spatio-temporal features using 3D CNN.</p><p>For object detection in images, Girshick et al. propose Region CNN (R-CNN) <ref type="bibr" target="#b3">[4]</ref>. In their approach region proposals are extracted using selective search. Then the candidate regions are warped to a fixed size and fed into ConvNet to extract CNN features. Finally, SVM model is trained for object classification. A fast version of R-CNN, Fast R-CNN, is presented in <ref type="bibr" target="#b2">[3]</ref>. Compared to the multi-stage pipeline of R-CNN, fast R-CNN incorporates object classifier in the network and trains object classifier and bounding box regressor simultaneously. Region of interest (RoI) pooling layer is introduced to extract fixed-length feature vectors for bounding boxes with different sizes. Recently, faster R-CNN is proposed in <ref type="bibr" target="#b20">[21]</ref>. It introduces a RPN (Region Proposal Network) to replace selective search for proposal generation. RPN shares full image convolutional features with the detection network, thus the proposal generation is almost cost-free. Faster R-CNN achieves state-ofthe-art object detection performance while being efficient during testing. Motivated by its high performance, in this paper we explore generalizing faster R-CNN from 2D image regions to 3D video volumes for action detection. name kernel dims output dims</p><formula xml:id="formula_0">(d × h × w) (C × D × H × W ) conv1 3 × 3 × 3 64 × 8 × 300 × 400 max-pool1 1 × 2 × 2 64 × 8 × 150 × 200 conv2 3 × 3 × 3 128 × 8 × 150 × 200 max-pool2 2 × 2 × 2 128 × 4 × 75 × 100 conv3a 3 × 3 × 3 256 × 4 × 75 × 100 conv3b 3 × 3 × 3 256 × 4 × 75 × 100 max-pool3 2 × 2 × 2 256 × 2 × 38 × 50 conv4a 3 × 3 × 3 512 × 2 × 38 × 50 conv4b 3 × 3 × 3 512 × 2 × 38 × 50 max-pool4 2 × 2 × 2 512 × 1 × 19 × 25 conv5a 3 × 3 × 3 512 × 1 × 19 × 25 conv5b 3 × 3 × 3 512 × 1 × 19 × 25 toi-pool2* - 128 × 8 × 8 × 8 toi-pool5 - 512 × 1 × 4 × 4 1x1 conv - 8192 fc6 - 4096 fc7</formula><p>-4096 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generalizing R-CNN from 2D to 3D</head><p>Generalizing R-CNN from 2D image regions to 3D video tubes is challenging due to the asymmetry between space and time. Different from images which can be cropped and reshaped into a fixed size, videos vary widely in temporal dimension. Therefore, we divide input videos into fixed length (8 frames) clips, so that video clips can be processed with a fixed-size ConvNet architecture. Also, clip based processing mitigates the cost of GPU memory.</p><p>To better capture the spatio-temporal information in video, we exploit 3D CNN for action proposal generation and action recognition. One advantage of 3D CNN over 2D CNN is that it captures motion information by applying convolution in both time and space. Since 3D convolution and 3D max pooling are utilized in our approach, not only in the spatial dimension but also in the temporal dimension, the size of video clip is reduced while distinguishable information is concentrated. As demonstrated in <ref type="bibr" target="#b27">[28]</ref>, the temporal pooling is important for recognition task since it better models the spatio-temporal information of video and reduces some background noise. However, the temporal order information is lost. That means if we arbitrarily change the order of the frames in a video clip, the resulting 3D max pooled feature cube will be the same. This is problematic in action detection, since it relies on the feature cube to get bounding boxes for the original frames. To this end, we incorporate temporal skip pooling to retain temporal order information residing in the original frames. More details are provided in the next section. Since a video is processed clip by clip, action tube proposals with various spatial and temporal sizes are generated for different clips. These clip proposals need to be linked into a tube proposal sequence, which is used for action label prediction and localization. To produce a fixed length feature vector, we propose a new pooling layer -Tube-of-Interest (ToI) pooling layer. The ToI pooling layer is a 3D generalization of Region-of-Interest (RoI) pooling layer of R-CNN. The classic max pooling layer defines the kernel size, stride and padding which determines the shape of the output. In contrast, for RoI pooling layer, the output shape is fixed first, then the kernel size and stride are determined accordingly. Compared to RoI pooling which takes 2D feature map and 2D regions as input, ToI pooling deals with feature cube and 3D tubes. Denote the size of a feature cube as d × h × w, where d, h and w respectively represent the depth, height and width of the feature cube. A ToI in the feature cube is defined by a d-by-4 matrix, which is composed of d boxes distributed in all the frames. The boxes are defined by a four-tuple (x i 1 , y i 1 , x i 2 , y i 2 ) that specifies the top-left and bottom-right corners in the i-th feature map. Since the d bounding boxes may have different sizes, aspect ratios and positions, in order to apply spatio-temporal pooling, pooling in spatial and temporal domains are performed separately. First, the h × w feature maps are divided into H × W bins, where each bin corresponds to a cell with size of approximately h/H × w/W . In each cell, max pooling is applied to select the maximum value. Second, the spatially pooled d feature maps are temporally divided into D bins. Similar to the first step, d/D adjacent feature maps are grouped together to perform the standard temporal max pooling. As a result the fixed output size of ToI pooling layer is D × H × W . A graphical illustration of ToI pooling is presented in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Back-propagation of ToI pooling layer routes the deriva-tives from output back to the input. Assume x i is the i-th activation to the ToI pooling layer, and y j is the j-th output. Then the partial derivative of the loss function (L) with respect to each input variable x i can be expressed as:</p><formula xml:id="formula_1">∂L ∂x i = j [i = f (j)] ∂L ∂y j .<label>(1)</label></formula><p>Each pooling output y j has a corresponding input position i. We use a function f (·) to represent the argmax selection. Thus, the gradient from the next layer ∂L/∂y j is passed back to only that neuron which achieved the max ∂L/∂x i . Since one input may correspond to multiple outputs, the partial derivatives are the accumulation of multiple sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">T-CNN Pipeline</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our T-CNN is an end-to-end deep learning framework that takes video clips as input. The core component is the Tube Proposal Network (TPN) (see <ref type="figure">Figure  3</ref>) to produce tube proposals for each clip. Linked tube proposal sequence represents spatio-temporal action detection in the video and is also used for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Tube Proposal Network</head><p>For a 8-frame video clip, 3D convolution and 3D pooling are used to extract spatio-temporal feature cube. In 3D ConvNet, convolution and pooling are performed spatiotemporally. Therefore, the temporal information of the input video is preserved. Our 3D ConvNet consists of seven 3D convolution layers and four 3D max-pooling layers. We denote the kernel shape of 3D convolution/pooling by d×h×w, where d, h, w are depth, height and width, respectively. In all convolution layers, the kernel sizes are 3×3×3, padding and stride remain as 1. The numbers of filters are 64, 128 and 256 respectively in the first 3 convolution layers and 512 in the remaining convolution layers. The kernel size is set to 1 × 2 × 2 for the first 3D max-pooling layer, and 2 × 2 × 2 for the remaining 3D max-pooling layers. The details of network architecture are presented in <ref type="table" target="#tab_0">Table 1</ref>. We use the C3D model <ref type="bibr" target="#b27">[28]</ref> as the pre-trained model and fine tune it on each dataset in our experiments.</p><p>After conv5, the temporal size is reduced to 1 frame (i.e. feature cube with depth D = 1). In the feature tube, each frame/slice consists of a number of channels specified in <ref type="table" target="#tab_0">Table 1</ref>. Here, we drop the number of channels for ease of explanation. Following faster R-CNN, we generate bounding box proposals based on the conv5 feature cube.</p><p>Anchor bounding boxes selection. In faster R-CNN, the bounding box dimensions are hand picked, i.e. 9 anchor boxes with 3 scales and 3 aspect ratios. We can directly adopt the same anchor boxes in our T-CNN framework. However, it has been shown in <ref type="bibr" target="#b12">[13]</ref> that if we choose better priors as initializations for the network, it will help the network learn better for predicting good detections. Therefore, instead of choosing hand-picked anchor boxes, we apply k-means clustering on the training set bounding boxes to learn 12 anchor boxes (i.e. clustering centroids). This data driven anchor box selection approach is adaptive to different datasets.</p><p>Each bounding box is associated with an "actionness" score, which measures the probability that the content in the box corresponds to a valid action. We assign a binary class label (of being an action or not) to each bounding box. Bounding boxes with actionness scores smaller than a threshold are discarded. In the training phase, the bounding box which has an IoU overlap higher than 0.7 with any ground-truth box or has the highest Intersection-over-Union (IoU) overlap with a ground-truth box (the later condition is considered in case the former condition may find no positive sample) is considered as a positive bounding box proposal.</p><p>Temporal skip pooling. Bounding box proposals generated from conv5 feature tube can be used for frame-level action detection by bounding box regression. However, due to temporal concentration (8 frames to 1 frame) of temporal max pooling, the temporal order of the original 8 frames is lost. Therefore, we use temporal skip pooling to inject the temporal order for frame-level detection. Specifically, we map each positive bounding box generated from conv5 feature tube to conv2 feature tube which has 8 feature frames/slices. Since these 8 feature slices correspond to the original 8 frames in the video clip, the temporal order information is preserved. As a result, if there are 5 bounding boxes in conv5 feature tube for example, 5 scaled bounding boxes are mapped in each conv2 feature slice at the corresponding locations. This creates 5 tube proposals as illustrated in <ref type="figure">Figure 3</ref>, which are paired with the corresponding 5 bounding box proposals for frame-level action detection. To form a fixed feature shape, ToI pooling is applied to the variable size tube proposals as well as the bounding box proposals. Since a tube proposal covers 8 frames, the ToI pooled bounding box is duplicated 8 times to form a tube. We then L2 normalize the paired two tubes and perform vectorization. For each frame, features are concatenated. Since we use the C3D model <ref type="bibr" target="#b27">[28]</ref> as the pre-trained model, we connect a 1x1 convolution to match the input dimension of fc6. Three fully-connected layers process each descriptor and produce the output: displacement of height, width and center coordinate of each bounding box ("bbox") in each frame. Finally, a set of refined tube proposals are generated as an output from the TPN representing spatio-temporal action localization of the input video clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Linking Tube Proposals</head><p>We obtain a set of tube proposals for each video clip after the TPN. We then link these tube proposals to form a proposal sequence for spatio-temporal action localization of the entire video. Each tube proposal from different clips can be linked in a tube proposal sequence (i.e. video tube proposal) for action detection. However, not all combinations of tube proposals can correctly capture the complete action. For example, a tube proposal in one clip may contain the action and a tube proposal in the following clip may only capture the background. Intuitively, the content within the selected tube proposals should capture an action and connected tube proposals in any two consecutive clips should have a large temporal overlap. Therefore, two criteria are considered when linking tube proposals: actionness and overlap scores. Each video proposal is then assigned a score defined as follows:</p><formula xml:id="formula_2">S = 1 m m i=1 Actionness i + 1 m − 1 m−1 j=1</formula><p>Overlap j,j+1 <ref type="bibr" target="#b1">(2)</ref> where Actionness i denotes the actionness score of the tube proposal from the i-th clip, Overlap j,j+1 measures the overlap between the linked two proposals respectively from the j-th and (j + 1)-th clips, and m is the total number of video clips. As shown in <ref type="figure">Figure 3</ref>, each bounding box proposal from conv5 feature tube is associated with an actionness score. The actionness scores are inherited by the corresponding tube proposals. The overlap between two tube proposals is calculated based on the IoU (Intersection Over Union) of the last frame of the j-th tube proposal and the first frame of the (j+1)-th tube proposal. The first term of S computes the average actionness score of all tube proposals in a video proposal and the second term computes the average overlap between the tube proposals in every two consecutive video clips. Therefore, we ensure the linked tube proposals can encapsulate the action and at the same time have temporal consistency. An example of linking tube proposals and computing scores is illustrated in <ref type="figure">Figure 4</ref>. We choose a number of linked proposal sequences with highest scores in a video (see more details in Sec. 5.1).  <ref type="figure">Figure 4</ref>: An example of linking tube proposals in each video clips using network flow. In this example, there are three video clips and each has two tube proposals, resulting in 8 video proposals. Each video proposal has a score, e.g. S1, S2, ..., S8, which is computed according to Eq. (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Action Detection</head><p>After linking tube proposals, we get a set of linked tube proposal sequences, which represent potential action instances. The next step is to classify these linked tube proposal sequences. The tube proposals in the linked sequences may have different sizes. In order to extract a fixed length feature vector from each of the linked proposal sequence, our proposed ToI pooling is utilized. Then the ToI pooling layer is followed by two fully-connected layers and a dropout layer. The dimension of the last fully-connected layer is N + 1 (N action classes and 1 background class).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To verify the effectiveness of the proposed T-CNN for action detection, we evaluate T-CNN on three trimmed video datasets including UCF-Sports <ref type="bibr" target="#b21">[22]</ref>, J-HMDB <ref type="bibr" target="#b7">[8]</ref>, UCF-101 <ref type="bibr" target="#b10">[11]</ref> and one un-trimmed video dataset -THU-MOS'14 <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>We implement our method based on the Caffe toolbox <ref type="bibr" target="#b9">[10]</ref>. The TPN and recognition network share weights in their common layers. Due to memory limitation, in training phase, each video is divided into overlapping 8-frame clips with resolution 300 × 400 and temporal stride 1. When training the TPN network, each anchor box is assigned a binary label. Either the anchor box which has the highest IoU overlap with a ground-truth box, or an anchor box that has an IoU overlap higher than 0.7 with any ground-truth box is assigned a positive label, the rest are assigned negative label. In each iteration, 4 clips are fed into the network. Since the number of background boxes is much more than that of action boxes, to well model the action, we randomly select some of the negative boxes to balance the number of positive and negative samples in a batch. For recognition network training, we choose 40 linked proposal sequences with highest scores in a video as Tubes of Interest.</p><p>Our model is trained in an alternative manner. First, Initialize TPN based on the pre-trained model in <ref type="bibr" target="#b27">[28]</ref>, then using the generated proposals to initialize recognition networks. Next, the weights tuned by recognition network are used to update TPN. Finally, the tuned weights and proposals from TPN are used for finalizing recognition network. For all the networks for UCF-Sports and J-HMDB, the learning rate is initialized as 10 −3 and decreased to 10 −4 after 30k batches. Training terminates after 50k batches. For UCF-101 and THUMOS'14, the learning rate is initialized as 10 −3 and decreased to 10 −4 after 60k batches. Training terminates after 100k batches.</p><p>During testing, each video is divided into nonoverlapping 8-frame clips. If the number of frames in video cannot be divided by 8, we pad zeros after the last frame to make it dividable. 40 tube proposals with highest actionness confidence through TPN are chosen for the linking process. Non-maximum suppression (NMS) is applied to linked proposals to get the final action detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets and Experimental Results</head><p>UCF-Sports. This dataset contains 150 short action videos of 10 different sport classes. Videos are trimmed to the action and bounding boxes annotations are provided for all frames. We follow the standard training and test split defined in <ref type="bibr" target="#b15">[16]</ref>.</p><p>We use the usual IoU criterion and generate ROC curve in <ref type="figure" target="#fig_4">Figure 5</ref>(a) when overlap criterion equals to α = 0.2. <ref type="figure" target="#fig_4">Figure 5</ref>(b) illustrates AUC (Area-Under-Curve) measured with different overlap criterion. In direct comparison, our T-CNN clearly outperforms all the competing methods shown in the plot. We are unable to directly compare the detection accuracy against Peng et al. <ref type="bibr" target="#b19">[20]</ref> in the plot, since they do not provide the ROC and AUC curves. As shown in <ref type="table">Table  2</ref>, the frame level mAP of our approach outperforms theirs in 8 actions out of 10. Moreover, by using the same metric, the video mAP of our approach reaches 95.2 (α = 0.2 and 0.5), while they report 94.8 (α = 0.2) and 94.7 (α = 0.5).</p><p>J-HMDB. This dataset consists of 928 videos with 21 different actions. All the video clips are well trimmed. There are three train-test splits and the evaluation is done on the average results over the three splits. The experiment results comparison is shown in <ref type="table" target="#tab_3">Table 3</ref>. We report our results with 3 metrics: frame-mAP, the average precision of detection at frame level as in <ref type="bibr" target="#b4">[5]</ref>; video-mAP, the average precision at video level as in <ref type="bibr" target="#b4">[5]</ref> with IoU threshold α = 0.2 and α = 0.5. It is evident that our T-CNN consistently outperforms the state-of-the-art approaches in terms of all three evaluation metrics.</p><p>UCF101. This dataset with 101 actions is commonly used for action recognition. For action detection task, a subset of 24 action classes and 3, 207 videos have spatiotemporal annotations. Similar to other methods, we perform the experiments on the first train/test split only. We report our results in <ref type="table" target="#tab_4">Table 4</ref> with 3 metrics: frame-mAP, video-mAP (α = 0.2) and video-mAP (α = 0.5). Our approach again yields the best performance. Moreover, we also report the action recognition results of T-CNN on the above three datasets in <ref type="table">Table 5</ref>.</p><p>THUMOS'14. To further validate the effectiveness of our proposed T-CNN approach for action detection, we evaluate it using the untrimmed videos from the THU-MOS'14 dataset <ref type="bibr" target="#b11">[12]</ref>. The THUMOS'14 spatio-temporal localization task consists of 4 classes of sports actions: BaseballPitch, golfSwing, TennisSwing and ThrowDiscus. There are about 20 videos per class and each video contains 500 to 3, 000 frames. The videos are divided into validation set and test set, but only video in the test set have spatial annotations provided by <ref type="bibr" target="#b24">[25]</ref>. Therefore, we use samples corresponding to those 4 actions in UCF-101 with spatial annotations to train our model.</p><p>In untrimmed videos, there often exist other unrelated actions besides the action of interests. For example, "walking" and "picking up a golf ball" are considered as unrelated actions when detecting "GolfSwing" in video. We denote clips which have positive ground truth annotation as positive clips, and the other clips as negative clips (i.e. clips contain only unrelated actions). If we randomly select negative samples for training, the number of boxes on unrelated   <ref type="table">Table 2</ref>: mAP for each class of UCF-Sports. The IoU threshold α for frame m-AP is fixed to 0.5.</p><formula xml:id="formula_3">f.-mAP v.-mAP v.-mAP (α = 0.5) (α = 0.2) (α = 0.5)</formula><p>Gkioxari et al. <ref type="bibr" target="#b4">[5]</ref> 36.   actions is much smaller than that of background boxes (i.e. boxes capturing only image background). Thus the trained model will have no capability to distinguish action of interest and unrelated actions.</p><p>To this end, we introduce a so called negative sample mining process. Specifically, when initializing the TPN, we only use positive clips. Then we apply the model on the whole training video (both positive clips and negative clips). Most false positives in negative clips should include unrelated actions to help our model learn the correlation between action of interest and unrelated actions. Therefore we select boxes in negative clips with highest scores as hard negatives because low scores probably infer image background. In updating TPN procedure, we choose 32 boxes which have IoU with any ground truth greater than 0.7 as positive samples and randomly pick another 16 samples as negative. We also select 16 samples from hard negative pool as negative. Therefore, we efficiently train a model, which is able to distinguish not only action of interest from background, but also action of interest from unrelated actions.</p><p>The mean ROC curves of different methods on THU-MOS'14 action detection are plotted in <ref type="figure" target="#fig_4">Figure 5</ref>(c). Our method without negative mining performs better than the baseline method Sultani et al. <ref type="bibr" target="#b24">[25]</ref>. Additionally, with negative mining, the performance is further boosted.</p><p>For qualitative results, we shows examples of detected action tubes in videos from UCF-Sports, JHMDB, UCF-101 (24 actions) and THUMOS'14 datasets (see <ref type="figure" target="#fig_5">Figure 6</ref>). Each block corresponds to a different video that is selected from the test set. We show the highest scoring action tube for each video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RunSide Diving</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCF-Sports</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J-HMDB THUMOS'14</head><p>BrushHair Clap</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Riding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCF-101</head><p>SkateBoarding GolfSwing TennisSwing  <ref type="table">Table 5</ref>: Action recognition results of our T-CNN approach on the four datasets.</p><p>6. Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">ToI Pooling</head><p>To evaluate the effectiveness of ToI pooling, we compare action recognition performance on UCF-101 dataset (101 actions) using C3D <ref type="bibr" target="#b27">[28]</ref> and our approach. For the C3D network, we use C3D pre-train model from <ref type="bibr" target="#b27">[28]</ref> and fine tune the weights on UCF-101 dataset. In the C3D fine tuning process, a video is divided into 16 frames clips first. Then the C3D network is fed by clips and outputs a feature vector for each clip. Finally, a SVM classifier is trained to predict the labels of all clips, and the video label is determined by the predictions of all clips belonging to the video. Compared to the original C3D network, we use the ToI pooling layer to replace the 5-th 3d-max-pooling layer in C3D pipeline. Similar to C3D network, our approach takes clips from a video as input. The ToI pooling layer takes the whole clip as tube of interest and the pooled depth is set to 1. As a result, each video will output one feature vector. Therefore, it is an end-to-end deep learning based video recognition approach. Video level accuracy is used as the metric. The results are shown in <ref type="table">Table 6</ref>. For a direct comparison, we only use the result from deep network without fusion with other features. Our approach shows a 5.2% accuracy improvement compared to the original C3D. Our ToI pooling based pipeline optimizes the weights for the whole video directly, while C3D performs clip-based optimization. Therefore, our approach can better capture the spatio-temporal information of the entire video. Furthermore, our ToI pooling can be combined with other deep learning based pipelines, such as two-stream CNN <ref type="bibr" target="#b22">[23]</ref>. C3D <ref type="bibr" target="#b27">[28]</ref> Ours Accuracy (%) 82.3 87.5 <ref type="table">Table 6</ref>: Video action recognition results on UCF-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Temporal Skip Connection</head><p>Since we use overlapping clips with temporal stride of 1 in training, a particular frame is included in multiple training clips at different temporal positions. The actual temporal information of that particular frame is lost if we only use the conv5 feature cube to infer action bounding boxes. Especially when the action happens periodically (i.e. Swing-Bench), it always fails to locate a phase of spinning. On the contrary, by combining conv5 with conv2 through temporal skip pooling, temporal order is preserved to localize actions more accurately. To verify the effectiveness of temporal skip pooling in our proposed TPN, we conduct an experiment using our method without skip connection. In other words, we perform bounding box regression to estimate bounding boxes in 8 frames simultaneously using only the conv5 feature cube. As shown in <ref type="table" target="#tab_3">Table 3</ref>, without skip connection, the performance decreases a lot, demonstrating the advantage of skip connection for extracting temporal order information and detailed motion in original frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Computational Cost</head><p>We carry out our experiments on a workstation with one GPU (Nvidia GTX Titan X). For a 40-frames video, it takes 1.1 seconds to generate tube proposals, 0.03 seconds to link tube proposals in a video and 0.9 seconds to predict action label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper we propose an end-to-end Tube Convolutional Neural Network (T-CNN) for action detection in videos. It exploits 3D convolutional network to extract effective spatio-temporal features and perform action localization and recognition in a unified framework. Coarse proposal boxes are densely sampled based on the 3D convolutional feature cube and linked for action recognition and localization. Extensive experiments on several benchmark datasets demonstrate the strength of T-CNN for spatiotemporal localizing actions, even in untrimmed videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Appendix</head><p>In this supplemental material, we provide more discussions on our proposed T-CNN framework for action detection. We also include some video clips showing the detection results of our T-CNN approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Analysis of Temporal Skip Pooling</head><p>In our paper, we already compared the performance of T-CNN with and without temporal skip pooling on J-HMDB dataset ( <ref type="table" target="#tab_3">Table  3</ref>). Here we show more results on UCF-Sports dataset to demonstrate the power of temporal skip pooling.</p><p>All the experiments use the same parameters. The only difference is which feature cubes are connected with the feature cube from conv5, denoted as C5. As shown in the table, C5 only has the lowest performance since it is not able to distinguish the differences between frames. C5 + C1 perform much better than C5 only. Since conv1 output maintains the temporal structure (8 frames) and preserves detailed motion information. However, the first convolution layer only captures some low level properties, which may not be useful. C5+C2, which we reported in our submission, has the best performance. On one hand, conv2 maintains the temporal structure as well. On the other hand, it captures more high level information than conv1, which may be directly related to some particular classes. Since conv3 feature cube has a reduced temporal dimension of 4, it preserves less temporal information. Therefore the performance of C5 + C3 is slightly worse than C5 + C2. For the same reason, C5 + C4 perform better than C5, but worse than C5 + C3. In summary, the spatio-temporal localization model needs descriptive as well as distinguishing capabilities. The output of latter convolutional layers are more distinguishable, but due to the temporal collapse, they have less descriptive property. When we concatenate conv5 with conv2, which preserves much more spatial and temporal information, the better performance is achieved. It is also worth noting that more convolution layers can be concatenated e.g. C5 + C2 + C3 to further improve the performance. However, it will increase the computational burden as well as memory cost. As a trade off between performance and computational &amp; memory efficiency, we only concatenate two convolutional feature cubes in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">More Action Detection Results</head><p>Besides the detection results presented in the paper ( <ref type="figure" target="#fig_5">Figure 6</ref>), we include some video clips showing the detection results.</p><p>The videos can be viewed at https://www.dropbox.com/sh/6bakdb9s88u8zif/ AAB3SxJkvvtdQm0JxtYVLkMma?dl=0</p><p>The first 4 videos are from different datasets. The green boxes represent the groundtruth annotation and red ones are our detection results. All the videos are encoded by DivX contained in an AVI.</p><p>The 5-th video is downloaded from YouTube, which does not appear in any existing dataset. We use the model trained on UCF-Sports and detect the runners. This video is much more complex than UCF-Sports. The background has crowded people with motions e.g. waving hands and waving flags, and the camera motion is more severe. Here, we show some key frames of the clip in <ref type="figure">Figure</ref> A. As can be seen, our T-CNN is able to successfully detect the action "running" from both runners given complicated background and noise disturbance (e.g. camera motion and background actions). This demonstrates the generalization ability of our approach to action detection in videos with complex actions.</p><p>The 6-th and 7-th videos are another two such videos downloaded from YouTube, corresponding to actions "horse riding" and "diving", respectively. Example action detection results of our T-CNN for these two videos are presented in <ref type="figure" target="#fig_8">Figure B</ref>     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the proposed Tube Convolutional Neural Network (T-CNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Tube of interest pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5 Figure 3 :</head><label>53</label><figDesc>Tube proposal network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The ROC and AUC curves for UCF-Sports Dataset<ref type="bibr" target="#b21">[22]</ref> are shown in (a) and (b), respectively. The results are shown for Jain et al.[6] (green), Tian et al. [27] (purple), Soomro et al. [24] (blue), Wang et al. [29] (yellow), Gkioxari et al. [5] (cyan) and Proposed Method (red). (c) shows the mean ROC curves for four actions of THUMOS'14. The results are shown for Sultani et al. [25] (green), proposed method (red) and proposed method without negative mining (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Action detection results by T-CNN on UCF-Sports, JHMDB, UCF-101 and THUMOS'14. Red boxes indicate the detections in the corresponding frames, and green boxes denote ground truth. The predicted label is overlaid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>and Figure C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A .</head><label>A</label><figDesc>Example frames of action detection results on a YouTube video "running".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure B .</head><label>B</label><figDesc>Example frames of action detection results on a YouTube video "horse riding".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure C .</head><label>C</label><figDesc>Example frames of action detection results on a YouTube video "diving".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Network architecture of T-CNN. We refer kernel with shape d×h×w where d is the kernel depth, h and w are height and width. Output matrix with shape C ×D×H ×W where C is number of channels, D is the number of frames, H and W are the height and width of frame. toi-pool2 only exists in TPN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison to the state-of-the-art on J-HMDB. The IoU threshold α for frame m-AP is fixed to 0.5.</figDesc><table><row><cell></cell><cell>f.-mAP</cell><cell></cell><cell cols="2">video-mAP</cell></row><row><cell>IoU th.</cell><cell></cell><cell>0.05</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell>Weinzaepfel et al. [30]</cell><cell>35.84</cell><cell>54.3</cell><cell>51.7</cell><cell cols="2">46.8 37.8</cell></row><row><cell>Peng et al. [19]</cell><cell>39.63</cell><cell>54.5</cell><cell>50.4</cell><cell cols="2">42.3 32.7</cell></row><row><cell>Ours</cell><cell>41.37</cell><cell>54.7</cell><cell>51.3</cell><cell cols="2">47.1 39.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison to the state-of-the-art on UCF-101 (24 actions). The IoU threshold α for frame m-AP is fixed to 0.5.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Temporal localization of actions with actoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2782" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242v1</idno>
		<title level="m">Yolo9000: Better, faster, stronger</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Event detection in crowded videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative figure-centric models for joint action localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2003" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Human action recognition in videos: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Negin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-region two-stream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="744" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-region two-stream R-CNN for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016 -European Conference on Computer Vision</title>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Action mach: a spatio-temporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action localization in videos through context walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3280" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What if we do not have multiple videos of the same action? -video action localization using web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4597" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatiotemporal deformable part models for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2642" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video action detection with relational dynamic-poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to track for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3164" to="3172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
