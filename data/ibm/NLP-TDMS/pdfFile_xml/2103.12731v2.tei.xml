<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling Local Self-Attention for Parameter Efficient Visual Backbones</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scaling Local Self-Attention for Parameter Efficient Visual Backbones</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-attention has the promise of improving computer vision systems due to parameter-independent scaling of receptive fields and content-dependent interactions, in contrast to parameter-dependent scaling and content-independent interactions of convolutions. Self-attention models have recently been shown to have encouraging improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50. In this work, we aim to develop self-attention models that can outperform not just the canonical baseline models, but even the high-performing convolutional models. We propose two extensions to selfattention that, in conjunction with a more efficient implementation of self-attention, improve the speed, memory usage, and accuracy of these models. We leverage these improvements to develop a new self-attention model family, HaloNets, which reach state-of-the-art accuracies on the parameterlimited setting of the ImageNet classification benchmark. In preliminary transfer learning experiments, we find that HaloNet models outperform much larger models and have better inference performance. On harder tasks such as object detection and instance segmentation, our simple local self-attention and convolutional hybrids show improvements over very strong baselines. These results mark another step in demonstrating the efficacy of self-attention models on settings traditionally dominated by convolutional models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision and natural language processing (NLP) systems divide the landscape of computational primitives. While self-attention is the primary workhorse in NLP, convolutions are ubiquitous in nearly all vision models. Convolutions embody the principle of local processing, to learn local spatial features such as edges and texture that are abundant in images. On the other hand, the Transformer <ref type="bibr" target="#b56">[57]</ref> showed that self-attention is an effective and computationally efficient mechanism for capturing global interactions between words in a sentence. The success of self-attention in NLP motivates research in understanding how self-attention can improve vision. Self-attention has several properties that make it a good fit for vision: (a) content-based interactions as opposed to content-independent interactions of convolution; (b) parameter-independent scaling of receptive field size as opposed to parameter-dependent scaling of convolution; (c) empirical ability to capture long-range dependencies for use in larger images; (d) flexibility to handle and integrate multiple types of data that appear in vision, such as pixels <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b65">66]</ref>, point clouds <ref type="bibr" target="#b62">[63]</ref>, sequence conditioning information <ref type="bibr" target="#b61">[62]</ref>, and graphs <ref type="bibr" target="#b31">[32]</ref>. Self-attention may also be regarded as an adaptive nonlinearity paralleling a long history of nonlinear processing techniques in computer vision, such as bilateral filtering <ref type="bibr" target="#b38">[39]</ref> and non-local means <ref type="bibr" target="#b3">[4]</ref>.</p><p>Several recent papers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b49">50]</ref> have attempted using self-attention primitives to improve image classification accuracy over the strong and commonly used ResNet backbones <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Among them, the Stand-Alone Self-Attention (SASA) <ref type="bibr" target="#b42">[43]</ref> is a fully self-attentive model that replaces every spatial convolution with local self-attention, which improves the performance of ResNet backbones while having fewer parameters and floating point operations. While conceptually promising, these models lag behind state-of-the-art convolutional models in image classification. State-of-the-art convolutional models <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b41">42]</ref> use a variety of scaling techniques to achieve strong performance across a range of computation and parameter regimes. In this work, we aim to develop and understand techniques for scaling local self-attention models to outperform some of the best convolutional models. Scaling self-attention models presents a unique set of challenges. For example, convolutions have been very efficiently mapped to matrix accelerators such as TPUs and GPUs that drive most deep learning workloads, but fast implementations of local 2D self-attention do not currently exist. To bridge this gap, we introduce a non-centered version of local attention that efficiently maps to existing hardware with haloing. While our formulation breaks translational equivariance, it improves both throughput and accuracies over the centered local selfattention used in SASA. We also introduce a strided selfattentive downsampling operation for multi-scale feature extraction.</p><p>We leverage these techniques to develop a new local selfattention model family, HaloNet, which achieves state-ofthe-art performance across different parameter regimes. The largest HaloNet achieves 84.9% top-1 accuracy on the Im-ageNet <ref type="bibr" target="#b46">[47]</ref> classification benchmark (Section 4.1). We perform a detailed study to uncover how self-attention and convolutional models scale differently. Our self-attention layers also show promising results on harder tasks such as object detection and instance segmentation (Section 4.6) using the Mask R-CNN framework on the COCO benchmark. Finally, we end with a discussion of current limitations and ideas for future work in applying self-attention to vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Models and Methods</head><p>Although our models use self-attention instead of convolutions for capturing spatial interactions between pixels, they adopt some important architectural features of modern convolutional neural networks (CNNs). Like CNNs, we compute multi-scale feature hierarchies <ref type="bibr" target="#b33">[34]</ref> which enable detecting objects at multiple sizes in tasks such as localization and instance segmentation. For this, we develop a strided selfattention layer, a natural extension of strided convolutions (Section 2.2). To deal with the computational cost in larger resolutions where global attention is infeasible, we follow the fairly general principle of local processing, which is at the heart of convolutions and natural perceptual systems <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, and use spatially restricted forms of self-attention. However, unlike the model of <ref type="bibr" target="#b42">[43]</ref>, that also use local self-attention, we abstain from enforcing translation equivariance in lieu of better hardware utilization, which improves the speedaccuracy tradeoff (Section 2.2). Also note that while we use local attention, our receptive fields per pixel are quite large (up to 18 × 18) and we show in Section 4.2.2 that larger receptive fields help with larger images. In the remainder of this section, we will motivate self-attention for vision tasks and describe how we relax translational equivariance to efficiently map local self-attention to hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Self-attention can generate spatially varying convolutional filters</head><p>Recent work <ref type="bibr" target="#b7">[8]</ref> has shown that self-attention with sufficient number of heads and the right geometric biases can simulate convolutions, suggesting a deeper relationship between self-attention and convolutions. Self-attention has been viewed as a method to directly capture relationships between distant pixels <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b57">58]</ref>. It has also been interpreted as a specific instantiation of the classic technique of non-local means <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b58">59]</ref>. The perspective that we discuss in this section is one that views self-attention as generating spatially varying filters, in contrast to the reuse of the same filter across every spatial location in standard convolutions <ref type="bibr" target="#b13">[14]</ref>. To observe this, we write self-attention and convolution as specific instances of a general spatial pooling function. Given an input x ∈ R H×W ×cin , where H is the height, W is the width, and c in is the number of input channels, we define a local 2D pooling function that computes an output at location (i, j), y ij ∈ R cout as</p><formula xml:id="formula_0">y ij = a,b∈N (i,j) f (i, j, a, b)x ab ,</formula><p>where f (i, j, a, b) is a function that returns a weight matrix W ∈ R cin×cout at every location in a 2D window N (i, j) of size k × k centered at (i, j). Note that later in this section, we introduce non-centered windows for selfattention, but we use centering here for ease of explanation. This computation is repeated for every pixel (i, j). For a convolution, f (i, j, a, b) returns a different linear transformation for each relative distance in neighborhood, and these weights are shared across all (i, j). Weight sharing significantly reduces parameters and encourages learning features that repeat spatially. In dot-product relative selfattention <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b2">3]</ref> (eqs. <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula" target="#formula_1">(3)</ref>), every pixel in the neighborhood shares the same linear transformation which is multiplied by a scalar probability that is a function of both content-content and content-geometry interactions resulting in weights that can vary spatially. As an example, for a ball and an orange at two different locations in an image, pixels inside the ball and the orange are likely to generate different p ij a−i,b−j because of the different content around them, such as color or texture.</p><formula xml:id="formula_1">f (i, j, a, b) conv = W a−i,b−j (1) f (i, j, a, b) self −att = softmax ab (W Q x ij ) W K x ab + (W Q x ij ) r a−i,b−j W V (2) = p ij a−i,b−j W v<label>(3)</label></formula><p>For self-attention, W Q , W K , and W V are learned linear transformations that are shared across all spatial locations, and respectively produce queries, keys, and values when used to transform x. Spatial geometry is captured by r a−i,b−j , which is a learned relative position based embedding. The (W Q x ij ) W K x ab component captures the content-content interaction between the query pixel and a key pixel in the window. The (W Q x ij ) r a−i,b−j component is the contentgeometry interaction that captures the relationship between     the query and the relative position of the key pixel <ref type="bibr" target="#b47">[48]</ref>. Note that this formulation preserves translational equivariance.</p><p>If an object translates in an image, for any pixel within the object, the content around it stays the same, generating the same p ij a−i,b−j , thereby producing the same output after selfattention. To increase expressivity, multi-headed attention <ref type="bibr" target="#b56">[57]</ref> is used, which repeats this computation multiple times in parallel with different parameters, analogous to group convolutions <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b60">61]</ref>.</p><p>In the SASA model of <ref type="bibr" target="#b42">[43]</ref>, the local window N (i, j) is a k × k window centered around (i, j), just like a convolution. The size of this local window k is an important setting to leverage in self-attention. Unlike dense convolutions, k can grow without significantly increasing the number of parameters. Since the projection parameters (W Q , W K , W V ) are independent of k, the only parameters that increase with k is r a−i,b−j . However, r a−i,b−j constitutes a trivial fraction of the parameters compared to the projection parameters 1 , so increasing k does not not impact the number of parameters of the layer significantly. In contrast, the number of parameters in a convolution layer scale quadratically with k (e.g., <ref type="bibr" target="#b0">1</ref> For a window size as large as 63, and 16 dimensions per attention head, r a−i,b−j would add only 63 * 16 = 1008 parameters per layer because r a−i,b−j are shared among heads. In contrast, if the dimensions of the attention layer were 512, W Q , W K , W V would contribute 786432 parameters. We show details in the appendix. a 5 × 5 convolution has <ref type="bibr">25 9</ref> times the parameters of a 3 × 3 convolution). On the other hand, the computational cost of self-attention grows quadratically with k, preventing the use of very large values for k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Improving the speed-memory tradeoff by relaxing translational equivariance</head><p>Global self-attention, in which all locations attend to each other, is too expensive for most image scales due to the quadratic computation cost with respect to k. Thus, multi-scale visual backbones need to use local attention to limit the size of k. We follow the intuitive form of local attention developed in <ref type="bibr" target="#b42">[43]</ref>, which tries to mimic the square neighborhoods used by convolutions. This form of local attention requires extracting local 2D grids around each pixel. Unfortunately, while deep learning libraries automatically handle neighborhood gathering for convolutions, no such neighborhood gathering function exists for local self-attention (or any general local function). Thus, implementing local self-attention requires explicitly gathering the local neighborhoods before the actual self-attention operation can be performed. While the implementation of this local neighborhood gathering function might initially appear to be a relatively minor implementation detail, in practice, it must actually be carefully designed to reduce memory usage while avoiding unnecessary extra computation. An unoptimized implementation can prevent self-attention models from scaling up due to either out-of-memory errors or excessive slowness. The following discussion frames the design considerations of this neighborhood gathering function.</p><p>A straightforward approach would gather k × k sized windows separately around each pixel. As summarized in <ref type="table">Table 1</ref> (Row 1), this method blows up the memory used by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Neighborhood Memory</head><p>Receptive Field</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FLOPs Per Pixel</head><p>Global HW c HW × HW 4(HW ) 2 c Per pixel windows <ref type="table">Table 1</ref>. Scaling behavior of self-attention mechanisms. f is the number of heads, b is the size of the block, c is the total number of channels, and h is the size of the halo a factor of k 2 due to replicating the pixel contents for each of the k 2 neighborhoods it participates in. This solution quickly leads to out-of-memory errors. Global attention (Row 4) is at the other end of the spectrum, where all pixels share the same neighborhood, lowering memory at the expense of considerably more FLOPs 2 . This solution slows down models significantly, while also imposing memory problems due the massize size of the attention matrix. A solution that lies in-between these two extremes should trade-off memory and compute appropriately, with the recognition that a small amount of waste is required. A compromise solution can be achieved by leveraging the idea that neighboring pixels share most of their neighborhood. For example, two pixels that are right next to each other share k × (k − 1) pixels of their neighborhoods. Thus a local neighborhood for a block of pixels can be extracted once together, instead of extracting separate neighborhoods per pixel. The FLOPs can be controlled by varying the number of pixels that form a block. We name this strategy blocked local self-attention. The two extremes discussed above are a special case of blocked local self-attention. Global attention corresponds to setting the block size to be the entire spatial extent, while the per-pixel extraction corresponds to setting the block size to be 1. <ref type="figure" target="#fig_0">Figure 1</ref> depicts the different steps involved in executing blocked local self-attention for an image with height H = 4, width W = 4, and c channels with stride 1. Blocking chops up the image into a H b , W b tensor of non-overlapping (b, b) blocks. Each block behaves as a group of query pixels and a haloing operation combines a band of h pixels around them (with padding at boundaries) to obtain the corresponding shared neighborhood block of shape</p><formula xml:id="formula_2">HW k 2 c k × k 4k 2 c SASA [43] HW b 2 (b + 2h) 2 c k × k, where h = k 2 4(b + 2h) 2 c Blocked local (ours) HW b 2 (b + 2h) 2 c (b + 2h) × (b + 2h) 4(b + 2h) 2 c</formula><formula xml:id="formula_3">( H b , W b , b + 2h, b + 2h, c) from which the keys and values are computed. H b × W b</formula><p>attention operations then run in parallel for each of the query blocks and their corresponding neighborhoods, illustrated with different colors in <ref type="figure" target="#fig_0">Figure 1</ref>. SASA <ref type="bibr" target="#b42">[43]</ref> used the same blocking strategy 3 , setting h = k 2 and uses attention masks to emulate pixel-centered neighborhood windows of size k × k. Our approach For example, to achieve a 7 × 7 pixel centered window, <ref type="bibr" target="#b42">[43]</ref> set h = 3. The use of attention masks gives the operation translational equivariance, since each pixel only looks at a square window around it.</p><p>However, the downside of using attention masks is that it wastes computation that must happen regardless due to the implementation of this algorithm. If attention masks are not used, the receptive field increases without any additional computation, as shown in <ref type="table">Table 1</ref> (Rows 2 and 3). However, pixel-level translational equivariance is lost because the nonsquare receptive fields means that the output of a pixel is dependent on which block it falls into. Take for example a pixel at the left edge of its block, which sees additional pixels that are to the right of its square receptive field. If the entire image is shifted one pixel to the right, the pixel now falls into right edge of a neighboring block, and now sees additional pixels that are to the left of its square receptive field. Thus the output of the pixel is dependent on its position in a block, which can change if the image shifts. Another perspective is that blocked local self-attention is only translational equivariant to shifts of size b. While pixellevel translational equivariance is considered important for achieving good performance <ref type="bibr" target="#b64">[65]</ref>, we find that empirically, using a non-masked block local self-attention actually improves the accuracy of the model (see <ref type="bibr">Section 4.3)</ref>. We suspect that the image shifting and cropping perturbations in common data augmentation strategies reduce the reliance on such inductive biases. Thus we adopt unmasked blocked local self-attention because it improves accuracy without sacrificing performance.</p><p>Another difference with SASA is our implementation of downsampling. We replace attention followed by postattention strided average pooling by a single strided attention layer that subsamples queries similar to strided convolutions, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. Note that we use the same neighborhood as is extracted in the stride 1 case <ref type="figure" target="#fig_0">(Figure 1</ref>). This change does not impact accuracy while also reducing the FLOPs 4× in the downsampling layers. We also implement some important algorithmic optimizations that improve our throughput primarily by avoiding reshapes and data formatting operations. In interest of space, we list them in the Appendix D. Taken together, the speedups produced by these improvements are significant as seen in <ref type="figure">Figure 3</ref>, with up to 2× improvements in step time. These improvements can be leveraged to train large self-attention models that were previously too expensive. We leave additional optimizations, such as fused operations and better pipelining of memory accesses with computation, to future work.</p><p>To conclude this section, it's important to note that in the deeper layers of multiscale architectures, smaller spatial dimensions and larger channels would shift the compute calculus in favor of global attention. The models we introduce in Section 4, also take advantage of this, typically using local   </p><formula xml:id="formula_4">× s 4    1 × 1, 64 attention(b, h), 64 · rv 1 × 1, 64 · r b    × 3 s 8 × s 8    1 × 1, 128 attention(b, h), 128 · rv 1 × 1, 128 · r b    × 3 s 16 × s 16    1 × 1, 256 attention(b, h), 256 · rv 1 × 1, 256 · r b    × l3 s 32 × s 32    1 × 1, 512 attention(b, h), 512 · rv 1 × 1, 512 · r b    × 3 s 32 × s 32 1 × 1, d f 1 × 1 global average pooling fc, 1000</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">HaloNet</head><p>Using the implementation of local 2D self-attention with haloing detailed above, we propose a new model, HaloNet that matches state-of-the-art convolutional models on the parameter-accuracy trade-off curve. We leverage the structure of ResNets <ref type="bibr" target="#b16">[17]</ref> that stack multiple residual bottleneck blocks together (see <ref type="table" target="#tab_2">Table 2</ref>). HaloNet uses a few minor modifications from ResNets: (a) adding a final 1 × 1 convo-lution before the global average pooling for larger models, following EfficientNet <ref type="bibr" target="#b54">[55]</ref>, (b) modifying the bottleneck block width factor, which is traditionally fixed at 4, (c) modifying the output width multiplier of the spatial operation, which is traditionally fixed at 1, (d) changing the number of blocks in the third stage from 4 to 3 for computational reasons because attention is more expensive in the higher resolution layers. We also fix the number of heads for each of the four stages to <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8)</ref>  Since the ResNet structure was initially designed for convolutions, we suspect that designing architectures specifically for attention may improve HaloNet. In our work we maintained homogeneity across all layers of model for hyperparameters such as the block (b) and halo (h) sizes. We also hope that using automated architecture search methods <ref type="bibr" target="#b54">[55]</ref> to optimize these hyperparameters for specific accelerators will lead to better local attention architectures. In our work, we train with comparable image sizes as EfficientNet models to determine if attention models can scale to larger images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Attention has steadily risen in adoption in vision models in recent years. First introduced in various forms of sequence modeling <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b56">57]</ref>, attention was used to attend to image features in the text generation module of image captioning models <ref type="bibr" target="#b61">[62]</ref>. Attention is also closely related to non-local means <ref type="bibr" target="#b3">[4]</ref>, a pairwise-weighted global sum of pixels originally developed for image denoising. <ref type="bibr" target="#b58">[59]</ref> applied non-local means on top of spatially downsampled convolutional features to improve video classification. However, since these methods scale quadratically with receptive field size, they cannot be used because the spatial size is too large. In order to apply self-attention, <ref type="bibr" target="#b39">[40]</ref> applies local attention on images for the task of image generation. <ref type="bibr" target="#b2">[3]</ref> spatially downsample the features for attention and concatenate the attention outputs to convolutional features. Instead, we directly build on top of the approach of <ref type="bibr" target="#b42">[43]</ref>, who compute attention on local regions in order to build a fully self-attentional vision model for classification and object detection. Different forms of attention for pure self-attention vision models have also been proposed <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b65">66]</ref>, which are orthogonal and complementary to the focus on scaling in this work. In addition to attention over the spatial extent that we focus on, components that perform attention over channels have also been used to augment convolutional models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33]</ref>. In recent and concurrent work, Vision Transformer <ref type="bibr" target="#b11">[12]</ref> show that apply-  ing transformers on projections of non-overlapping image patches can achieve accuracies comparable to SOTA when pre-trained on very large (JFT-300M <ref type="bibr" target="#b51">[52]</ref>) and medium sized (ImageNet-21k <ref type="bibr" target="#b10">[11]</ref>) classification datasets. However, their models do not adopt a multiscale architecture and our focus in this work is training on ImageNet <ref type="bibr" target="#b46">[47]</ref> from scratch. In Section 4.5, we conduct transfer experiments and compare with ViT and BiT <ref type="bibr" target="#b28">[29]</ref>. Generally, the performance of computational primitives tend to improve over time due to algorithmic changes to the primitive and better software implementations. In particular, convolution have improved over the last decade through changes in (a) the computation of the primitive <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b30">31]</ref>; (b) the software implementation <ref type="bibr" target="#b5">[6]</ref>; (c) the structure of the primitive itself, through for example, grouped convolution <ref type="bibr" target="#b60">[61]</ref> and depthwise separable convolution <ref type="bibr" target="#b48">[49]</ref>. Attention is in the beginning phases of this performance improvement trajectory, and given its importance in sequence modeling <ref type="bibr" target="#b56">[57]</ref>, it will likely see sustained effort to enhance performance. Local attention could also receive performance improvements if it is adopted more widely to combat the general problem of processing large inputs. Our work introduces blocked local attention to efficiently process immediate neighbors. Other forms of nonglobal pixel interaction can also be implemented efficiently <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b1">2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Each HaloNet model (H0-H7) is designed by successively growing the values of the hyperparameters defined in <ref type="table" target="#tab_2">Table 2</ref>. In interest of space, we leave the exact configurations of our models to the Appendix C.2. We also leave the training and evaluation of larger HaloNet models that compare with larger EfficientNet models for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">HaloNets are competitive with state-of-the-art convolutional models</head><p>We train our HaloNet models on ImageNet <ref type="bibr" target="#b46">[47]</ref> (ILSVRC-2012) benchmark with a batch size of 4096 and learning rate of 1.6, which is linearly warmed up for 10 epochs and followed by cosine decay <ref type="bibr" target="#b35">[36]</ref>. The models are trained for 350 epochs with Nesterov's Accelerated Gradient <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b52">53]</ref>, and regularized with dropout <ref type="bibr" target="#b50">[51]</ref>, weight decay, RandAugment <ref type="bibr" target="#b9">[10]</ref> and stochastic depth <ref type="bibr" target="#b23">[24]</ref>.</p><p>We find that HaloNets perform at par or slightly better <ref type="figure" target="#fig_7">(Figure 4</ref>) than EfficientNet models for the same parameters, outperforming other model families. Our best model, H7, achieves 84.9% top-1 ImageNet validation accuracy and 74.7% top-1 accuracy on ImageNet V2 <ref type="bibr" target="#b44">[45]</ref> (with a -0.5% gap to the linear fit in <ref type="bibr" target="#b44">[45]</ref>). For each of our HaloNet models, we use image sizes comparable to the corresponding Effi-cientNet model, training on images sizes up to 600 × 600. <ref type="table" target="#tab_2">(Table A2</ref>). For a comparison of our latencies with Effi-cientNet, the reader can refer to Section 5. To the best of our knowledge, these results are the first to show that selfattention based models for vision perform on par with the SOTA for image classification when trained on imagenet from scratch. Note that for all our experiments, we report accuracies at the end of training and we tune regularization hyperparameters such as augmentation hyperparameters for the baselines and HaloNet models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model study 1: comparing self-attention and convolutions</head><p>In the following sections, we will focus on model studies to distinguish the advantages of self-attention over convolutions for vision and and understand how to best design self-attention vision architectures. This knowledge is important since much of the progress in convolutional networks comes from improvements in architecture design while keeping the core convolution primitive the same <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b16">17]</ref>. We believe our study is the first to explicitly examine the design of optimal self-attention vision architectures.</p><p>For the remainder of the experimental section, we compare with ResNet-50 <ref type="bibr" target="#b17">[18]</ref>, the canonical vision model, because many of the components that we ablate have been well studied for ResNet-50, allowing us to use best practices for the baseline model. We tune our baseline ResNet-50 implementation to achieve a better accuracy, 77.6%, compared to commonly reported numbers in the literature. For example, <ref type="bibr" target="#b16">[17]</ref> report 76.3%. We then create a new HaloNet architecture, HaloNet-50, that exactly matches the ResNet-50 architecture by replacing spatial convolutions with local self-attention. HaloNet-50 and ResNet-50 have about 18.0 million and 25.5 million parameters respectively. We train both for 150 epochs on 256 × 256 image size. We share other training details of the ablation set-up in the appendix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Transfer of convolutional components to selfattention</head><p>Utilizing regularizations and architectural modules beyond the core primitive is critical for achieving strong results <ref type="bibr" target="#b18">[19]</ref>.</p><p>In this section, we study the effects of these additional components on self-attention models. The components we study were all designed for use in convolutional models, as they were developed through experimentation (either human or automated search) on convolutional models. We examine whether these components can successfully transfer to the new model family of self-attention networks. We focus on 4 different components based on the design of EfficientNet <ref type="bibr" target="#b54">[55]</ref>, 2 architecture modules and 2 regularizations: Squeeze-and-Excitation (SE) <ref type="bibr" target="#b22">[23]</ref>, a channel attention module used after the spatial convolution; SiLU/Swish-1 <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref>, an activation function with the form x · sigmoid(x); RandAugment (RA) <ref type="bibr" target="#b9">[10]</ref>, a data augmentation scheme that simplifies AutoAugment <ref type="bibr" target="#b8">[9]</ref>; and Label Smoothing (LS) <ref type="bibr" target="#b53">[54]</ref>, a smoothing of the label distribution.</p><p>The results of adding these various components to the baseline is in <ref type="table" target="#tab_4">Table 3</ref>. Suprisingly, regularizations of the same strength improve HaloNet accuracies significantly more than ResNet, despite HaloNet having around 30% fewer parameters than ResNet. When label smoothing and RandAugment are added, HaloNet improves by 1.3% while ResNet improves by 0.8%. This result suggests that selfattention models may require regularizations that are typical of larger convolutional models, perhaps due to the expressivity of self-attention.</p><p>However, the architecture modules that were developed for convolutional models only improve attention models by a small amount. When Squeeze-and-Excitation (SE) and SiLU/Swish-1 are added, ResNet improves by 1.3% while HaloNet only improves by 0.4%. We speculate that HaloNet models benefit from the gating and multiplicative interactions that comprise self-attention and do not need explicit gating such as SE. Further research must be conducted in order to discover architecture modules that can consistently improve a variety of self-attention models. Inspired by these findings, we decided to use label smoothing, SiLU/Swish-1, and RandAugment in our HaloNet H0 − H7 models. We also use stochastic depth for our larger models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b54">55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Increasing image sizes improve accuracies</head><p>A beneficial property of self-attention is attention is that the receptive field size can scale along with image size without significantly impacting the number of parameters (see Section 2.1). As shown in <ref type="figure" target="#fig_10">Figure 6</ref>, HaloNet consistently improves when using larger images. Although we also see improvements with convolutional models, the accuracy gap between HaloNets and ResNets is maintained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Components</head><p>HaloNet Accuracy   In this section, we will study the impact of relaxing translational equivariance and the relationship of neighborhood window and halo sizes. In the interest of space, a detailed study of scaling various components of our models such as r v , r qk etc can be found in the Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model study 2: HaloNet architecture study</head><p>Relaxing translational equivariance: In <ref type="figure" target="#fig_8">Figure 5</ref>, we see that HaloNet-50 with b = 8, and h = 3 achieves better accuracies using the same block and halo to achieve 7 × 7 neighborhoods with attention masks <ref type="bibr" target="#b42">[43]</ref> and the gap widens with more regularizations. This suggests that larger receptive fields are more important than inductive biases such as translational equivariance.</p><p>Window and halo size: When using the blocked input format, there are two ways of changing the window size of attention: changing the query block size or the halo size. For the same window size w, smaller query blocks and larger halos require more memory than larger query blocks and smaller halos, as discussed in section 2.2.  We see in <ref type="figure" target="#fig_11">Figure 7</ref> that accuracy consistently improves as the window size increases. In particular, doubling the window size from 6 × 6 to 12 × 12 produces a 1.3% accuracy gain. These results suggest that increasing window size can be successfully used to scale models without increasing the number of parameters, potentially beneficial for production environments. Furthermore, for a fixed window size, the choice of query block size does not impact results, enabling the usage of larger query block sizes to reduce memory. <ref type="figure" target="#fig_11">Figure 7</ref> also shows that eschewing haloing for non-overlapping attention, can lower accuracy significantly unless the blocks are quite large. For example using a block size of 4 and a halo of 1 results in better accuracy than using a block size of 8 with 0 halo, despite a smaller neighborhood size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Convolution-Attention hybrids improve the speed-accuracy tradeoff</head><p>In our final set of ablations, we replace self-attention with convolutions to understand where attention layers are currently most beneficial. In <ref type="table">Table 4</ref>, we show results for replacing attention layers with convolutions with squeezeand-excitation modules in each of the stages of our best performing model (HaloNet H7). Having convolutions in all stages except the last yields the fastest model albeit with a significant loss in top-1 accuracy (1%). Splitting the allocation between convolutions (in stages 1-2) and attention (in stages 3-4) minimally detriments predictive accuracy while significantly improving training and inference step times. We leave a detailed study of improved hybrid models for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Transfer from ImageNet-21k</head><p>Our experiments thus far have focused on training from scratch on ImageNet-ILSVRC-2012 <ref type="bibr" target="#b46">[47]</ref>, where regularizations and longer training are critical for good accuracies. Papers such <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref> have shown that a short finetuning step after pretraining models on larger labelled datasets such  <ref type="table">Table 4</ref>. Replacing attention layers with convolutions in stages 1 and 2 exhibit the best speed vs. accuracy tradeoff. All the models had about 67 million parameters and the train and inference times are normalized to the corresponding times for EfficientNet B7. Please see <ref type="figure">Figure 8</ref> for a comparison of step time with other HaloNet models.  as ImageNet-21k <ref type="bibr" target="#b10">[11]</ref> or JFT-300M <ref type="bibr" target="#b51">[52]</ref> can achieve better accuracies without the need for regularization. To understand the transfer properties of HaloNet models, we scale up HaloNet-H4 by increasing the base width to 128 and evaluate the transfer protocol from <ref type="bibr" target="#b28">[29]</ref>, pretraining on the public ImageNet-21k dataset, and finetuning on ImageNet. Following our observation in <ref type="table">Table 4</ref>, we train a hybrid version of this model with convolutions in the first two stages. Note that our hybrids can be seen as using a series of convolution layers to downsample the image. Since we For a fair comparison with <ref type="bibr" target="#b28">[29]</ref>, we do not use squeeze-and-excitation <ref type="bibr" target="#b22">[23]</ref> in the stages with convolutions. We also investigate the effect of replacing the convolutional stem with linear projections of 4 × 4 non-overlapping patches, similar to the vision Transformer. The details of the models can be found in the Appendix E. ImageNet-21k contains 14.2 million annotated images, and 21k labels, both an order of magnitude larger than ImageNet. Following <ref type="bibr" target="#b28">[29]</ref>, we pretrain for 90 epochs with a batch size of 4096, and a base learning rate of 0.16, which is linearly warmed up for 2 epochs followed by cosine decay <ref type="bibr" target="#b35">[36]</ref>. We also use a weight decay of 0.00008, and train with Nesterov's Accelerated Gradient <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b52">53]</ref> during pretraining and finetuning. We pretrain on 256 × 256 size images and finetune on different image sizes, as shown in <ref type="table">Table 6</ref>. Our wider H4 and hybrid-H4 models achieves better accuracy than the Vision Transformer and a 4× wide ResNet-152 from <ref type="bibr" target="#b28">[29]</ref> and are also faster at inference on larger images. We finetune for 8 epochs on ImageNet, initializing with the parameters learned from pretraining except for the label embedding matrix, which is initialized to zeros. We train with a batch size of 512, a learning rate of 0.016 and cosine decay after linearly warming it up for 0.5 epochs. We benefit from finetuning with a label smoothing of 0.1 during finetuning despite pretrainig on a larger dataset. We do not use Polyak averaging <ref type="bibr" target="#b40">[41]</ref>, and other regulariations during finetuning.</p><p>Our preliminary results on transfer are promising since we achieve better parameter-accuracy and speed-accuracy tradeoffs than other models on this dataset. We leave the study of transfer with larger HaloNet and HaloNet hybrids for future work. The speed advantages of our models on larger images make them desirable for challenging structured prediction tasks on large images such as object detection and instance segmentation, which we briefly explore in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Detection and instance segmentation</head><p>To understand if our primitives will generalize to structured prediction tasks on larger images, we conduct initial investigations with the simple attention-convolutional hybrids on detection and instance segmentation, using the Mask R-CNN <ref type="bibr" target="#b15">[16]</ref> framework. These hybrids are also faster and consume less memory than pure attention models, enabling faster experimental cycles. We only replace the last 3 convolutional layers in the ResNet-50 and ResNet-101 backbones with two halo layers with block size, b = 8 and halo size h = 3 (Rows 3 and 6 in <ref type="table">Table 5</ref>). For ResNet-50, we also examine using b = 32 and halo size h = 3 to understand benefits from larger receptive fields. We also use squeeze-and-excitation with convolutions and pre-train them on 512 × 512 images with the regularizations mentioned in Section 4.2.1: label smoothing, RandAugment, and stochastic depth. We train our models on the COCO dataset <ref type="bibr" target="#b34">[35]</ref> with 1024×1024 size images for 32 epochs, using the Cloud TPU Detection Codebase 4 . We provide more training details in the Appendix C.4.</p><p>Our ResNet-50 baseline in row 2 of <ref type="table">Table 5</ref>, is significantly better than what is usually reported in the literature HaloNet vs EfficientNet step times HaloNet EfficientNet <ref type="figure">Figure 8</ref>. Pure attention based HaloNet models are currently slower to train than efficient net models. The times are the TPUv3 compute time needed to process a batch size of 32 per core. The points in green with annotations C1, C12, and C123 correspond to the hybrid models with convolutions in stages 1, 1-2 and 1-3 respectively. (see <ref type="table">Table 4</ref>).</p><p>(row 1). Our attention variants achieve at least 0.7 mAP gains on bounding box detection and at least 0.6 mAP gains on instance segmentation on top of our stronger baselines (denoted by ++ in rows 3, 4 and 6 in <ref type="table">Table 5</ref>). The gain from local attention with block size b = 8 closes half of the mAP gap between the R50 and R101 baselines in detection and 70% of the gap in instance segmentation despite being less than a third of the gap in terms of wall-clock time. Local attention with b = 8 and h = 3 also improves on top of the deep R101 backbone. Interestingly, localization of large objects (AP * l ) shows the largest improvement when attention is used. Larger block sizes (b = 32 in row 4) achieve very close performance to b = 8 while being slower. However, we see that b = 32 does much better than b = 8 on small objects (AP * s ). Future work can combine the best of these two settings. Note that with b = 32, the last two attention layers do global attention since the image is downsampled to 1024 32 = 32 pixels in each spatial dimension. Concurrent work, BoTNet <ref type="bibr" target="#b49">[50]</ref>, uses global self-attention in ResNet-Attention hybrids for structured prediction tasks and classification. See <ref type="bibr" target="#b49">[50]</ref> for additional details on the efficacy of global attention for localization tasks These models have only three layers of self-attention, and more layers could alter these results. We leave the study of detection and instance segmentation with pure attention models to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this work, we built multiscale self-attention models that are competitive with the best convolutional models. To achieve this result, we developed two attention improvements: blocked local attention and attention downsampling. We also performed multiple ablations to understand how to improve the scaling of self-attention models.</p><p>Our results demonstrate that self-attention is competitive accuracy-wise when training on ImageNet from scratch. <ref type="figure">Figure 8</ref> shows that pure self-attention <ref type="bibr" target="#b4">5</ref> based HaloNets are currently slower to train than the corresponding Efficient-Nets and require further optimizations for large batch training. However, our hybrids have the same speed-accuracy tradeoff as EfficientNets. On transfer from ImageNet-21k, our models outperform very strong models such as BiT <ref type="bibr" target="#b28">[29]</ref> and ViT <ref type="bibr" target="#b11">[12]</ref>, on both accuracy and speed. Model optimizations, such as using architecture search methods to find better speed-accuracy tradeoffs or different forms of more powerful and/or efficient attention forms <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b45">46]</ref>, are promising directions for machine learning researchers. Implementation optimizations, such as better memory management, can improve the practicality of these models. Also, scaling up our models to larger widths might cause our operations to transition from being memory bound to compute bound, and lead to better speed-accuracy tradeoffs. We leave this study for future work. Overall, our work shows that self-attention can be competitive in regimes traditionally dominated by computer vision. Future work can push these boundaries further, both in terms of scale and efficiency. recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8697-8710, 2018. 1 <ref type="table" target="#tab_2">Table A2</ref>. Configurations of HaloNet models, each of which matches a model from the EfficientNet family in terms of parameters. The number of heads in the four stages are <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8)</ref>. The notations are: image size s, query block size b, halo size h, attention output width multiplier rv, bottleneck output width multiplier r b , number of bottleneck blocks in the third group l3, and final 1 × 1 conv width d f 1024 image size used in detection setting. The models were regularized with RandAug at a magnitude of 15 and stochastic depth with probability 0.1, and use Squeeze-Excitation with a reduction factor of <ref type="bibr">1 8</ref> . The detection code and hyperparameters directly used the open-source TPU detection and segmentation framework <ref type="bibr" target="#b5">6</ref> . During the detection / instance segmentation phase, the backbone is initialized with the pretrained weights, while the other parameters are initialized from scratch. The model is trained for 67500 steps with 0.1x learning rate decays at 60000 and 65000 steps, uses a learning rate of 0.1 in SGD with 0.9 momentum, a warmup of 500 steps with a fixed learning rate of 2 300 , a batch size of 64 spread across 32 TPUv3 cores, 1024 × 1024 image size, an L2 weight decay of 4e −5 , and multi-scale jitter with magnitudes between [ 4 5 , <ref type="bibr">5 4</ref> ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimizations</head><p>We endeavor to avoid data formatting operations whenever possible, which can slow down the model, resulting in the following two key optimizations • Persistent blocking: Once the image is blocked, we flatten the (b, b) blocks to sequences of length b 2 , and we do not reshape it back to 4D until the end of the network, implementing operations such as batch normalization <ref type="bibr" target="#b26">[27]</ref> to handle the blocked format. The image is thus processed in 5D: (Batch, H b , W b , b 2 , c) instead of (Batch, H, W, c).</p><p>• Gathers with convolutions: The haloing described in Section 2.2 is also carried out in 5D resulting in flattened neighborhoods. For speed, we implement haloing with 3D convolutions used as gathering operations instead of slices and concatenations. <ref type="bibr" target="#b5">6</ref> https://github.com/tensorflow/tpu/tree/master/ models/official/detection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. ImageNet-21k Models</head><p>For our ImageNet-21k transfer experiments <ref type="table">Table 6</ref>), we make 4 changes to our HaloNet H4 model (See <ref type="table" target="#tab_2">Table A2</ref> for specification of the H4 model). To increase the number of parameters in the model body, We increase the base width r w to 2.0 (Making the base width 128, twice the normal width), and we also change r b from 3.0 to the default 4.0. We remove the final extra 1 × 1 convolution, so that the label embeddings have a large number of filters to account for the larger number of labels. Finally, we increase the number of layers in the second stage from 3 to 4. For the hybrid model, we use convolutions in the first two stages.</p><p>For pretraining on 256 × 256 images, we set b = 8 and h = 2. For finetuning on 384 × 384 images, we use b = 12, h = 2, and for finetuning on 512 × 512 size images, we use b = 16, h = 1. When transferring the pretrained model, we initialize all the parameters from the pretrained checkpoint at the final step of pretraining except for the label embeddings, which are initialized to zeros, and the relative embeddings, that are initialized by linearly interpolating from the ones learned at pretraining. F. Impact of relative position encodings <ref type="bibr" target="#b42">[43]</ref> showed that using relative position was important for achieving good accuracies. We find the same outcome with HaloNet. Using absolute factorized abosolute position encodings, which are added to the activations before local self-attention in every layer, drops accuracy from to 78.6% (the first row in <ref type="table" target="#tab_4">Table 3</ref>) to 77.5%</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>HaloNet local self-attention architecture: The different stages of blocked local attention for a [4, 4, c] image, block size b = 2, and halo h = 1. The image is first blocked into non-overlapping [2, 2, c] images from which the queries are computed. The subsequent haloing step then extracts a [4, 4, c] memory around each of the blocks which linearly transform to keys and values. The spatial dimensions after attention are the same as the queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Blocked</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The attention downsampling layer subsamples the queries but keeps the neighborhood the same as the the stride=1 case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 . 4 7 × 7</head><label>347</label><figDesc>Optimizations improve performance. The improvements here are a result of reducing FLOPs with our attention downsampling and improved local self-attention algorithms that avoid reshapes and data formatting. In some cases, we halve the training step time computed on TPU v3. attention in the higher resolutions and global attention when the image resolutions are the smallest. conv stride 2, 64 3 × 3 max pool stride 2 s 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>because heads are more expensive at higher resolutions. To summarize, the scaling dimensions in HaloNet are: image size s, query block size b, halo size h, attention output width multiplier r v , bottleneck output width multiplier r b , number of bottleneck blocks in the third group l 3 , and final 1 × 1 conv width d f . Our attention neighborhoods range from 14 × 14 (b = 8, h = 3) to 18 × 18 (b = 14, h = 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>HaloNets can match EfficientNets on the accuracy vs. parameter trade-off. The accuracies for EfficientNets B5 and B7 were obtained using RandAugment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Relaxing translational equivariance improves accuracies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>The accuracy gap between HaloNet-50 and ResNet-50 is maintained with increasing image sizes. The HaloNet experiments are annotated with block size (b), halo size (h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Increasing window sizes improves accuracy up to a point. The experiments in the graph have been annotated with their block size (b), halo size (h), h = 0 implies attention with non-overlapping blocks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>HaloNet model family specification.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>HaloNet improves more than ResNet with regularizations, but does not improve significantly with architectural modules that strongly benefit ResNet.</figDesc><table><row><cell>Starting from a baseline</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Accuracies on object detection and instance segmentation. We experiment with two settings for self-attention in the last stage: A block size of (b) of 8 and a halo size (h) of 3 and also with (b = 32, h = 3) for ResNet-50. bb (bounding box) refers to detection, and mk (mask) refers to segmentation. The identifiers s, m, and l refer to small, medium, and large objects respectively. Speed is measured as the milliseconds taken by only the backbone (and not the FPN) for a batch size of 32 on 2 TPUv3 cores. The train time the total training time calculated from the peak images/sec of the Mask-RCNN training run on 8 TPUv3 cores with a batch size of 64. HaloNet models pretrained on ImagetNet-21k perform well when finetuned on ImageNet. For HaloNet and ViT, we finetuned on 384 × 384 and 512 × 512 size images. The pretraining step time reports the TPUv3 compute time for a batch size of 32 per core. The inference speed is also computed on a single TPUv3 core.</figDesc><table><row><cell>Model</cell><cell>AP bb</cell><cell>AP bb s</cell><cell>AP bb m</cell><cell>AP bb l</cell><cell>AP mk</cell><cell></cell><cell>AP mk s</cell><cell>AP mk m</cell><cell>AP mk l</cell><cell>Speed (ms)</cell><cell>Train time (hrs)</cell></row><row><cell>R50 baseline in lit</cell><cell>42.1</cell><cell>22.5</cell><cell>44.8</cell><cell>59.1</cell><cell>37.7</cell><cell></cell><cell>18.3</cell><cell>40.5</cell><cell>54.9</cell><cell>409</cell><cell>14.6</cell></row><row><cell>R50 + SE (our baseline)</cell><cell>44.5 (+2.4)</cell><cell>25.5</cell><cell>47.7</cell><cell>61.2</cell><cell cols="2">39.6 (+1.9)</cell><cell>20.4</cell><cell>42.6</cell><cell>57.6</cell><cell>446</cell><cell>15.2</cell></row><row><cell>R50 + SE + Local Att (b = 8)</cell><cell>45.2 (++0.7)</cell><cell>25.4</cell><cell>48.1</cell><cell>63.3</cell><cell cols="2">40.3 (++0.7)</cell><cell>20.5</cell><cell>43.1</cell><cell>59.0</cell><cell>540</cell><cell>15.8</cell></row><row><cell cols="2">R50 + SE + Local Att (b = 32) 45.4 (++0.9)</cell><cell>25.9</cell><cell>48.2</cell><cell>63.0</cell><cell cols="2">40.5 (++0.9)</cell><cell>21.2</cell><cell>43.5</cell><cell>58.8</cell><cell>613</cell><cell>16.5</cell></row><row><cell>R101 + SE (our baseline)</cell><cell>45.9 (+3.8)</cell><cell>25.8</cell><cell>49.5</cell><cell>62.9</cell><cell cols="2">40.6 (+2.9)</cell><cell>20.9</cell><cell>43.7</cell><cell>58.7</cell><cell>740</cell><cell>17.9</cell></row><row><cell cols="2">R101 + SE + Local Att (b = 8) 46.8 (++0.9)</cell><cell>26.3</cell><cell>50.0</cell><cell>64.5</cell><cell cols="2">41.2 (++0.6)</cell><cell>21.4</cell><cell>44.3</cell><cell>59.8</cell><cell>799</cell><cell>18.4</cell></row><row><cell>Model</cell><cell>Parameters (Millions)</cell><cell cols="2">Pretraining Image Size (Pixels)</cell><cell cols="2">Pretraining Step Time (32 per core)</cell><cell cols="2">Finetuning Image Size</cell><cell cols="2">Finetuning Top-1 Accuracy (%)</cell><cell cols="2">Inference Speed img/sec/core</cell></row><row><cell>H4 (base 128)</cell><cell>85</cell><cell></cell><cell>256</cell><cell cols="2">377 ms</cell><cell cols="2">384/512</cell><cell cols="2">85.6/85.8</cell><cell cols="2">121.3/48.6</cell></row><row><cell>H4 (base 128, 4 × 4 patch)</cell><cell>85</cell><cell></cell><cell>256</cell><cell cols="2">366 ms</cell><cell cols="2">384/512</cell><cell cols="2">85.4/85.4</cell><cell cols="2">125.7/56.5</cell></row><row><cell>H4 (base 128, Conv-12)</cell><cell>87</cell><cell></cell><cell>256</cell><cell cols="2">213 ms</cell><cell cols="2">384/512</cell><cell cols="2">85.5/85.8</cell><cell cols="2">257.6/120.2</cell></row><row><cell>ViT-L/16</cell><cell>300</cell><cell></cell><cell>224</cell><cell cols="2">445 ms</cell><cell cols="2">384/512</cell><cell cols="2">85.2/85.3</cell><cell cols="2">74.6/27.4</cell></row><row><cell>BiT-M</cell><cell>928</cell><cell></cell><cell>224</cell><cell cols="2">1021 ms</cell><cell></cell><cell>384</cell><cell cols="2">85.4</cell><cell>54.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">To illustrate this, on a 128 × 128 resolution with 64 channels, global self-attention would incur about 28 times more FLOPs than a 3 × 3 convolution with 64 input and output channels<ref type="bibr" target="#b2">3</ref> Code for both SASA and HaloNet will be made available, along with the checkpoints for HaloNet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/tensorflow/tpu/tree/master/ models/official/detection</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">By pure attention we mean models that use self-attention in all layers except the stem, which is convolutional.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>We would like to thank David Fleet for valuable discussions. We would also like to thank Irwan Bello, Barret Zoph, Mingxing Tan, and Lucas Beyer for valuable commentary on earlier drafts of the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. Relative embeddings add very few parameters to the model Our parameters grow very slowly with receptive field. In this section, we will show that the number of parameters in the relative embeddings, the only spatially dependent parameters, is quite small. As described in the paper, the output of local 2D self-attention at position (i, j) is computed as:</p><p>softmax ab q ij k ab + q ij r a−i,b−j v ab <ref type="bibr" target="#b3">(4)</ref> where the queries q ij = W Q x ij , keys k ab = W K x ab , and values v ab = W V x ab are linear transformations of the pixels, and r a−i,b−j is a learned relative position based embedding. Following the Transformer <ref type="bibr" target="#b56">[57]</ref>, we also use multihead attention, where we run multiple instances of the self-attention in parallel with different parameters. However, each head shares the parameters for the relative embeddings r a−i,b−j . For an attention window of size k around each pixel, we factorize the relative embeddings along height and width following <ref type="bibr" target="#b42">[43]</ref>, and we allocate half the channels within a head to each of these. Keeping the dimension per head fixed at 16 as mentioned in the paper, this gives a constant 2(k − 1) * 16 parameters per attention layer layer for r a−i,b−j . In contrast, if the channels in an attention layer are d, then each of the three linear transformations has d 2 parameters. Thus the ratio of parameters in the relative embeddings as compared with the linear projections is 2(k−1) * 16 3d 2 , which is small for typical values of k and d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimension</head><p>Values Accuracy Baseline ∆ Baseline Scaled  <ref type="table">Table A1</ref>. Increasing the number of channels for the values and number of layers has the most impact on accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Study of enlarging self-attention models</head><p>In Section 4.3, we presented some scaling properties of our models. In <ref type="table">Table A1</ref>, we try to understand which other parts of our models most impact accuracy. For our study, we increase the size of HaloNet-50 by scaling different hyperparameters to reach a parameter budget of 30 million. We find that adding more computation in the attention by increasing r v and adding more layers are most fruitful scaling dimensions for increasing accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental details, hyperparameters</head><p>In this section, we list the experimental details and model configurations that were omitted from the main body in interest of space C.1. Experimental details for model studies In Sections 4.2 and 4.3, all the HaloNet-50 models use the same layer allocations and channels widths as the standard ResNet-50 <ref type="bibr" target="#b16">[17]</ref> model. Both ResNet-50 and HaloNet-50 models were trained for 150 epochs on 256 × 256 size images with a learning rate of 0.1. For the experiments with RandAugment, we used a weight decay of 0.00004 for the settings that used RandAugment <ref type="bibr" target="#b9">[10]</ref>, and 0.00008 otherwise. Using a weight decay of 0.00008 with RandAugment seemed to have a negative impact on accuracies with ResNet-50. We used a RandAugment magnitude of 10 in these sections. For HaloNet-50, we used a block size b = 8, and halo h = 3. We fixed the number of channels per head to be 16. For the SASA models in section, we used a pixel centered window of size 7 × 7 following <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. HaloNet Models</head><p>In <ref type="table">Table A2</ref>, we describe the configurations of our HaloNet models, H1 − H7. The hyperparameters in the HaloNet family are: image size s, query block size b, halo size h, attention output width multiplier r v , bottleneck output width multiplier r b , number of bottleneck blocks in the third group l 3 , and final 1 × 1 conv width d f . Each of our HaloNet models is trained on a comparable image size to the corresponding EfficientNet <ref type="bibr" target="#b54">[55]</ref> model, which can be found in <ref type="table">Table A2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Classification hyperparameters</head><p>In this section we complete the details of our training and regularization setup. We used a weight decay of 2e −5 and using a cosine annealing scheme <ref type="bibr" target="#b35">[36]</ref> with learning rate 0.1. The largest models consistently overfit at the very end of training, which we attribute to the learning rate going to 0 at the end of training <ref type="bibr" target="#b63">[64]</ref>. To combat this, we set the end of the cosine annealing to be 1.0 128 of the original learning rate instead of 0. For RandAugment <ref type="bibr" target="#b9">[10]</ref>, we grow our Ran-gAugment magnitudes for the smallest H0 to the the largest H7 models as <ref type="bibr">6, 8, 10, 14, 17, 19, 24 and 31</ref>. Note that we have not extensively tuned the RandAugment magnitudes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Detection and instance segmentation hyperparameters</head><p>We use Mask-RCNN <ref type="bibr" target="#b15">[16]</ref> for all detection and instance segmentation experiments. We pretrain the backbone on ImageNet, mostly reusing the same hyperparameters as in Section C.3. Backbones are pretrained for 350 epochs using an image size of 512, which was chosen to be closer to the </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambdanetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08602</idno>
		<title level="m">Modeling long-range interactions without attention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High performance convolutional neural networks for document processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Chellapilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidd</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Workshop on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cudnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03584</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Randaugment: Practical data augmentation with no separate search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sigmoidweighted linear units for neural network function approximation in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiji</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Revisiting spatial invariance with low-rank local connectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Gamaleldin F Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kornblith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02959</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shape and arrangement of columns in cat&apos;s striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hubel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">559</biblScope>
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Receptive fields and functional architecture of monkey striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Torsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="243" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast algorithms for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4013" to="4021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relationaware graph attention network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10313" to="10322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5851</idno>
		<title level="m">Fast training of convolutional networks through ffts</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate o(1/k 2 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. Akad. Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Bilateral filtering: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Kornprobst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13678</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05997</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Selfattention with relative position representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serkan</forename><surname>Piantino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7580</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Fast convolutional nets with fbfft: A gpu performance evaluation</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Axial-deeplab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07853</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Arithmetic complexity of computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="volume">33</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Modeling point clouds with self-attention and gumbel subset sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3323" to="3332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11142</idno>
		<title level="m">Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with big single-stage models</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Making convolutional networks shiftinvariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11486</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13621</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Learning transferable architectures for scalable image</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
