<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structural Embedding of Syntactic Trees for Machine Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liu</surname></persName>
							<email>ruil@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
							<email>junjieh@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
							<email>weiwei@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structural Embedding of Syntactic Trees for Machine Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees. In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reading comprehension such as SQuAD <ref type="bibr" target="#b15">(Rajpurkar et al., 2016)</ref> or NewsQA <ref type="bibr" target="#b18">(Trischler et al., 2016)</ref> requires identifying a span from a given context, which is an extension to the traditional question answering task, aiming at responding questions posed by human with natural language <ref type="bibr" target="#b12">(Nyberg et al., 2002;</ref><ref type="bibr" target="#b4">Ferrucci et al., 2010;</ref><ref type="bibr" target="#b8">Liu, 2017;</ref><ref type="bibr" target="#b23">Yang, 2017)</ref>. Many works have been proposed to leverage deep neural networks for such question answering tasks, most of which involve learning the query-aware context representations <ref type="bibr" target="#b3">(Dhingra et al., 2016;</ref><ref type="bibr" target="#b16">Seo et al., 2017;</ref><ref type="bibr" target="#b20">Wang and Jiang, 2016;</ref><ref type="bibr" target="#b22">Xiong et al., 2017)</ref>. Although deep learning based methods demonstrated great potentials for question answering, none them take syntactic information of the sentences such as con- * Authors contributed equally to this work. stituency tree and dependency tree into consideration. Such techniques have been proven to be useful in many natural language understanding tasks in the past and illustrated noticeable improvements such as the work by <ref type="bibr" target="#b15">(Rajpurkar et al., 2016)</ref>. In this paper, we adopt similar ideas but apply them to a neural attention model for question answering.</p><p>The constituency tree <ref type="bibr" target="#b9">(Manning et al., 1999</ref>) of a sentence defines internal nodes and terminal nodes to represent phrase structure grammars and the actual words. <ref type="figure">Figure 1</ref> illustrates the constituency tree of the sentence "the architect or engineer acts as the project coordinator". Here, "the architect or engineer" and "the project coordinator" are labeled as noun phrases ("NP"), which is critical for answering the question below. Here, the question asks for the name of certain occupation that can be best answered using an noun phrase. Utilizing the know ledge of a constituency relations, we can reduce the size of the candidate space and help the algorithm to identify the correct answer.</p><p>Whose role is to design the works, prepare the specifications and produce construction drawings, administer the contract, tender the works, and manage the works from inception to completion?</p><p>On the other hand, a dependency tree <ref type="bibr" target="#b9">(Manning et al., 1999)</ref> is constructed based on the dependency structure of a sentence. <ref type="figure" target="#fig_1">Figure 2</ref> displays the dependency tree for sentence</p><p>The Annual Conference, roughly the equivalent of a diocese in the Anglican Communion and the Roman Catholic Church or a synod in some Lutheran denominations such as the Evangelical Lutheran Church in America, is the basic unit of organization within the UMC.  <ref type="figure">Figure 1</ref>: The constituency tree of context "the architect or engineer acts as the project coordinator" "The Annual Conference" being the subject of "the basic unit of organization within the UMC" provides a critical clue for the model to skip over a large chunk of the text when answering the question "What is the basic unit of organization within the UMC". As we show in the analysis section, adding dependency information dramatically helps identify dependency structures within the sentence, which is otherwise difficult to learn.</p><p>In this paper, we propose Structural Embedding of Syntactic Trees (SEST) that encode syntactic information structured by constituency tree and dependency tree into neural attention models for the question answering task. Experimental results on SQuAD dataset illustrates that the syntactic information helps the model to choose the answers that are both succinct and grammatically coherent, which boosted the performance on both qualitative studies and numerical results. Our focus is to show adding structural embedding can improve baseline models, rather than directly compare to published SQuAD results. Although the methods proposed in the paper are demonstrated using syntactic trees, we note that similar approaches can be used to encode other types of tree structured information such as knowledge graphs and ontology relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The general framework of our model is illustrated in <ref type="figure">Figure 3</ref>. Here the input of the model is the embedding of the context and question while the output is two indices begin and end which indicate the begin and end indices of the answer in the context space.</p><p>The input of the model contains two parts: the word/character model and the syntactic model.</p><p>The shaded portion of our model in <ref type="figure">Figure 3</ref> represents the encoded syntactic information of both context and question that are fed into the model. To gain an insight of how the encoding works, consider a sentence which syntactic tree consists of four nodes (o1, o2, o3, o4). A specific word is represented to be a sequence of nodes from its leave all the way to the root. We cover how this process work in detail in Section 3.1.1 and 3.1.2. Another input that will be fed into deep learning model is the embedding information for words and characters respectively. There are many ways to convert words in a sentence into a highdimensional embedding. We choose GloVe <ref type="bibr" target="#b14">(Pennington et al., 2014b)</ref> to obtain a pre-trained and fixed vector for each word. Instead of using a fixed embedding, we use Convolutional Neural Networks (CNN) to model character level embedding, which values can be changed during training <ref type="bibr" target="#b6">(Kim, 2014)</ref>. To integrate both embeddings into the deep neural model, we feed the concatenation of them for the question and the context to be the input of the model.</p><p>The inputs are processed in the embedding layer to form more abstract representations. Here we choose a multi-layer bi-directional Long Short Term Memory (LSTM) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997)</ref> to obtain more abstract representations for words in the contexts and questions.</p><p>After that, we employ an attention layer to fuse information from both the contexts and the questions. Various matching mechanisms using attentions have been extensively studied for machine comprehension tasks <ref type="bibr" target="#b22">(Xiong et al., 2017;</ref><ref type="bibr" target="#b16">Seo et al., 2017;</ref><ref type="bibr" target="#b20">Wang and Jiang, 2016)</ref>. We use the Bi-directional Attention flow model <ref type="bibr" target="#b16">(Seo et al., 2017)</ref> which performs contextto-question and question-to-context attentions in both directions. The context-to-question attention signifies which question words are most relevant to each context word. For each context word, the attention weight is first computed by a softmax function with question words, and the attention vector of each context word is then computed by a weighted sum of the question words' embeddings obtained from the embedding layer. The questionto-context attention summarizes a context vector by performing a soft attention with context words given the question. We then represent each context word as the concatenation of the embedding vector obtained from the embedding layer, the atten-The Annual Conference , ... , is the basic unit of organization within the UMC . tion vector obtained from the context-to-question attention and the context vector obtained from the question-to-context attention. We then feed the concatenated vectors to a stacked bi-directional LSTM with two layers to obtain the final representations of context words. We note that our proposed structural embedding of syntactic trees can be easily applied to any attention approaches mentioned above. For the machine comprehension task in this paper, the answer to the question is a phrase in the context. In the output layer, we use two softmax functions over the output of the attention layer to predict the begin and end indices of the phrase in the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Structural Embedding of Syntactic Tree</head><p>We detail the procedures of two alternative implementation of our methods: the Structural Embedding of Constituency Trees model (SECT) and the Structural Embedding of Dependency Trees model (SEDT). We assume that the syntactic information has already been generated in the preprocessing step using tools such as the Stanford CoreNLP <ref type="bibr" target="#b10">(Manning et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Syntactic Sequence Extraction</head><p>We first extract a syntactic collection C(p) for each word p, which consists of a set of nodes {o 1 , o 2 , . . . , o d−1 , o d } in the syntactic parse tree T . Each node o i can be a word, a grammatical category (e.g., part-of-speech tagging), or a dependency link label, depending on the type of syntactic tree we use. To construct syntactic embeddings, the first thing we need to do is to define a specific processing order A over the syntactic collection C(p), in which way we can extract a syntactic sequence S(p) for the word p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Structural Embedding of Constituency Trees (SECT)</head><p>The constituency tree is a syntactic parse tree constructed by phrase structure grammars <ref type="bibr" target="#b9">(Manning et al., 1999)</ref>, which defines the way to hierarchically construct a sentence from words in a bottomup manner based on constituency relations. Words in the contexts or the questions are represented by leaf nodes in the constituency tree while the non-terminal nodes are labeled by categories of the grammar. Non-terminal nodes summarize the grammatical function of the sub-tree. <ref type="figure">Figure 1</ref> shows an example of the constituency tree with "the architect or engineer" being annotated as a noun phrase (NP). A path originating from the leaf node to the root node captures the syntactic information in the constituency tree in a hierarchical way. The higher the node is, the longer span of words the sub-tree of this node covers. Hence, to extract the syntactic sequence S(p) for a leaf node p, it is reasonable to define the processing order A(p) from the leaf p all the way to its root. For example, the syntactic sequence for the phrase "the project coordinator" in <ref type="figure">Figure 1</ref> is detected as (NP, PP, VP, S). In practice, we usually take the last hidden units of Bi-directional encoding mechanisms such as Bidirectional LSTM to represent the sequence state, as is indicated in <ref type="figure" target="#fig_2">Figure 4</ref> (a). We set a window size to limit the amount of information that is used in our models. For example, if we choose the window size as 2, then the syntactic sequence becomes (NP, PP). This process is introduced for both performance and memory utilization consideration, which is discussed in detail in Section 4.5.</p><p>In addition, a non-terminal node at a particular position in the syntactic sequence defines the begin and end indices of a phrase in the context. By measuring the similarity between syntactic se-  <ref type="figure">Figure 3</ref>: Model Framework. The neural network for training and testing is built by components with solid lines, which includes the embedding layer, attention layer, and output layer. The shaded area highlights the part of the framework that involves syntactic information. Components with dashed lines is an example to illustrate how syntactic information is decoded. Here a sentence is decomposed into a syntactic tree with four nodes and the syntactic information for a specific word is recorded as the path from its position in the syntactic tree to the root, i.e. (o 1 , o 2 , o 3 ) in this case. quences S(p) extracted for each word p of both the question and the context, we are able to locate the boundaries of the answer span. This is done in the attention layer shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Structural Embedding of Dependency Trees (SEDT)</head><p>The dependency tree is a syntactic tree constructed by dependency grammars <ref type="bibr" target="#b9">(Manning et al., 1999)</ref>, which defines the way to connect words by directed links that represent dependencies. A dependency link is able to capture both long and short distance dependencies of words. Relations on links vary in their functions and are labeled with different categories. For example, in the dependency tree plotted in <ref type="figure" target="#fig_1">Figure 2</ref>, the link from "unit" to "Conference" indicates that the target node is a nominal subject (i.e. NSUBJ) of the source node.</p><p>The syntactic collection C(p) for dependency tree is defined as p's children, each represented by its word embedding concatenated with a vec-tor that uniquely identifies the dependency label. The processing order A(p) for dependency tree is then defined to be the dependent's original order in the sentence. Take the word "unit" as an example, we encode the dependency sub-tree using a Bi-directional LSTM, as indicated in <ref type="figure" target="#fig_2">Figure 4 (b)</ref>. In such as a sub-tree, since children are directly linked to the root, they are position according to the original sequence in the sentence. Similar to the syntactic encoding of C-Tree, we take the last hidden states as its embedding.</p><p>Similar to SECT, we use a window of size l to limit the amount of syntactic information for the learning models by choosing only the l-nearest dependents, which is again reported in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Syntactic Sequence Encoding</head><p>Similar to previous work <ref type="bibr" target="#b0">(Cho et al., 2014;</ref><ref type="bibr" target="#b6">Kim, 2014)</ref>, we use a neural network to encode a variable-length syntactic sequence into a fixedlength vector representation. The encoder can be a Recurrent Neural Network (RNN) or a Convolutional Neural Network (CNN) that learns a structural embedding for each node such that embedding of nodes under similar syntactic trees are close in their embedding space.</p><p>We can use a Bi-directional LSTM as our RNN encoder, where the hidden state v p t is updated according to Eq. 1. Here x p t is the t th node in the syntactic sequence of the word p, which is a vector that uniquely identifies each syntactic node. We obtain the structural embedding of the given word p, v p Bi-LSTM = v p T to be the final hidden state.</p><formula xml:id="formula_0">v p t = Bi-LSTM(v p t−1 , x p t )<label>(1)</label></formula><p>Alternatively, we can also use CNN to obtain embeddings from a sequence of syntactic nodes. We denote l as the length of the filter of the CNN encoder. We define x p i:i+l as the concatenation of the vectors from x p i to x p i+l−1 within the filter. The i th element in the j th feature map can be obtained in Eq. 2. Finally we obtain the structural embedding of the given word p by v p CNN in Eq. 3.</p><formula xml:id="formula_1">c p i,j = f (w j · x i:i+l−1 + b j ) (2) v p CNN = max row (c p )<label>(3)</label></formula><p>where w j and b j are the weight and bias of the j th filter respectively, f is a non-linear activation function and max row (·) takes the maximum value along rows in a matrix. </p><formula xml:id="formula_2">; v 4 ], where</formula><p>Ew is the word embedding for "coordinator" that is 100 dimensions in our experiments and each of the encoded vector u and v can be 30 dimensional. For SEDT, we encode the word "unit" in <ref type="figure" target="#fig_1">Figure 2</ref> with its dependent nodes including "Conference", "is", "the", "basic", "organization", ordered by their positions in the original sentence. Each word is represented with its word embedding. Similar to SECT, the final representation is the concatenation [Ew; u 0 ; v 6 ], which will be sent to the input layer of a neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted systematic experiments on the SQuAD dataset <ref type="bibr" target="#b15">(Rajpurkar et al., 2016)</ref>. We compared our methods against Bi-Directional Attention Flow (BiDAF), as well as the SEST models described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preprocessing</head><p>A couple of preprocessing steps is performed to ensure that the deep neural models get the correct input. We segmented context and questions into sentences by using NLTK's Punkt sentence segmenter 1 . Words in the sentences were then converted into symbols by using PTB Tokenizer 2 . Syntactic information including POS tags and syntactic trees were acquired by Stanford CoreNLP utilities <ref type="bibr" target="#b10">(Manning et al., 2014)</ref>. For the parser, we collected constituent relations and dependency relations for each word by using tree annotation and enhanced dependencies annotation respectively. To generate syntactic sequence, we removed sequences whose first node is a punctuation ("$", ":", "#", ".", " " ", " " ", ","). To use dependency labels, we removed all the subcategories (e.g., "nmod:poss" ⇒ "nmod"). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Setting</head><p>We run our experiments on a machine that contains a single GTX 1080 GPU with 8GB VRAM. All of the models being compared have the same settings on character embedding and word embedding. As introduced in Section 2, we use a variable character embedding with a fixed pre-trained word embedding to serve as part of the input into the model. The character embedding is implemented using CNN with a one-dimensional layer consists of 100 units with a channel size of 5. It has an input depth of 8. The max length of SQuAD is 16 which means there are a maximum 16 words in a sentence. The fixed word embedding has a dimension of 100, which is provided by the GloVe data set <ref type="bibr" target="#b13">(Pennington et al., 2014a)</ref>. The settings for syntactic embedding are slightly different for each model. The BiDAF model does not deal with syntactic information. The POS model contains syntactic information with 39 different POS tags that serve as both input and output. For SECT and SEDT the input of the model has a size of 8 with 30 units to be output. Both of them has a maximum length size that is set to be 10 and 20 respectively, which values will be further discussed in Section 4.5. They also have two different ways to encode the syntactic information as indi-cated in Section 3: LSTM and CNN. We apply the same sets of parameters when we experiment them with the two models. We report the results on the SQuAD development set and the blind test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Predictive Performance</head><p>We first compared the performance of single models between the baseline approach BiDAF and the proposed SEST approaches, including SE-POS, SECT-LSTM, SECT-CNN, SEDT-LSTM, and SEDT-CNN, on the development dataset of SQuAD. For each model, we conducted 5 different single experiments and evaluated them using two metrics: "Exact match" (EM), which calculates the ratio of questions that are answered correctly by strict string comparison, and the F1 score, which calculates the harmonic mean of the precision and recall between predicted answers and ground true answers at the character level. As shown in <ref type="table">Table 1</ref>, we reported the maximum, the mean, and the standard deviation of EM and F1 scores across all single runs for each approach, and highlighted the best model using bold font. SECT-LSTM is the second best method, which confirms the predictive powers of different types of syntactic information. We could see that SEDT-LSTM model outperforms the baseline method and other proposed methods in terms of both EM and F1. Another observation is that our propose models achieve higher relative improvements in EM scores than F1 scores over the baseline methods, providing the evidence that syntactic information can accurately locate the boundaries of the answer. Moreover, we found that both SECT-LSTM and SEDT-LSTM have better performance than their CNN counterparts, which suggests that LSTM can more effectively preserve the syntactic information. As a result, we conducted further analysis of only SECT-LSTM and SEDT-LSTM models in the subsequent subsections and drop the suffix "-LSTM" for abbreviation. We built an ensemble model from the 5 single models for the baseline method BiDAF and our proposed methods SEPOS, SECT-LSTM, and SEDT-LSTM. The ensemble model choose the answer with the highest sum of confidence scores among the 5 single models for each question. We compared these models on both the development set and official test set and reported the results in <ref type="table" target="#tab_3">Table 2</ref>. We found that the models have higher performance on the test set than the development set, which coincides with the previous results on the same data set <ref type="bibr" target="#b16">(Seo et al., 2017;</ref><ref type="bibr" target="#b22">Xiong et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Contribution of Syntactic Sequence</head><p>To take a closer look at how syntactic sequences affect the performance, we removed the character/word embedding from our model seen in <ref type="figure">Figure 3</ref> and conducted experiments based on the syntactic input alone. In particular, we are interested in two aspects related to syntactic sequences: First, the ability to predict answers of questions of syntactic sequences compared to complete random sequences. Second, the amount of impacts brought by our proposed ordering introduced in Section 3.1.1 and Section 3.1.2 compared to random ordering.</p><p>We compared the performance of the models using syntactic information along in their original order (i.e. SECT-Only and SEDT-Only) against their counterparts with the same syntactic tree nodes but with randomly shuffled order (i.e. SECT-Random-Order and SEDT-Random-Order) as well as the baselines with randomly generated tree nodes (i.e. SECT-Random and SEDT-Random). Here we choose the length of window size to be 10. The predictive results in terms of EM and F1 metrics are reported in <ref type="table">Table 3</ref>. From the table we see that both the ordering and the contents of the syntactic tree are important for the models to work properly: constituency and dependency trees achieved over 20% boost on performance compared to the randomly generated ones and our proposed ordering also out-performed the random ordering. It also worth mentioning that the ordering of dependency trees seems to have less impact on the performance compared to that of the constituency trees. This is because sequences extracted from constituency trees contain hierarchical information, which ordering will affect the output of the model significantly. However, sequences extracted from dependency trees are all children nodes, which are often interchangeable and don't seem to be affected by ordering much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Window Size Analysis</head><p>As we have mentioned in the earlier sections, limiting the window size is an important technique to prevent excessive usage on VRAM. In practice, we found that limiting the window size also benefits the performance of our models. In <ref type="table" target="#tab_5">Table 4</ref> we compared the predictive performance of SECT   <ref type="table">Table 3</ref>: Performance comparisons of models with only syntactic information against their counterparts with randomly shuffled node sequences and randomly generated tree nodes using the SQuAD Dev set and SEDT models by varying the length of their window sizes from 1 to maximum on the development set. In general the results illustrate that performances of the models increase with the length of the window. However, we found that for SECT model, its mean performance reached the peak while standard deviations narrowed when window size reaches 10. We also observed that larger window size does not generate predictive results that is as good as the one with window size set to 10. This suggests that there exists an optimal window size for the constituency tree. One possible ex-  planation is increasing the window size leads to the increase in the number of syntactic nodes in the extracted syntactic sequence. Although subtrees might be similar between context and question, it is very unlikely that the complete trees are the same. Because of that, allowing the syntactic sequence to extend beyond the certain heights will introduce unnecessary noise into the learned representation, which will compromise the performance of the models. Similar conclusion holds for the SEDT model, which has an improved performance and decreased variance with the window size is set to 10. We did not perform experiments with window size beyond 10 for SEDT since it will consume VRAM that exceeds the capacity of our computing device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Overlapping Analysis</head><p>To further understand the performance benefits of incorporating syntactic information into the question answering problem, we can take a look at the questions on which models disagree. <ref type="figure" target="#fig_3">Figure 5</ref> is the Venn Diagram on the questions that have been corrected identified by SECT, SEDT and the baseline BiDAF model. Here we see that the vast Question BiDAF SECT Whose role is to design the works, prepare the specifications and produce construction drawings, administer the contract, tender the works, and manage the works from inception to completion?  These advances led to the development of a layered model of the Earth, with a crust and lithosphere on top, the mantle below (separated within itself by seismic discontinuities at 410 and 660 kilometers), and the outer core and inner core below that. seismic discontinuities at 410 and 660 kilometers), and the outer core and inner core the outer core and inner core What percentage of farmland grows wheat?</p><p>More than 50% of this area is sown for wheat, 33% for barley and 7% for oats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>33% 50%</head><p>What is the basic unit of organization within the UMC?</p><p>The Annual Conference, roughly the equivalent of a diocese in the Anglican Communion and the Roman Catholic Church or a synod in some Lutheran denominations such as the Evangelical Lutheran Church in America, is the basic unit of organization within the UMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evangelical Lutheran Church in America</head><p>The Annual Conference  To understand the types of the questions that syntactic models can do better, we extracted three questions that were correctly answered by SECT and SEDT but not the baseline model. In <ref type="table" target="#tab_7">Table 5</ref>, all of the three questions are "Wh-questions" and expect the answer of a noun phrase (NP). Without knowing the syntactic information, BiDAF answered questions with unnecessary structures such as verb phrases (vp) (e.g. "acts as · · · ", "represented · · · ") or prepositional phrases (pp) (e.g. "in · · · ") in addition to NPs (e.g. "the architect engineer", "uncertainty" and "powerful high frequency currents") that normal human would answer. For that reason, answers provided by BiDAF failed the exact match although its answers are semantically equivalent to the ones provided by SECT. Having incorporated constituency information provided an huge advantage in inferring the answers that are most natural for a human.</p><p>The advantages of using the dependency tree in our model can be illustrated using the questions in <ref type="table" target="#tab_8">Table 6</ref>. Here again we listed the ones that are correctly identified by SEDT but not BiDAF. As we can see that the answer provided by BiDAF for first question broke the parenthesis incorrectly, this problem that can be easily solved by utilizing dependency information. In the second example, BiDAF failed to identify the dependency structures between "50%" and the keyword being asked "wheat", which resulted in an incorrect answer that has nothing to do with the question. SEDT, on the other hand, answered the question correctly. In the third question, the key to the answer is to correctly identify the subject of question phrase "is the basic unit of organization". Using the dependency tree as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, SEDT is able to identify the subject phrase correctly, namely "The Annual Conference". However, BiDAF failed to anwer the question correctly and selected a noun phrase as the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Reading Comprehension. Reading comprehension is a challenging task in NLP research. Since the release of the SQuAD data set, many works have been done to construct models on this massive question answering data set. <ref type="bibr">Rajpurkar et. al.</ref> are among the first authors to explore the SQuAD. They used logistic regression with pos tagging information <ref type="bibr" target="#b15">(Rajpurkar et al., 2016)</ref> and provided a strong baseline for all subsequent models. A steep improvement was given by the RaSoR model <ref type="bibr" target="#b7">(Lee et al., 2016)</ref> which utilized recurrent neural networks to consider all possible subphrases of the context and evaluated them one by one. To avoid comparing all possible candidates and to improve the performance, Match-LSTM <ref type="bibr" target="#b20">(Wang and Jiang, 2016)</ref> was proposed by using a pointer network <ref type="bibr" target="#b19">(Vinyals et al., 2015)</ref> to extract the answer span from the context. The same idea was taken to the BiDAF <ref type="bibr" target="#b16">(Seo et al., 2017)</ref> model by introducing a bi-directional attention mechanism. Despite the above-mentioned strong models for the machine comprehension task, none of them considers syntactic information into their prediction models.</p><p>Representations of Texts and Words. One of the main issues in reading comprehension is to identify the latent representations of texts and words <ref type="bibr" target="#b2">(Cui et al., 2016;</ref><ref type="bibr" target="#b7">Lee et al., 2016;</ref><ref type="bibr" target="#b22">Xiong et al., 2017;</ref><ref type="bibr" target="#b24">Yu et al., 2016)</ref>.</p><p>Many pre-trained libraries such as word2vec <ref type="bibr" target="#b11">(Mikolov et al., 2013)</ref> and Glove <ref type="bibr" target="#b13">(Pennington et al., 2014a)</ref> have been widely used to map words into a high dimensional embedding space. Another approach is to generate embeddings by using neural networks models such as Character Embedding <ref type="bibr" target="#b6">(Kim, 2014)</ref> and Tree-LSTM <ref type="bibr" target="#b17">(Tai et al., 2015)</ref>. One thing that worth mentioning is that although Tree-LSTM does utilize syntactic information, it targets at the phrases or sentences level embedding other than the word level embedding we have discussed in this paper. Many machine comprehension models include both pre-trained embeddings and variable embeddings that can be changed through a training stage <ref type="bibr" target="#b16">(Seo et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed methods to embed syntactic information into the deep neural models to improve the accuracy of our model in the machine comprehension task. We formally defined our SEST framework and proposed two instances to it: the structural embedding of constituency trees (SECT) and the structural embedding of dependency trees (SEDT). Experimental results on SQuAD data set showed that our proposed approaches outperform the state-of-the-art BiDAF model, proving that the proposed embeddings play a significant part in correctly identifying answers for the machine comprehension task. In particular, we found that our model can perform especially well on exact match metrics, which requires syntactic information to accurately locate the boundaries of the answers. Similar approaches can be used to encode other tree structures such as knowledge graphs and ontology relations.</p><p>This work opened several potential new lines of research: 1) In the experiments of our paper we utilized the BiDAF model to retrieve answers from the context. Since there are no structures in the BiDAF models to specifically optimize for syntactic information, an attention mechanism that is designed for to utilize syntactic information should be studied. 2) Another direction of research is to incorporate SEST with deeper neural networks such as VD-CNN <ref type="bibr" target="#b1">(Conneau et al., 2017)</ref> to improve learning capacity for syntactic embedding.</p><p>3) Tree structured information such as knowledge graphs and ontology structure should be studied and improve question answering tasks using similar techniques to the ones proposed in the paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Partial dependency parse tree of an example context "The Annual Conference, roughly the equivalent of a diocese in the Anglican Communion and the Roman Catholic Church or a synod in some Lutheran denominations such as the Evangelical Lutheran Church in America, is the basic unit of organization within the UMC."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Two examples are used to illustrate how the syntactic information is encoded for SECT and SEDT respectively. Take Bi-directional LSTM as examples, where x is a vector such as word embedding, v and u are the outputs of the forward and backward LSTMs respectively. For SECT, we encode the syntactic sequence (NP, PP, VP) for the word "coordinator" inFigure 1. We use fixed vectors for syntactic tags (e.g., NP, PP and VP), initialized with multivariate normal distribution. The final representation for the target word "coordinator" can be represented as the concatenation [Ew; u 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Venn Diagram on the number of correct answers predicted by BiDAF, SECT and SEDT majority of the correctly answered questions are shared across all three models. The rest of them indicates questions that models disagree and are distributed fairly evenly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>LSTM 67.91 67.65 (±0.31)• 77.47 77.19 (±0.21) • 71.76 80.09 SEDT-LSTM 68.13 67.89 (±0.10)• 77.58 77.42 (±0.19) • 72.03 80.28 Performance comparison on the development set. Each setting contains five runs trained consecutively. Standard deviations across five runs are shown in the parenthesis for single models. Dots indicate the level of significance.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Single</cell><cell></cell><cell cols="2">Ensemble</cell></row><row><cell cols="2">Method</cell><cell>Max</cell><cell>EM Mean (±SD)</cell><cell>Max</cell><cell>F1 Mean (±SD)</cell><cell>EM</cell><cell>F1</cell></row><row><cell>BiDAF</cell><cell></cell><cell cols="2">67.10 66.92 (±0.23)</cell><cell cols="2">76.92 76.79 (±0.08)</cell><cell cols="2">70.97 79.53</cell></row><row><cell>SEPOS</cell><cell></cell><cell cols="2">67.65 66.05 (±2.94)</cell><cell cols="2">77.25 75.80 (±2.65)</cell><cell cols="2">71.46 79.70</cell></row><row><cell cols="2">SECT-SECT-CNN</cell><cell cols="2">67.29 64.04 (±4.28)</cell><cell cols="2">76.91 73.99 (±3.89)</cell><cell cols="2">69.70 78.49</cell></row><row><cell cols="2">SEDT-CNN</cell><cell cols="2">67.88 66.53 (±1.91)</cell><cell cols="2">77.27 76.21 (±1.67)</cell><cell cols="2">71.58 79.80</cell></row><row><cell>Table 1: Method</cell><cell cols="2">Single EM F1</cell><cell>Ensemble EM F1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BiDAF</cell><cell cols="3">67.69 77.07 72.33 80.33</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SECT-LSTM 68.12 77.21 72.83 80.58</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SEDT-LSTM 68.48 77.97 73.02 80.84</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on the official blind test set. Ensemble models are trained over the five single runs with the identical network and hyper-parameters.</figDesc><table><row><cell>Method</cell><cell>EM</cell><cell>F1</cell></row><row><cell>SECT-Random</cell><cell cols="2">5.64 12.85</cell></row><row><cell cols="3">SECT-Random-Order 30.04 39.98</cell></row><row><cell>SECT-Only</cell><cell cols="2">34.21 44.53</cell></row><row><cell>SEDT-Random</cell><cell>0.92</cell><cell>8.82</cell></row><row><cell cols="3">SEDT-Random-Order 31.82 43.65</cell></row><row><cell>SEDT-Only</cell><cell cols="2">32.96 44.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance means and standard deviations of different window sizes on the development set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Questions that are correctly answered by SECT but not BiDAF</figDesc><table><row><cell>Question</cell><cell>Context</cell><cell>BiDAF</cell><cell>SEDT</cell></row><row><cell>In the layered</cell><cell></cell><cell></cell><cell></cell></row><row><cell>model of the Earth,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>the mantle has two</cell><cell></cell><cell></cell><cell></cell></row><row><cell>layers below it.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>What are they?</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Questions that are correctly answered by SEDT but not BiDAF</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Fethi Bougares, Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building watson: An overview of the deepqa project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01436</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A Phased Ranking Model for Information Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">999</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The javelin questionanswering system at trec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jaime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevyn</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hiyakumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09830</idno>
		<title level="m">Newsqa: A machine comprehension dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine comprehension using match-lstm and answer pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-perspective context matching for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analytics Meta Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">End-to-end answer chunk extraction and ranking for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09996</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
