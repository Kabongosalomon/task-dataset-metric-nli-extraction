<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dark Model Adaptation: Semantic Image Segmentation from Daytime to Nighttime</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
						</author>
						<title level="a" type="main">Dark Model Adaptation: Semantic Image Segmentation from Daytime to Nighttime</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work addresses the problem of semantic image segmentation of nighttime scenes. Although considerable progress has been made in semantic image segmentation, it is mainly related to daytime scenarios. This paper proposes a novel method to progressive adapt the semantic models trained on daytime scenes, along with large-scale annotations therein, to nighttime scenes via the bridge of twilight time -the time between dawn and sunrise, or between sunset and dusk. The goal of the method is to alleviate the cost of human annotation for nighttime images by transferring knowledge from standard daytime conditions. In addition to the method, a new dataset of road scenes is compiled; it consists of 35,000 images ranging from daytime to twilight time and to nighttime. Also, a subset of the nighttime images are densely annotated for method evaluation. Our experiments show that our method is effective for knowledge transfer from daytime scenes to nighttime scenes, without using extra human annotation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Autonomous vehicles will have a substantial impact on people's daily life, both personally and professionally. For instance, automated vehicles can largely increase human productivity by turning driving time into working time, provide personalized mobility to non-drivers, reduce traffic accidents, or free up parking space and generalize valet service <ref type="bibr" target="#b2">[3]</ref>. As such, developing automated vehicles is becoming the core interest of many, diverse industrial players. Recent years have witnessed great progress in autonomous driving <ref type="bibr" target="#b13">[14]</ref>, resulting in announcements that autonomous vehicles have driven over many thousands of miles and that companies aspire to sell such vehicles in a few years. All this has fueled expectations that fully automated vehicles are coming soon. Yet, significant technical obstacles must be overcome before assisted driving can be turned into full-fletched automated driving, a prerequisite for the above visions to materialize.</p><p>While perception algorithms based on visible light cameras are constantly getting better, they are mainly designed to operate on images taken at daytime under good illumination <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Yet, outdoor applications can hardly escape from challenging weather and illumination conditions. One of the big reasons that automated cars have not gone mainstream yet is because it cannot deal well with nighttime and adverse weather conditions. Camera sensors can become untrustworthy at nighttime, in foggy weather, and in wet weather. Thus, computer vision systems have to function well 1 Dengxin Dai and Luc Van Gool are with the Toyota TRACE-Zurich team at the Computer Vision Lab, ETH Zurich, 8092 Zurich, Switzerland firstname.lastname@vision.ee.ethz.ch <ref type="bibr" target="#b1">2</ref> Luc Van Gool is also with the Toyota TRACE-Leuven team at the Dept of Electrical Engineering ESAT, KU Leuven 3001 Leuven, Belgium also under these adverse conditions. In this work, we focus on semantic object recognition for nighttime driving scenes.</p><p>Robust object recognition using visible light cameras remains a difficult problem. This is because the structural, textural and/or color features needed for object recognition sometimes do not exist or highly disbursed by artificial lights, to the point where it is difficult to recognize the objects even for human. The problem is further compounded by camera noise <ref type="bibr" target="#b31">[32]</ref> and motion blur. Due to this reason, there are systems using far-infrared (FIR) cameras instead of the widely used visible light cameras for nighttime scene understanding <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Far-infrared (FIR) cameras can be another choice <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b10">[11]</ref>. They, however, are expensive and only provide images of relatively low-resolution. Thus, this work adopts visible light cameras for semantic segmentation of nighttime road scenes. Another reason of this choice is that large-scale datasets are available for daytime images by visible light cameras <ref type="bibr" target="#b7">[8]</ref>. This makes model adaptation from daytime to nighttime feasible.</p><p>High-level semantic tasks is usually tackled by learning from many annotations of real images. This scheme has achieved a great success for good weather conditions at daytime. Yet, the difficulty of collecting and annotating images for all other weather and illumination conditions renders this standard protocol problematic. To overcome this problem, we depart from this traditional paradigm and propose another route. Instead, we choose to progressively adapt the semantic models trained for daytime scenes to nighttime scenes, by using images taken at the twilight time as intermediate stages. The method is based on progressively self-learning scheme, and its pipeline is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The main contributions of the paper are: 1) a novel model adaptation method is developed to transfer semantic knowledge from daytime scenes to nighttime scenes; 2) a new dataset, named Nighttime Driving, consisting of images of real driving scenes at nighttime and twilight time, with 35, 000 unlabeled images and 50 densely annotated images. These contributions will facilitate the learning and evaluation of semantic segmentation methods for nighttime driving scenes. Nighttime Driving is available at http: //people.ee.ethz.ch/˜daid/NightDriving/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Semantic Understanding of Nighttime Scenes</head><p>A lot of work for nighttime object detection/recognition has focused on human detection, by using FIR cameras <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b9">[10]</ref> or visible light cameras <ref type="bibr" target="#b17">[18]</ref>, or a combination of both <ref type="bibr" target="#b4">[5]</ref>  road traffic objects such as cars <ref type="bibr" target="#b18">[19]</ref> and their rear lights <ref type="bibr" target="#b28">[29]</ref>. Another group of work is to develop methods robust to illumination changes for robust road area detection <ref type="bibr" target="#b1">[2]</ref> and semantic labeling <ref type="bibr" target="#b24">[25]</ref>. Most of the research in this vein had been conducted before deep learning was widely used. Semantic understanding of visual scenes have recently undergone rapid growth, making accurate object detection feasible in images and videos in daytime scenes <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b20">[21]</ref>. It is natural to raise the question of how to extend those sophisticated methods to other weather conditions and illumination conditions, and examine and improve the performance therein. A recent effort has been made for foggy weather <ref type="bibr" target="#b26">[27]</ref>. This work would like to initiate the same research effort for nighttime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Adaptation</head><p>Our work bears resemblance to works from the broad field of transfer learning. Model adaptation across weather conditions to semantically segment simple road scenes is studied in <ref type="bibr" target="#b19">[20]</ref>. More recently, domain adaptation based approach was proposed to adapt semantic segmentation models from synthetic images to real environments <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b6">[7]</ref>. The supervision transfer from daytime scenes to nighttime scenes in this paper is inspired by the stream of work on model distillation/imitation <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Our approach is similar in that knowledge is transferred from one domain to the next by distilled from the previous domain. The concurrent work in <ref type="bibr" target="#b25">[26]</ref> on adaptation of semantic models from clear weather condition to light fog then to dense fog is closely related to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Road Scene Understanding</head><p>Road scene understanding is a crucial enabler for applications such as assisted or autonomous driving. Typical examples include the detection of roads <ref type="bibr" target="#b3">[4]</ref>, traffic lights <ref type="bibr" target="#b16">[17]</ref>, cars and pedestrians <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b26">[27]</ref>, and tracking of such objects <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b21">[22]</ref>. We refer the reader to the excellent surveys <ref type="bibr" target="#b23">[24]</ref>. The aim of this work is to extend/adapt the advanced models developed recently for road scene understanding at daytime to nighttime, without manually annotating nighttime images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>Training a segmentation model with large amount of human annotations should work for nighttime images, similar to what has been achieved for daytime scene understanding <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>. However, applying this protocol to other weather conditions and illumination conditions is problematic as it is hardly affordable to annotate the same amount of data for all different conditions and their combinations. We depart from this protocol and investigate an automated approach to transfer the knowledge from existing annotations of daytime scenes to nighttime scenes. The approach leverages the fact that illumination changes continuously between daytime and nighttime, through the twilight time. Twilight is the time between dawn and sunrise, or between sunset and dusk. Twilight is defined according to the solar elevation angle, which is the position of the geometric center of the sun relative to the horizon <ref type="bibr" target="#b0">[1]</ref>. See <ref type="figure" target="#fig_1">Figure 2</ref> for an illustration.</p><p>During a large portion of twilight time, solar illumination suffices enough for cameras to capture the terrestrial objects and suffices enough to alleviate the interference of artificial lights to a limited amount. See <ref type="figure" target="#fig_0">Figure 1</ref> for examples of road scenes at twilight time. These observations lead to our conjecture that the domain discrepancy between daytime scenes and twilight scenes, and the the domain discrepancy between twilight scenes and nighttime scenes are both smaller than the domain discrepancy between daytime scenes and nighttime scenes. Thus, images captured during twilight time can serve our purpose well -transfer knowledge from daytime to nighttime. That is, twilight time constructs a bridge for knowledge transfer from our source domain daytime to our target domain nighttime.</p><p>In particular, we train a semantic segmentation model on daytime images using the standard supervised learning paradigm, and apply the model to a large dataset recorded at civil twilight time to generate the class responses. The three subgroups of twilight are used: civil twilight, nautical twilight, and astronomical twilight <ref type="bibr" target="#b0">[1]</ref>. Since the domain gap between daytime condition and civil twilight condition is relatively small, these class responses, along with the images, can then be used to fine-tune the semantic segmentation model so that it can adapt to civil twilight time. The same procedure is continued through nautical twilight and astronomical twilight. We then apply the final fine-tuned model to nighttime images.</p><p>This learning approach is inspired by the stream of work on model distillation <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Those methods either transfer supervision from sophisticated models to simpler models for efficiency <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b8">[9]</ref>, or transfer supervision from the domain of images to other domains such as depth maps <ref type="bibr" target="#b11">[12]</ref>. We here transfer the semantic knowledge of annotations of daytime scenes to nighttime scenes via the unlabeled images recorded at twilight time.</p><p>Let us denote an image by x, and indicate the image taken at daytime, civil twilight time, nautical twilight time, astronomical twilight time and nighttime by x 0 , x 1 , x 2 , x 3 , and x 4 , respectively. The corresponding human annotation for x 0 is provided and denoted by y 0 , where y 0 (m, n) ∈ {1, ..., C} is the label of pixel (m, n), and C is the total number of classes. Then, the training data consist of labeled data at daytime D 0 = {(x 0 i , y 0 i )} l 0 i=1 , and three unlabeled datasets for the three twilight categories:</p><formula xml:id="formula_0">D 1 = {x 1 j } l 1 j=1 , D 2 = {x 2 k } l 2 k=1 , and D 3 = {x 3 q } l 3 q=1</formula><p>, where l 0 , l 1 , l 2 , and l 3 are the total number of images in the corresponding datasets.</p><p>The method consists of eight steps and it is summarized below.</p><p>1: train a segmentation model with daytime images and the human annotations:</p><formula xml:id="formula_1">min φ 0 1 l 0 l 0 i=1 L(φ 0 (x 0 i ), y 0 i ),<label>(1)</label></formula><p>where L(., .) is the cross entropy loss function; 2: apply segmentation model φ 0 to the images recorded at civil twilight time to obtain "noisy" semantic labels: y 1 = φ 0 (x 1 ), and augment dataset D 1 toD 1 :</p><formula xml:id="formula_2">D 1 = {(x 1 j ,ŷ 1 j )} l 1 j=1 ; 3</formula><p>: instantiate a new model φ 1 by duplicating φ 0 , and then fine-tune (retrain) the semantic model on D 0 andD 1 :</p><formula xml:id="formula_3">φ 1 ← φ 0 ,<label>(2)</label></formula><p>and min φ 1</p><formula xml:id="formula_4">1 l 0 l 0 i=1 L(φ 1 (x 0 i ), y 0 i ) + λ 1 l 1 l 1 j=1 L(φ 1 (x 1 j ),ŷ 1 j ) ,<label>(3)</label></formula><p>where λ 1 is a hyper-parameter balancing the weights of the two data sources; 4: apply segmentation model φ 1 to the images recorded at nautical twilight time to obtain "noisy" semantic labels:ŷ 2 = φ 1 (x 2 ), and augment dataset D 2 toD 2 :</p><formula xml:id="formula_5">D 2 = {(x 2 k ,ŷ 2 k )} l 2 k=1</formula><p>; 5: instantiate a new model φ 2 by duplicating φ 1 , and finetune (train) semantic model on D 0 ,D 1 andD 2 :</p><formula xml:id="formula_6">φ 2 ← φ 1 ,<label>(4)</label></formula><p>and then</p><formula xml:id="formula_7">min φ 2 1 l 0 l 0 i=1 L(φ 2 (x 0 i ), y 0 i ) + λ 1 l 1 l 1 j=1 L(φ 2 (x 1 j ),ŷ 1 j ) + λ 2 l 2 l 2 k=1 L(φ 2 (x 2 k ),ŷ 2 k ) ,<label>(5)</label></formula><p>where λ 1 and λ 2 are hyper-parameters regulating the weights of the datasets; 6: apply segmentation model φ 2 to the images recorded at astronomical twilight data to obtain "noisy" semantic labels:ŷ 3 = φ 2 (x 3 ), and augment dataset D 3 toD 3 :</p><formula xml:id="formula_8">D 3 = {(x 3 q ,ŷ 3 q )} l 3 q=1 ; ; 7</formula><p>: instantiate a new model φ 3 by duplicating φ 2 , and finetune (train) the semantic model on all four datasets D 0 , D 1 ,D 2 andD 3 :</p><formula xml:id="formula_9">φ 3 ← φ 2 ,<label>(6)</label></formula><p>and then</p><formula xml:id="formula_10">min φ 3 1 l 0 l 0 i=1 L(φ 3 (x 0 i ), y 0 i ) + λ 1 l 1 l 1 j=1 L(φ 3 (x 1 j ),ŷ 1 j ) + λ 2 l 2 l 2 k=1 L(φ 3 (x 2 k ),ŷ 2 k ) + λ 3 l 3 l 3 q=1 L(φ 3 (x 3 q ),ŷ 3 q ) ,<label>(7)</label></formula><p>where λ 1 , λ 1 and λ 3 are hyper-parameters regulating the weights of the datasets; 8: apply model φ 3 to nighttime images to perform the segmentation:ŷ 4 = φ 3 (x 4 ). We term our method Gradual Model Adaptation. During training, in order to balance the weights of different data sources (in Equation 3, Equation 5 and Equation 7), we empirically give equal weight to all training datasets. An optimal value can be obtained via cross-validation. The optimization of Equation 3, Equation 5 and Equation 7 are implemented by feeding to the training algorithm a stream of hybrid data, for which images in the considered datasets are sampled proportionally according to the parameters λ 1 , λ 2 , and λ 3 . In this work, they all set to 1, which means all datasets are sampled at the same rate.</p><p>Rather than applying the model trained on daytime images directly to nighttime images, Gradual Model Adaptation breaks down the problem to three progressive steps to adapt the semantic model. In each of the step, the domain gap is much smaller than the domain gap between daytime domain and nighttime domain. Due to the unsupervised nature of this domain adaptation, the algorithm will also be affected by the noise in the labels. The daytime dataset D 1 is always used for the training, to balance between noisy data of similar domains and clean data of a distinct domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Data Collection</head><p>Nighttime Driving was collected during 5 rides with a car inside multiple Swiss cities and their suburbs using a GoPro Hero 5 camera. We recorded 5 large video sequence with length of about 2 hours. The video recording starts from daytime, goes through twilight time and ends at full nighttime. The video frames are extracted at a rate of one frame per second, leading to 35,000 images in total. According to <ref type="bibr" target="#b0">[1]</ref> and the sunset time of each recording day, we partition the dataset into five parts: daytime, civil twilight time, nautical twilight time, astronomical twilight time, and nighttime. They consist of 8000, 8750, 8750, 8750, and 9500 images, respectively.</p><p>We manually select 50 nighttime images of diverse visual scenes, and construct the test set of Nighttime Driving therefrom, which we term Nighttime Driving-test. The aforementioned selection is performed manually in order to guarantee that the test set has high diversity, which compensates for its relatively small size in terms of statistical significance of evaluation results. We annotate these images with fine pixellevel semantic annotations using the 19 evaluation classes In addition, we assign the void label to pixels which do not belong to any of the above 19 classes, or the class of which is uncertain due to insufficient illumination. Every such pixel is ignored for semantic segmentation evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Evaluation</head><p>Our model of choice for experiments on semantic segmentation is the RefineNet <ref type="bibr" target="#b20">[21]</ref>. We use the publicly available RefineNet-res101-Cityscapes model, which has been trained on the daytime training set of Cityscapes. In all experiments of this section, we use a constant base learning rate of 5 × 10 −5 and mini-batches of size 1.</p><p>Our segmentation experiment showcases the effectiveness of our model adaptation pipeline, using twilight time as a bridge. The models which are obtained after the initial adaptation step are further fine-tuned on the union of the daytime Cityscapes dataset and the previously segmented twilight datasets, where the latter sets are labeled by the adapted models one step ahead.</p><p>We evaluate four variants of our method and compare them to the original segmentation model trained on daytime images directly. Using the pipeline described in Section III, three models can be obtained, in particular φ 1 , φ 2 , and φ 3 .</p><p>We also compare to an alternative adaptation approach which generates labels (by using the original model trained on daytime data) for all twilight images at once and finetunes the original daytime segmentation model once. To put in another word, the three-step progressive model adaptation is reduced to a one-step progressive model adaptation.</p><p>Quantitative Results. The overall intersection over union (IoU) over all classes of the semantic segmentation by all methods are reported in Tables I. The table shows that all variants of our adaptation method improve the performance of the original semantic model trained with daytime data. This is mainly due to the fact that twilight time fall into the middle ground of daytime and nighttime, so the domain gaps from twilight to the other two domains are smaller than the direct domain gap of the two.</p><p>Also, it can be seen from the table that our method benefits from the progressive adaptation in three steps, i.e. from daytime to civil twilight, from civil twilight to nautical twilight, and from nautical twilight to astronomical twilight. The complete pipeline outperforms the two incomplete alternatives. This means that the gradual adaptation closes the domain gap progressively. As the model is adapted one more step forward, the gap to the target domain is further narrowed. Data recorded through twilight time constructs a trajectory between the source domain (daytime) and the target domain (nighttime) and makes daytime-to-nighttime knowledge transfer feasible.</p><p>Finally, we find that our three-step progressive pipeline outperforms the one-step progressive alternative. This is mainly due to the unsupervised nature of the model adap-tation: the method learns from generated labels for model adaptation. This means that the accuracy of the generated labels directly affect the quality of the adaptation. The onestep adaptation alternative proceeds more aggressively and in the end learns from more noisy generated labels than than our three-step complete pipeline. The three-step model adaptation method generate labels only on data which falls slightly off the training domain of the previous model. Our three-step model adaptation strikes a good balance between computational cost and quality control.</p><p>Qualitative Results. We also show multiple segmentation examples by our method (the three-step complete pipeline) and the original daytime RefineNet model in <ref type="figure">Figure 3</ref>. From the two figures, one can see that our method generally yields better results than the original RefineNet model. For instance, in the second image of <ref type="figure">Figure 3</ref>, the original RefineNet model misclassified some road area as car.</p><p>While improvement has been observed, the performance of for nighttime scenes is still a lot worse than that for daytime scenes. Nighttime scenes are indeed more challenging than daytime scenes for semantic understanding tasks. There are more underlying causal factors of variation that generated night data, which requires either more training data or more intelligent learning approaches to disentangle the increased number of factors. Also, the models are adapted in an unsupervised manner. Introducing a reasonable amount of human annotations of nighttime scenes will for sure improve the results. This constitutes our future work.</p><p>Limitation. Many regions in nighttime images are uncertain for human annotators. Those areas should be treated as a separate, special class; algorithms need to be trained to predict this special class as well. It is misleading to assign a class label to those areas. This will be implemented in our next work. We also argue that street lamps should be considered as a separate class in addition to the classes considered in Cityscapes' daytime driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>This work has investigated the problem of semantic image segmentation of nighttime scenes from a novel perspective. This paper has proposed a novel method to progressive adapts the semantic models trained on daytime scenes to nighttime scenes via the bridge of twilight time -the time between dawn and sunrise, or between sunset and dusk. Data recorded during twilight times are further grouped into three subgroups for a three-step progressive model adaptation, which is able to transfer knowledge from daytime to nighttime in an unsupervised manner. In addition to the method, a new dataset of road driving scenes is compiled. It consists of 35,000 images ranging from daytime to twilight time and to nighttime. Also, 50 diverse nighttime images are densely annotated for method evaluation. The experiments show that our method is effective for knowledge transfer from daytime scenes to nighttime scenes without using human supervision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The pipeline of our approach for semantic segmentation of nighttime scenes, by transferring knowledge from daytime scenes via the bridge of twilight time scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Twilight is defined according to the solar elevation angle and is categorized into three subcategories: civil twilight, nautical twilight, and astronomical twilight. (picture is from wikipedia).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. There are also notable examples for detecting other</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Infer</cell><cell></cell><cell></cell></row><row><cell cols="5">Daytime images Human Annotation</cell><cell cols="2">Nighttime image</cell><cell></cell><cell cols="2">Result</cell></row><row><cell></cell><cell></cell><cell cols="2">Train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Segmentation</cell><cell></cell><cell cols="2">Segmentation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Model (daytime)</cell><cell></cell><cell cols="2">Model (twilight)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Train</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Infer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">… Twilight images</cell><cell></cell><cell></cell><cell></cell><cell cols="4">… Generated Semantic Labels</cell></row><row><cell>Void</cell><cell>Road</cell><cell>Sidewalk</cell><cell>Building</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell>Traffic Light</cell><cell>Traffic Sign</cell><cell>Vegetation</cell></row><row><cell>Terrain</cell><cell>Sky</cell><cell>Person</cell><cell>Rider</cell><cell>Car</cell><cell>Truck</cell><cell>Bus</cell><cell>Train</cell><cell>Motorcycle</cell><cell>Bicycle</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>COMPARISON BETWEEN THE VARIANTS OF OUR METHOD TO THE ORIGINAL SEGMENTATION MODEL.</figDesc><table><row><cell>Model</cell><cell>Fine-tuning on twilight data</cell><cell>Mean IoU</cell></row><row><cell>Refinenet [21]</cell><cell>-</cell><cell>35.2</cell></row><row><cell>Refinenet</cell><cell>φ 1 (→ civil)</cell><cell>38.6</cell></row><row><cell>Refinenet</cell><cell>φ 2 (→ civil → nautical)</cell><cell>39.9</cell></row><row><cell>Refinenet</cell><cell>φ 3 (→ civil → nautical → astronomical)</cell><cell>41.6</cell></row><row><cell>Refinenet</cell><cell>→ all twilight (1-step adaptation)</cell><cell>39.1</cell></row><row><cell cols="3">of the Cityscapes dataset [8]: road, sidewalk, building, wall,</cell></row><row><cell cols="3">fence, pole, traffic light, traffic sign, vegetation, terrain, sky,</cell></row><row><cell cols="3">person, rider, car, truck, bus, train, motorcycle and bicycle.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Qualitative results for semantic segmentation on Nighttime Driving-test.</figDesc><table><row><cell cols="2">(a) nighttime image</cell><cell></cell><cell cols="2">(b) ground truth</cell><cell cols="2">(c) refineNet [21]</cell><cell></cell><cell cols="2">(d) our method</cell></row><row><cell>Void</cell><cell>Road</cell><cell>Sidewalk</cell><cell>Building</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell>Traffic Light</cell><cell>Traffic Sign</cell><cell>Vegetation</cell></row><row><cell>Terrain</cell><cell>Sky</cell><cell>Person</cell><cell>Rider</cell><cell>Car</cell><cell>Truck</cell><cell>Bus</cell><cell>Train</cell><cell>Motorcycle</cell><cell>Bicycle</cell></row><row><cell></cell><cell></cell><cell>Fig. 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work is supported by Toyota Motor Europe via the research project TRACE-Zurich.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Definitions from the us astronomical applications dept (usno)</title>
		<imprint>
			<date type="published" when="2011-07-22" />
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Road detection based on illuminant invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="193" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Autonomous Vehicle Technology: A Guide for Policymakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nidhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Oluwatola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>RAND Corporation</publisher>
			<pubPlace>Santa Monica, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recent progress in road and lane detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vision Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="727" to="745" />
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Night-time pedestrian detection by visualinfrared video fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Congress on Intelligent Control and Automation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Metric imitation by manifold transfer for efficient vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time pedestrian detection and tracking at nighttime for driver-assistance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pedestrian detection at day/night time with visible and fir cameras: A comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzlez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Socarras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vzquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lpez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end learning of driving models with surround-view cameras and route planners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cycada: Cycle consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vision for looking at traffic lights: Issues, survey, and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Philipsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mgelmose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1800" to="1815" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional neural networkbased human detection in nighttime images using visible light camera sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bayes saliency-based object proposal generator for nighttime traffic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="814" to="825" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequential bayesian model update under structured scene prior for semantic road scenes labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks with identity mappings for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pathtrack: Path supervision for efficient video annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vision and the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Looking at humans in the age of self-driving and highly automated vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="104" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised image transformation for outdoor semantic labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Looking at vehicles in the night: Detection and dynamics of rear lights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Satzoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Looking at vehicles on the road: A survey of vision-based vehicle detection, tracking, and behavior analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1773" to="1795" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pedestrian detection and tracking with night vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fujimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="71" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time stereo vision system at nighttime with noise reduction using simplified non-local matching cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tehrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ishimaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shirai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
