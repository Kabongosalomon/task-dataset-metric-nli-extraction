<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EPSNet: Efficient Panoptic Segmentation Network with Cross-layer Attention Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Yuan</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National University of Kaohsiung</orgName>
								<address>
									<settlement>Kaohsiung</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EPSNet: Efficient Panoptic Segmentation Network with Cross-layer Attention Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0000−0001−5841−3455] , Shuo-En Chang 1[0000−0003−4916−499X] , Pei-Yung Hsiao 2[0000−0003−1750−7118] , and Li-Chen Fu 1[0000−0002−6947−7646]</p><p>Abstract. Panoptic segmentation is a scene parsing task which unifies semantic segmentation and instance segmentation into one single task. However, the current state-of-the-art studies did not take too much concern on inference time. In this work, we propose an Efficient Panoptic Segmentation Network (EPSNet) to tackle the panoptic segmentation tasks with fast inference speed. Basically, EPSNet generates masks based on simple linear combination of prototype masks and mask coefficients. The light-weight network branches for instance segmentation and semantic segmentation only need to predict mask coefficients and produce masks with the shared prototypes predicted by prototype network branch. Furthermore, to enhance the quality of shared prototypes, we adopt a module called "cross-layer attention fusion module", which aggregates the multi-scale features with attention mechanism helping them capture the long-range dependencies between each other. To validate the proposed work, we have conducted various experiments on the challenging COCO panoptic dataset, which achieve highly promising performance with significantly faster inference speed (51ms on GPU). Code is available at: github.com/neo85824/epsnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to Convolutional Neural Networks (CNNs) and other advances in deep learning, computer vision systems have achieved considerable success especially on computer vision tasks such as image recognition <ref type="bibr" target="#b0">[1]</ref>, semantic segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and instance segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. In particular, semantic segmentation aims to assign specific class label for each image pixel, whereas instance segmentation predicts foreground object masks. However, the former is not capable of separating objects of the same class, and the latter only focuses on segmenting of things (i.e countable objects such as people, animals, and tools) rather than stuff (i.e amorphous regions such as grass, sky, and road). To overcome the respective shortcomings, combination of semantic segmentation and instance segmentation leads to the so-called panoptic segmentation <ref type="bibr" target="#b7">[8]</ref>. More specifically, the goal of panoptic segmentation is to assign a semantic label and an instance ID to every pixel in an image.  Several methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> have been proposed for panoptic segmentation in the literature. Detection-based approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> usually exploit an instance segmentation network like Mask R-CNN <ref type="bibr" target="#b5">[6]</ref> as the main stream and attach light-weight semantic segmentation branch after the shared backbone. Then, they combine those outputs by heuristic fusion <ref type="bibr" target="#b7">[8]</ref> to generate the final panoptic prediction. Despite such detection-based fashions which achieve the state-of-the-art results, they solely aim to improve the performance but may sacrifice the computation load and speed. In fact, detection-based methods suffer from several limitations. First, due to the two-stage detector, instance segmentation branch costs the major computation time and drags down the inference speed. Second, most detection-based approaches commonly employ the outputs of backbone, like feature pyramid network <ref type="bibr" target="#b14">[15]</ref>, as shared features without further enhancement, causing sub-optimality of features used by the following branches. Lastly, the independent branches unfortunately lead to inconsistency when generating final prediction.</p><p>To address the above problems, we propose a novel one-stage framework called Efficient Panoptic Segmentation Network (EPSNet), as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. It adopts parallel networks to generate prototype masks for the entire image and predicts a set of coefficients for instance and semantic segmentation. Instance and semantic segments can be easily generated by linearly combining the prototypes with predicted coefficients from the branches. The proposed semantic branch only needs to produce coefficients for each class instead of pixel-wise predictions. Moreover, the prototypes are shared by both branches, which save time for producing large-size masks and help them solve their tasks simultaneously. Further, we introduce an innovative fusion module called cross-layer attention fusion module, which enhances the quality of shared features with attention mechanism. Instead of directly using suboptimal features in FPN, we choose certain layer as the target feature and other layers as source features and then apply an attention module on them to capture spatial dependencies for any two positions of the feature maps. For each position in target feature, it is updated via aggregating source features at all positions with weighted summation. To verify the efficiency of EPSNet, we conduct experiments on COCO <ref type="bibr" target="#b15">[16]</ref> dataset. The experimental results manifest that our method achieves competitive performances with much faster inference compared to current approaches, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Panoptic Segmentation</head><p>Panoptic segmentation is originally proposed by <ref type="bibr" target="#b7">[8]</ref>. In panoptic segmentation tasks, each pixel in the image needs to be assigned a semantic label and an instance ID. In <ref type="bibr" target="#b7">[8]</ref>, separate networks are used for semantic segmentation and instance segmentation, respectively, and then the results are combined with heuristic rules. The recent approaches of panoptic segmentation train semantic and instance segmentation network in end-to-end fashion with shared backbone. These methods can be categorized into two groups, namely, detection-based methods and bottom-up methods.</p><p>Detection-based. Most detection-based methods exploit Mask R-CNN <ref type="bibr" target="#b5">[6]</ref> as their instance segmentation network and attach semantic segmentation branch with FCN <ref type="bibr" target="#b16">[17]</ref> after shared backbone. These approaches are also considered as two-stage methods because of the additional stage to generate proposals. For instances, JSIS <ref type="bibr" target="#b17">[18]</ref> firstly trains instance and semantic segmentation network jointly. TASCNet <ref type="bibr" target="#b18">[19]</ref> ensures the consistency of stuff and thing prediction through binary mask. OANet <ref type="bibr" target="#b11">[12]</ref> uses spatial ranking module to deal with the occlusion problem between the predicted instances. Panoptic FPN <ref type="bibr" target="#b13">[14]</ref> endows Mask R-CNN <ref type="bibr" target="#b5">[6]</ref> with a semantic segmentation branch. AUNet <ref type="bibr" target="#b12">[13]</ref> adds RPN and thing segmentation mask attentions to stuff branch to provide object-level and pixel-level attentions. UPSNet <ref type="bibr" target="#b8">[9]</ref> introduces a parameter-free panoptic head which solves the panoptic segmentation via pixel-wise classification. The First weakly-supervised method is proposed by <ref type="bibr" target="#b19">[20]</ref> to diminish the cost of pixel-level annotations. SOGNet <ref type="bibr" target="#b10">[11]</ref> encodes overlap relations without direct supervision to solve overlapping. AdaptIS <ref type="bibr" target="#b9">[10]</ref> adapts to the input point with a help of AdaIN layers <ref type="bibr" target="#b20">[21]</ref> and produce masks for different objects on the same image. Although detection-based methods achieve better performance, they are usually slow in inference because of two-stage Mask R-CNN <ref type="bibr" target="#b5">[6]</ref> in instance head. In addition, the inconsistency of semantic and instance segmentation needs to be solved when the two are merged into panoptic segmentation.</p><p>Bottom-up. Unlike the above approaches, some methods tackle panoptic segmentation tasks by associating pixel-level predictions to each object instance <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. In these approaches, they first predict the foreground mask with semantic segmentation, and then use several types of heatmaps to group foreground pixels into objects. DeeperLab <ref type="bibr" target="#b25">[26]</ref> predicts instance keypoint as well as multi-range offset heatmap and then groups them into class-agnostic instance segmentation. In semantic segmentation head, they follow the design of DeepLab <ref type="bibr" target="#b2">[3]</ref>. At the end, panoptic segmentation is generated by merging class-agnostic instance masks and semantic output. SSAP <ref type="bibr" target="#b26">[27]</ref> groups pixels based on a pixel-pair affinity pyramid with an efficient graph partition method. Despite the single-shot architecture of bottom-up approaches, their post-processing step still needs major computational time. Also, the performance of the bottom-up methods usually is inferior to that of the detection-based methods.</p><p>Recently, the proposed methods obtain shared feature for semantic and instance head. The quality of shared feature is highly essential for the following network head to produce better results. Still, the proposed approaches do not take this into consideration, and they usually make use of the output of shared backbone as shared feature directly.</p><p>In this work, we aim to propose a panoptic segmentation network based on one-stage detector to attain fast inference speed and competitive performance.</p><p>To increase the quality of shared feature, our proposed cross-layer attention fusion, which is a lightweight network, provides the target feature map with richer information in different feature pyramid layers using attention mechanism.</p><p>3 Efficient Panoptic Segmentation Network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Efficient Panoptic Segmentation Network</head><p>Our method consists of five major components including (1) shared backbone, (2) protohead for generating prototypes, (3) instance segmentation head, (4) semantic segmentation head, and (5) cross-layer attention fusion module for aggregating multi-scale feature.</p><p>Backbone. Our backbone exploits a deep residual network (ResNet) <ref type="bibr" target="#b0">[1]</ref> with a feature pyramid network (FPN) <ref type="bibr" target="#b14">[15]</ref>, which takes a standard network with features at multiple spatial resolutions and adds a light top-down pathway with lateral connections. It generates a pyramid feature with scales from 1/8 to 1/128 resolution (F 3 to F 7 ), where each pyramid channel dimension is set to 256, see <ref type="figure">Fig. 3</ref>. For these features, F 7 is fed to the semantic head, and F 3 to F 5 are sent to instance head and protohead as inputs.</p><p>Protohead. Rather than producing masks with FCN <ref type="bibr" target="#b16">[17]</ref>, inspired by Yolact <ref type="bibr" target="#b27">[28]</ref>, we choose to combine prototypes and mask coefficients with linear combination to generate masks. Our network heads only need to deal with mask coefficients and construct masks with shared prototypes. The goal of protohead is to provide high-quality prototypes which contain semantic information and details of high-resolution feature. <ref type="figure">Fig. 3</ref>. Architecture of EPSNet. We adopt ResNet-101 <ref type="bibr" target="#b0">[1]</ref> with FPN <ref type="bibr" target="#b14">[15]</ref> as our backbone and only exploit F3, F4, F5 for cross-layer attention fusion module. The prototypes are shared by semantic and instance head. k denotes the number of prototypes. Na denotes the number of anchors. N thing and N stuff stands for the number of thing and stuff classes, respectively. ⊗ means matrix multiplication.</p><p>To generate higher resolution prototypes with more semantic values, we perform cross-layer attention fusion module to aggregate multi-scale features in backbone into information-richer feature maps for protohead as inputs. Then, we apply three convolutional blocks, 2× bilinear upsampling and 1 × 1 convolution to produce output prototypes which are at 1/4 scale with k channels.</p><p>Instance Segmentation Head. In most panoptic segmentation networks, they adopt Mask R-CNN <ref type="bibr" target="#b5">[6]</ref> as their instance segmentation branch. Yet, Mask R-CNN <ref type="bibr" target="#b5">[6]</ref> needs to generate proposals first and then classify and segment those proposals in the second stage. Inspired by one-stage detector, Yolact <ref type="bibr" target="#b27">[28]</ref>, our instance head directly predicts object detection results and mask coefficients to make up the segmentation with prototypes without feature localization (e.g. ROI Align <ref type="bibr" target="#b5">[6]</ref>) and refinement.</p><p>The instance head aims to predict box regression, classification confidences and mask coefficients. There are three branches in instance head. Regression branch predicts 4 box regression values, classification branch predicts N thing class confidences, and mask branch predicts k mask coefficients. Thus, there are totally 4+N thing +k values for each anchor. We perform a convolutional block on input features (F 3 to F 5 ) first and send them to each branch to predict respective results. In mask branch, we choose tanh as the activation function, which allows subtraction when linearly combining the coefficients.</p><p>In inference, we choose the mask coefficients whose corresponding bounding boxes survive after NMS procedure. Then, we combine mask coefficients and pro- <ref type="figure">Fig. 4</ref>. The design of proposed semantic head. We adopt F7 as input feature and apply two convolutional blocks on it without padding. The semantic coefficients are produced after average pooling, which predicts k mask coefficients for each stuff classes. Here, ⊗ denotes matrix multiplication.</p><p>totypes generated from protohead with linear combination followed by sigmoid to produce instance masks and crop final mask with predicted bounding box. During training, we crop mask with ground truth bounding box and divide mask segmentation loss by the area of ground truth bounding box.</p><p>Semantic Segmentation Head. Usually, semantic segmentation masks are generated by decoder network <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17]</ref>, which applies FCN <ref type="bibr" target="#b16">[17]</ref> networks and up-sampling layer on the features from backbone to make sure that the size of semantic segmentation outputs is similar to the original input size. However, due to the large feature maps, the computation speed is limited by image size.</p><p>To reduce the computation of large feature maps, we propose a novel semantic segmentation head which only produces mask coefficients for each class, see <ref type="figure">Fig.  4</ref>. The semantic masks can be easily generated by combining the coefficients and prototypes, and each semantic class only demands k mask coefficients. Therefore, with smaller feature maps, the proposed light-weight semantic head can achieve faster inference speed.</p><p>We adopt last layer F 7 from backbone as the input of semantic head. Two convolution blocks are performed. In the second convolution block, the output channel is set to k ×N stuff and tanh is used as activation function. Because of the channel size k×N stuff , every position in the feature map can predict k coefficients for each class to construct semantic segmentation. Accordingly, we perform average pooling to aggregate the mask coefficients from all positions to generate final semantic coefficients. Further, prototypes from protohead and semantic coefficients are reshaped to 2d matrix and applied with linear combination followed by sof tmax to produce semantic segmentation result. The operation is able to be implemented by matrix multiplication which is defined as  where P ∈ R N ×k denotes prototypes , Y ∈ R k×N stuff stands for the reshaped semantic coefficients, and S ∈ R N ×N stuff is semantic segmentation result. N stuff represents the number of stuff classes including 'other' class and N denotes the number of locations in prototypes.</p><formula xml:id="formula_0">S = sof tmax(P · Y ),<label>(1)</label></formula><p>Although the channels of semantic coefficients depend on the number of classes and prototypes, the feature maps in semantic head are much smaller than the large feature maps in other approaches. Our semantic head provides faster semantic segmentation generation and less computation cost.</p><p>Cross-layer Attention Fusion. Since all of the semantic or instance masks are derived from linear combination of mask coefficients and prototypes which are shared to semantic and instance head, the quality of prototypes significantly influences the generated masks. Hence, we decide to enrich the input features of protohead to help protohead produce preferable prototypes.</p><p>To enhance inputs of protohead, we propose a module called Cross-layer Attention (CLA) Fusion Module. This fusion module aims to efficiently aggregate the multi-scale feature maps in FPN layers. Certain layer within the module is chosen to be the target feature map and extract values from source feature maps with attention mechanism, as shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. To provide high resolution details, the layer F 3 , which is the highest resolution feature map in FPN, is selected as target feature map, and we choose other layers (e.g. F 3 to F 5 ) as source features to provide more semantic values for target.</p><p>Instead of directly merging multi-scale features with element-wise addition or concatenation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, we combine them with a block called Cross Attention Block. Inspired by non-local <ref type="bibr" target="#b32">[33]</ref> and self-attention <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, which are able to capture long-range dependencies efficiently in feature map, the proposed cross attention block follows their concepts and further finds the long-range relationships from two different feature maps, as shown in <ref type="figure" target="#fig_3">Fig. 6</ref>. Each position in target feature map is updated with the weighted sum of all positions in source feature, where attention weight is calculated by similarities between the corresponding positions.</p><p>For the target feature map T ∈ R C×N and source feature map O ∈ R C×N , we first transform these feature maps into two feature spaces θ(x) = W θ x and φ(x) = W φ x and calculate the attention score s with dot product as shown below</p><formula xml:id="formula_1">s i,j = θ(T i ) T φ(O j ),<label>(2)</label></formula><p>where s i,j measures the attention score of position j in O and position i in T .</p><p>Here, C denotes the number of channels, and N denotes the number of locations from feature maps. After that, we obtain the attention weight α by normalizing attention score for each position j with sof tmax</p><formula xml:id="formula_2">α i,j = exp(s i,j ) N j=1 exp(s i,j ) ,<label>(3)</label></formula><p>where α i,j stands for the normalized impact of position j in source feature map to position i in target feature map. Then, each position in output feature map A ∈ R C×N is produced by calculating weighted sum of source features across all positions. The operation is shown as follows</p><formula xml:id="formula_3">A i = v( N j=1 α i,j h(O j )).<label>(4)</label></formula><p>Here, A i denotes output feature on position i, and both v(x) = W v x and h(x) = W h x stand for embedding functions. The embedding functions θ, φ, h and v are implemented by 1 × 1 convolution, and their output channels are set to 128, which is 1/2 of input channel to reduce computation cost. Finally, we apply cross attention block on each layer including F 5 , F 4 and F 3 and consider F 3 as target feature map. The overall operation is defined as</p><formula xml:id="formula_4">Z = A F3,F5 + A F3,F4 + A F3,F3 + F 3 ,<label>(5)</label></formula><p>where Z denotes the aggregated result from source and target features. Also, we adopt residual connection that makes a new cross attention block easier to insert without interfering the initial behaviors. With the cross attention block, each position in the target feature is able to obtain spatial dependencies over all positions in feature maps from other layers. Moreover, we also select F 3 as source feature map even F 3 is target feature. In this case, it is same as self-attention, which helps target feature capture longdependencies on its own feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training and Inference</head><p>During training, our EPSNet contains 4 loss functions in total, namely, classification loss L c , box regression loss L b , instance mask segmentation loss L m and semantic segmentation loss L s . Because each loss function is in different scales and normalized policies, different weights on different loss functions actually affect the final performance on instance and semantic branch. Thus, we set several hyper-parameters on those loss functions, which is defined as</p><formula xml:id="formula_5">L = λ c L c + λ b L b + λ m L m + λ s L s .</formula><p>In inference, since we won't allow overlaps on each pixel in panoptic segmentation, we resolve overlaps in instance segmentation with post-processing proposed in <ref type="bibr" target="#b7">[8]</ref>. After getting non-overlapping instance segmentation results, we resolve possible overlaps between instance and semantic segmentation in favor of instance. Further, the stuff regions which are predicted as 'other' or under a predefined threshold are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct experiments on COCO <ref type="bibr" target="#b15">[16]</ref> panoptic dataset. Experimental results demonstrate that EPSNet achieves fast and competitive performance on COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. COCO <ref type="bibr" target="#b15">[16]</ref> panoptic segmentation task consists of 80 thing classes and 53 stuff classes. There are approximately 118K images on training set and 5K on validation set.</p><p>Metrics. We adopt the evaluation metric called panoptic quality (PQ), which is introduced by <ref type="bibr" target="#b7">[8]</ref>. Panoptic quality is defined as:</p><formula xml:id="formula_6">P Q = (p,g)∈TP IoU(p, g) |TP| SQ |TP| |TP| + 1 2 |FP| + 1 2 |FN| RQ ,<label>(6)</label></formula><p>which can be considered as the multiplication of semantic quality (SQ) and recognition quality (RQ). Here, p and g are predicted and ground truth segments. TP, FP and FN represent the set of true positives, false positives and false negatives, respectively.</p><p>Implementation Details. We implement our method based on Pytorch <ref type="bibr" target="#b36">[37]</ref> with single GPU RTX 2080Ti. The models are trained with batch size 2. Owing to the small batch size, we freeze the batch normalization layers within backbone and add group normalization <ref type="bibr" target="#b37">[38]</ref> layers in each head. The initial learning rate and weight decay are set to 10 −3 and 5 × 10 −4 . We train with SGD for 3200K iterations and decay the learning rate by a factor of 10 at 1120k, 2400K, 2800k and 3000k iterations and a momentum of 0.9. The loss weights λ c , λ b , λ m and λ s are 1, 1.5, 6.125 and 2, respectively. Our models are trained with ResNet-101 <ref type="bibr" target="#b0">[1]</ref> backbone using FPN with Ima-geNet <ref type="bibr" target="#b38">[39]</ref> pre-trained weights. We adopt similar training strategies in backbone and instance segmentation head as Yolact <ref type="bibr" target="#b27">[28]</ref>. The number of prototypes k is set to 32. Our instance head only predicts thing classes, and semantic head predicts stuff classes viewing thing class as other. The cross attention blocks are shared in cross-layer attention fusion module. The base image size is 550 × 550. We do not preserve aspect ratio in order to get consistent evaluation times per image. We perform random flipping, random cropping and random scaling on images for data augmentation. The image size is randomly scaled in range [550, 825] and then randomly cropped into 550 × 550.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>To verify the performance of training decisions and our cross-layer fusion attention module, we conduct the experiments with different settings in <ref type="table">Table 1</ref> on COCO panoptic val set. The empty cells in the table indicate that the corresponding component is not used.</p><p>Data Augmentation. We compare the model without using data augmentation during training. The first and second rows in <ref type="table">Table 1</ref> show that the model trained with data augmentation improves by 2% in PQ. It proves that data augmentation plays important role during training.</p><p>Cross-layer Attention Fusion. For the model without using cross-layer fusion module, it is replaced by another fusion module similar to Panoptic-FPN <ref type="bibr" target="#b13">[14]</ref>. We adopt convolution blocks for different layers in backbone and combine them together. The layers F 5 , F 4 and F 3 are attached with 3, 2 and 1 convolution blocks respectively and 2× bilinear upsampling layers between each block. Output features are obtained by combining them with element-wise addition. As shown in second and third rows in <ref type="table">Table 1</ref>, the model employing the cross-layer attention fusion module yields 38.4% in PQ and 2.8% improvement in PQ St . Loss Balance. In order to balance the loss values in similar order of magnitude, we assign different weights for each loss during training. With loss balance, the weights are same as experimental setting. Without loss balance, all weights are set to 1. As shown in the third and fourth rows in <ref type="table">Table 1</ref>, the model with loss balance performs better especially on PQ Th .</p><p>Prototypes. We further conduct experiments on the number of prototypes. As shown in <ref type="table">Table 2</ref>, the number of prototypes barely influence the overall performance. However, it will affect the inference and post-processing time. We choose 32 prototypes for the setting of EPSNet.</p><p>The ablation results show that our cross-layer attention fusion and training strategies bring significant improvement on panoptic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Semantic Head</head><p>We further compare our semantic segmentation head to other design choice. In most proposed panoptic segmentation networks, they adopt feature maps in backbone and perform FCN <ref type="bibr" target="#b16">[17]</ref> to obtain pixel-wise predictions. The semantic head of compared model is constructed by apply 1 × 1 convolutional layer on the the fused feature maps with the Panoptic-FPN fusion, whose size is same as F 3 . Note that we only replace the semantic head in EPSNet without CLA fusion. We count the multiply-adds to evaluate the computation cost of different structures for semantic heads.</p><p>The experimental results in <ref type="table" target="#tab_1">Table 3</ref> show that the computation cost of the proposed semantic head is about 0.3 times less than the standard semantic head, although our semantic head is deeper. Unlike the standard semantic segmentation, because of the small input feature maps for computation, the proposed semantic head using mask coefficients does not slow down inference speed and outperforms the standard semantic head.</p><p>In semantic head, the coefficients k × N stuff in each position is able to be used to generate semantic segmentation before average pooling. To verify the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Ground Truth Normal Fusion CLA Fusion <ref type="figure">Fig. 7</ref>. Visualization results of cross-layer attention fusion on COCO panoptic val set. Normal fusion stands for the Panoptic-FPN <ref type="bibr" target="#b13">[14]</ref> fusion with F3,F4 and F5. impact of coefficients from different position, we use the coefficients before average pooling from corner positions and center position to perform the semantic segmentations. In <ref type="table">Table 4</ref>, the comparison shows that the result using coefficients from center position is superior than other positions. Moreover, we compare the different options on pooling operation. The coefficients produced by average pooling yield better performance than using max pooling, as shown in <ref type="table">Table 4</ref>.</p><p>To sum up, the proposed semantic head predicts mask coefficients of each class with faster inference speed and efficiently exploits shared prototypes without dragging down panoptic segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Cross-layer Attention Fusion Module</head><p>In this subsection, we investigate different strategies of using cross-layer attention (CLA) fusion. We compare our EPSNet to the model using other fusion method like Panoptic-FPN <ref type="bibr" target="#b13">[14]</ref> with F 3 , F 4 and F 5 and another model only employing F 3 as source feature map with CLA fusion module. As shown in <ref type="table" target="#tab_2">Table 5</ref>, the model with CLA fusion outperforms the Panoptic-FPN fusion espescially on PQ St . Also, more layers are adopted in CLA fusion can yield slight improvement and inference time.</p><p>The comparison of using cross-layer attention fusion can be visualized as <ref type="figure">Fig. 7</ref>. The details of background are much better and clearer. CLA fusion helps model generate higher quality segmentation especially for the stuff classes. For instance, the segments of the table in the first row and the ground in the second rows are much more complete. To further understand what has been learned in CLA fusion module, we select two query points in input image in the first and fourth columns and visualize their corresponding sub-attention maps on other source features (F 3 and F 4 ) in remaining columns. In <ref type="figure">Fig. 8</ref>, we observe that CLA fusion module can capture long-range dependencies according to the similarity. For example, in first row, the red point #1 on bus pays more attention on positions labeled as bus (second and third columns). For the point # 2 on ground, it highlights most areas labeled as ground(fifth and sixth columns).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with Other Methods on COCO</head><p>We compare our method on COCO val set with panoptic quality and inference speed measured from input image to panoptic segmentation output including post-processing time. Specifically, our model is only trained on COCO training dataset with ResNet-101-FPN and tested using single-scale 550 × 550 image. As shown in <ref type="table">Table 6</ref>, EPSNet outperforms every one-stage method and improves the performance over DeeperLab <ref type="bibr" target="#b25">[26]</ref> with Xception-71 <ref type="bibr" target="#b39">[40]</ref> backbone by 4.8% PQ. Also, our inference speed is much faster than all existing panoptic segmentation methods. EPSNet only takes 51ms for inference, which is 1.4× faster than DeeperLab with Light Wider MobileNet-V2 <ref type="bibr" target="#b40">[41]</ref> backbone and 3.1× faster than UPSNet <ref type="bibr" target="#b8">[9]</ref>. Compared to the two-stage methods, we bring better performance especially on PQ St , which outperforms Panoptic-FPN [14] by 1.8%, indicating that our approach provides better results on semantic segmentation. Despite the one-stage detector of EPSNet, with the fusion module and efficient architecture, we not only achieve competitive result for panoptic segmentation but also boost the inference speed.</p><p>In COCO test set, the inference setting is the same as COCO val set experiment. As shown in <ref type="table">Table 7</ref>, we outperform SSAP <ref type="bibr" target="#b26">[27]</ref>, which adopts horizontal flipping and multi-scale input images for testing , by 2% PQ. Without any additional tricks for inference, we still achieve competitive result compared to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we present a one-stage Efficient Panoptic Segmentation Network. The masks are efficiently constructed by linear combination of prototypes generated by protohead and mask coefficients produced by instance and semantic branches. The proposed cross-layer attention fusion module aggregates multiscale features in different layers with attention mechanism to enhance the quality of shared prototypes. The experiments show that our method achieves competitive performance on COCO panoptic dataset and outperforms other one-stage approaches. Also, EPSNet is significantly faster than the existing panoptic segmentation networks. In the future, We would like to explore a more effective way to replace the heuristic merging algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Speed-performance trade-off of panoptic segmentation methods on COCO. The inference time is measured end-to-end from input image to panoptic segmentation output. Our approach achieves 19 fps and 38.6% PQ on COCO val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of Efficient Panoptic Network. EPSNet predicts prototypes and mask coefficients for semantic and instance segmentation. Both segmentation, obtained by linear combination of prototypes and mask coefficients, are fused using heuristic merging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>The architecture of cross-layer attention module. The layers F3, F4 and F5 in FPN are used. F3 is considered as target feature, and all of them are set as source features. ⊕ denotes element-wise addition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>The architecture of cross attention block. ⊗ denotes matrix multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Ablation study on COCO panoptic val set with panoptic quality (PQ), semantic quality (SQ) and recognition quality (RQ). PQ Th and PQ St indicate PQ on thing and stuff classes. Data Aug denotes data augmentation. CLA Fusion stands for cross-layer attention fusion.Data Aug CLA Fusion Loss Balance PQ PQ Th PQ Ablation study on number of prototypes.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>St SQ RQ</cell></row><row><cell>√ √ √</cell><cell>√ √</cell><cell>√</cell><cell cols="2">35.4 40.5 27.7 77.2 43.5 37.4 43.2 28.6 77.6 45.7 38.4 43.0 31.4 77.7 47.6 38.6 43.5 31.3 77.9 47.3</cell></row><row><cell cols="5">Prototypes PQ PQ Th PQ St Inf time (ms)</cell></row><row><cell>16</cell><cell></cell><cell cols="2">38.4 43.4 30.8</cell><cell>50.7</cell></row><row><cell>32</cell><cell></cell><cell cols="2">38.6 43.5 31.3</cell><cell>51.1</cell></row><row><cell>64</cell><cell></cell><cell cols="2">38.4 43.2 31.2</cell><cell>52.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison on different design of semantic head.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Table 4. Performance comparison of</cell></row><row><cell></cell><cell></cell><cell>stan-</cell><cell cols="2">using different options on semantic co-</cell></row><row><cell cols="3">dard denotes EPSNet with other de-</cell><cell>efficients.</cell></row><row><cell cols="3">sign choice on semantic head, which di-rectly generates semantic segmentation</cell><cell>Method</cell><cell>PQ PQ St</cell></row><row><cell cols="3">with convolutional layers. Note that,</cell><cell>top-left</cell><cell>33.2 17.7</cell></row><row><cell cols="3">the EPSNet here does not use CLA fu-</cell><cell>top-right</cell><cell>33.4 18.1</cell></row><row><cell cols="3">sion. M-adds denotes multiply-adds.</cell><cell>bottom-left</cell><cell>33.5 18.3</cell></row><row><cell>Method</cell><cell cols="2">PQ PQ Th PQ St M-Adds (M)</cell><cell>bottom-right center</cell><cell>33.6 18.5 37.5 28.5</cell></row><row><cell cols="2">standard 37.2 42.9 28.5</cell><cell>33.8</cell><cell>max pooling</cell><cell>37.6 28.8</cell></row><row><cell cols="2">coefficients 37.4 43.2 28.6</cell><cell>9.4</cell><cell cols="2">average pooling 38.6 31.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison on different strategies for fusion module. The inference time is measured without considering post-processing.</figDesc><table><row><cell>Method</cell><cell>PQ PQ Th PQ St Inf time</cell></row><row><cell cols="2">Panoptic-FPN fusion [14] (F3, F4, F5) 37.4 43.2 28.6 23ms</cell></row><row><cell>CLA fusion (F3)</cell><cell>38.3 43.5 30.5 27ms</cell></row><row><cell>CLA fusion (F3, F4, F5)</cell><cell>38.6 43.5 31.3 29ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Visualization results of cross attention block on COCO panoptic val set. In each row, we show the input images with different marked points in 1 st and 4 th columns and two sub-attention maps on source features (F3 and F4) corresponding to the marked point in 2 nd , 3 th , 5 th and 6 th columns.</figDesc><table><row><cell>Image</cell><cell>Sub-attention</cell><cell>Sub-attention</cell><cell>Image</cell><cell>Sub-attention</cell><cell>Sub-attention</cell></row><row><cell>(point #1)</cell><cell>map (F3)</cell><cell>map (F4)</cell><cell>(point #2)</cell><cell>map (F3)</cell><cell>map (F4)</cell></row><row><cell>Fig. 8.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Panoptic segmentation results on COCO val set. LW-MNV2 denotes Light Wider MobileNet-V2. W-MNV2 means Wider MobileNet-V2. Panoptic segmentation results on COCO test-dev set. LW-MNV2 denotes Light Wider MobileNet-V2. W-MNV2 means Wider MobileNet-V2. Flip and MS stands for horizontal flipping and multi-scale inputs during testing.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Backbone</cell><cell cols="4">Input Size PQ PQ Th PQ St Inf time (ms)</cell></row><row><cell></cell><cell></cell><cell cols="2">Two Stage</cell><cell></cell><cell></cell></row><row><cell>JSIS [18]</cell><cell></cell><cell>ResNet-50</cell><cell cols="4">400 × 400 26.9 29.3 23.3</cell><cell>-</cell></row><row><cell>AUNet [13]</cell><cell cols="2">ResNet-50-FPN</cell><cell>-</cell><cell cols="3">39.6 49.1 25.2</cell><cell>-</cell></row><row><cell cols="3">Panoptic-FPN [14] ResNet-101-FPN</cell><cell>-</cell><cell cols="3">40.3 47.5 29.5</cell><cell>-</cell></row><row><cell>AdaptIS [10]</cell><cell cols="2">ResNeXt-101</cell><cell>-</cell><cell cols="3">42.3 49.2 31.8</cell><cell>-</cell></row><row><cell>UPSNet [9]</cell><cell cols="6">ResNet-50-FPN 800 × 1333 42.5 48.6 33.4</cell><cell>167</cell></row><row><cell></cell><cell></cell><cell cols="2">Single Stage</cell><cell></cell><cell></cell></row><row><cell>DeeperLab [26]</cell><cell cols="2">LW-MNV2</cell><cell cols="2">641 × 641 24.1</cell><cell>-</cell><cell>-</cell><cell>73</cell></row><row><cell>DeeperLab [26]</cell><cell></cell><cell>W-MNV2</cell><cell cols="2">641 × 641 27.9</cell><cell>-</cell><cell>-</cell><cell>83</cell></row><row><cell>DeeperLab [26]</cell><cell cols="2">Xception-71</cell><cell cols="2">641 × 641 33.8</cell><cell>-</cell><cell>-</cell><cell>119</cell></row><row><cell>SSAP [27]</cell><cell cols="2">ResNet-101</cell><cell cols="2">512 × 512 36.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="6">ResNet-101-FPN 550 × 550 38.6 43.5 31.3</cell><cell>51</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Backbone</cell><cell cols="3">Flip MS PQ PQ Th PQ St</cell></row><row><cell></cell><cell></cell><cell cols="2">Two Stage</cell><cell></cell><cell></cell></row><row><cell>JSIS [18]</cell><cell></cell><cell cols="2">ResNet-50</cell><cell></cell><cell cols="2">27.2 29.6 23.4</cell></row><row><cell cols="4">Panoptic-FPN [14] ResNet-101-FPN</cell><cell></cell><cell cols="2">40.9 48.3 29.7</cell></row><row><cell cols="2">AdaptIS [10]</cell><cell cols="2">ResNeXt-101</cell><cell></cell><cell cols="2">42.8 50.1 31.8</cell></row><row><cell>AUNet [13]</cell><cell></cell><cell cols="2">ResNeXt-101-FPN</cell><cell></cell><cell cols="2">46.5 55.8 32.5</cell></row><row><cell>UPSNet [9]</cell><cell></cell><cell cols="2">ResNet-101-FPN</cell><cell></cell><cell cols="2">46.6 53.2 36.7</cell></row><row><cell></cell><cell></cell><cell cols="2">Single Stage</cell><cell></cell><cell></cell></row><row><cell cols="2">DeeperLab [26]</cell><cell cols="2">Xception-71</cell><cell></cell><cell cols="2">34.3 37.5 29.6</cell></row><row><cell>SSAP [27]</cell><cell></cell><cell cols="2">ResNet-101</cell><cell></cell><cell cols="2">36.9 40.1 32.0</cell></row><row><cell>Ours</cell><cell></cell><cell cols="2">ResNet-101-FPN</cell><cell></cell><cell cols="2">38.9 44.1 31.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolo9000: Better, faster, stronger. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9396" to="9405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptis: Adaptive instance selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">SOGNet: Scene Overlap Graph Network for Panoptic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07527</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An end-to-end network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6165" to="6174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention-guided unified network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7019" to="7028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Panoptic Segmentation with a Joint Semantic and Instance Segmentation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Geus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02110</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01192</idno>
		<title level="m">Learning to Fuse Things and Stuff</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4396" to="4405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2614" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>GCPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2978" to="2991" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05093</idno>
		<title level="m">DeeperLab: Single-Shot Image Parser</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssap: Singleshot instance segmentation with affinity pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Seamless scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Colovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Self-Attention Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks. The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
