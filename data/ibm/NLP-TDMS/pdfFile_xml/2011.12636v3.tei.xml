<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Augmentation and Evaluation Schemes for Semantic Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Katiyar</surname></persName>
							<email>prateek.katiyar@de.bosch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
							<email>anna.khoreva@de.bosch.com</email>
							<affiliation key="aff1">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Augmentation and Evaluation Schemes for Semantic Image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite data augmentation being a de facto technique for boosting the performance of deep neural networks, little attention has been paid to developing augmentation strategies for generative adversarial networks (GANs). To this end, we introduce a novel augmentation scheme designed specifically for GAN-based semantic image synthesis models. We propose to randomly warp object shapes in the semantic label maps used as an input to the generator. The local shape discrepancies between the warped and nonwarped label maps and images enable the GAN to learn better the structural and geometric details of the scene and thus to improve the quality of generated images. While benchmarking the augmented GAN models against their vanilla counterparts, we discover that the quantification metrics reported in the previous semantic image synthesis studies are strongly biased towards specific semantic classes as they are derived via an external pre-trained segmentation network. We therefore propose to improve the established semantic image synthesis evaluation scheme by analyzing separately the performance of generated images on the biased and unbiased classes for the given segmentation network. Finally, we show strong quantitative and qualitative improvements obtained with our augmentation scheme, on both class splits, using state-of-the-art semantic image synthesis models across three different datasets. On average across COCO-Stuff, ADE20K and Cityscapes datasets, the augmented models outperform their vanilla counterparts by ∼3 mIoU and ∼10 FID points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The reliance of modern deep learning methods on significant amounts of training data has made data augmentation techniques ubiquitous for large-scale machine learning tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b68">69]</ref>. Besides being used to increase the size and diversity of the training set, data augmentation can also serve as an implicit regularizer and prevent the overfitting of models with high capacity <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3]</ref>. While in principle augmentation strategies can be applied to any class of models, their utility in improving the training of generative adversarial networks (GANs) for image synthesis has not been widely studied, and very little attention has been paid to developing novel augmentation strategies specific for GANs. For the task of semantic image synthesis (SIS), which aims to generate realistic images from the corresponding semantic label maps, recent work has primarily focused on the refinement of conditional GAN architectures <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">57,</ref><ref type="bibr" target="#b78">79]</ref>, adopting routine augmentation schemes that were originally designed for image classification <ref type="bibr" target="#b27">[28]</ref>, such as random cropping or flipping. Although these transformations incorporate geometric invariances about the data domain in the trained models, they are not specifically tuned for the SIS task, and thus do not assist in alleviating its common pitfalls.</p><p>Despite the recent successes of the SIS models <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">57,</ref><ref type="bibr">39]</ref>, one can still observe unsatisfactory artifacts in the synthesized images, mainly in the generation of object-level fine-grained structures. Since the input label maps do not provide any supervision about the structural content within the semantic segments, the generated images often lack class-relevant structural information and additionally contain undesirable distortions. <ref type="figure" target="#fig_0">Fig. 1</ref> shows two such cases, where a recently proposed SIS model <ref type="bibr" target="#b31">[32]</ref> adds significant distortions in the building and road segments of the synthesized images, and overall lacks relevant structural details. Inspired by the task-specific augmentation studies in other vision applications <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b60">61]</ref>, in this work, we propose an augmentation method specifically designed to overcome the above mentioned limitations of the SIS models.</p><p>Our proposed augmentation approach allows to greatly improve the quality of the synthetic images by enabling the generator to focus more on the local shapes and structural details (see <ref type="figure" target="#fig_0">Fig. 1c</ref>). We achieve this by randomly warping objects in the semantic label map fed to the SIS model as an input. The local shape discrepancies between the semantic input and the non-warped real image enable the generator to learn geometric properties of the scene better, which may otherwise be ignored since the generator has access to the ground truth scene layout that is perfectly aligned with the real image. Besides the perceptual details, the discriminator , and the bottom row shows the respective image segmentations obtained using a pre-trained segmentation model <ref type="bibr">[67]</ref> and the mIoU scores. Despite the artifact in the CC-FPSE image, the segmentation model is able to infer the building segment (in yellow), achieving the highest mIoU score. This also applies for the Real-perturbed case, where even after replacing the entire building segment information with its average pixel value, the segmentation model achieves an mIoU score similar to that for the real image. A non-cropped augmentation example is presented in panels (d)-(f).</p><p>also utilizes this misalignment between the real image and the warped label map to distinguish between the real and fake images, forcing the generator to correct distortions introduced to its input by learning the object-level shape details. To provide an even fine-grained guidance about the class-specific structural details missing from the label map, we add a structure preserving edge loss term to the generator objective. This ensures that the edges in the synthetic image are faithful to those in the corresponding real image.</p><p>We demonstrate the efficacy of our augmentation scheme by improving recently proposed SIS models on three different datasets, both quantitatively and qualitatively. Following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b41">42]</ref>, besides standard image synthesis metrics, we also use semantic segmentation metrics for the SIS evaluation, which were adopted with a two-fold reasoning. Firstly, a good SIS model should generate images with the layout that closely matches the ground truth semantic label map. Secondly, realistically-looking semantic classes in the generated image should be recognized well by an external semantic segmentation network trained on the real images from an independent dataset. However, we discover that the biases learned by the segmentation network during training leak into the quantification of the synthetic images, resulting in an overestimation of the SIS model's performance (see <ref type="figure" target="#fig_0">Fig. 1</ref>, bottom panel). We therefore propose to mitigate this issue by identifying the biased and unbiased classes in all datasets for the given segmentation networks and show the advantages of our proposed augmentation scheme using an extended evaluation on both class splits.</p><p>In summary, our main contributions are: (1) We propose a simple and novel data augmentation strategy designed specifically for GAN-based SIS models. Our augmentation scheme improves the quality of the generated images by encouraging the SIS model to focus more on the local image details and structures. <ref type="bibr" target="#b1">(2)</ref> We showcase a fundamental issue present in the evaluation of SIS models due to the biases learned by the pre-trained segmentation models and propose a fix by extending the SIS model evaluation on dataset-specific biased and unbiased class splits.</p><p>(3) We conduct extensive experiments using state-of-the-art SIS models on three different datasets to demonstrate the efficacy of our proposed augmentation scheme and perform ablation studies to carefully study the effect of augmentation on GAN-based SIS model components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic image synthesis. Conditional GANs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> can generate images via side information, such as class labels <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b4">5]</ref>, textual descriptions <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref>, images <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b21">22]</ref>, scene graphs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b59">60]</ref>, or semantic label maps <ref type="bibr" target="#b41">[42,</ref><ref type="bibr">57,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr">39]</ref> as in this work. Using the label maps as a guidance, SIS aims to create photo-realistic images. Pix2Pix <ref type="bibr" target="#b22">[23]</ref> first proposed to use GANs for this task, employing an encoder-decoder generator which takes semantic label map as input and a PatchGAN discriminator. Aiming at high-resolution image synthesis, Pix2PixHD <ref type="bibr" target="#b62">[63]</ref> improved upon <ref type="bibr" target="#b22">[23]</ref> by using multi-scale PatchGAN discriminators and introducing a coarse-to-fine generator architecture. SPADE <ref type="bibr" target="#b41">[42]</ref> proposed to fuse the semantic information in the label maps to the generator more effectively via a spatially-adaptive normalization layer. To better exploit the semantic maps and adaptively control the synthesis process, CC-FPSE <ref type="bibr" target="#b31">[32]</ref> further introduced a semantics-embedding discriminator and spatially-varying conditional convolution kernels in the generator. Most recently, LGGAN [57] focused on improving synthesis of small objects and local details, by designing a generator with separate branches that jointly learn the local class-specific and global image-level generation. Likewise, the authors in <ref type="bibr" target="#b55">[56]</ref> improve the synthesis of local structures via an attention-based edge guided generator network. Besides GANs, CRN <ref type="bibr" target="#b7">[8]</ref> and SIMS <ref type="bibr" target="#b42">[43]</ref> train a cascaded refinement convolutional network with a regression loss. This work proposes a novel data augmentation scheme for training GAN-based SIS models, which is orthogonal to the previous approaches and can be used for training the above SIS methods. Augmentation. Data augmentation is a widely used technique for generating additional data to train machine learning systems <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b58">59]</ref>. On natural images, the most common form of data augmentation is based on label preserving geometric and photometric transformations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b54">55]</ref>, e.g. when images are rotated, scaled, or colorjittered without altering their class labels. These transformations are known to suppress the overfitting effects and to improve generalization. Recently, label-perturbing data augmentation methods have received a lot of attention <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b16">17]</ref>, proposing to create augmented images by mixing samples from different classes and interpolating their labels accordingly. Though these approaches have shown to be effective, their potential shortcoming is that the model may learn a biased decision boundary, as the augmented samples are not drawn directly from an underlying distribution. Instead of hand-crafted techniques, several works have focused on finding automatic augmentation strategies <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b44">45]</ref> or generating samples via GANs <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Very little attention has been paid to data augmentation policies for GANs themselves, which also benefit from more training data and regularization. For image augmentations, the mainstream GAN models adopt only random cropping and horizontal flipping as their augmentation strategy. A few augmentation techniques have been proposed to improve the training stability of GANs, dealing with the issue of vanishing gradient by adding noise to the images <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b1">2]</ref>, blurring images [48], progressively appending the discriminator's input with random bits <ref type="bibr" target="#b70">[71]</ref> or using a wide range of standard augmentations techniques <ref type="bibr" target="#b24">[25]</ref> to prevent the discriminator from overfitting. Most recently, <ref type="bibr" target="#b75">[76]</ref> provided some guidelines of the effectiveness of various existing augmentation techniques for GAN training. However, the above works mostly focus on the unconditional and class-conditional image synthesis. In contrast, we propose a novel data augmentation strategy for GAN-based SIS models. Our augmentation scheme improves the quality of the generated samples by encouraging the SIS model to focus more on the local image details and structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Augmentation</head><p>In SIS, training dataset consists of a pair of data samples (s, x), where s denotes the input semantic label map 1 and x is the respective real image. In this task, the generator is trained to learn the distribution of real images conditioned on the semantic input. Thus, the loss functions for the generator G and the discriminator D take the following form:</p><formula xml:id="formula_0">L G min = −E s [log D(G(s), s)], L D max = E (x,s) [log D(x, s)] + E s [log (1 − D(G(s), s))].</formula><p>(1) To stabilize the training and improve the fidelity of synthetic images, recent SIS models have also added feature matching and perceptual losses <ref type="bibr" target="#b62">[63]</ref> to the generator:</p><formula xml:id="formula_1">L G min = − E s [log D(G(s), s)] + λ F M E (x,s) L F M (D(x, s), D(G(s), s)) + λ P E (x,s) L P (F (x), F (G(s))),<label>(2)</label></formula><p>where λ F M and λ P are the weights of the respective loss terms, and F is a pre-trained CNN.</p><p>While the modifications in the objective functions and model architectures <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">57]</ref> have led to steady improvements in the performance of SIS models, they do not explicitly guide the generator network to learn the local shape details of objects in the real image. In fact, as the generator architectures proposed in the state-of-the-art SIS models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b41">42]</ref> condition the features of all intermediate layers on the semantic input, the generator can simply copy the global structural layout of the scene directly from the conditioning input. This direct dependency may further weaken the generator's ability to learn finer structural details, because the semantic input itself lacks the information about the composition of various classes, e.g. windows/doors for the building class. Thus, to prevent the generator from naively copying the scene layout and encourage it to learn local shape properties of different classes, we propose to warp the objects in the semantic label map fed to the generator. The geometric mismatches between the semantic input and the corresponding real image force the generator to not only learn the perceptual content of different classes, but also to correct the shape distortions introduced to its input. More specifically, we obtain the warped semantic label maps using a transformer function:s = t(s). Here, the thin-plate spline transform t is obtained by estimating the affine and non-affine warping coefficients, which minimize a bending energy function for a set of fixed {u, v} and moving {ú,v} points <ref type="bibr" target="#b3">[4]</ref>. To selectively warp the objects in the input semantic label map s, we sample the key-points {u, v} uniformly from boundary pixels in the edge map (see <ref type="figure">Fig. 3</ref>). Afterwards, the moving points {ú,v} are obtained by adding random horizontal and vertical pixel shifts to the previously sampled key-points within a defined range. The amount of pixel shift controls the degree of warping. For each dataset, we determined this parameter experimentally by training multiple models with varying levels of distortions. <ref type="figure" target="#fig_1">Fig. 2</ref> shows examples of such warped label maps. We train the augmented models by warping the input label maps from the entire training dataset and conditioning the generator and the discriminator on the warped semantic layouts. The real images fed to the discriminator remain unmodified. During inference, the non-warped semantic label maps are used to generate the synthetic images.  <ref type="figure">Figure 3</ref>. A warping example comparing the effects of sampling key-points from the boundary pixels versus random selection. The first column shows an image of a plane (a) and its edge map extracted using the edge detection network (b). The second column shows key-points sampled from the boundary pixels in the edge map overlaid on the image (c) and the corresponding warped image obtained after applying the thin-plate spline transformer function t, estimated using the fixed and moving boundary key-points (d). The last column shows key-points randomly sampled from the image (e) and the respective warped image (f). It is evident from the panels (d) and (f) that compared to random selection, sampling key-points from the boundary pixels in the edge map allows to precisely distort the local shape details of the objects in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Edge Loss</head><p>Although warping semantic label maps enables the generator to focus on the local shape details (as described previously), it does not provide class specific fine-grained structural guidance necessary for high fidelity SIS. For example, it is evident from the building class segment (dark gray color) in the label map in <ref type="figure" target="#fig_0">Fig. 1</ref> that the generator can only infer the class identity and its spatial extent from the semantic input. While the feature matching and perceptual losses aid the generator in learning class specific content, they do not provide explicit structural clues about the fine-grained scene details. We therefore propose to add a structure preserving edge loss <ref type="bibr" target="#b25">[26]</ref> to the generator's output using the edge maps extracted by the edge detector E for the label map warping. We implement the edge loss L E as the L2 difference between the edge maps extracted from the synthetic and the real images: L E = E(x) − E(G(s)) 2 . Jointly, the augmentation and the edge loss complement each other, as they allow the generator to focus on different aspects of the target scene geometry. The final G and D losses of the augmented SIS model with the edge loss are:</p><formula xml:id="formula_2">L G min = − E s [log D(G(s),s)] + λ F M E (x,s) L F M (D(x,s), D(G(s),s)) + λ P E (x,s) L P (F (x), F (G(s))) + λ E E (x,s) L E (E(x), E(G(s))),<label>(3)</label></formula><formula xml:id="formula_3">L D max = E (x,s) [log D(x,s)] + E s [log (1 − D(G(s),s))].</formula><p>An illustrative overview of the complete augmentation model is presented in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Bias</head><p>The use of semantic segmentation metrics for evaluating the quality of synthetic images has become a common practice in SIS <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32]</ref>. Such evaluation is based on the hypothesis that realistically looking synthetic images can be classified correctly by a semantic segmentation network that was originally trained on real images <ref type="bibr" target="#b22">[23]</ref>. Thus, we first introduce the preliminaries associated with SIS evaluation. Thereafter, we present the image perturbations with which we determine the semantic classes that lead to a biased evaluation of SIS models for a given segmentation network.</p><p>Let X ∈ R H×W and Y ∈ {1, 2..., N cl } H×W denote an input image and its densely labelled semantic map with N cl categories. Here, H and W represent the image height and width. For simplicity we restrict the discussion below to single channel images, although it is fully generalizable to multi-channel images. Given an arbitrary image input X to a pre-trained segmentation network, let Y pred be the predicted segmentation map. Using Y and Y pred , we can calculate the following evaluation metrics for each class i: Pixel Accuracy (P A i ) = n ii /t i and, Intersection over Union (IoU i ) = n ii /(t i + j n ji -n ii ). Here, n ji and t i are the number of pixels of class j that are labelled as class i in Y pred and the total number of pixels of class i in Y <ref type="bibr" target="#b32">[33]</ref>.</p><p>To assess whether the segmentation model is biased towards specific semantic classes, we modify X by using different perturbation schemes and evaluate the metrics defined above by feeding the perturbed images into the segmentation network. As contextual information <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref> and statistics <ref type="bibr" target="#b63">[64]</ref> have been shown to strongly influence the predictions of a semantic segmentation model, we aim to utilize the segmentations of perturbed images to determine classes that can be correctly inferred by the segmentation network even when the information within the class segment is artificially altered.</p><formula xml:id="formula_4">For (u, v) ∈ ({1, 2, ..., H}, {1, 2, ..., W }), let M i (u, v) = 1{Y (u, v)</formula><p>= i} be the mask for class i, where 1 is the indicator function. We can define the perturbed im-ageX i for class i as:</p><formula xml:id="formula_5">X i (M i , X) = X • (1 − M i ) + P • M i .</formula><p>Here, P ∈ R H×W is the applied perturbation and • denotes the Hadamard product. Since SIS relies on pre-trained segmentation networks to evaluate the fake image pool, our choice of perturbations include approaches that replace the class segment information with the statistics derived from the segment itself. The following perturbations (constant, average, Gaussian blur and lognormal) are a basic approximation of a SIS model's ability to perform label-to-image translation:</p><formula xml:id="formula_6">P (u, v) =            c 0 1 u v M (u, v) u v M (u, v) * X(u, v) G(σ 0 ) X ∼ lognormal(µ, σ)<label>(4)</label></formula><p>where c 0 is a fixed grayscale value, σ 0 is the standard deviation parameter of the Gaussian kernel, and the mean µ and standard deviation σ are determined from the masked image segment. In our experiments, we determine σ 0 for each dataset by taking into account the class-wise segment area statistics across the entire validation set. The constant perturbation additionally covers classes that can be identified by a segmentation model solely based on the class silhouette and the neighboring context. Examples of all four perturbation schemes are presented in <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><p>By feeding the perturbed images into the segmentation model, we obtain the set M pi that contains the aforementioned metrics calculated for class i for all perturbation schemes. Given the score m i ∈ {P A i , IoU i } for the original image, the class i is considered as biased if the following criterion is met for any of the two respective perturbed metric sets:</p><formula xml:id="formula_7">i = biased if ∃ m pi m pi &gt; δ * m i , ∀ m pi ∈ M pi , unbiased otherwise.</formula><p>(5) Where the factor δ * m i determines the threshold for the perturbed metrics for class i to be regarded as biased. It is considered as biased because the segmentation network is able to identify the class segment inX i with sufficiently high accuracy for any of the four perturbation schemes. In practice, we group classes in the biased and unbiased splits using the cumulative P A i and IoU i metrics calculated for the entire validation set. We chose δ = 2/3 and additionally compute the following metrics to evaluate SIS models across both splits: Mean Accuracy (M A s ) = 1/N cls i P A i and Mean IoU (mIoU s ) = 1/N cls i IoU i , where N cls denotes the number of classes in the split s.   Baselines. As baselines, we choose three recently proposed GAN-based SIS models: Pix2PixHD <ref type="bibr" target="#b62">[63]</ref>, SPADE <ref type="bibr" target="#b41">[42]</ref> and CC-FPSE <ref type="bibr" target="#b31">[32]</ref>. We compare the qualitative and quantitative performances of the baselines with the respective augmented models, where we warp the semantic input and add the edge loss. We denote these models with -AUG suffix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Segmentation and edge detection models. For consistent quantitative evaluations with the published baselines, we use the following segmentation models: DeepLabV2 (COCO-Stuff) <ref type="bibr">[7,</ref><ref type="bibr" target="#b37">38]</ref>, UperNet101 (ADE20K) <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b9">10]</ref>, and DRN-D-105 (Cityscapes) <ref type="bibr" target="#b67">[68,</ref><ref type="bibr">67]</ref>. In addition to mIoU, PA and MA metrics defined in Sec. 4, we calculate Fŕechet Inception Distance (FID) <ref type="bibr" target="#b19">[20]</ref> to compare the distributions of real and fake image samples. We use a pre-trained Bi-Directional Cascade Network <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref> to estimate edges.</p><p>Implementation details. For a fair comparison, we use the original code provided by the authors <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40]</ref>. We train each model using 4 NVIDIA Tesla V100 GPUs. Except the batch sizes (provided in the supp. data), all other settings were kept to the defaults of the respective models. The resolution of the synthesized images are 256 × 256 for COCO-Stuff and ADE20K, and 512 × 256 for Cityscapes. We use λ E of 10 for the edge loss. The Cityscapes and ADE20K models are trained for 200 epochs, whereas the COCO-Stuff models are trained for 100 epochs. We retrain each baseline model from scratch for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Bias Results</head><p>Biased Classes. Following Sec. 4, we first identify the biased and unbiased classes in all datasets. To this end, we apply the pre-trained models on the perturbed validation set images created for each class in the respective dataset. We also separately test the models on the corresponding real validation set images. Based on the perturbed and unperturbed cumulative validation set metrics (P A i and IoU i ), we group the classes in the biased and unbiased categories. Note that we focus only on strongly biased classes. For a δ value of 2/3 in Eq. 5, we find 29, 52 and 5 biased classes in COCO-Stuff, ADE20K and Cityscapes (provided in the supp. data). Smaller values of δ dilute the biased class split. <ref type="figure" target="#fig_4">Fig. 4</ref> shows examples of original and perturbed image segmentations for all four perturbation schemes defined in Eq. 4. For the constant perturbation image, the segmentation network is able to identify the bird class almost entirely without leveraging any class-specific texture. Likewise, the average perturbation example shows that the average color and (quite likely) the positioning of the grass segment are sufficient for the segmentation model to achieve a nearperfect classification. In the Gaussian blur perturbation example, the network correctly identifies the mouse in front of the keyboard, while misses the one behind. This example shows the effect of context bias picked up by the segmentation model, as in majority of training images the mouse is placed either to the front or alongside the keyboard. The final lognormal perturbation example shows bidirectional effects of the bias learned by the segmentation network during training. Here, the perturbed wall segment is classified accurately, whereas the unaltered shower-door segment is misclassified as mirror. These examples highlight that for the biased classes, even unrealistic SIS model images may lead to high evaluation metrics. We investigate this in the next section by performing evaluations on the synthetic images generated using all three baselines. Analysis on Baselines. In <ref type="table">Table 1</ref> we report the results of evaluating the synthetic images of all baselines and the real images across all datasets. To our surprise-when considering all classes-the segmentation networks perform better  on the synthetic images than on the real images for COCO-Stuff and ADE-20K. The reason for better segmentation performance on the synthetic images becomes clear, as we bifurcate the metrics into the biased and unbiased classes. While overall the segmentation networks perform better on the biased classes than the unbiased ones, this performance gap is significantly higher for the synthetic images generated using the SIS baselines compared to the real images, especially for COCO-Stuff and ADE-20K. We also notice large differences between Pix2PixHD and the other baselines, indicating that newer architectures improve upon the synthesis of the unbiased classes. Having verified the adverse effects of the segmentation network's bias in the evaluation of SIS models, in the next section we extend the analysis of the augmentation results to both class splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Augmentation Results</head><p>Quantitative results. We report quantitative results of the baselines and the respective augmented models in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Here we show only FID and mIoU metrics. The full table including PA and MA metrics is provided in the supp. data. Even without optimizing the weight for the edge loss, we see that the augmented models outperform the corresponding baselines on each dataset. As the augmented generators focus more on the local shape and structural details, we particularly see gains on the unbiased class metrics. In some cases, the overall gain is only contributed by the improvements on the unbiased classes, e.g. CC-FPSE mIoU on COCO-Stuff and SPADE mIoU on Cityscapes. In fact, for each model pair and dataset combination in <ref type="table" target="#tab_2">Table 2</ref>, the augmented models show consistent gains over the baselines on the overall and unbiased mIoU (this also applies to MA metric) indicating that the general fine-grained improvements of the augmentation extend across a variety of GAN-based SIS architectures as well as datasets containing diverse scenes and objects. Adding only the edge loss to the baselines improves the model performance, but not as much as the full augmentation scheme (see the supp. data). The CC-FPSE and SPADE baselines trained on COCO-Stuff and ADE20K achieve lower FID scores than the respective augmented models. However, for both cases the augmented models perform better on almost all other metrics. We notice that the proposed class splits provide an additional level of granularity when benchmarking SIS models, as they al-  <ref type="figure" target="#fig_5">Fig. 5</ref>. Here, we illustrate that the proposed augmentation scheme reduces distortions, adds fine-grained structural details and enhances the perceptual realism of the synthetic images. The first 2 rows exemplify the distortions commonly present in the synthetic images of both baselines by the red boxes (Cityscapes examples shown in <ref type="figure" target="#fig_0">Fig. 1</ref>). We found that the proposed augmentation significantly reduce such irregularities. Moreover, the images in the second row show how the augmented models gradually add local structural details to the synthetic images and produce a high fidelity image. The final row shows that compared to the baselines, the augmented models also improve the overall perceptual realism of the translated images. Additional examples (including the results from the edge loss baselines) supporting these findings are provided in the supp. data.</p><p>Ablations. The results of ablations on the CC-FPSE model on Cityscapes are provided in <ref type="table">Table 3</ref>. For other SIS models and datasets, the reader is referred to the supp. data. We first compare our proposed augmentation technique with the CutMix augmentation <ref type="bibr" target="#b69">[70]</ref>, which has recently been used to regularize unconditional GAN training <ref type="bibr" target="#b49">[50]</ref>. While the CutMix variant improves upon the baseline model on mIoU metrics, it remains inferior to the proposed (task-specific) augmentation approach. Next, to understand which component of the SIS model benefits most with the proposed augmentation, we selectively augment only the generator (Gonly) or discriminator (Donly) input label maps. We observe that while augmenting both networks results in the best performance, augmenting the generator's input is most  <ref type="table">Table 3</ref>. Ablations on Cityscapes. BC and U C denote the biased and unbiased class splits. Bold indicates the best model. For Donly and Gonly models, the augmentation was applied only to the discriminator and the generator input, respectively. Augmented model without the edge loss is denoted as AUG-woEL.</p><p>critical for a GAN-based SIS model. Finally, to confirm the utility of both the label map warping and the edge loss in our proposed approach, we train one additional augmented model on each dataset in which we remove the edge loss (-AUG-woEL). This ablation confirms complementary benefits of the full scheme (-AUG), as adding the edge loss on top of the warping leads on average to a better performance (decreasing FID and increasing mIoU U C ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we propose a novel data augmentation method for GAN-based SIS models. Targeting the shortcomings of the recent SIS studies, the proposed augmentation scheme allows to significantly improve the overall quality of the synthetic images, in particular the local shapes and structures inside the semantic classes. In addition, to better analyze and benchmark the improvements of the SIS models, we extend the semantic segmentation metrics used for the SIS evaluation. Specifically, to mitigate the adverse effects of biases picked up by the segmentation network, we split the evaluation of semantic classes into biased and unbiased groups. Enabled by this new analysis, we observe that the conventional SIS models strongly underperform on unbiased classes, while our proposed augmentation method improves their results on both class groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Details Network Architectures</head><p>As we use the implementations provided by the authors for all experiments conducted in this study, we refer the reader to the original source code and articles for complete details of the generator and discriminator network architectures <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32]</ref>. In our experiments, we found that Pix2PixHD adds significant distortions to the generated images in the form of vertical and horizontal stripes. We also observe significant (unsatisfactory) variations in the performance of Pix2PixHD model and its variants. We tried improving Pix2PixHD baseline by adding spectral normalization <ref type="bibr" target="#b35">[36]</ref> to both generator and discriminator networks. However, this modification only helped in removing the artifacts from the Cityscapes baseline. Therefore, for all Cityscapes experiments in this study we use the spectral norm variant of the original Pix2PixHD model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>In addition to the baseline and augmented models, we also train strong baselines by adding the edge loss to the vanilla baseline models. We denote the strong baselines with -EL suffix. The learning objective of Pix2PixHD for the adversarial loss is least squares as proposed in <ref type="bibr" target="#b33">[34]</ref>. Whereas, SPADE and CC-FPSE use the Hinge loss formulation <ref type="bibr" target="#b29">[30]</ref>. Pix2PixHD and SPADE use a weight of 10 for both feature matching and perceptual losses. CC-FPSE uses a weight of 20 for the feature matching loss and a weight of 10 for the perceptual loss. We do not train SPADE and CC-FPSE models with the image encoder option. All models use ADAM <ref type="bibr" target="#b26">[27]</ref> optimizer. The batch sizes used in all experiments are provided in Supplementary Table 1. SPADE and CC-FPSE use a learning rate of 0.0001 and 0.0004 for the generator and discriminator, respectively. Pix2PixHD uses a common learning rate of 0.0002 for both networks. All models linearly decay the learning rate to zero from epoch 100 to 200 for Cityscapes and ADE20K experiments. To determine the appropriate warping regime, we train multiple models with varying levels of maximum pixel shifts. For each model, we sample the displacement values between the fixed and moving points from a uniform distribution U as following: ∼ U(−a, a). Where a denotes the maximum pixel shift. We found that a maximum pixel shift of 4 units works well for almost all the models. The average statistic of a across the best augmented models is 4.11 ± 1.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dataset COCO-Stuff ADE20K Cityscapes <ref type="table" target="#tab_2">CC-FPSE  16  16  8  SPADE  48  48  32  Pix2PixHD  48  48  16   Table 1</ref>. Batch sizes used for all experiments detailed in the main text and the supplementary data. Depending on the model and the dataset, the batch size was selected as the largest input that could fit in the GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative Results</head><p>In <ref type="table" target="#tab_2">Supplementary Table 2</ref>, we provide the full quantitative evaluations of the baselines, the strong baselines (baselines with the edge loss) and the respective augmented models. Overall, we notice that the edge loss and the augmented variants provide incremental improvements over the baseline models. On the unbiased classes, with the exception of SPADE-EL model's PA U C metric on COCO-Stuff (66.3), the augmented models perform better than their edge loss counterparts across all datasets and metrics. In fact, for this exception too, MA U C metric of the augmented model (47.7) is higher than that of the edge loss model (47.1), suggesting more general benefits of the full augmentation scheme across all COCO-Stuff classes. The CC-FPSE-EL model achieves lower FID score than the respective augmented model on COCO-Stuff, however, the latter model performs better on almost all other metrics. We observe that the newer architectures benefit more with the addition of the edge loss. For example, CC-FPSE-EL model overall performs better than the respective baseline across all datasets, however, for SPADE we only see clear improvements on ADE20K and mixed results on Cityscapes.</p><p>Whereas Pix2PixHD-EL model shows strong gains over the baseline model on COCO-Stuff, the opposite holds true for ADE20K. On Cityscapes, the results are mixed similar to SPADE. The augmented models, however, show consistent increments in evaluation metrics across all models and datasets.</p><p>The full results of CutMix and selective augmentation (-Donly and -Gonly) ablations are provided in <ref type="table">Supplementary  Table 3</ref>. The CutMix augmentation <ref type="bibr" target="#b69">[70]</ref> has recently been shown to be effective for image classification. The authors in <ref type="bibr" target="#b49">[50]</ref> further employ CutMix to regularize GAN training. We augment the baseline SIS models using CutMix by following the approach presented in <ref type="bibr" target="#b49">[50]</ref>. More specifically, to prevent the discriminator from learning a biased decision boundary, we set the class label for the CutMix image as fake. The results in <ref type="table">Supplementary Table 3 clearly</ref> show that for each model, our task-specific augmentation approach (-AUG) outperforms CutMix across all datasets. While in some cases, the -CutMix model variants achieve lower FID scores than the -AUG counterparts (CC-FPSE-CutMix on COCO-Stuff and SPADE-CutMix on ADE20K), they perform poorly on all other metrics.</p><p>The purpose of selective augmentation was to individually study the effects of the proposed augmentation approach on the generator and discriminator networks. Therefore, in this experiment, we selectively augment only the generator (Gonly) or discriminator (Donly) input label maps. For Donly ablation, we additionally remove the edge loss term from the generator objective to account for increments/decrements in the performance arising from augmenting the discriminator network only. In the case of Donly augmentation, both fake and real images are misaligned from the input label map fed to the discriminator due to the applied warping. Whereas, in the case of Gonly augmentation, the fake image fed to the discriminator is misaligned from the real image and the respective label map input, as long as the generator is unable to fully correct the distortions introduced to its input. From the results presented in <ref type="table">Supplementary Table 3</ref>, we observe that overall augmenting both networks leads to the best performance. However, we notice that on an average augmenting the generator is more effective than augmenting the discriminator. These findings imply that in the case of Gonly augmentation, the misalignment between the fake and real images as well the label map input fed to the discriminator allow the discriminator to better distinguish between the real and fake samples, thus forcing the generator to learn fine grained structural details in order to correct the distortions introduced to its input.</p><p>Supplementary <ref type="table">Table 4</ref> shows the results for the augmented models trained without the edge loss. We train one such ablation model for each dataset, whichever is fastest to train depending on the dataset size. The main objective of this ablation experiment is to assess the contributions of both-the edge loss and warping-components in the full augmentation scheme. While the CC-FPSE augmented model without the edge loss yields semantic segmentation scores comparable to those of the corresponding augmented model, its FID increases by 1.3 points. The SPADE and Pix2PixHD augmented models without the edge loss perform worse than their original augmented variants on all metrics (except for SPADE-AUG-woEL FID), suggesting the complementary benefits of both components in the full augmentation scheme. Since the edge loss provides additional structural guidance to the generator for the details that are missing from the semantic segments in the input label maps, its combination with the proposed warping scheme provides complementary advantages to the augmented SIS models.</p><p>Supplementary <ref type="table">Table 5</ref> presents the evaluation for the augmented models with the matching aware loss. The intuition behind adding the matching aware loss is to utilize the unwarped semantics by letting the discriminator of the augmented SIS model classify the real image and warped semantics pair as fake and the respective pair with the unwarped semantics as real. If the discriminator is able to successfully distinguish between these two pairs, then it should be able to provide a highly fine-grained geometric guidance to the generator. Such an auxiliary pair has been shown to improve the performance of conditional GANs previously <ref type="bibr" target="#b45">[46]</ref>. We train the augmented variants of all three baselines with the matching aware loss on ADE20K dataset. For all models, we do not see any improvements in adding the matching aware loss on top of the proposed augmentation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>In <ref type="figure" target="#fig_0">Supplementary Figures 1-9</ref>, we show several image examples comparing the baseline and its edge loss and augmented variants for all three SIS models in a dataset-wise manner. In each figure, we also highlight image areas with distortions using red boxes. Consistent with the quanti-tative evaluation, the augmented models greatly improve the fidelity of the generated images. We notice that the augmentation substantially minimizes the distortions commonly present in the images synthesized using the vanilla baseline models. Furthermore, as the edge loss and warping provide additional structural guidance to the generator, we observe clear fine-grained improvements within the semantic classes in the generated images of the augmented models compared to those of the baselines. Whereas the edge loss models improve over the baselines on both the aforementioned factors to some extent, these improvements are not consistent across all models and datasets, as is the case for the augmented models. Lastly, we also notice that the images generated by the augmented models are perceptually more consistent than the samples generated using the respective baselines and the edge loss models. We hypothesize that these perceptual improvements are a result of the better overlap between the supports of the real and fake image distributions. Our interpretation is supported by previous evidence <ref type="bibr" target="#b53">[54]</ref>, where the authors increase the overlap between the supports of the two distributions by means of simple augmentation approaches, such as adding Gaussian noise to both real and fake samples.  <ref type="table">Table 4</ref>. Augmented models without the edge loss (AUG-woEL). The CC-FPSE, SPADE and Pix2PixHD models are trained on Cityscapes, ADE20k and COCO-Stuff, respectively. BC and U C denote the biased and unbiased class splits. For the ease of comparison, we repeat the metrics from the respective augmented models (-AUG). The augmented models refer to the baseline models trained with the full augmentation scheme proposed in this work (warping + edge loss).  <ref type="table">Table 5</ref>. Scores obtained using augmented models with the matching aware GAN loss (AUG-MAL) on ADE20K dataset. BC and U C denote the biased and unbiased class splits. For all baselines, this extension did not improve the performance of the augmented models. For the ease of comparison, we repeat the metrics from the respective augmented models (-AUG). The augmented models refer to the baseline models trained with the full augmentation scheme proposed in this work (warping + edge loss).</p><formula xml:id="formula_8">Method FID ↓ mIoU ↑ mIoU BC mIoU U C PA ↑ PA BC PA U C MA ↑ MA BC MA U C COCO-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label map</head><p>Real image CC-FPSE CC-FPSE-EL CC-FPSE-AUG </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pix2PixHD</head><p>Pix2PixHD-EL Pix2PixHD-AUG <ref type="figure">Figure 9</ref>. Cityscapes results obtained using Pix2PixHD baselines and the augmented model. For clarity, we omit the label maps and the real images. Best viewed in color.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples from Cityscapes [9] illustrating the image quality improvements obtained using our proposed augmentation scheme and the adverse effects of segmentation model's bias in the evaluation of SIS models. For clarity, the images in the top and bottom panels have been cropped to size 256 × 256. Top panel: label map input (a) and the associated synthetic images generated using the baseline (b) and augmented (c) CC-FPSE [32] models. The proposed augmentation scheme fixes the artifact (shown with red box) introduced by the baseline model and improves the overall structural and perceptual details of the synthesized image. Bottom panel: the top row shows real, synthesized (baseline and augmented) and perturbed images for the label map shown in (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Augmented SIS pipeline. Left panel: in contrast to a vanilla model, the input to the generator G in the augmented model is a label map warped based on the edges, estimated by the edge detector E. Not shown is the real image and the warped label map fed into the discriminator D. Right panel: examples of original and the respective warped label maps. Zoom in the boxes for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Image perturbation examples exhibiting the bias present in pre-trained segmentation models. The perturbed class segments are highlighted with yellow contours. For each perturbed image (ptb-image), the segmentation model is able to correctly infer the semantic category of the affected segment (as in image-seg), despite the corruption of perceptual and structural details. More details in Sec. 5.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>ADE20K results using SPADE and CC-FPSE baselines, and the augmented models. The red boxes highlight image artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 .Figure 2 .Figure 3 .Figure 4 .Figure 5 .Figure 6 .</head><label>123456</label><figDesc>COCO-Stuff results obtained using CC-FPSE baselines and the augmented model. Best viewed in color. The red boxes highlight image artifacts. ADE20K results obtained using CC-FPSE baselines and the augmented model. Best viewed in color. The red boxes highlight image artifacts. Cityscapes results obtained using CC-FPSE baselines and the augmented model. For clarity, we omit the label maps and the real images. Best viewed in color. The red boxes highlight image artifacts. COCO-Stuff results obtained using SPADE baselines and the augmented model. Best viewed in color. The red boxes highlight image artifacts. ADE20K results obtained using SPADE baselines and the augmented model. Best viewed in color. The red boxes highlight image artifacts. Cityscapes results obtained using SPADE baselines and the augmented model. For clarity, we omit the label maps and the real images. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>BC mIoU U C FID ↓mIoU ↑mIoU BC mIoU U C FID ↓ mIoU ↑ mIoU BC mIoU U C Evaluation comparison across datasets. BC and U C denote the biased and unbiased class splits. Bold indicates the best model between the baseline and its augmented variant (-AUG).</figDesc><table><row><cell cols="3">COCO-Stuff FID ↓mIoU ↑mIoU CC-FPSE Model 18.9 41.0 47.3</cell><cell>39.7</cell><cell cols="2">33.2 42.6</cell><cell>ADE20K 44.5</cell><cell>41.6</cell><cell cols="3">Cityscapes 53.6 61.8 79.3</cell><cell>55.5</cell></row><row><cell>CC-FPSE-AUG</cell><cell>19.1 42.1</cell><cell>46.3</cell><cell>41.2</cell><cell cols="2">32.6 44.0</cell><cell>45.8</cell><cell>43.1</cell><cell cols="2">52.1 63.1</cell><cell>80.1</cell><cell>57.0</cell></row><row><cell>SPADE</cell><cell>22.5 37.8</cell><cell>43.5</cell><cell>36.7</cell><cell cols="2">34.4 39.6</cell><cell>41.7</cell><cell>38.6</cell><cell cols="2">64.7 59.2</cell><cell>79.9</cell><cell>51.9</cell></row><row><cell>SPADE-AUG</cell><cell>22.7 38.2</cell><cell>43.5</cell><cell>37.1</cell><cell cols="2">34.6 41.2</cell><cell>43.2</cell><cell>40.2</cell><cell cols="2">62.3 60.4</cell><cell>79.8</cell><cell>53.5</cell></row><row><cell>Pix2PixHD</cell><cell>128.7 12.0</cell><cell>21.2</cell><cell>10.1</cell><cell cols="2">59.1 24.4</cell><cell>27.9</cell><cell>22.6</cell><cell cols="2">73.6 56.7</cell><cell>78.8</cell><cell>48.8</cell></row><row><cell cols="2">Pix2PixHD-AUG 54.2 21.9</cell><cell>31.8</cell><cell>19.9</cell><cell cols="2">41.5 32.5</cell><cell>36.0</cell><cell>30.7</cell><cell cols="2">72.7 58.0</cell><cell>79.1</cell><cell>50.5</cell></row><row><cell>Average ∆</cell><cell>24.7 3.8</cell><cell>3.2</cell><cell>3.9</cell><cell>6</cell><cell>3.7</cell><cell>3.6</cell><cell>3.7</cell><cell>1.6</cell><cell>1.3</cell><cell>0.3</cell><cell>1.6</cell></row><row><cell cols="5">low to identify cases of pseudo-improvements where the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">overall boost in performance is caused mainly due to a gain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">in the biased class metric. For instance on Cityscapes, PA of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">SPADE (93.1) is higher than that of CC-FPSE (92.8) 2 . But,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">this improvement is only caused by the biased classes. On</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">the unbiased classes, CC-FPSE (84.5) outperforms SPADE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(82.3) by more than 2 points.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Qualitative results. The qualitative examples comparing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">SPADE and CC-FPSE baselines, and the augmented mod-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>els are shown in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Evaluation comparison across datasets. BC and U C denote the biased and unbiased class splits. Bold indicates the best model among the baseline and its edge loss (-EL) and augmented (-AUG) variants. For the ease of comparison, we repeat the metrics from the baselines and the augmented models from the main text. The augmented models refer to the baseline models trained with the full augmentation scheme proposed in this work (warping + edge loss).MethodFID ↓ mIoU ↑ mIoU BC mIoU U C PA ↑ PA BC PA U C MA ↑ MA BC MA U C Cityscapes Method FID ↓ mIoU ↑ mIoU BC mIoU U C PA ↑ PA BC PA U C MA ↑ MA BC MA U C</figDesc><table><row><cell></cell><cell>CC-FPSE</cell><cell>18.9</cell><cell>41.0</cell><cell>47.3</cell><cell>39.7</cell><cell>71.0 77.3 68.4</cell><cell>52.4 61.5</cell><cell>50.5</cell></row><row><cell></cell><cell>CC-FPSE-EL</cell><cell>18.6</cell><cell>41.8</cell><cell>46.4</cell><cell>40.9</cell><cell>71.1 76.7 68.7</cell><cell>52.8 61.2</cell><cell>51.1</cell></row><row><cell>Stuff</cell><cell>CC-FPSE-AUG SPADE SPADE-EL SPADE-AUG</cell><cell>19.1 22.5 22.8 22.7</cell><cell>42.1 37.8 37.8 38.2</cell><cell>46.3 43.5 43.4 43.5</cell><cell>41.2 36.7 36.7 37.1</cell><cell>71.5 78.1 68.8 68.6 74.5 66.2 68.3 73.1 66.3 68.8 75.3 66.1</cell><cell>53.1 61.8 48.8 56.9 48.9 57.6 49.4 57.4</cell><cell>51.3 47.1 47.1 47.7</cell></row><row><cell></cell><cell>Pix2PixHD</cell><cell>128.7</cell><cell>12.0</cell><cell>21.2</cell><cell>10.1</cell><cell>36.9 47.6 32.5</cell><cell>18.0 28.7</cell><cell>15.8</cell></row><row><cell></cell><cell>Pix2PixHD-EL</cell><cell>91.2</cell><cell>17.9</cell><cell>27.3</cell><cell>15.9</cell><cell>43.9 55.7 39.0</cell><cell>25.5 37.7</cell><cell>23.0</cell></row><row><cell></cell><cell cols="2">Pix2PixHD-AUG 54.2</cell><cell>21.9</cell><cell>31.8</cell><cell>19.9</cell><cell>54.1 69.2 47.8</cell><cell>29.5 46.8</cell><cell>26.0</cell></row><row><cell></cell><cell>CC-FPSE</cell><cell>33.2</cell><cell>42.6</cell><cell>44.5</cell><cell>41.6</cell><cell>82.2 87.6 72.0</cell><cell>49.8 52.7</cell><cell>48.3</cell></row><row><cell></cell><cell>CC-FPSE-EL</cell><cell>34.2</cell><cell>43.1</cell><cell>44.7</cell><cell>42.2</cell><cell>81.7 87.5 70.8</cell><cell>50.4 53.4</cell><cell>48.8</cell></row><row><cell>ADE20K</cell><cell>CC-FPSE-AUG SPADE SPADE-EL SPADE-AUG</cell><cell>32.6 34.4 34.9 34.6</cell><cell>44.0 39.6 40.8 41.2</cell><cell>45.8 41.7 42.2 43.2</cell><cell>43.1 38.6 40.1 40.2</cell><cell>83.0 88.2 73.5 81.0 85.8 72.1 81.3 86.2 72.2 81.3 86.0 72.5</cell><cell>51.3 54.5 46.6 49.1 47.7 50.0 48.0 50.8</cell><cell>49.6 45.3 46.5 46.5</cell></row><row><cell></cell><cell>Pix2PixHD</cell><cell>59.1</cell><cell>24.4</cell><cell>27.9</cell><cell>22.6</cell><cell>71.8 80.5 55.3</cell><cell>29.5 34.7</cell><cell>26.8</cell></row><row><cell></cell><cell>Pix2PixHD-EL</cell><cell>65.2</cell><cell>20.4</cell><cell>24.1</cell><cell>18.4</cell><cell>69.1 77.8 52.9</cell><cell>25.0 30.8</cell><cell>22.0</cell></row><row><cell></cell><cell cols="2">Pix2PixHD-AUG 41.5</cell><cell>32.5</cell><cell>36.0</cell><cell>30.7</cell><cell>77.9 84.4 65.9</cell><cell>38.2 42.7</cell><cell>35.8</cell></row><row><cell></cell><cell>CC-FPSE</cell><cell>53.6</cell><cell>61.8</cell><cell>79.3</cell><cell>55.5</cell><cell>92.8 94.6 84.5</cell><cell>71.6 85.6</cell><cell>66.6</cell></row><row><cell></cell><cell>CC-FPSE-EL</cell><cell>54.7</cell><cell>63.0</cell><cell>80.1</cell><cell>56.9</cell><cell>93.4 95.4 84.5</cell><cell>72.0 85.9</cell><cell>67.0</cell></row><row><cell>Cityscapes</cell><cell>CC-FPSE-AUG SPADE SPADE-EL SPADE-AUG</cell><cell>52.1 64.7 62.8 62.3</cell><cell>63.1 59.2 59.4 60.4</cell><cell>80.1 79.9 79.8 79.8</cell><cell>57.0 51.9 52.1 53.5</cell><cell>93.5 95.4 85.2 93.1 95.6 82.3 93.2 95.5 82.8 93.2 95.5 83.0</cell><cell>72.4 85.6 68.3 85.5 68.2 85.4 69.7 85.2</cell><cell>67.7 62.1 62.0 64.2</cell></row><row><cell></cell><cell>Pix2PixHD</cell><cell>73.6</cell><cell>56.7</cell><cell>78.8</cell><cell>48.8</cell><cell>92.7 95.3 80.7</cell><cell>65.4 84.2</cell><cell>58.7</cell></row><row><cell></cell><cell>Pix2PixHD-EL</cell><cell>74.8</cell><cell>56.6</cell><cell>78.6</cell><cell>48.8</cell><cell>92.4 95.2 80.4</cell><cell>65.9 84.2</cell><cell>59.4</cell></row><row><cell></cell><cell cols="2">Pix2PixHD-AUG 72.7</cell><cell>58.0</cell><cell>79.1</cell><cell>50.5</cell><cell>92.7 95.3 81.2</cell><cell>66.9 84.6</cell><cell>60.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>FID ↓ mIoU ↑ mIoU BC mIoU U C PA ↑ PA BC PA U C MA ↑ MA BC MA U C</figDesc><table><row><cell cols="2">CC-FPSE-AUG-MAL 35.4</cell><cell>42.7</cell><cell>44.8</cell><cell>41.6</cell><cell>82.3 87.4 72.6</cell><cell>50.2 53.4 48.5</cell></row><row><cell>CC-FPSE-AUG</cell><cell>32.6</cell><cell>44.0</cell><cell>45.8</cell><cell>43.1</cell><cell>83.0 88.2 73.5</cell><cell>51.3 54.5 49.6</cell></row><row><cell>SPADE-AUG-MAL</cell><cell>35.6</cell><cell>39.5</cell><cell>41.4</cell><cell>38.5</cell><cell>80.9 85.7 71.9</cell><cell>46.2 48.7 44.9</cell></row><row><cell>SPADE-AUG</cell><cell>34.6</cell><cell>41.2</cell><cell>43.2</cell><cell>40.2</cell><cell>81.3 86.0 72.5</cell><cell>48.0 50.8 46.5</cell></row><row><cell cols="2">Pix2PixHD-AUG-MAL 69.4</cell><cell>23.7</cell><cell>26.7</cell><cell>22.1</cell><cell>72.1 80.8 56.0</cell><cell>28.8 33.5 26.2</cell></row><row><cell>Pix2PixHD-AUG</cell><cell>40.7</cell><cell>33.1</cell><cell>36.3</cell><cell>31.5</cell><cell>77.5 84.0 65.4</cell><cell>39.2 43.2 37.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For certain datasets, SIS models also concatenate instance maps with the label maps.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Following DRN-D-105[67], unlike<ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b31">32]</ref>, we ignore the unlabelled class in the Cityscapes evaluation.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This supplementary material is structured as follows:</p><p>A: Additional information on biased semantic classes across datasets.</p><p>B: A summary of the network architectures and the training details.</p><p>Supplementary <ref type="table">Table 1</ref>: Batch sizes used in all experiments.</p><p>C: Additional quantitative results.</p><p>Supplementary <ref type="table">Table 2</ref>: Full evaluation comparison of the baselines with the edge loss and the augmented variants across datasets.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Biased Classes</head><p>In this section, we detail the list of classes that are identified as biased using the perturbation approach presented in Sec. 4. For a δ value of 2/3, we identify 29, 52 and 5 biased classes in COCO-Stuff, ADE20K and Cityscapes, respectively. The Gaussian perturbation parameter σ 0 for COCO-Stuff, ADE20K and Cityscapes was set to 25.0, 35.0 and 27.0, respectively. The parameter σ 0 is a function of the kernel size K (σ 0 = 1/3 * K), which was detemined by calculating the average class area statistics for all semantic categories in the dataset. The biased class names and their associated ids are provided below in a dictionary format. : " g r a s s " , 1 2 6 : " ground − o t h e r " , 1 3 5 : " m o u n t a i n " , 1 4 2 : " p l a n t − o t h e r " , 1 4 4 : " p l a t f o r m " , 1 4 5 : " p l a y i n g f i e l d " , 1 4 9 : " r o a d " , 1 5 4 : " s a n d " , 1 5 7 : " sky − o t h e r " , 1 5 9 : " snow " , 1 6 7 : " t e x t i l e − o t h e r " , 1 7 2 : " w a l l − c o n c r e t e " , 1 7 3 : " w a l l − o t h e r " ADE20K 1 :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO-Stuff</head><p>" w a l l " , 3 :</p><p>" s k y " , 4 :</p><p>" f l o o r f l o o r i n g " , 5 :</p><p>" t r e e " , 6 :</p><p>" c e i l i n g " , 7 :</p><p>" r o a d r o u t e " , 8 :</p><p>" bed " , 1 0 : " g r a s s " , 1 2 : " s i d e w a l k p a v e m e n t " , 1 4 : " e a r t h g r o u n d " , 1 6 : " t a b l e " , 1 7 : " m o u n t a i n mount " , 1 8 : " p l a n t f l o r a p l a n t l i f e " , 2 0 : " c h a i r " , 2 9 : " r u g c a r p e t c a r p e t i n g " , 3 1 : " a r m c h a i r " , 3 4 : " d e s k " , 3 7 : " lamp " , 4 0 : " c u s h i o n " , 4 1 : " b a s e p e d e s t a l s t a n d " , 4 2 : " box " , 4 3 : " column p i l l a r " , 4 7 : " s a n d " , 5 1 : " r e f r i g e r a t o r i c e b o x " , 5 3 : " p a t h " , 6 5 : " c o f f e e t a b l e c o c k t a i l t a b l e " , 6 9 : " h i l l " , 7 1 : " c o u n t e r t o p " , 12 7 4 : " k i t c h e n i s l a n d " , 7 6 : " s w i v e l c h a i r " , 8 3 : " l i g h t l i g h t s o u r c e " , 8 5 : " t o w e r " , 8 6 : " c h a n d e l i e r p e n d a n t p e n d e n t " , 8 7 : " awning s u n s h a d e s u n b l i n d " , 9 4 : " p o l e " , 9 6 : " b a n n i s t e r b a n i s t e r b a l u s t r a d e b a l u s t e r s h a n d r a i l " , 1 0 7 : " c a n o p y " , 1 1 5 : " t e n t c o l l a p s i b l e s h e l t e r " , 1 2 6 : " p o t f l o w e r p o t " , 1 2 9 : " l a k e " , 1 3 1 : " s c r e e n s i l v e r s c r e e n p r o j e c t i o n s c r e e n " , 1 3 2 : " b l a n k e t c o v e r " , 1 3 4 : " hood e x h a u s t hood " , 1 3 6 : " v a s e " , 1 3 8 : " t r a y " , 1 3 9 : " a s h c a n t r a s h c a n g a r b a g e c a n w a s t e b i n a s h b i n ash − b i n a s h b i n d u s t b i n t r a s h b a r r e l t r a s h b i n " , 1 4 0 : " f a n " , 1 4 1 : " p i e r w h a r f w h a r f a g e dock " , 1 4 2 : " c r t s c r e e n " , 1 4 3 : " p l a t e " , 1 4 4 : " m o n i t o r m o n i t o r i n g d e v i c e " , 1 4 8 : " g l a s s d r i n k i n g g l a s s " Cityscapes 7 :</p><p>" r o a d " , 1 1 : " b u i l d i n g " , 1 7 : " p o l e " , 2 1 : " v e g e t a t i o n " , 2 3 : " s k y "</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04340</idno>
		<title level="m">Data augmentation generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Principal warps: Thin-plate splines and the decomposition of deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semantic Segmentation on MIT ADE20K dataset in PyTorch</title>
		<ptr target="https://github.com/CSAILVision/semantic-segmentation-pytorch.6" />
		<imprint/>
	</monogr>
	<note>CSAILVision</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<title level="m">Practical data augmentation with no separate search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling visual context is key to augmenting object detection datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cut, paste and learn: Surprisingly easy synthesis for instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fmix: Enhancing mixed sample data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesan</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Prügel-Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bi-Directional Cascade Network for Perceptual Edge Detection(BDCN)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/pkuCactus/BDCN.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bi-directional cascade network for perceptual edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Grid saliency for context explanations of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Device and method for training a generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaspar</forename><surname>Sakmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Patent:EP3767590A1</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Smart augmentation learning an optimal data augmentation strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Lemley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shabab</forename><surname>Bazrafkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Corcoran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Geometric gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://github.com/xh-liu/CC-FPSE.6" />
		<imprint>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to predict layout-to-image conditional convolutions for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<ptr target="https://github.com/kazuto1011/deeplab-pytorch.6" />
	</analytic>
	<monogr>
		<title level="j">Kazuto Nakashima. DeepLab with PyTorch</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sesame: Semantic editing of scenes by adding, manipulating or erasing objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ntavelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iason</forename><surname>Kastanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/pix2pixHD" />
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Semantic Image Synthesis with SPADE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nvlabs</surname></persName>
		</author>
		<ptr target="https://github.com/NVlabs/SPADE.6" />
		<imprint>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semi-parametric image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mirrorgan: Learning text-to-image generation by redescription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to compose domain-specific transformations for data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeshan</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Tempered adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giambattista</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Mehrjou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Incorporating invariances in support vector learning machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A u-net based discriminator for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient pattern recognition using a new transformation distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rendergan: Generating realistic labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Sixt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Landgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Robotics and AI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Amortised map inference for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Casper Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huszár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Edge guided gans with semantic preserving for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13898</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Local class-specific and global image-level generative adversarial networks for semantic-guided scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Improving deep learning with generic data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Nitschke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Between-class learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Using scene graph context to improve image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anahita</forename><surname>Bhiwandiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Bastidas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlin</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03762</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning to generate synthetic data via compositing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visesh</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Manifold mixup: Better representations by interpolating hidden states. In ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learnable histogram: Statistical context features for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Attngan: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drn</surname></persName>
		</author>
		<ptr target="https://https://github.com/fyu/drn.2" />
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Progressive augmentation of gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Stack-gan++: Realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Image augmentations for gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02595,2020.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Sean: Image synthesis with semantic region-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Data augmentation in emotion classification using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00648</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Semantically multi-modal image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">BC and U C denote the biased and unbiased class splits. Bold indicates the best model. For Donly and Gonly models, the augmentation was applied only to the discriminator and the generator input, respectively. For the ease of comparison, we repeat the metrics from the respective baseline and augmented models (-AUG) obtained using Pix2PixHD baselines and the augmented model</title>
	</analytic>
	<monogr>
		<title level="m">Table 3. Ablation results of each model on COCO-Stuff, ADE20K and Cityscapes</title>
		<imprint/>
	</monogr>
	<note>Best viewed in color</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
