<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTI-MODAL EMOTION RECOGNITION ON IEMOCAP WITH NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Tripathi</surname></persName>
							<email>samarthtripathi@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Tripathi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Homayoon</forename><surname>Beigi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Advanced AI</orgName>
								<orgName type="institution">LG Silicon Valley Lab Santa Clara</orgName>
								<address>
									<postCode>94041</postCode>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Uber Technologies</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">â€  Recognition Technologies, Inc. South Salem</orgName>
								<address>
									<postCode>10590</postCode>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MULTI-MODAL EMOTION RECOGNITION ON IEMOCAP WITH NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Emotion Recognition</term>
					<term>Multimodal clas- sification</term>
					<term>IEMOCAP</term>
					<term>Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion recognition has become an important field of research in human computer interactions and there is a growing need for automatic emotion recognition systems. One of the directions the research is heading is the use of neural networks which are adept at estimating complex functions that depend on a large number and diverse source of input data. In this paper we attempt to exploit this effectiveness of neural networks to enable us to perform multimodal emotion recognition on IEMOCAP dataset using data from speech, text, and motions captured from face expressions, rotation and hand movements. Our approach first identifies best individual architectures for classification on each modality and performs fusion only at the final layer which allows for a more robust and accurate emotion detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Emotion is a psycho-physiological process that can be triggered by conscious and/or unconscious perception of objects and situations, associated with multitude of factors such as mood, temperament, personality, disposition, and motivation <ref type="bibr" target="#b0">[1]</ref>. Emotions are very important in human decision handling, interaction and cognitive process <ref type="bibr" target="#b1">[2]</ref>. With the advancement of technology and as our understanding of emotions is advancing, there is a growing need for automatic emotion recognition systems. Emotion recognition has been studied widely using speech <ref type="bibr" target="#b2">[3]</ref> [4] <ref type="bibr" target="#b4">[5]</ref>, text <ref type="bibr" target="#b5">[6]</ref>, facial cues <ref type="bibr" target="#b6">[7]</ref>, and EEG based brain waves <ref type="bibr" target="#b7">[8]</ref> individually. One of the biggest opensourced multimodal resources available in emotion detection is IEMOCAP dataset <ref type="bibr" target="#b8">[9]</ref> which consists of approximately 12 hours of audio-visual data, including facial recordings, speech and text transcriptions.</p><p>In this paper we combine these modes to make a stronger and more robust detector for emotions. We explore various deep learning based architectures to first get the best individual detection accuracy from each of the different modes. We then combine them in an ensemble based architecture to allow for training across the different modalities using the variations of the better individual models. Our ensemble consists of Long Short Term Memory networks, Convolution Neural Networks, fully connected Multi-Layer Perceptrons and we complement them using techniques such as Dropout, adaptive optimizers such as Adam, pretrained word-embedding models and Attention based RNN decoders. This allows us to individually target each modality and only perform feature fusion at the final stage. The advantages of our study are two-fold. First, since we target each modality individually, lack of availability of any modality does not cripple our algorithm and would not require retraining of other modalities but only the prefinal layer. This also allows our approach to be modular. Second, we use Motion-capture data instead of Video recording, hence we do not use 3D-Convolutions but 2D-Convolutions which are faster have less memory requirements. We also use advanced hyperparameter optimization tools to achieve the best possible model configuration depending on our resource constraints. Our code is open sourced for other researchers to repeat and enhance our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>Most of the early research on IEMOCAP has concentrated specifically on emotion detection using speech data. One of the early important papers on this dataset is <ref type="bibr" target="#b9">[10]</ref> where they used segment level feature extraction, to feed those features to a MLP based architecture, where the input is 750 dimensional feature vector, followed by 3 hidden layer of 256 neurons each with rectilinear units as non-linearity. <ref type="bibr" target="#b2">[3]</ref> follows <ref type="bibr" target="#b9">[10]</ref> and they train long short-term memory (LSTM) based recurrent neural network. First they divide each utterance into small segments with voiced region, then assume that the label sequences of each segment follows a Markov chain. They extract 32 features for every frame with 12-dimensional Mel-frequency cepstral coefficients (MFCC) with log energy, and their first time derivatives among others. The network contains 2 hidden layers with 128 BLSTM cells (64 forward nodes and 64 backward nodes).</p><p>Another research we closely follow is <ref type="bibr" target="#b3">[4]</ref>, where they use CTC loss function to improve upon RNN based Emotion prediction. They use 34 features including 12 MFCC, chromagram-based and spectrum properties like flux and rolloff. For all speech intervals they calculate features in 0.2 second window and moving it with 0.1 second step. The use of CTC loss helps, as often, almost the whole utterance has no emotion, but emotionality is contained only in a few words or phonemes in an utterance which the CTC loss handles well. Unlike <ref type="bibr" target="#b2">[3]</ref> which uses only the improv data, Chernykh et. al. use all the session data for the emotion classification.</p><p>Multi-modal emotion classification has recently gathered more traction and IEMOCAP remains the significant dataset for this research direction. The current state-of-art classification on IEMOCAP is provided by <ref type="bibr" target="#b10">[11]</ref> which builds on the prior work <ref type="bibr" target="#b11">[12]</ref>. They use 3D-CNN for visual feature extraction, text-CNN for textual features extraction and openSMILE for audio feature extraction. They use Contextual LSTM Architecture on top of these unimodal input features. They are then merged with multi-modal contextual LSTM layers which performs feature fusion. This layer finally feeds to the classification module. In our paper we adopt a different approach to this study and achieve similar performance with certain advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SETUP</head><p>IEMOCAP has 12 hours of audio-visual data from 10 actors where the recordings follow dialogues between a male and a female actor in both scripted or improvised topics. After the audio-visual data has been collected it is divided into small utterances of length between 3 to 15 seconds which are then labelled by evaluators. Each utterance is evaluated by 3-4 assessors. The evaluation form contained 10 options (neutral, happiness, sadness, anger, surprise, fear, disgust frustration, excited, other). We consider only 4 of them anger, excitement (happiness), neutral and sadness so as to remain consistent with the prior research. We consider emotions where atleast 2 experts were consistent with their decision, which is more than 70 % of the dataset, consistent with prior research.</p><p>Along with the .wav file for the dialogue we also have the transcript for each the utterance. For each session, one actor wears the Motion Capture (MoCap) camera data which records the facial expression, head and hand movements of the actor. The Mocap data contains column tuples, for facial expressions the tuples are contained in 165 dimensions, 18 for hand positions and 6 for head rotations. As this Mocap data is very extensive we use it instead of the video recording in the dataset. These three modes (Speech, Text, Mocap) of data form the basis of our multi-modal emotion detection pipeline.</p><p>Next we preprocess the IEMOCAP data for these modes. For the speech data our preprocessing follows the work of <ref type="bibr" target="#b3">[4]</ref>. We use the Fourier frequencies and energy-based features Mel-frequency Cepstral Coefficients (MFCC) for a total of 34 features. They include 13 MFCC, 13 chromagram-based and 8 Time Spectral Features like zero crossing rate, short-term energy, short-term entropy of energy, spectral centroid and spread, spectral entropy, spectral flux, spectral rolloff. We calculate features in 0.2 second window and moving it with 0.1 second step and with 16 kHz sample rate. We keep a maximum of 100 frames or approximately for 10 seconds of the input, and zero pad the extra signal and end up with (100,34) feature vector for each utterance. We also experiment with delta and double-delta features of MFCC but they dont produce any performance improvement while adding extra computation overhead.</p><p>For the text transcript of each of the utterance we use pretrained Glove embeddings <ref type="bibr" target="#b12">[13]</ref> of dimension 300, along with the maximum sequence length of 500 to obtain a (500,300) vector for each utterance. For the Mocap data, for each different mode such as face, hand, head rotation we sample all the feature values between the start and finish time values and split them into 200 partitioned arrays. We then average each of the 200 arrays along the columns (165 for faces, 18 for hands, and 6 for rotation), and finally concatenate all of them to obtain (200,189) dimension vector for each utterance.</p><p>The total dataset consists of 4936 dialogues. For individual modalities we divide our dataset with a randomly chosen 20% validation splits. For the final combined model we use 3838 (77.7%) as our training set, these correspond to first 4 sessions of the data with 8 actors. We use the final 1098 (22.2%) dialogues as our test set. These correspond to Session 5, with 2 actors (Male and Female). This ensures we remain speaker agnostic in our predictions. Unlike <ref type="bibr" target="#b10">[11]</ref> we do not use 10-fold cross validation, since cross validation on Neural Networks is unfeasible due to time and compute requirements. For HyperParameter Optimization (HPO) we use Auptimizer 1 an open-sourced HPO tool. We use 838 dialogues from 3838 training set as our validation set for HPO. Once best parameters have been found we use both training and validation to evaluate on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Speech Based Emotion Detection</head><p>Our first model -Speech Model1 consists of three layered fully connected MLP layers with 1024, 512, 256 hidden neural units with Relu as activation and 4 output neurons with We also try many variations of the speech data including using MelSpectrogram, smaller window (0.08s) with longer context (200 timestamps) but do not achieve improvements.</p><p>To compare our results with prior research we use our best model (Speech Model4) and evaluate it in the manner similar to various conditions of the previous researches. Like <ref type="bibr" target="#b2">[3]</ref> we use only the improvisation session for both training and testing and achieve similar results. To compare with <ref type="bibr" target="#b3">[4]</ref> [5] <ref type="bibr" target="#b13">[14]</ref> who use the both scripted and improvisation sessions we again achieve similar results. One important insight of our results is with minimal preprocessing and no complex loss functions or noise injection into the training, we can easily match prior research's performance using Attention based Bidirectional LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Text based Emotion Recognition</head><p>Our task of performing emotion detection using only the text transcripts of our data resembles that of sentiment analysis, a    We then perform hyperparamter optimization on this model. We choose the number of LSTM neurons in Speech Model4, LSTM neurons in Text Model2, neurons of the final fully connected layer of the combined model and net Dropout on all the models as hyperparameters. We use Random proposer to optimize training on the validation set. We then evaluate the best hyperparameter configuration on the test set, using train and validation set. Our performance matches the prior state of the art, however the comparison is not fair. [11] use 10 fold cross validation, while we use less training data in a 77%-22% split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper we perform multimodal emotion recognition on IEMOCAP dataset using data from speech, text, and motions capture and identify best individual architectures for classification on each modality. We perform fusion only at the final layer which allows for a more robust and accurate emotion detection. Our approach has certain advantages. Firstly, since we only perform fusion at the final stage, lack of a modality would only require retraining the final fully connected layer. Also since we optimize individual modalities our combined model has a modular approach. This allows any individual model to be replaced by a better model, without affecting rest of the modalities in the combined model. Secondly, since we use motion captured data and 2D convolutions instead of video recordings and 3D convolutions, we have a faster training and inference time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Final Combined Neural Network Accuracy graph of our Final Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Speech emotion detection models and accuracy Adadelta as the optimizer. As we can see the final Attention based bidirectional LSTM model performs the best.</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>Speech Model1</cell><cell>50.6%</cell></row><row><cell>Speech Model2</cell><cell>51.32%</cell></row><row><cell>Speech Model3</cell><cell>54.15%</cell></row><row><cell>Speech Model4</cell><cell>55.65%</cell></row><row><cell cols="2">Table 2. Comparison between our Speech emotion detection</cell></row><row><cell>models and previous research</cell><cell></cell></row><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>Lee and Tashev [3]</cell><cell>62.85%</cell></row><row><cell>Ours (improv only)</cell><cell>62.72%</cell></row><row><cell>Chernykh [4]</cell><cell>54%</cell></row><row><cell>Neumann [5]</cell><cell>56.10%</cell></row><row><cell>Lakomkin [14]</cell><cell>56%</cell></row><row><cell>Ours (all)</cell><cell>55.65%</cell></row><row><cell cols="2">Softmax (like [10]). The model takes the flattened speech vec-</cell></row><row><cell cols="2">tors as input and trains using cross entropy loss with Adadelta</cell></row><row><cell cols="2">as the optimizer. Speech Model2 uses two stacked LSTM</cell></row><row><cell cols="2">layers with 512 and 256 units followed by a Dense layer with</cell></row><row><cell cols="2">512 units and Relu activation (like [3]). Speech Model3 uses</cell></row><row><cell cols="2">2 LSTM layers with 128 units each but the second LSTM</cell></row><row><cell cols="2">layer has Attention implementation as well, followed by 512</cell></row><row><cell cols="2">units of Dense layer with Relu activation. Speech Model4</cell></row><row><cell cols="2">improves both the encoding LSTM and Attention based de-</cell></row><row><cell cols="2">coding LSTM by making them bi-directional. All the last 3</cell></row><row><cell>models use</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Text emotion detection models and accuracy ModelAccuracy</figDesc><table><row><cell>Text Model1</cell><cell>62.55%</cell></row><row><cell>Text Model2</cell><cell>64.68%</cell></row><row><cell>Text Model3</cell><cell>64.78%</cell></row><row><cell cols="2">very common and well researched task of Natural Language</cell></row><row><cell cols="2">Processing. Here we try two approaches Text Model1 which</cell></row><row><cell cols="2">uses 1D convolutions of kernel size 3 each, with 256, 128,</cell></row><row><cell cols="2">64 and 32 filters using Relu as Activation and Dropout of</cell></row><row><cell cols="2">0.2 probability, followed by 256 dimension fully connected</cell></row><row><cell cols="2">layer and Relu, feeding to 4 output neurons with Softmax.</cell></row><row><cell cols="2">Text Model2 uses two stacked LSTM layers with 512 and</cell></row><row><cell cols="2">256 units followed by a Dense layer with 512 units and Relu</cell></row><row><cell cols="2">Activation. Both these models are initialized with Glove Em-</cell></row><row><cell cols="2">beddings based word-vectors. We also try Randomized ini-</cell></row><row><cell cols="2">tialization with 128 dimensions in Text Model3 and obtain</cell></row><row><cell cols="2">similar performance as Text Model2. The LSTM based mod-</cell></row><row><cell cols="2">els use Adadelta and Convolution based models use Adam as</cell></row><row><cell>optimizers.</cell><cell></cell></row><row><cell cols="2">4.3. MoCap based Emotion Detection</cell></row><row><cell cols="2">For the Mocap based emotion detection we use LSTM and</cell></row><row><cell cols="2">Convolution based models. For emotion detection using</cell></row><row><cell cols="2">only the head rotation we try 2 models, Head Model1 uses</cell></row><row><cell cols="2">LSTM with 256 units followed by Dense layer and Relu</cell></row><row><cell cols="2">activation, while Head Model2 uses just 256 hidden unit</cell></row><row><cell cols="2">based Dense Layer with Relu and achieves better perfor-</cell></row><row><cell cols="2">mance. We use the two models again for Hand movement</cell></row><row><cell cols="2">based emotion detection and Hand Model2 again achieves</cell></row><row><cell cols="2">better performance. For the facial expression based Mocap</cell></row><row><cell cols="2">data (which has a larger dimensionality than Mocap head</cell></row><row><cell cols="2">and hand data), we use two stacked LSTM layers with 512</cell></row><row><cell cols="2">and 256 units followed by a Dense layer with 512 units</cell></row><row><cell cols="2">and Relu Activation as Face Model1. Face Model2 uses 5</cell></row><row><cell cols="2">2D Convolutions each with kernel size 3, Stride 2 and 32,</cell></row><row><cell cols="2">64, 64, 128, 128 filters, along with Relu activation and 0.2</cell></row><row><cell cols="2">Dropout. These layers are then followed by a Dense Layer</cell></row><row><cell cols="2">with 256 neurons and Relu followed by 4 output neurons and</cell></row><row><cell cols="2">Softmax. Since Face Model2 achieves the best performance</cell></row><row><cell cols="2">we use Face Model2 based architecture for the concatenated</cell></row><row><cell cols="2">MoCap data architecture with 189 input feature length as</cell></row><row><cell cols="2">Mocap Model1. The LSTM based models use Adadelta and</cell></row><row><cell cols="2">Convolution and fully connected based models use Adam as</cell></row><row><cell>optimizers.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>MoCap emotion detection models and accuracy</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>MoCap-head Head Model1</cell><cell>37.75%</cell></row><row><cell>MoCap-head Head Model2</cell><cell>40.28%</cell></row><row><cell>MoCap-hand Hand Model1</cell><cell>33.70%</cell></row><row><cell>MoCap-hand Hand Model2</cell><cell>36.94%</cell></row><row><cell>MoCap-face Face Model1</cell><cell>48.99%</cell></row><row><cell>MoCap-face Face Model2</cell><cell>48.58%</cell></row><row><cell>MoCap-combined Mocap Model1</cell><cell>51.11%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Multimodal emotion detection models and accuracy</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>Text + Speech + Mocap Combined</cell><cell>71.04%</cell></row><row><cell>Poria [11]</cell><cell>71.59%</cell></row><row><cell>5. RESULTS</cell><cell></cell></row><row><cell cols="2">5.1. Combined Multi-Modal Emotion Detection</cell></row><row><cell cols="2">For our final model we take the best individual models</cell></row><row><cell cols="2">for each modality without their final softmax layers. The</cell></row><row><cell cols="2">Text Model2 with stacked LSTMs and Glove word embed-</cell></row><row><cell cols="2">dings is chosen for text modality, Speech Model4 for the</cell></row><row><cell cols="2">speech modality with 2 stacked bidirections LSTMs with At-</cell></row><row><cell cols="2">tention, and combined Mocap Model1 with stacked convolu-</cell></row><row><cell cols="2">tion layers. We then perform feature fusion by concatenating</cell></row><row><cell cols="2">their final fully connected layers. We add another final fully-</cell></row><row><cell cols="2">connected layer with 256 neurons followed by a softmax</cell></row><row><cell>layer. This forms our combined final model.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/LGE-ARC-AdvancedAI/auptimizer</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A multimodal database for affect recognition and implicit tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="55" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification of human emotion from deap eeg signal using hybrid improved neural networks with cuckoo search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sreeshakthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Preethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BRAIN. Broad Research in Artificial Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="60" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">High-level feature representation using recurrent neural network for speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Tashev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Emotion recognition from speech with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Chernykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigoriy</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Prihodko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08071</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emotion recognition: the role of facial movement and the relative importance of upper and lower areas of the face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bassili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2049</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using deep and convolutional neural networks for accurate emotion classification on deap dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrinivas</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranti Dev</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samit</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4746" to="4752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iemocap: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language resources and evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">335</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using deep neural network and extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis: Addressing key issues and setting up the baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="17" to="25" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reusing neural speech representations for auditory emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Lakomkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelius</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Magg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11508</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
