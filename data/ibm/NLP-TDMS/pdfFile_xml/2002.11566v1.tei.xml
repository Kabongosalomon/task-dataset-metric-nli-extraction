<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Relational Graph with Teacher-Recommended Learning for Video Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
							<email>zhangziqi2017@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaya</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">PeopleAI, Inc</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">State Key Laboratory of Communication Content Cognition</orgName>
								<address>
									<settlement>People&apos;s Daily Online</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peijin</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
							<email>wmhu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<orgName type="institution" key="instit1">CAS 5 Aerospace Information Research Institute</orgName>
								<orgName type="institution" key="instit2">CAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjun</forename><surname>Zha</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object Relational Graph with Teacher-Recommended Learning for Video Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Taking full advantage of the information from both vision and language is critical for the video captioning task. Existing models lack adequate visual representation due to the neglect of interaction between object, and sufficient training for content-related words due to long-tailed problems. In this paper, we propose a complete video captioning system including both a novel model and an effective training strategy. Specifically, we propose an object relational graph (ORG) based encoder, which captures more detailed interaction features to enrich visual representation. Meanwhile, we design a teacher-recommended learning (TRL) method to make full use of the successful external language model (ELM) to integrate the abundant linguistic knowledge into the caption model. The ELM generates more semantically similar word proposals which extend the groundtruth words used for training to deal with the long-tailed problem. Experimental evaluations on three benchmarks: MSVD, MSR-VTT and VATEX show the proposed ORG-TRL system achieves state-of-the-art performance. Extensive ablation studies and visualizations illustrate the effectiveness of our system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video captioning aims to generate natural language descriptions automatically according to the visual information of given videos. There are many wonderful visions of video captioning such as blind assistance and autopilot assistance. Video captioning needs to consider both spatial appearance and temporal dynamics of video contents, * Equal contribution. † Corresponding author.</p><p>Words frequency before TRL Words frequency after TRL % % Head a th e m a n in ta lk d o g m o o n s c ie n ti s t te n n is d a n c e Tail <ref type="figure">Figure 1</ref>. Long-tailed problem of the caption corpus. The top-50 words frequency of MSR-VTT are shown. Under the guidance of TRL, more potential content-specific words are exposed to the caption model. Compared with the words frequency before, the words in tail region get an overall boosting.</p><p>which is a promising and challenging task. The key problems in this task are twofold: how to extract discriminative features to represent the contents of videos, and how to leverage the existing visual features to match the corresponding captioning corpus. The ultimate aim is to cross the gap between vision and language. For vision representation, previous works <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">35]</ref> always leverage appearance features of keyframes and motion features of segments to represent video contents. These features extract global information and hard to capture the detailed temporal dynamics of objects in the video. The most recent works <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b11">12]</ref> apply a pretrained object detector to obtain some object proposals in each keyframe and use spatial/temporal attention mechanisms to fuse object features. However, they neglect the relationship between objects in the temporal and spatial domains. Some researches in the field of Visual Question Answering, Image Captioning even Action Recognition demonstrate that the relationship between objects is vital, which also plays an important role in generating a more detailed and diverse description for a video.</p><p>For sentence generation, according to statistics of word frequency in caption corpus, it is found that the majority of words are function words and common words e.g. "the" and "man", which are far more than the real content-specific words in number. This is the so-called "long-tailed" problem, as shown in <ref type="figure">Fig.1</ref>. This problem will cause insufficient training for a large number of meaningful words. Although the long-tailed problem can be relieved by giving different weights to different words <ref type="bibr" target="#b7">[8]</ref>, it can not be solved fundamentally. Furthermore, a caption model should not only comprehend visual information but also grasp linguistic ability using such a small number of samples, which is a so heavy task! Why not employ a ready-made ELM e.g. BERT <ref type="bibr" target="#b6">[7]</ref> or GPT <ref type="bibr" target="#b25">[26]</ref> as a teacher, to directly impart linguistic knowledge to the caption model, to mitigate the problem caused by insufficient samples.</p><p>In this paper, we propose a novel model with the assistance of an original training strategy to deal with above two issues for video captioning: 1) We construct a learnable ORG to fully explore the spatial and temporal relationships between objects. With the help of graph convolutional networks(GCNs) <ref type="bibr" target="#b15">[16]</ref>, object representations can be enhanced during the process of relational reasoning. Specifically, we explore two kinds of graphs: the partial object relational graph (P-ORG) connects objects in the same frame, and the complete object relational graph (C-ORG) builds a connection for all the objects in video. Scaled dot-product is utilized to implicitly compute relationships between each object, which is learnable during training. Finally, object features are updated by GCNs to be more informative features. 2) Generally, the caption model is forced to learn the ground-truth word at each training step, so we call this process as the teacher-enforced learning (TEL) and these words as hard target. However, TEL doesn't consider the long-tailed problem. Therefore, we propose a TRL method, which makes full use of external language model (ELM) to generate some word proposals according to the prediction probability of current ground-truth words. These proposals are called soft targets, which are often semantically similar with the ground-truth words and extended them. Specifically, the ELM is off-line well-trained on a large-scale external corpus, and it is employed as an experienced teacher, who has contained a wealth of linguistic knowledge. By contrast, the caption model can be regarded as a student. Under the guidance of TRL, excellent linguistic knowledge from ELM is transformed into the caption model.</p><p>The contributions of this work can be summarized as following: 1) We construct novel ORGs to connect each object in video and utilizes GCNs to achieve relational reasoning, which enrich the representation of detailed objects further.</p><p>2) The TRL is proposed as a supplement of the TEL, to integrate linguistic knowledge from an ELM to the caption model. Several times words are trained at each time step more than before. It's effective to relieve long-tailed prob-lem and improve generalization of the caption model. 3) Our model achieves state-of-the-art performances on three benchmarks: MSVD, MSR-VTT, and newly VATEX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Video Captioning. Recent researches mainly focus on sequence-learning based methods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b24">25]</ref>, which adopt encoder-decoder structure. Yao et al. <ref type="bibr" target="#b44">[44]</ref> propose a temporal attention mechanism to dynamically summarize the visual features. Wang et al. <ref type="bibr" target="#b35">[35]</ref> try to enhance the quality of generated captions by reproducing the frame features from decoding hidden states. More recently, there are some researches concerning the object-level information <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b11">12]</ref>. Zhang et al. <ref type="bibr" target="#b48">[48]</ref> use a bidirectional temporal graph to capture detailed temporal dynamics for the salient objects in the video. Hu et al. <ref type="bibr" target="#b11">[12]</ref> use twolayers stacked LSTM as an encoder to construct the temporal structure at frame-level and object-level successively.</p><p>However, these methods mainly work on the global information or temporal structure of salient objects without considering the interactions between each object in frames. In this work, we propose a graph-based approach, which constructs a temporal-spatial graph on all the objects in a video to enhance object-level representation.</p><p>Visual Relational Reasoning. Some researches have shown that visual relational reasoning is effective for computer vision tasks, such as Image Captioning <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b42">42]</ref>, VQA <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref> and Action Recognition <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref>. Yao et al. <ref type="bibr" target="#b45">[45]</ref> exploit predefined semantic relations learned from the scene graph parsing task <ref type="bibr" target="#b47">[47]</ref> and embed the graph structure into vector representations by using a modified GCN. Li et al. <ref type="bibr" target="#b16">[17]</ref> use both explicit graph and learnable implicit graph to enrich image representation and apply GAT <ref type="bibr" target="#b31">[31]</ref> to update relations in attentive weight. Wang et al. <ref type="bibr" target="#b38">[38]</ref> compute both implicit similarity relation and relative positional relation of each object in the video, and then apply GCNs to perform reasoning. There are few efforts utilizing relational reasoning for video captioning.</p><p>External Language Model for Seq2Seq Generation Tasks. ELM has been applied to many natural language generation tasks such as neural machine translation (NMT) and automatic speech recognition (ASR). An early attempt to use ELM for NMT in <ref type="bibr" target="#b8">[9]</ref> is also known as shallow fusion and deep fusion. Kannan et al. <ref type="bibr" target="#b12">[13]</ref> fully explore the behavior of shallow fusion with different ELMs and test them on a large-scale ASR task. Sriram et al. <ref type="bibr" target="#b28">[28]</ref> propose cold fusion to improve ASR performance.</p><p>These above fusion methods illustrate promising performance but also have some limitations. Shallow fusion may bring bias when output logits are used directly because of the difference in the data distribution between language model and task model. Deep fusion also needs ELM during inference and cold fusion relies on additional gating mech-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GroundTruth</head><p>Two chefs talk about how to cook a pork chop while demonstrating the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detector 3DCNNs 2DCNNs</head><p>Spatial Attention  anisms and networks, which will bring heavy calculations and complexity to the task model. In comparison, our introduced TRL method only calculates the KL divergence between "soft targets" and output distribution of task model during training which can well overcome above-mentioned limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t a l k t a l k s h o w p l a y c o o k i l l u s t r a t e t e l l</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Fig <ref type="figure" target="#fig_1">.2</ref> illustrates the overview of our system. An encoderdecoder framework is followed. Appearance, motion and detailed objects features are extracted by diverse networks. Specifically, we construct a graph based object encoder whose core is a learnable object relational graph (ORG), which can learn the interaction among different objects dynamically. The description generator generates each word by steps, with attentively aggregating visual features in space and time. For the learning process, not only normal teacher-enforced learning (TEL) but also proposed teacherrecommended learning (TRL) strategy are leveraged to learn task-specific knowledge and external linguistic knowledge separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Object Relational Graph based Visual Encoder</head><p>Formally, given a sequence of video frames, we uniformly extract T frames as keyframes, and collect a shortrange video frames around keyframes as segments which reflects the temporal dynamics of a video. The pretrained 2D CNNs and 3D CNNs are employed to extract the appearance features F = {f i } of each keyframe and motion features M = {m i } of each segment separately, where f i and m i denote the features of the i th frame and segment respectively; i = 1, . . . , L; L denotes the number of keyframes.</p><p>People always describe an object based on its relationships with others in the video. In order to get the detailed object representations, the pretrained object detector is ap-plied to capture several class-agnostic object proposals in each keyframe and extract their features R i = r i k , i = 1, . . . , L, k = 1, . . . , N , where r i k represents the k th object feature in i th keyframe, L is the number of keyframes and N is the number of objects in each frame. These original object features are independent, and they have no interaction with each other in time and space.</p><p>To learn the relation message from surrounding objects, we define a relational graph for a object set and then use it to update the object features. Specifically, given K objects, each object is considered as a node. Let R ∈ R K×d denote K object nodes with d dimensional feature, and A ∈ R K×K denote the relation coefficient matrix between K nodes. We define A as:</p><formula xml:id="formula_0">A = φ(R) · ψ(R) T (1) φ(R) = R · W i + b i , ψ(R) = R · W j + b j (2) where W i , W j ∈ R d×d and b i ∈ R d , b j ∈ R d are learn- able parameters.</formula><p>Subsequently, A is normalized to make the sum of edges, connecting to the same node, equals to 1:</p><formula xml:id="formula_1">A = sof tmax(A, dim = 1)<label>(3)</label></formula><p>whereÂ can be seen as how much information the center object gets from the surrounding objects. We apply GCNs to perform relational reasoning, then original objects features R are updated toR:</p><formula xml:id="formula_2">R =Â · R · W r<label>(4)</label></formula><p>whereR ∈ R K×d is enhanced object features with interaction message between objects, and W r ∈ R d×d is learnable parameters. We explore two kinds of relational graphs as shown in <ref type="figure" target="#fig_2">Fig.3</ref>, the P-ORG and the C-ORG. Specifically, the P-ORG only build the relationship between N objects in the same frame thus a A ∈ R N ×N relational graph is constructed. Note that learnable parameters of relational graph are shared with all L frames. Although object proposals appearing in different frames may belong to the same entity, they are considered as different nodes because of diverse states. Meanwhile, the C-ORG constructs a complete graph A ∈ R (N ×L)×(N ×L) which connects each object with all the other N × L objects in the video. It's noisy to directly connect center node with all N × L nodes, thus we select top-k corresponding nodes to connect. Finally, the enhanced object features are computed by performing relational reasoning. They are together with appearance and motion features to sufficiently present videos.</p><formula xml:id="formula_3">Share Share N R 1 N × L P-ORG R 2 R L R 1 R 2 R L Top-k A ∈ R N × N 0 1 C-ORG Sort A ∈ R N × L × N × L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Description Generation</head><p>After getting the sufficient video features, we propose a hierarchical decoder with a temporal-spatial attention module to generate linguistic descriptions by steps. The hierarchical decoder consists of the Attention LSTM and the Language LSTM.</p><p>Firstly, the Attention LSTM is to summarize current semantics h attn t according to the history hidden state h lang t−1 of Language LSTM, concatenated with mean-pooled global video featurev = 1 L v i and the previous word w t−1 at the decoding step t:</p><formula xml:id="formula_4">h attn t = LSTM attn v, W e w t−1 , h lang t−1 ; h attn t−1<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">v i = [f i , m i ], f i ∈ F, m i ∈ M,</formula><p>is the concatenation of appearance feature and motion feature, W e is the learnable word embedding matrix. Following the current semantics h attn t , the temporal attention module dynamically decides when (frames) to attend, and abstracts the global context features c g t :</p><formula xml:id="formula_6">c g t = L i=1 α t,i v i α t,i = sof tmax w T a tanh W a v i + U a h attn t<label>(6)</label></formula><p>where α t,i is the weight of the i th global feature at the t th decoding step; L is the number of keyframes; w a , W a and U a are learnable parameters. For local object feature, objects in different frames are firstly aligned to merge together, and then the spatial attention module chooses which objects should be focused on. We use a simple but effective method to align objects in different frames. The process is shown on the left pictures in <ref type="figure" target="#fig_1">Fig.2</ref>, and the dotted line trajectories present the objects alignment. We set objects in the first frame as anchors, and define sim i (j, j ) as the cosine distance between the j th object in anchor frame and the j th in i th frame:</p><formula xml:id="formula_7">sim i j, j = cos r 1 j , r i j<label>(7)</label></formula><p>where j, j = 1, . . . , N ; i = 2, . . . , L. Considering the similarity between two objects themselves, we use original object features R to calculate similarity rather than enhanced featuresR. The object in each frame is aligned to the anchors according to the maximum similarity. These aligned objects ideally belong to the same entity. Enhanced featureŝ R, following the group of aligned objects, are weighted sum by {α t,i }, i = 1, . . . , L. In this way, objects in different frames are merged into one frame as local aligned features R according to alignment operation and temporal attention. Then, the spatial attention module decides where (objects) to attend, and abstracts local context feature c l t :</p><formula xml:id="formula_8">c l t = N j=1 β t,j u j β t,j = sof tmax w T b tanh W b u j + U b h attn t<label>(8)</label></formula><p>where u j ∈R denotes one of the N local aligned features; w b , W b and U b are learnable parameters. Finally, the Language LSTM summarizes both global and local context features to generate current hidden state h lang t . The probability distribution of the caption model P t is acquired, followed with a single layer perceptron and the softmax operation at decoding step t:</p><formula xml:id="formula_9">h lang t = LSTM lang c g t , c l t , h attn t ; h lang t−1<label>(9)</label></formula><formula xml:id="formula_10">P t = sof tmax(W z h lang t + b z )<label>(10)</label></formula><p>where [·, ·] denotes concatenation; P t is a D-dimensional vector of vocabulary size; W z and b z are learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Teacher-recommended Learning via External Language Model</head><p>For a sufficient training of content-specific words, the proposed model is jointly trained under the guidance of common TEL and proposed TRL.  <ref type="table">Table 1</ref>. An example of "soft targets" and "hard target" (colored words) at three positions of the same sentence. Given the words before "hard targets", the ELM generate 10 "soft targets" and their probabilities in descending order.</p><p>As for conventional TEL process, the caption model is forced to generate the ground-truth word at each time step. This word is the so-called "hard target", which are expressed as X hard = x h 1 , x h 2 , . . . , x h Ts , where x h t is one ground-truth word at the t th decoding step; T s denotes training step in total of the given sentence. We refer to our designed caption model as CAP, and the output probability distribution of CAP is P t = CAP (w &lt;t |θ CAP ), where w &lt;t is history words; θ CAP stands all the parameters of the CAP. The training criterion is based on Cross-Entropy loss, only the probability corresponding to ground-truth participate in calculation:</p><formula xml:id="formula_11">L CE (θ) = − T t=1 δ(x h t ) T · logP t<label>(11)</label></formula><p>where δ(d) ∈ R D denotes one-hot vector, and the value equals to 1 only at the word d position; P t ∈ R D is the output distribution of the CAP; x h t is the "hard targets". The TEL is lack of sufficient training for content-related words due to long-tailed problems. Therefore, we propose the TRL to integrate the knowledge from ELM. There are many ready-made models that can be employed as ELM e.g. Bert and GPT. Suppose we got an ELM that has been well trained on a large scale monolingual corpus. When given the previous t − 1 words w &lt;t , the probability distribution of ELM at time step t is:</p><formula xml:id="formula_12">Q t = ELM (w &lt;t , T e |θ ELM )<label>(12)</label></formula><p>where Q t ∈ R D is a D-dimensional vector representing the output distribution of ELM; θ ELM are parameters of ELM which are fixed during the training phase of the CAP; T e is the temperature used to smooth output distribution. Generally, in order to transfer the knowledge from ELM to the CAP, it's easy to minimize the KL divergence between probability distribution of the CAP and the ELM during decoding step. To make P t fit Q t , the KL divergence is formulated as:</p><formula xml:id="formula_13">D KL (Q t ||P t ) = − d∈D Q d t · log P d t Q d t<label>(13)</label></formula><p>where P d t and Q d t are the output probability of word d in the CAP and the ELM respectively. Q t is the probability distribution of all the words for task vocabulary, but most of the values(&lt; 10 −4 ) are extremely small. These semantic irrelevant words may confuse the model and increase computation. Therefore we only extract top-k words as "soft targets":</p><formula xml:id="formula_14">X sof t = x x x s 1 , x x x s 2 , . . . , x x x s Ts<label>(14)</label></formula><p>where x x x s t = {x s i |i = 1, 2, . . . , k} are a set of words in descending order of probability distribution Q t at the t th decoding step. Furthermore, the ELM is fixed while the CAP is training, so the KL-loss function is simplified as:</p><formula xml:id="formula_15">L KL (θ) = − T t=1 d∈x x x s t Q d t · logP d t<label>(15)</label></formula><p>In most cases, "hard target" is concluded in "soft targets", because ELM is trained on the large-scale corpora. Tab.1 shows an example, our ELM can generate some syntactically correct and semantically reasonable proposals, which can be regarded as supplements to ground-truth word.</p><p>For the overall training process, our CAP is under the coguidance of both TEL and TRL to learn task-specific knowledge and external linguistic knowledge separately. We set a trade-off parameter λ ∈ [0, 1] to balance the degree of TEL and TRL, thus the criterion of the whole system is shown as:</p><formula xml:id="formula_16">L(θ) = λL KL (θ) + (1 − λ)L CE (θ)<label>(16)</label></formula><p>The TRL exposes a large number of potential words to the CAP. To some extent, it effectively alleviates the longtailed problem of the caption training corpus. Moreover, there is no extra computational burden on sentence generation at inference time, because the TRL only participates in the training process of the CAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our proposed model on three datasets: MSVD <ref type="bibr" target="#b1">[2]</ref>, MSR-VTT <ref type="bibr" target="#b41">[41]</ref> and VATEX <ref type="bibr" target="#b39">[39]</ref>, via four popular used metrics including BLEU-4 <ref type="bibr" target="#b23">[24]</ref>, ME-TEOR <ref type="bibr" target="#b5">[6]</ref>, CIDEr <ref type="bibr" target="#b30">[30]</ref> and ROUGE-L <ref type="bibr" target="#b17">[18]</ref>. Our results are compared with state-of-the-art results, which demonstrate the effectiveness of our methods. Besides, we verify the interpretation of our modules through two groups of experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>MSVD contains 1970 YouTube short video clips. Each video is annotated with multilingual sentences, but we experiment with the roughly 40 captions in English. Similar to the prior work <ref type="bibr" target="#b33">[33]</ref>, we separate the dataset into 1,200 train, 100 validation and 670 test videos.</p><p>MSR-VTT is another benchmark for video captioning which contains 10,000 open domain videos and each video is annotated with 20 English descriptions. There are 20 simple-defined categories, such as music, sports, movie etc. we use the standard splits in <ref type="bibr" target="#b41">[41]</ref> for fair comparison which separates the dataset into 6,513 training, 497 validation and 2,990 test videos.</p><p>VATEX 1 is a most recently released large-scale dataset that reuses a subset of the videos from the Kinetics-600 dataset <ref type="bibr" target="#b13">[14]</ref> and contains 41,269 videos. Each video is annotated with 10 English and 10 Chinese descriptions. We only utilize English corpora in experiments. Following the official split: 25,991 videos for training, 3,000 videos for validation and 6,000 public test videos for test. Compared with the two datasets mentioned above, the captions are longer and higher-quality, the visual contents are richer and more specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Features and Words Preprocessing. We uniformly sample 28 keyframes/clips for each video and 5 objects for each keyframe. The 1536-D appearance features are extracted by InceptionResNetV2 <ref type="bibr" target="#b29">[29]</ref> pretrained on the Im-ageNet dataset <ref type="bibr" target="#b26">[27]</ref>. The 2048-D motion features are extracted by C3D <ref type="bibr" target="#b9">[10]</ref> which is pretrained on the Kinetics-400 dataset, with ResNeXt-101 <ref type="bibr" target="#b40">[40]</ref> backbone. These features are concatenated and projected into hidden space with 512-D. We utilize a ResNeXt-101 backbone based Faster-RCNN pretrained on MSCOCO <ref type="bibr" target="#b2">[3]</ref> to extract object features. The object features are captured from the output of FC7 layer without category information and then embedded to 512-D before fed into ORG.</p><p>For the sentences longer than 24 words are truncated (30 for VATEX); the punctuation are removed (for VATEX are retained); all words are converted into lower case. We build a vocabulary on words with at least 2 occurrences. We embed the word to 300-D word vector initialized with GloVe by spaCy toolkits.</p><p>External Language Model Settings. To guarantee the quality of generated "soft targets", we employ the off-theshelf Bert model provided by pytorch-transformers 2 . Its a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. Specifically, the bert-base-uncased model with 12 layers, 768 hidden and 12 self-attention heads is utilized. We then simply fine-tune it on the corpus of corresponding training dataset using Adam <ref type="bibr" target="#b14">[15]</ref> optimizer with 3e − 5 learning rate and 128 batch size for 10 epochs. During the captioning model training phase, the parameters of ELM are fixed and we inference the "soft targets" of current time step with masking all the words after that.</p><p>Captioning Model Settings. The model is optimized by the Adam with a learning rate of 3e-4 and batch size of 128 at training, and we use beam search with size 5 for generation at inference. The two-layers LSTMs used in our decoder have 512 hidden units. The state sizes of both temporal and spatial attentions are set to 512. The dimension of feature vectors in the ORG is 512. We also explore the diverse influences on the system, with the different numbers of top soft targets in TRL and the different number of top collections in P-ORG. In general, top-50 soft targets and top-5 connections are better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance Comparison</head><p>To evaluate the effectiveness of our models, we compare our model with state-of-the-art models listed in Tab.2. Due to diverse modalities for video captioning, we list the models that only contain visual modalities i.e. appearance, motion and object features. Even so, it's also hard to achieve a completely fair comparison because of different feature extraction methods. Therefore, we try to employ the same feature extractors and preprocessing as the most recent models.</p><p>The quantitative results in Tab.2 illustrate our model gets significant improvement on MSVD and MSR-VTT datasets, which verifies the effectiveness of our proposed methods. Specifically, compared with GRU-EVE, MGSA, POS+CG and POS+VCT using the same features as ours, which demonstrate the superior performance without the effects of features. The remarkable improvement under CIDEr on both datasets demonstrates the ability to generate novel words of our model. Since the mechanism of CIDEr is to punish the often-seen but uninformative n-grams in the dataset. This phenomenon verifies that our model captures the detailed information from videos and acquires wealthy knowledge via ELM.</p><p>Moreover, we compare our model with the existing video captioning models that use detailed object information. GRU-EVE tries to derive high-level semantics from an object detector to enrich the representation with spatial dynamics of the detected objects. OA-BTG applies a bidirectional temporal graph to capture temporal trajectories for each object. However, these two methods ig-  nore the relationship between objects. Our ORG method achieves better performances than OA-BTG on MSR-VTT in Tab.2, which illustrates the benefits of object relations. Note that, POS+VCT achieves higher scores under ME-TEOR and ROUGE-L on MSR-VTT, and these are probably caused by the reason that their POS method can learn the syntactic structure representation. Besides, we also report the results of our model on the public test set of recent published VATEX dataset as shown in Tab.3. These results come from the online test system. Compared with the baseline model, we train the model on English corpus without sharing Encoder and Decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Experiments</head><p>Effectiveness of each component. We design 4 control experiments to demonstrate the effectiveness of the proposed ORG module and TRL. Tab.4 gives the control results on the testing set of MSVD and MSR-VTT datasets. The baseline model only applies appearance and motion features, and the same encoder-decoder architecture as mentioned above except without object encoder. It follows the Cross-Entropy criterion, and the results are shown in The evaluation of ORG. We explore two proposed ORGs: the P-ORG and the C-ORG, and we connect top-5 object nodes for C-ORG. The top half of Tab.5 demonstrates that C-ORG is better than P-ORG. It is probable that P-ORG can get more comprehensive information from the whole video than P-ORG. Moreover, both ORGs achieve significant improvement compared with the baseline model, which attributes to the association between objects. We also explore the effect of different Top-k for the C-ORG which is listed in the bottom half of Tab.5. "All" means each node acquires information from all nodes. We find that the highest performances are achieved at the sweat point when k = 5. A proper explanation is that, when k is too small, there are not enough related objects to update the relation of node; when k is too large, a few unrelated nodes will be introduced and bring noise.</p><p>The evaluation of TRL. We analyze the effect of different ELM temperatures T e and different ratios of KL-loss λ. <ref type="figure" target="#fig_4">Fig.5</ref> illustrates the performances on CIDEr: If T e is too low, the distribution of soft targets is sharp, thus large noise will be introduced if top-one is not the content related word. Otherwise, the distribution is too smooth to reflect the importance of soft targets. On the other hand, the weight of λ reflects the degree of the TRL: the generation will devi-GT:</p><p>female models are walking down a runway in dresses Baseline: a woman walks down a runway ORG-TRL: a woman in a dress is walking on a stage GT:</p><p>the man in the green shirt is cutting potatoes in thin slices Baseline: a man in a kitchen is slicing a piece of bread ORG-TRL: a man in a black shirt is cutting some vegetables in a kitchen GT:</p><p>two men are competing in a fierce table tennis game Baseline: there is a man in red is playing table tennis ORG-TRL: two men are playing a game of ping pong GT: a woman is mixing something in a bowl Baseline: there is a woman is making a dish ORG-TRL: a person is mixing some food in a bowl  ate from the content of the video itself if λ is too high; it plays no role if too low. <ref type="figure">Fig.6</ref> shows comparisons of the intermediate states of the baseline model and our TRL based model at inference time, and two models are trained with the same epoch: the red word is the next word to be predicted; the green box and blue box show the predictions and their probabilities of baseline model and our TRL method respectively. See the first clip, "climate change" is a very common noun phrase, but it rarely appears in the caption task. As shown in the second clip, the caption model can predict various proper combinations after "basketball" according to the sentence context. Moreover, the most of words are relative with video content. Our TRL method can help the model to learn some common matches and content-related words. To some extent, it effectively alleviates the long-tailed problem of video captioning task. We also experiment various top-k soft targets, see the appendix for detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Analysis</head><p>We show some examples in <ref type="figure" target="#fig_3">Fig.4</ref>. It can be seen, the content of captions generated by our model is richer than the baseline model, and more activity associations are involved. For instance, the example at top-left shows that the baseline model can only understand the general meaning of the video. By contrast, our model can recognize more detailed objects, and the relation "mixing" between "person" and "food", even the position "in a bowl". The rest of the examples have similar characteristics.  <ref type="figure">Figure 6</ref>. Two instances of the baseline model and baseline+TRL model in inference. The red word is the word to be predicted. The left-green box is the prediction of baseline model; the right-blue box is under the guidance of TRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a complete system, which contains a novel model and a training strategy for video captioning. By constructing relational graph between objects and performing relational reasoning, we can acquire more detailed and interactive object features. Furthermore, the novel TRL introduces external language model to guide the caption model to learn abundant linguistic knowledge, which is the supplement of the common TEL. Our system has achieved competitive performances on MSVD, MSR-VTT and VATEX datasets. The experiments and visualizations have demonstrates the effectiveness of our methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overview of our proposed ORG-TRL system. It mainly consists of the ORG based object encoder presented in the top-left box, and the hierarchical decoder with temporal/spatial attention in the top-right box. Our model is under the co-guidance of the novel TRL in the bottom-left box and the common TEL in the bottom-right. It also illustrates a virtual example during training: when t = 3, the TEL forces the model to learn "talk", but the TRL recommends the model to learn more words via the knowledge from ELM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The diagrams of the proposed P-ORG and C-ORG. Each colored square represents the vector of the object. A is the relational coefficient matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Examples of generations on MSR-VTT with the baseline model and our proposed ORG-TRL system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Analysis of different temperatures of ELM and different ratios of KL-loss on MSR-VTT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>GT: narrator talks about some people not believing in climate change[EOS] and the to [UNK] 0.531 0.031 0.026 0.021 0.0170 change effect [EOS] country weather 0.673 0.072 0.055 0.005 0.004 GT: a guy in shorts and a white shirt is teaching different basketball moves [EOS] on ball and in 0.308 0.060 0.058 0.049 0.043 [EOS] skills moves tricks in 0.308 0.060 0.057 0.049 0.043</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Performance comparisons on MSVD and MSR-VTT benchmarks. The best results and corresponding features are listed. The results of the VATEX online evaluation system.</figDesc><table><row><cell></cell><cell cols="2">Models</cell><cell>Year</cell><cell></cell><cell cols="2">Appearence</cell><cell cols="2">Features Motion</cell><cell></cell><cell>Object</cell><cell>B@4</cell><cell>MSVD M R</cell><cell>C</cell><cell>B@4</cell><cell>MSR-VTT M R</cell><cell>C</cell></row><row><cell></cell><cell cols="3">SA-LSTM [35] 2018</cell><cell></cell><cell cols="2">Inception-V4</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>45.3 31.9 64.2 76.2 36.3 25.5 58.3 39.9</cell></row><row><cell></cell><cell cols="2">M3 [36]</cell><cell>2018</cell><cell></cell><cell cols="2">VGG</cell><cell></cell><cell>C3D</cell><cell></cell><cell>-</cell><cell>52.8 33.3</cell><cell>-</cell><cell>-</cell><cell>38.1 26.6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">RecNet [35]</cell><cell>2018</cell><cell></cell><cell cols="2">Inception-V4</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>52.3 34.1 69.8 80.3 39.1 26.6 59.3 42.7</cell></row><row><cell></cell><cell cols="2">PickNet  *  [5]</cell><cell>2018</cell><cell></cell><cell cols="2">ResNet-152</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>52.3 33.3 69.6 76.5 41.3 27.7 59.8 44.1</cell></row><row><cell></cell><cell cols="2">MARN [25]</cell><cell>2019</cell><cell></cell><cell cols="2">ResNet-101</cell><cell></cell><cell>C3D</cell><cell></cell><cell>-</cell><cell>48.6 35.1 71.9 92.2 40.4 28.1 60.7 47.1</cell></row><row><cell></cell><cell cols="2">SibNet [19]</cell><cell>2019</cell><cell></cell><cell cols="2">GoogleNet</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>54.2 34.8 71.7 88.2 40.9 27.5 60.2 47.5</cell></row><row><cell></cell><cell cols="2">OA-BTG [48]</cell><cell>2019</cell><cell></cell><cell cols="2">ResNet-200</cell><cell></cell><cell>-</cell><cell></cell><cell cols="2">Mask-RCNN 56.9 36.2</cell><cell>-</cell><cell>90.6 41.4 28.2</cell><cell>-</cell><cell>46.9</cell></row><row><cell></cell><cell cols="2">GRU-EVE [1]</cell><cell cols="4">2019 InceptionResnetV2</cell><cell></cell><cell>C3D</cell><cell></cell><cell>YOLO</cell><cell>47.9 35.0 71.5 78.1 38.3 28.4 60.7 48.1</cell></row><row><cell></cell><cell cols="2">MGSA [4]</cell><cell cols="4">2019 InceptionResnetV2</cell><cell></cell><cell>C3D</cell><cell></cell><cell>-</cell><cell>53.4 35.0</cell><cell>-</cell><cell>86.7 42.4 27.6</cell><cell>-</cell><cell>47.5</cell></row><row><cell></cell><cell cols="2">POS+CG [34]</cell><cell cols="7">2019 InceptionResnetV2 OpticalFlow</cell><cell>-</cell><cell>52.5 34.1 71.3 88.7 42.0 28.2 61.6 48.7</cell></row><row><cell></cell><cell cols="6">POS+VCT [11] 2019 InceptionResnetV2</cell><cell></cell><cell>C3D</cell><cell></cell><cell>-</cell><cell>52.8 36.1 71.8 87.8 42.3 29.7 62.8 49.1</cell></row><row><cell></cell><cell cols="2">ORG-TRL</cell><cell cols="4">Ours InceptionResnetV2</cell><cell></cell><cell>C3D</cell><cell></cell><cell cols="2">FasterRCNN 54.3 36.4 73.9 95.2 43.6 28.8 62.1 50.9</cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell>B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Shared Enc [39]</cell><cell></cell><cell></cell><cell cols="4">28.9 21.9 47.4 46.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Shared Enc-Dec [39]</cell><cell></cell><cell cols="4">28.7 21.9 47.2 45.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Baseline(Ours)</cell><cell></cell><cell></cell><cell cols="4">30.2 21.3 47.9 44.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Baseline+ORG(Ours)</cell><cell></cell><cell cols="4">31.5 21.9 48.7 48.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Baseline+TRL(Ours)</cell><cell></cell><cell cols="4">31.5 22.1 48.7 49.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">Baseline+ORG+TRL(Ours) 32.1 22.2 48.9 49.7</cell><cell></cell><cell></cell></row><row><cell cols="2">Methods</cell><cell></cell><cell cols="2">MSVD</cell><cell></cell><cell cols="3">MSR-VTT</cell><cell></cell><cell></cell></row><row><cell cols="3">ORG TRL B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell><cell>B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell><cell></cell></row><row><cell>×</cell><cell>×</cell><cell cols="8">53.3 35.2 72.4 91.7 41.9 27.5 61.0 47.9</cell><cell></cell></row><row><cell></cell><cell>×</cell><cell cols="8">54.0 36.0 73.2 94.1 43.3 28.4 61.5 50.1</cell><cell></cell></row><row><cell>×</cell><cell></cell><cell cols="8">54.0 36.0 73.7 93.3 43.2 28.6 61.7 50.4</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">54.3 36.4 73.9 95.2 43.6 28.8 62.1 50.9</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Ablation Studies of the ORG and the TRL on MSVD and MSR-VTT benchmarks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ablation for two kinds of ORGs (top-half), and performance comparisons of the C-ORG with different top-k objects (bottom-half) on MSR-VTT.the first row of the table. Compared with the baseline model, both ORG and TRL achieve improvement when added alone. The combination of two methods can further enhance the performance which is illustrated as the last row.</figDesc><table><row><cell>Methods</cell><cell cols="2">Top-k B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell></row><row><cell>Baseline(B)</cell><cell>-</cell><cell cols="4">41.9 27.5 61.0 47.9</cell></row><row><cell>B+P-ORG</cell><cell>-</cell><cell cols="4">43.1 28.3 61.4 50.4</cell></row><row><cell>B+C-ORG</cell><cell>5</cell><cell cols="4">43.3 28.4 61.5 50.1</cell></row><row><cell>B+C-ORG</cell><cell>1</cell><cell cols="4">42.4 28.4 61.2 49.3</cell></row><row><cell>B+C-ORG</cell><cell>20</cell><cell cols="4">42.9 28.4 61.8 50.0</cell></row><row><cell>B+C-ORG</cell><cell>All</cell><cell cols="4">42.8 28.2 61.2 49.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://vatex.org/main/index.html 2 https://huggingface.co/transformers/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatio-temporal dynamics and semantic attribute enriched visual encoding for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayyer</forename><surname>Aafaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Syed Zulqarnain Gilani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12487" to="12496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Motion guided spatial attention for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8191" to="8198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Less is more: Picking informative frames for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weigang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="367" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Not all words are equal: Videospecific information loss for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Ç Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1503.03535</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint syntax representation learning and visual cue translation for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical global-local temporal modeling for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaosi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia, MM &apos;19</title>
		<meeting>the 27th ACM International Conference on Multimedia, MM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="774" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An analysis of incorporating an external language model into a sequenceto-sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5824" to="5828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Relationaware graph attention network for visual question answering. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sibnet: Sibling convolutional encoder for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference, MM 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1425" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Out of the box: Reasoning with graph convolution nets for factual visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Medhini</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2659" to="2670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning conditioned graph structures for interpretable visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Norcliffe-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stathis</forename><surname>Vafeias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8344" to="8353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4594" to="4602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Memory-attended recurrent network for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8347" to="8356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cold fusion: Training seq2seq models together with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graph attention networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1494" to="1504" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2015</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Controllable video captioning with pos sequence guidance based on gated fusion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bairui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reconstruction network for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bairui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7622" to="7631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">M3: multimodal memory modelling for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7512" to="7520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="413" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Vatex: A large-scale, highquality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MSR-VTT: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Catching the temporal regions-of-interest for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference, MM 2017</title>
		<meeting>the 2017 ACM on Multimedia Conference, MM 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="711" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4584" to="4593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5831" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Object-aware aggregation with bidirectional temporal graph for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8327" to="8336" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
