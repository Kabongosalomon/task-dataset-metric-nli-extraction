<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 DRNAS: DIRICHLET NEURAL ARCHITECTURE SEARCH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
							<email>xiangning@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Wang</surname></persName>
							<email>ruocwang@ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
							<email>mhcheng@ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Tang</surname></persName>
							<email>xiaochengtang@didiglobal.com</email>
							<affiliation key="aff1">
								<orgName type="department">DiDi AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
							<email>chohsieh@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 DRNAS: DIRICHLET NEURAL ARCHITECTURE SEARCH</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel differentiable architecture search method by formulating it into a distribution learning problem. We treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlet distribution. With recently developed pathwise derivatives, the Dirichlet parameters can be easily optimized with gradient-based optimizer in an end-to-end manner. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. Furthermore, to alleviate the large memory consumption of differentiable NAS, we propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of our method. Specifically, we obtain a test error of 2.46% for CIFAR-10, 23.7% for ImageNet under the mobile setting. On NAS-Bench-201, we also achieve state-of-the-art results on all three datasets and provide insights for the effective design of neural architecture search algorithms. * Equal Contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, Neural Architecture Search (NAS) has attracted lots of attentions for its potential to democratize deep learning. For a practical end-to-end deep learning platform, NAS plays a crucial role in discovering task-specific architecture depending on users' configurations (e.g., dataset, evaluation metric, etc.). Pioneers in this field develop prototypes based on reinforcement learning <ref type="bibr" target="#b49">(Zoph &amp; Le, 2017)</ref>, evolutionary algorithms <ref type="bibr" target="#b33">(Real et al., 2019)</ref> and Bayesian optimization <ref type="bibr" target="#b26">(Liu et al., 2018)</ref>. These works usually incur large computation overheads, which make them impractical to use. More recent algorithms significantly reduce the search cost including one-shot methods <ref type="bibr" target="#b32">(Pham et al., 2018;</ref><ref type="bibr" target="#b1">Bender et al., 2018)</ref>, a continuous relaxation of the space  and network morphisms <ref type="bibr" target="#b4">(Cai et al., 2018)</ref>. In particular,  proposes a differentiable NAS framework -DARTS, converting the categorical operation selection problem into learning a continuous architecture mixing weight. They formulate a bi-level optimization objective, allowing the architecture search to be efficiently performed by a gradient-based optimizer.</p><p>While current differentiable NAS methods achieve encouraging results, they still have shortcomings that hinder their real-world applications. Firstly, several works have cast doubt on the stability and generalization of these differentiable NAS methods <ref type="bibr" target="#b8">(Chen &amp; Hsieh, 2020;</ref><ref type="bibr" target="#b44">Zela et al., 2020a)</ref>. They discover that directly optimizing the architecture mixing weight is prone to overfitting the validation set and often leads to distorted structures, e.g., searched architectures dominated by parameter-free operations. Secondly, there exist disparities between the search and evaluation phases, where proxy tasks are usually employed during search with smaller datasets or shallower and narrower networks, due to the large memory consumption of differentiable NAS.</p><p>In this paper, we propose an effective approach that addresses the aforementioned shortcomings named Dirichlet Neural Architecture Search <ref type="bibr">(DrNAS)</ref>. Inspired by the fact that directly optimizing the architecture mixing weight is equivalent to performing point estimation (MLE/MAP) from a probabilistic perspective, we formulate the differentiable NAS as a distribution learning problem Published as a conference paper at ICLR 2021 instead, which naturally induces stochasticity and encourages exploration. Making use of the probability simplex property of the Dirichlet samples, DrNAS models the architecture mixing weight as random variables sampled from a parameterized Dirichlet distribution. Optimizing the Dirichlet objective can thus be done efficiently in an end-to-end fashion, by employing the pathwise derivative estimators to compute the gradient of the distribution <ref type="bibr" target="#b30">(Martin Jankowiak, 2018)</ref>. A straightforward optimization, however, turns out to be problematic due to the uncontrolled variance of the Dirichlet, i.e., too much variance leads to training instability and too little variance suffers from insufficient exploration. In light of that, we apply an additional distance regularizer directly on the Dirichlet concentration parameter to strike a balance between the exploration and the exploitation. We further derive a theoretical bound showing that the constrained distributional objective promotes stability and generalization of architecture search by implicitly controlling the Hessian of the validation error.</p><p>Furthermore, to enable a direct search on large-scale tasks, we propose a progressive learning scheme, eliminating the gap between the search and evaluation phases. Based on partial channel connection <ref type="bibr" target="#b39">(Xu et al., 2020)</ref>, we maintain a task-specific super-network of the same depth and number of channels as the evaluation phase throughout searching. To prevent loss of information and instability induced by partial connection, we divide the search phase into multiple stages and progressively increase the channel fraction via network transformation <ref type="bibr" target="#b7">(Chen et al., 2016)</ref>. Meanwhile, we prune the operation space according to the learnt distribution to maintain the memory efficiency.</p><p>We conduct extensive experiments on different datasets and search spaces to demonstrate DrNAS's effectiveness. Based on the DARTS search space , we achieve an average error rate of 2.46% on CIFAR-10, which ranks top amongst NAS methods. Furthermore, DrNAS achieves superior performance on large-scale tasks such as ImageNet. It obtains a top-1/5 error of 23.7%/7.1%, surpassing the previous state-of-the-art (24.0%/7.3%) under the mobile setting. On NAS-Bench-201 <ref type="bibr" target="#b14">(Dong &amp; Yang, 2020)</ref>, we also set new state-of-the-art results on all three datasets with low variance. Our code is available at https://github.com/xiangning-chen/DrNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE PROPOSED APPROACH</head><p>In this section, we first briefly review differentiable NAS setups and generalize the formulation to motivate distribution learning. We then layout our proposed DrNAS and describe its optimization in section 2.2. In section 2.3, we provide a generalization result by showing that our method implicitly regularizes the Hessian norm over the architecture parameter. The progressive architecture learning method that enables direct search is then described in section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PRELIMINARIES: DIFFERENTIABLE ARCHITECTURE SEARCH</head><p>Cell-Based Search Space The cell-based search space is constructed by replications of normal and reduction cells . A normal cell keeps the spatial resolution while a reduction cell halves it but doubles the number of channels. Every cell is represented by a DAG with N nodes and E edges, where every node represents a latent representation x i and every edge (i, j) is associated with an operations o (i,j) (e.g., max pooling or convolution) selected from a predefined candidate space O. The output of a node is a summation of all input flows, i.e., x j = i&lt;j o (i,j) (x i ), and a concatenation of intermediate node outputs, i.e., concat(x 2 , ..., x N −1 ), composes the cell output, where the first two input nodes x 0 and x 1 are fixed to be the outputs of previous two cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient-Based Search via Continuous Relaxation</head><p>To enable gradient-based optimization,  apply a continuous relaxation to the discrete space. Concretely, the information passed from node i to node j is computed by a weighted sum of all operations alone the edge, forming a mixed-operationô <ref type="bibr">(i,j)</ref> </p><formula xml:id="formula_0">(x) = o∈O θ (i,j) o o(x).</formula><p>The operation mixing weight θ (i,j) is defined over the probability simplex and its magnitude represents the strength of each operation. Therefore, the architecture search can be cast as selecting the operation associated with the highest mixing weight for each edge. To prevent abuse of terminology, we refer to θ as the architecture/operation mixing weight, and concentration parameter β in DrNAS as the architecture parameter throughout the paper.</p><p>Bilevel-Optimization with Simplex Constraints With continuous relaxation, the network weight w and operation mixing weight θ can be jointly optimized by solving a constraint bi-level optimization problem:</p><formula xml:id="formula_1">min θ L val (w * , θ) s.t. w * = arg min w L train (w, θ), |O| o=1 θ (i,j) o = 1, ∀ (i, j), i &lt; j,<label>(1)</label></formula><p>where the simplex constraint |O| o=1 θ (i,j) o = 1 can be either solved explicitly via Lagrangian function , or eliminated by substitution method (e.g., θ = Sof tmax(α), α ∈ R |O|×|E| ) . In the next section we describe how this generalized formulation motivates our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DIFFERENTIABLE ARCHITECTURE SEARCH AS DISTRIBUTION LEARNING</head><p>Learning a Distribution over Operation Mixing Weight Previous differentiable architecture search methods view the operation mixing weight θ as learnable parameters that can be directly optimized <ref type="bibr" target="#b39">Xu et al., 2020;</ref><ref type="bibr" target="#b25">Li et al., 2020)</ref>. This has been shown to cause θ to overfit the validation set and thus induce large generalization error <ref type="bibr" target="#b44">(Zela et al., 2020a;</ref><ref type="bibr" target="#b15">b;</ref><ref type="bibr" target="#b8">Chen &amp; Hsieh, 2020)</ref>. We recognize that this treatment is equivalent to performing point estimation (e.g., MLE/MAP) of θ in probabilistic view, which is inherently prone to overfitting <ref type="bibr" target="#b2">(Bishop, 2016;</ref><ref type="bibr" target="#b15">Gelman et al., 2004)</ref>. Furthermore, directly optimizing θ lacks sufficient exploration in the search space, and thus cause the search algorithm to commit to suboptimal paths in the DAG that converges faster at the beginning but plateaus quickly <ref type="bibr" target="#b34">(Shu et al., 2020)</ref>.</p><p>Based on these insights, we formulate the differentiable architecture search as a distribution learning problem. The operation mixing weight θ is treated as random variables sampled from a learnable distribution. Formally, let q(θ|β) denote the distribution of θ parameterized by β. The bi-level objective is then given by:</p><formula xml:id="formula_2">min β E q(θ|β) L val (w * , θ) + λd(β,β) s.t. w * = arg min w L train (w, θ).</formula><p>( <ref type="formula">2)</ref> where d(·, ·) is a distance function. Since θ lies on the probability simplex, we select Dirichlet distribution to model its behavior, i.e., q(θ|β) ∼ Dir(β), where β represents the Dirichlet concentration parameter. Dirichlet distribution is a widely used distribution over the probability simplex <ref type="bibr" target="#b21">(Joo et al., 2019;</ref><ref type="bibr" target="#b12">David M. Blei, 2003;</ref><ref type="bibr" target="#b23">Lee et al., 2020;</ref><ref type="bibr" target="#b22">Kessler et al., 2019)</ref>, and it enjoys nice properties that enables gradient-based training (Martin Jankowiak, 2018).</p><p>The concentration parameter β controls the sampling behavior of Dirichlet distribution and is crucial in balancing exploration and exploitation during the search phase. Let β o denote the concentration parameter assign to operation o. When β o 1 for most o = 1 ∼ |O|, Dirichlet tends to produce sparse samples with high variance, reducing the training stability; when β o 1 for most o = 1 ∼ |O|, the samples will be dense with low variance, leading to insufficient exploration. Therefore, we add a penalty term in the objective (2) to regularize the distance between β and the anchorβ = 1, which corresponds to the symmetric Dirichlet. In section 2.3, we also derive a theoretical bound showing that our formulation additionally promotes stability and generalization of the architecture search by implicitly regularizing the Hessian of validation loss w.r.t. architecture parameters.</p><p>Learning Dirichlet Parameters via Pathwise Derivative Estimator Optimizing objective (2) with gradient-based methods requires back-propagation through stochastic nodes of Dirichlet samples. The commonly used reparameterization trick does not apply to Dirichlet distribution, therefore we approximate the gradient of Dirichlet samples via pathwise derivative estimators (Martin Jankowiak, 2018)</p><formula xml:id="formula_3">dθ i dβ j = − ∂F Beta ∂βj (θ j |β j , β tot − β j ) f Beta (θ j |β j , β tot − β j ) × δ ij − θ i 1 − θ j i, j = 1, ..., |O|,<label>(3)</label></formula><p>where F Beta and f Beta denote the CDF and PDF of beta distribution respectively, δ ij is the indicator function, and β tot is the sum of concentrations. F Beta is the iregularised incomplete beta function, for which its gradient can be computed by simple numerical approximation. We refer to (Martin Jankowiak, 2018) for the complete derivations.</p><p>Joint Optimization of Model Weight and Architecture Parameter With pathwise derivative estimator, the model weight w and concentration β can be jointly optimized with gradient descent. Concretely, we draw a sample θ ∼ Dir(β) for every forward pass, and the gradients can be obtained easily through backpropagation. Following DARTS , we approximate w * in the lower level objective of equation 2 with one step of gradient descent, and run alternative updates between w * and β.</p><p>Selecting the Best Architecture At the end of the search phase, a learnt distribution of operation mixing weight is obtained. We then select the best operation for each edge by the most likely operation in expectation:</p><formula xml:id="formula_4">o (i,j) = arg max o∈O E q(θ (i,j) o |β (i,j) ) θ (i,j) o .<label>(4)</label></formula><p>In the Dirichlet case, the expectation term is simply the Dirichlet mean</p><formula xml:id="formula_5">β (i,j) o o β (i,j) o .</formula><p>Note that under the distribution learning framework, we are able to sample a wide range of architectures from the learnt distribution. This property alone has many potentials. For example, in practical settings where both accuracy and latency are concerned, the learnt distribution can be used to find architectures under resource restrictions in a post search phase. We leave these extensions to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">THE IMPLICIT REGULARIZATION ON HESSIAN</head><p>It has been observed that the generalization error of differentiable NAS is highly related to the dominant eigenvalue of the Hessian of validation loss w.r.t. architecture parameter. Several recent works report that the large dominant eigenvalue of ∇ 2 θL val (w, θ) in DARTS results in poor generalization performance <ref type="bibr" target="#b44">(Zela et al., 2020a;</ref><ref type="bibr" target="#b8">Chen &amp; Hsieh, 2020)</ref>. Our objective <ref type="formula">(2)</ref> is the Lagrangian function of the following constraint objective:</p><formula xml:id="formula_6">min β E q(θ|β) L val (w * , θ) s.t. w * = arg min w L train (w, θ) , d(β,β) ≤ δ,<label>(5)</label></formula><p>Here we derive an approximated lower bound based on (5), which demonstrates that our method implicitly controls this Hessian matrix.</p><p>Proposition 1 Let d(β,β) = β −β 2 ≤ δ andβ = 1 in the bi-level formulation (5). Let µ denote the mean under the Laplacian approximation of Dirichlet. If ∇ 2 µLval (w * , µ) is Positive Semi-definite, the upper-level objective can be approximated bounded by:</p><formula xml:id="formula_7">E q(θ|β) (L val (w, θ)) L val (w * , µ) + 1 2 ( 1 1 + δ (1 − 2 |O| ) + 1 |O| 1 1 + δ )tr ∇ 2 µLval (w * , µ) (6) with:L val (w * , µ) = L val (w * , Sof tmax(µ)), µ o = log β o − 1 |O| o log β o , o = 1, . . . , |O|.</formula><p>This proposition is driven by the Laplacian approximation to the Dirichlet distribution <ref type="bibr" target="#b29">(MacKay, 1998;</ref><ref type="bibr" target="#b0">Akash Srivastava, 2017)</ref>. The lower bound <ref type="formula">(6)</ref> indicates that minimizing the expected validation loss controls the trace norm of the Hessian matrix. Empirically, we observe that DrNAS always maintains the dominant eigenvalue of Hessian at a low level (Appendix A.4). The detailed proof can be found in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">PROGRESSIVE ARCHITECTURE LEARNING</head><p>The GPU memory consumption of differentiable NAS methods grows linearly with the size of operation candidate space. Therefore, they usually use a easier proxy task such as training with a smaller dataset, or searching with fewer layers and number of channels <ref type="bibr" target="#b5">(Cai et al., 2019)</ref>. For instance, the architecture search is performed on 8 cells and 16 initial channels in DARTS . But during evaluation, the network has 20 cells and 36 initial channels. Such gap makes it hard to derive an optimal architecture for the target task <ref type="bibr" target="#b5">(Cai et al., 2019)</ref>.</p><p>PC-DARTS <ref type="bibr" target="#b39">(Xu et al., 2020)</ref> proposes a partial channel connection to reduce the memory overheads of differentiable NAS, where they only send a random subset of channels to the mixed-operation while directly bypassing the rest channels in a shortcut. However, their method causes loss of information and makes the selection of operation unstable since the sampled subsets may vary widely across iterations. This drawback is amplified when combining with the proposed method since we learn the architecture distribution from Dirichlet samples, which already injects certain stochasticity. As shown in <ref type="table" target="#tab_0">Table 1</ref>, when directly applying partial channel connection with distribution learning, the test accuracy of the searched architecture decreases over 3% and 18% on CIFAR-10 and CIFAR-100 respectively if we send only 1/8 channels to the mixed-operation.</p><p>To alleviate such information loss and instability problem while being memory-efficient, we propose a progressive learning scheme which gradually increases the fraction of channels that are forwarded to the mixed-operation and meanwhile prunes the operation space based on the learnt distribution. We split the search process into consecutive stages and construct a task-specific super-network with the same depth and number of channels as the evaluation phase at the initial stage. Then after each stage, we increase the partial channel fraction, which means that the super-network in the next stage will be wider, i.e., have more convolution channels, and in turn preserve more information. This is achieved by enlarging every convolution weight with a random mapping function similar to Net2Net <ref type="bibr" target="#b7">(Chen et al., 2016)</ref>. The mapping function g : {1, 2, . . . , q} → {1, 2, . . . , n} with q &gt; n is defined as</p><formula xml:id="formula_8">g(j) = j j ≤ n random sample from {1, 2, . . . , n} j &gt; n<label>(7)</label></formula><p>To widen layer l, we replace its convolution weight</p><formula xml:id="formula_9">W (l) ∈ R Out×In×H×W with a new weight U (l) . U (l) o,i,h,w = W (l) g(o),g(i),h,w ,<label>(8)</label></formula><p>where Out, In, H, W denote the number of output and input channels, filter height and width respectively. Intuitively, we copy W (l) directly into U (l) and fulfill the rest part by choosing randomly as defined in g. Unlike Net2Net, we do not divide U (l) by a replication factor here because the information flow on each edge has the same scale no matter the partial fraction is. After widening the super-network, we reduce the operation space by pruning out less important operations according to the Dirichlet concentration parameter β learnt from the previous stage, maintaining a consistent memory consumption. As illustrated in <ref type="table" target="#tab_0">Table 1</ref>, the proposed progressive architecture learning scheme effectively discovers high accuracy architectures and retains a low GPU memory overhead. Early methods in NAS usually include a full training and evaluation procedure every iteration as the inner loop to guide the consecutive search <ref type="bibr" target="#b49">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b33">Real et al., 2019)</ref>. Consequently, their computational overheads are beyond acceptance for practical usage, especially on large-scale tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DISCUSSIONS AND RELATIONSHIP TO PRIOR WORK</head><p>Differentiable NAS Recently, many works are proposed to improve the efficiency of NAS <ref type="bibr" target="#b32">(Pham et al., 2018;</ref><ref type="bibr" target="#b4">Cai et al., 2018;</ref><ref type="bibr" target="#b1">Bender et al., 2018;</ref><ref type="bibr" target="#b43">Yao et al., 2020b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b31">Mei et al., 2020)</ref>. Amongst them, DARTS  proposes a differentiable NAS framework, which introduces a continuous architecture parameter that relaxes the discrete search space. Despite being efficient, DARTS only optimizes a single point on the simplex every search epoch, which has no guarantee to generalize well after the discretization during evaluation. So its stability and generalization have been widely challenged <ref type="bibr" target="#b24">(Li &amp; Talwalkar, 2019;</ref><ref type="bibr" target="#b44">Zela et al., 2020a;</ref><ref type="bibr" target="#b8">Chen &amp; Hsieh, 2020;</ref><ref type="bibr" target="#b37">Wang et al., 2021)</ref>. Following DARTS, SNAS  and GDAS <ref type="bibr" target="#b13">(Dong &amp; Yang, 2019)</ref> leverage the gumbel-softmax trick to learn the exact architecture parameter. However, their reparameterization is motivated from reinforcement learning perspective, which is an approximation with softmax rather than an architecture distribution. Besides, their methods require tuning of temperature schedule <ref type="bibr" target="#b40">(Yan et al., 2017;</ref><ref type="bibr" target="#b3">Caglar Gulcehre, 2017)</ref>. GDAS linearly decreases the temperature from 10 to 1 while SNAS anneals it from 1 to 0.03. In comparison, the proposed method can automatically learn the architecture distribution without the requirement of handcrafted scheduling. BayesNAS <ref type="bibr" target="#b48">(Zhou et al., 2019)</ref> applies Bayesian Learning in NAS. Specifically, they cast NAS as model compression problem and use Bayes Neural Network as the super-network, which is difficult to optimize and requires oversimplified approximation. While our method considers the stochasticity in architecture mixing weight, as it is directly related to the generalization of differentiable NAS algorithms <ref type="bibr" target="#b44">(Zela et al., 2020a;</ref><ref type="bibr" target="#b8">Chen &amp; Hsieh, 2020)</ref>.  <ref type="bibr" target="#b13">(Dong &amp; Yang, 2019)</ref> and DSNAS <ref type="bibr" target="#b19">(Hu et al., 2020)</ref> both enforce a discrete constraint after the gumbel-softmax reparametrization. However, such discretization manifests premature convergence and cause search instability <ref type="bibr" target="#b45">(Zela et al., 2020b;</ref>. Our experiments in section 4.3 also empirically demonstrate this phenomenon. As an alternative, PC-DARTS <ref type="bibr" target="#b39">(Xu et al., 2020)</ref> proposes a partial channel connection, where only a portion of channels is sent to the mixed-operation. However, partial connection can cause loss of information as shown in section 2.4 and PC-DARTS searches on a shallower network with less channels, suffering the search and evaluation gap. Our solution, by progressively pruning the operation space and meanwhile widening the network, searches in a task-specific manner and achieves superior accuracy on challenging datasets like ImageNet (+2.8% over BayesNAS, +2.3% over GDAS, +2.3% over PARSEC, +2.0% over DSNAS, +1.2% over ProxylessNAS, and +0.5% over PC-DARTS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory overhead</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate our proposed DrNAS on two search spaces: the CNN search space in DARTS  and NAS-Bench-201 <ref type="bibr" target="#b14">(Dong &amp; Yang, 2020)</ref>. For DARTS space, we conduct experiments on both CIFAR-10 and ImageNet in section 4.1 and 4.2 respectively. For NAS-Bench-201, we test all 3 supported datasets (CIFAR-10, CIFAR-100, ImageNet-16-120 <ref type="bibr" target="#b10">(Chrabaszcz et al., 2017)</ref>) in section 4.3. Furthermore, we empirically study the dynamics of exploration and exploitation throughout the search process in section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RESULTS ON CIFAR-10</head><p>Architecture Space For both search and evaluation phases, we stack 20 cells to compose the network and set the initial channel number as 36. We place the reduction cells at the 1/3 and 2/3 of the network and each cell consists of N = 6 nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Settings</head><p>We equally divide the 50K training images into two parts, one is used for optimizing the network weights by momentum SGD and the other for learning the Dirichlet architecture distribution by an Adam optimizer. Since Dirichlet concentration β must be positive, we apply the shifted exponential linear mapping β = ELU(η) + 1 and optimize over η instead. We use l 2 norm to constrain the distance between η and the anchorη = 0. The η is initialized by standard Gaussian with scale 0.001, and λ in (2) is set to 0.001. The ablation study in Appendix A.3 reveals the effectiveness of our anchor regularizer, and DrNAS is insensitive to a wide range of λ. These settings are consistent for all experiments. For progressive architecture learning, the whole search process consists of 2 stages, each with 25 iterations. In the first stage, we set the partial channel parameter K as 6 to fit the super-network into a single GTX 1080Ti GPU with 11GB memory, i.e., only 1/6 features are sampled on each edge. For the second stage, we prune half candidates and meanwhile widen the network twice, i.e., the operation space size reduces from 8 to 4 and K becomes 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrain Settings</head><p>The evaluation phase uses the entire 50K training set to train the network from scratch for 600 epochs. The network weight is optimized by an SGD optimizer with a cosine  <ref type="bibr" target="#b20">(Huang et al., 2017)</ref> 3.46 25.6 -manual NASNet-A  2.65 3.3 2000 RL AmoebaNet-A <ref type="bibr" target="#b33">(Real et al., 2019)</ref> 3.34 ± 0.06 3.2 3150 evolution AmoebaNet-B <ref type="bibr" target="#b33">(Real et al., 2019)</ref> 2.55 ± 0.05 2.8 3150 evolution PNAS <ref type="bibr" target="#b26">(Liu et al., 2018)</ref> 3.41 ± 0.09 3.2 225 SMBO ENAS <ref type="bibr" target="#b32">(Pham et al., 2018)</ref> 2.89 4.6 0.5 RL DARTS (1st)  3.00 ± 0.14 3.3 0.4 gradient DARTS (2nd)  2.76 ± 0.09 3.3 1.0 gradient SNAS (moderate)  2.85 ± 0.02 2.8 1.5 gradient GDAS <ref type="bibr" target="#b13">(Dong &amp; Yang, 2019)</ref> 2.93 3.4 0.3 gradient BayesNAS <ref type="bibr" target="#b48">(Zhou et al., 2019)</ref> 2. annealing learning rate initialized as 0.025, a momentum of 0.9, and a weight decay of 3 × 10 −4 . To allow a fair comparison with previous work, we also employ cutout regularization with length 16, drop-path  with probability 0.3 and an auxiliary tower of weight 0.4.</p><p>Results <ref type="table" target="#tab_2">Table 2</ref> summarizes the performance of DrNAS compared with other popular NAS methods, and we also visualize the searched cells in Appendix A.2. DrNAS achieves an average test error of 2.46%, ranking top amongst recent NAS results. ProxylessNAS is the only method that achieves lower test error than us, but it searches on a different space with a much longer search time and has larger model size. We also perform experiments to assign proper credit to the two parts of our proposed algorithm, i.e., Dirichlet architecture distribution and progressive learning scheme. When searching on a proxy task with 8 stacked cells and 16 initial channels as the convention <ref type="bibr" target="#b39">Xu et al., 2020)</ref>, we achieve a test error of 2.54% that surpasses most baselines. Our progressive learning algorithm eliminates the gap between the proxy and target tasks, which further reduces the test error. Consequently, both of the two parts contribute a lot to our performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESULTS ON IMAGENET</head><p>Architecture Space The network architecture for ImageNet is slightly different from that for CIFAR-10 in that we stack 14 cells and set the initial channel number as 48. We also first downscale the spatial resolution from 224 × 224 to 28 × 28 with three convolution layers of stride 2 following previous works <ref type="bibr" target="#b39">(Xu et al., 2020;</ref>. The other settings are the same with section 4.1.</p><p>Search Settings Following PC-DARTS <ref type="bibr" target="#b39">(Xu et al., 2020)</ref>, we randomly sample 10% and 2.5% images from the 1.3M training set to alternatively learn network weight and Dirichlet architecture distribution by a momentum SGD and an Adam optimizer respectively. We use 8 RTX 2080 Ti GPUs for both search and evaluation, and the setup of progressive pruning is the same with that on CIFAR-10, i.e., 2 stages with operation space size shrinking from 8 to 4, and the partial channel K reduces from 6 to 3.</p><p>Retrain Settings For architecture evaluation, we train the network for 250 epochs by an SGD optimizer with a momentum of 0.9, a weight decay of 3 × 10 −5 , and a linearly decayed learning rate initialized as 0.5. We also use label smoothing and an auxiliary tower of weight 0.4 during training. The learning rate warm-up is employed for the first 5 epochs following previous works <ref type="bibr" target="#b39">Xu et al., 2020)</ref>.  <ref type="bibr" target="#b35">(Szegedy et al., 2015)</ref> 30.1 10.1 6.6 -manual MobileNet <ref type="bibr" target="#b18">(Howard et al., 2017)</ref> 29.4 10.5 4.2 -manual ShuffleNet 2× (v1)  26.4 10.2 ∼ 5 -manual ShuffleNet 2× (v2) <ref type="bibr" target="#b28">(Ma et al., 2018)</ref> 25.1 -∼ 5 -manual NASNet-A  26.0 8.4 5.3 2000 RL AmoebaNet-C <ref type="bibr" target="#b33">(Real et al., 2019)</ref> 24.3 7.6 6.4 3150 evolution PNAS <ref type="bibr" target="#b26">(Liu et al., 2018)</ref> 25.8 8.1 5.1 225 SMBO MnasNet-92 <ref type="bibr" target="#b36">(Tan et al., 2019)</ref> 25.2 8.0 4.4 -RL DARTS (2nd)  26.7 8.7 4.7 1.0 gradient SNAS (mild)  27.3 9.2 4.3 1.5 gradient GDAS <ref type="bibr" target="#b13">(Dong &amp; Yang, 2019)</ref> 26.0 8.5 5.3 0.3 gradient BayesNAS <ref type="bibr" target="#b48">(Zhou et al., 2019)</ref> 26.5 8.9 3.9 0.2 gradient DSNAS <ref type="bibr" target="#b19">(Hu et al., 2020</ref> Results As shown in <ref type="table" target="#tab_4">Table 3</ref>, we achieve a top-1/5 test error of 23.7%/7.1%, outperforming all compared baselines and achieving state-of-the-art performance in the ImageNet mobile setting. The searched cells are visualized in Appendix A.2. Similar to section 4.1, we also report the result achieved with 8 cells and 16 initial channels, which is a common setup for the proxy task on ImageNet <ref type="bibr" target="#b39">(Xu et al., 2020)</ref>. The obtained 24.2% top-1 accuracy is already highly competitive, which demonstrates the effectiveness of the architecture distribution learning on large-scale tasks. Then our progressive learning scheme further increases the top-1/5 accuracy for 0.5%/0.2%. Therefore, learning in a task-specific manner is essential to discover better architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RESULTS ON NAS-BENCH-201</head><p>Recently, some researchers doubt that the expert knowledge applied to the evaluation protocol plays an important role in the impressive results achieved by leading NAS methods <ref type="bibr" target="#b24">Li &amp; Talwalkar, 2019)</ref>. So to further verify the effectiveness of DrNAS, we perform experiments on NAS-Bench-201 <ref type="bibr" target="#b14">(Dong &amp; Yang, 2020)</ref>, where architecture performance can be directly obtained by querying in the database. NAS-Bench-201 provides support for 3 dataset (CIFAR-10, CIFAR-100, ImageNet-16-120 <ref type="bibr" target="#b10">(Chrabaszcz et al., 2017)</ref>) and has a unified cell-based search space containing 15,625 architectures. We refer to their paper <ref type="bibr" target="#b14">(Dong &amp; Yang, 2020)</ref> for details of the space. Our experiments are performed in a task-specific manner, i.e., the search and evaluation are based on the same dataset. The hyperparameters for all compared methods are set as their default and for DrNAS, we use the same search settings with section 4.1. We run every method 4 independent times with different random seeds and report the mean and standard deviation in <ref type="table" target="#tab_6">Table 4</ref>.</p><p>As shown, we achieve the best accuracy on all 3 datasets. On CIFAR-100, we even achieve the global optimal. Specifically, DrNAS outperforms DARTS, GDAS, DSNAS, PC-DARTS, and SNAS by 103.8%, 35.9%, 30.4%, 6.4%, and 4.3% on average. We notice that the two methods (GDAS and DSNAS) that enforce a discrete constraint, i.e., only sample a single path every search iteration, perform undesirable especially on CIFAR-100. In comparison, SNAS, employing a similar Gumbelsoftmax trick but without the discretization, performs much better. Consequently, a discrete constraint during search can reduce the GPU memory consumption but empirically suffers instability. In comparison, we develop the progressive learning scheme on top of the architecture distribution learning, enjoying both memory efficiency and strong search performance.  <ref type="bibr" target="#b24">(Li &amp; Talwalkar, 2019)</ref> 84.16 ± 1.69 87.66 ± 1.69 45.78 ± 6.33 46.60 ± 6.57 31.09 ± 5.65 30.78 ± 6.12 Reinforce  91.09 ± 0.37 93.85 ± 0.37 70.05 ± 1.67 70.17 ± 1.61 43.04 ± 2.18 43.16 ± 2.28 ENAS <ref type="bibr" target="#b32">(Pham et al., 2018)</ref> 39.77 ± 0.00 54.30 ± 0.00 10.23 ± 0.12 10.62 ± 0.27 16.43 ± 0.00 16.32 ± 0.00 DARTS (1st)  39.77 ± 0.00 54.30 ± 0.00 38.57 ± 0.00 38.97 ± 0.00 18.87 ± 0.00 18.41 ± 0.00 DARTS (2nd)  39.77 ± 0.00 54.30 ± 0.00 38.57 ± 0.00 38.97 ± 0.00 18.87 ± 0.00 18.41 ± 0.00 GDAS <ref type="bibr" target="#b13">(Dong &amp; Yang, 2019)</ref> 90.01 ± 0.46 93.23 ± 0.23 24.05 ± 8.12 24.20 ± 8.08 40.66 ± 0.00 41.02 ± 0.00 SNAS  90.10 ± 1.04 92.77 ± 0.83 69.69 ± 2.39 69.34 ± 1.98 42.84 ± 1.79 43.16 ± 2.64 DSNAS <ref type="bibr" target="#b19">(Hu et al., 2020)</ref> 89.66 ± 0.29 93.08 ± 0.13 30.87 ± 16.40 31.01 ± 16.38 40.61 ± 0.09 41.07 ± 0.09 PC-DARTS <ref type="bibr" target="#b39">(Xu et al., 2020)</ref> 89 We further conduct an empirical study on the dynamics of exploration and exploitation in the search phase of DrNAS on NAS-Bench-201. After every search epoch, We sample 100 θs from the learned Dirichlet distribution and take the arg max to obtain 100 discrete architectures. We then plot the range of their accuracy along with the architecture selected by Dirichlet mean (solid line in <ref type="figure">Figure 1</ref>). Note that in our algorithm, we simply derive the architecture according to the Dirichlet mean as described in Section 2.2. As shown in <ref type="figure">Figure 1</ref>, the accuracy range of the sampled architectures starts very wide but narrows gradually during the search phase. It indicates that DrNAS learns to encourage exploration in the search space at the early stages and then gradually reduces it towards the end as the algorithm becomes more and more confident of the current choice. Moreover, the performance of our architectures can consistently match the best performance of the sampled architectures, indicating the effectiveness of DrNAS.</p><p>(a) CIFAR-10 (b) CIFAR-100 (c) ImageNet16-120 <ref type="figure">Figure 1</ref>: Accuracy range (min-max) of the 100 sampled architectures. Note that the solid line is our derived architecture according to the Dirichlet mean as described in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose Dirichlet Neural Architecture Search (DrNAS). We formulate the differentiable NAS as a constraint distribution learning problem, which explicitly models the stochasticity in the architecture mixing weight and balances exploration and exploitation in the search space. The proposed method can be optimized efficiently via gradient-based algorithm, and possesses theoretical benefit to improve the generalization ability. Furthermore, we propose a progressive learning scheme to eliminate the search and evaluation gap. DrNAS consistently achieves strong performance across several image classification tasks, which reveals its potential to play a crucial role in future end-to-end deep learning platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 PROOF OF PROPOSITION 1</p><p>Preliminaries: Before the development of Pathwise Derivative Estimator, Laplace Approximate with Softmax basis has been extensively used to approximate the Dirichlet Distribution <ref type="bibr" target="#b29">(MacKay, 1998;</ref><ref type="bibr" target="#b0">Akash Srivastava, 2017)</ref>. The approximated Dirichlet distribution is:</p><formula xml:id="formula_10">p(θ(h)|β) = Γ( o β o ) o Γ(β o ) o θ βo o g(1 T h)<label>(9)</label></formula><p>Where θ(h) is the softmax-transformed h, h follows multivariate normal distribution, and g(·) is an arbitrary density to ensure integrability (Akash Srivastava, 2017). The mean µ and diagonal covariance matrix Σ of h depends on the Dirichlet concentration parameter β:</p><formula xml:id="formula_11">µ o = log β o − 1 |O| o log β o Σ o = 1 β o (1 − 2 |O| ) + 1 |O| 2 o 1 β o<label>(10)</label></formula><p>It can be directly obtained from (10) that the Dirichlet mean βo o β o = Sof tmax(µ). Sampling from the approximated distribution can be down by first sampling from h and then applying Softmax function to obtain θ. We will leverage the fact that this approximation supports explicit reparameterization to derive our proof.</p><p>Proof: Apply the above Laplace Approximation to Dirichlet distribution, the unconstrained upperlevel objective in (5) can then be written as:</p><formula xml:id="formula_12">E θ∼Dir(β) L val (w * , θ) (11) ≈E ∼N (0,Σ) L val (w * , Sof tmax(µ + ))<label>(12)</label></formula><formula xml:id="formula_13">≡E ∼N (0,Σ) L val (w * , µ + )<label>(13)</label></formula><formula xml:id="formula_14">≈E ∼N (0,Σ) L val (w * , µ) + T ∇ µLval (w * , µ) + 1 2 T ∇ 2 µLval (w * , µ) (14) =L val (w * , µ) + 1 2 tr E ∼N (0,Σ) T ∇ 2 µLval (w * , µ)<label>(15)</label></formula><p>=L val (w * , µ) + 1 2 tr Σ∇ 2 µLval (w * , µ)</p><p>In our full objective, we constrain the Euclidean distance between learnt Dirichlet concentration and fixed prior concentration ||β − 1|| 2 ≤ δ. The covariance matrix Σ of approximated softmax Gaussian can be bounded as:</p><formula xml:id="formula_16">Σ o = 1 β o (1 − 2 |O| ) + 1 |O| 2 o 1 β o (17) ≥ 1 1 + δ (1 − 2 |O| ) + 1 |O| 1 1 + δ<label>(18)</label></formula><p>Then <ref type="formula" target="#formula_1">(11)</ref> becomes:</p><formula xml:id="formula_17">E θ∼Dir(β) L val (w * , θ) (19) ≈L val (w * , µ) + 1 2 tr Σ∇ 2 µLval (w * , µ) (20) ≥L val (w * , µ) + 1 2 ( 1 1 + δ (1 − 2 |O| ) + 1 |O| 1 1 + δ )tr ∇ 2 µLval (w * , µ)<label>(21)</label></formula><p>The last line holds when ∇ 2 µLval (w * , µ) is positive semi-definite. In Appendix A.4 we provide an empirical justification for this implicit regularization effect of DrNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 SEARCHED ARCHITECTURES</head><p>We visualize the searched normal and reduction cells in <ref type="figure">Figure 2</ref> and 3, which is directly searched on CIFAR-10 and ImageNet respectively.  <ref type="table" target="#tab_8">Table 5</ref> shows the accuracy of the searched architecture using different value of λ while keeping all other settings the same. Using anchor regularizer? for a wide range of value can boost the accuracy and DrNAS performs quite stable under different λs. We track the anytime Hessian norm on NAS-Bench-201 in <ref type="figure" target="#fig_1">Figure 4</ref>. The result is obtained by averaging from 4 independent runs. We observe that the largest eigenvalue expands about 10 times when searching by DARTS for 100 epochs. In comparison, DrNAS always maintains the Hessian norm at a low level, which is in agreement with our theoretical analysis in section 2.3. <ref type="figure" target="#fig_2">Figure 5</ref> shows the regularization effect under various λs. As we can see, DrNAS can keep hessian norm at a low level for a wide range of λs, which is in accordance to the relatively stable performance in <ref type="table" target="#tab_8">Table 5</ref>.</p><p>Moreover, we compare DrNAS with DARTS and R-DARTS on 4 simplified space proposed in <ref type="bibr" target="#b44">(Zela et al., 2020a)</ref> and record the endpoint dominant eigenvalue. The first space S1 contains 2 popular operators per edge based on DARTS search result. For S2, S3, and S4, the operation sets are {3 × 3 separable convolution, skip connection}, {3 × 3 separable convolution, skip connection, zero}, and {3 × 3 separable convolution, noise} respectively. As shown in <ref type="table" target="#tab_9">Table 6</ref>, DrNAS consistently outperforms DARTS and R-DARTS. The endpoint eigenvalues for DrNAS are 0.0392, 0.0390, 0.0286, 0.0389 respectively. <ref type="figure" target="#fig_2">Figure 5</ref> shows the Hessian norm trajectory under various λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 CONNECTION TO VARIATIONAL INFERENCE</head><p>In this section, we draw a connection between DrNAS and Variational Inference <ref type="bibr" target="#b11">(David M. Blei, 2016)</ref>. We use w, θ, and β to denote the model weight, operation mixing weight, and Dirichlet concentration parameters respectively, following the main text. The true posterior distribution can  be written as p(θ|w, D), where D = {x n , y n } N n=1 is the dataset. Let q(θ|β) denote the variational approximation of the true posterior; and assume that q(θ|β) follows Dirichlet distribution. We follow <ref type="bibr" target="#b21">Joo et al. (2019)</ref> to assume a symmetric Dirichlet distribution for the prior p(θ) as well, i.e., p(θ) = Dir(1). The goal is to minimize the KL divergence between the true posterior and the approximated form, i.e., min β KL(q(θ|β)||p(θ|w, D)). It can be shown that this objective is equivalent to maximizing the evidence lower bound as below <ref type="bibr" target="#b11">(David M. Blei, 2016)</ref>:</p><p>L(β) = E q(θ|β) log p(D|θ, w) − KL(q(θ|β)||p(θ|w))</p><p>The upper level objective of the bilevel optimization under variational inference framework is then given as:</p><p>min β E q(θ|β) − log p(D valid |θ, w * ) + KL(q(θ|β)||p(θ))</p><p>Note that eq. (23) resembles eq. (2) if we use the negative log likelihood as the loss function and replace d(·, ·) with KL divergence. In practice, we find that using a simple l2 distance regularization works well across datasets and search spaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Normal and Reduction cells discovered by DrNAS on CIFAR-10. Normal and Reduction cells discovered by DrNAS on ImageNet. A.3 ABLATION STUDY ON ANCHOR REGULARIZER PARAMETER λ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Trajectory of the Hessian norm on NAS-Bench-201 when searching with CIFAR-10 (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Trajectory of the Hessian norm under various λs on NAS-Bench-201 when searching with CIFAR-10 (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Test accuracy of the derived</cell></row><row><cell cols="3">architectures when searching on NAS-</cell></row><row><cell cols="3">Bench-201 with different partial channel</cell></row><row><cell cols="3">fraction, where 1/K channels are sent</cell></row><row><cell cols="2">to the mixed-operation.</cell><cell></cell></row><row><cell></cell><cell>CIFAR-10</cell><cell></cell></row><row><cell>K</cell><cell>Test Accuracy (%)</cell><cell>GPU Memory (MB)</cell></row><row><cell>1</cell><cell>94.36 ± 0.00</cell><cell>2437</cell></row><row><cell>2</cell><cell>93.49 ± 0.28</cell><cell>1583</cell></row><row><cell>4</cell><cell>92.85 ± 0.35</cell><cell>1159</cell></row><row><cell>8</cell><cell>91.06 ± 0.00</cell><cell>949</cell></row><row><cell>Ours</cell><cell>94.36 ± 0.00</cell><cell>949</cell></row><row><cell></cell><cell cols="2">CIFAR-100</cell></row><row><cell>K</cell><cell>Test Accuracy (%)</cell><cell>GPU Memory (MB)</cell></row><row><cell>1</cell><cell>73.51 ± 0.00</cell><cell>2439</cell></row><row><cell>2</cell><cell>68.48 ± 0.41</cell><cell>1583</cell></row><row><cell>4</cell><cell>66.68 ± 3.22</cell><cell>1161</cell></row><row><cell>8</cell><cell>55.11 ± 13.78</cell><cell>949</cell></row><row><cell>Ours</cell><cell>73.51 ± 0.00</cell><cell>949</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>When dealing with the large memory consumption of differentiable NAS, previous works mainly restrain the number of paths sampled during the search phase. For instance, ProxylessNAS (Cai et al., 2019) employs binary gates and samples two paths every search epoch. PARSEC (Casale et al., 2019) samples discrete architectures according to a categorical distribution to save memory. Similarly, GDAS</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art image classifiers on CIFAR-10.</figDesc><table><row><cell>Architecture</cell><cell>Test Error (%)</cell><cell>Params (M)</cell><cell>Search Cost (GPU days)</cell><cell>Search Method</cell></row><row><cell>DenseNet-BC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Obtained on a different space with PyramidNet<ref type="bibr" target="#b16">(Han et al., 2017)</ref> as the backbone.</figDesc><table><row><cell></cell><cell>81 ± 0.04</cell><cell>3.4</cell><cell>0.2</cell><cell>gradient</cell></row><row><cell>ProxylessNAS (Cai et al., 2019)  †</cell><cell>2.08</cell><cell>5.7</cell><cell>4.0</cell><cell>gradient</cell></row><row><cell>PARSEC (Casale et al., 2019)</cell><cell>2.81 ± 0.03</cell><cell>3.7</cell><cell>1</cell><cell>gradient</cell></row><row><cell>P-DARTS (Chen et al., 2019)</cell><cell>2.50</cell><cell>3.4</cell><cell>0.3</cell><cell>gradient</cell></row><row><cell>PC-DARTS (Xu et al., 2020)</cell><cell>2.57 ± 0.07</cell><cell>3.6</cell><cell>0.1</cell><cell>gradient</cell></row><row><cell cols="2">SDARTS-ADV (Chen &amp; Hsieh, 2020) 2.61 ± 0.02</cell><cell>3.3</cell><cell>1.3</cell><cell>gradient</cell></row><row><cell cols="2">GAEA + PC-DARTS (Li et al., 2020) 2.50 ± 0.06</cell><cell>3.7</cell><cell>0.1</cell><cell>gradient</cell></row><row><cell cols="2">DrNAS (without progressive learning) 2.54 ± 0.03</cell><cell>4.0</cell><cell>0.4  ‡</cell><cell>gradient</cell></row><row><cell>DrNAS</cell><cell>2.46 ± 0.03</cell><cell>4.1</cell><cell>0.6  ‡</cell><cell>gradient</cell></row><row><cell cols="2">Obtained without cutout augmentation.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>†‡ Recorded on a single GTX 1080Ti GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art image classifiers on ImageNet in the mobile setting.</figDesc><table><row><cell>Architecture</cell><cell>Test Error(%) Params (M) top-1 top-5</cell><cell>Search Cost (GPU days)</cell><cell>Search Method</cell></row><row><cell>Inception-v1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The architecture is searched on ImageNet, otherwise it is searched on CIFAR-10 or CIFAR-100.</figDesc><table><row><cell>)  †</cell><cell>25.7</cell><cell>8.1</cell><cell>-</cell><cell>-</cell><cell>gradient</cell></row><row><cell>ProxylessNAS (GPU) (Cai et al., 2019)  †</cell><cell>24.9</cell><cell>7.5</cell><cell>7.1</cell><cell>8.3</cell><cell>gradient</cell></row><row><cell>PARSEC (Casale et al., 2019)</cell><cell>26.0</cell><cell>8.4</cell><cell>5.6</cell><cell>1</cell><cell>gradient</cell></row><row><cell>P-DARTS (CIFAR-10) (Chen et al., 2019)</cell><cell>24.4</cell><cell>7.4</cell><cell>4.9</cell><cell>0.3</cell><cell>gradient</cell></row><row><cell cols="2">P-DARTS (CIFAR-100) (Chen et al., 2019) 24.7</cell><cell>7.5</cell><cell>5.1</cell><cell>0.3</cell><cell>gradient</cell></row><row><cell>PC-DARTS (CIFAR-10) (Xu et al., 2020)</cell><cell>25.1</cell><cell>7.8</cell><cell>5.3</cell><cell>0.1</cell><cell>gradient</cell></row><row><cell>PC-DARTS (ImageNet) (Xu et al., 2020)  †</cell><cell>24.2</cell><cell>7.3</cell><cell>5.3</cell><cell>3.8</cell><cell>gradient</cell></row><row><cell>GAEA + PC-DARTS (Li et al., 2020)  †</cell><cell>24.0</cell><cell>7.3</cell><cell>5.6</cell><cell>3.8</cell><cell>gradient</cell></row><row><cell>DrNAS (without progressive learning)  †</cell><cell>24.2</cell><cell>7.3</cell><cell>5.2</cell><cell>3.9</cell><cell>gradient</cell></row><row><cell>DrNAS  †</cell><cell>23.7</cell><cell>7.1</cell><cell>5.7</cell><cell>4.6</cell><cell>gradient</cell></row><row><cell>†</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art NAS methods on NAS-Bench-201.</figDesc><table><row><cell>Method</cell><cell cols="2">CIFAR-10 validation</cell><cell>test</cell><cell cols="2">CIFAR-100 validation</cell><cell>test</cell><cell>ImageNet-16-120 validation test</cell></row><row><cell>ResNet (He et al., 2016)</cell><cell>90.83</cell><cell></cell><cell>93.97</cell><cell>70.42</cell><cell cols="2">70.86</cell><cell>44.53</cell><cell>43.63</cell></row><row><cell>Random (baseline)</cell><cell>90.93 ± 0.36</cell><cell cols="2">93.70 ± 0.36</cell><cell>70.60 ± 1.37</cell><cell cols="2">70.65 ± 1.38</cell><cell>42.92 ± 2.00</cell><cell>42.96 ± 2.15</cell></row><row><cell>RSPS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Test accuracy of the searched architecture with different λs on NAS-Bench-201 (CIFAR-10). λ = 1e −3 is what we used for all of our experiments. λ 0 5e −4 1e −3 5e −3 1e −2 1e −1 1 Accuracy 93.78 94.01 94.36 94.36 94.36 93.76 93.76 A.4 EMPIRICAL STUDY ON THE HESSIAN REGULARIZATION EFFECT</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>CIFAR-10 test error on 4 simplified spaces.</figDesc><table><row><cell></cell><cell>s1</cell><cell>s2</cell><cell>s3</cell><cell>s4</cell></row><row><cell>DARTS</cell><cell cols="4">3.84 4.85 3.34 7.20</cell></row><row><cell cols="5">R-DARTS (DP) 3.11 3.48 2.93 3.58</cell></row><row><cell cols="5">R-DARTS (L2) 2.78 3.31 2.51 3.56</cell></row><row><cell>DrNAS</cell><cell cols="4">2.74 2.47 2.4 2.59</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Autoencoding variational inference for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.01488" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/bender18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Memory augmented neural networks with wormhole connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio Caglar Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HylVB3AqYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Probabilistic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Paolo Casale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolo</forename><surname>Fusi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Net2net: Accelerating learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.05641" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stabilizing differentiable architecture search via perturbationbased regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v119/chen20f.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>Hal Daumé III and Aarti Singh</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Latent dirichlet allocation. The Journal of Machine Learning Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I. Jordan</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2017.243" />
		<imprint>
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJxyZkBKDr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bayesian Data Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
	<note>2nd ed. edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.668</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2017.668" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dsnas: Direct neural architecture search without parameter retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2017.243" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weonyoung</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgsvoA9K7" />
		<editor>Il-Chul Moon. Dirichlet variational autoencoder</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Hierarchical indian buffet neural networks for bayesian continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Zohren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A neural dirichlet process mixture model for task-free continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soochan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsoo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on learning representations. In ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Geometry-aware gradient algorithms for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01246-5_2</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-030-01246-5_2" />
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="19" to="35" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1eYHoC5FX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Choice of basis for laplace approximation. Machine Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/article/10.1023/A:1007558615313</idno>
		<ptr target="https://link.springer.com/article/10.1023/A:1007558615313" />
		<imprint>
			<date type="published" when="1998-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pathwise derivatives beyond the reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fritz</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jankowiak</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1806.01851" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Atomnas: Fine-grained end-to-end neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieru</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BylQSxHFwr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/pham18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33014780</idno>
		<ptr target="http://dx.doi.org/10.1609/aaai.v33i01.33014780" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding architectures learnt by cell-based neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofeng</forename><surname>Cai</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJxH22EKPS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.4842" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking architecture selection in differentiable NAS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=PKubaeJkw3" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rylqooRqK7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PC-DARTS: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJlS634tPr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailing</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nas evaluation is frustratingly hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Esperança</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HygrdpVKvr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient neural interaction function search for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380237</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380237" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020, WWW &apos;20</title>
		<meeting>The Web Conference 2020, WWW &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1660" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via proximal iterations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Wei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i04.6143</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/6143" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6664" to="6671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1gDNyrKDS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">NAS-BENCH-1SHOT1: Benchmarking and dissecting one-shot neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJx9ngStPH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Overcoming multi-model forgetting in one-shot nas with diversity maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7806" to="7815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bayesnas: A bayesian approach for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongpeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Pan</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/zhou19e.html" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7603" to="7613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.01578" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00907</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2018.00907" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
